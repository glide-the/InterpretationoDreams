{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "585bf5f5-284b-465b-9226-84528587e83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5273deb3-9ccb-45aa-bbc3-97a219732eb6",
   "metadata": {},
   "source": [
    "## 使用\n",
    "我们提供了一键运行脚本，由于使用了多线程，并不支持jupyter中运行，\n",
    "### 如何运行\n",
    "- 安装依赖\n",
    "```\n",
    "pip install dreamsboard[\"vector\"] -U\n",
    "```\n",
    "\n",
    "我们对每个脚本提供了一些环境变量，除了基本的推理服务环境之外，还有一些资源配置的环境变量\n",
    "- 服务商环境\n",
    "```\n",
    "\n",
    "export DEEPSEEK_API_BASE=\"https://api.deepseek.com/v1\"\n",
    "export DEEPSEEK_API_MODEL=\"deepseek-chat\"\n",
    "export DEEPSEEK_API_KEY=\"sk-api\"\n",
    "export ZHIPUAI_API_BASE=\"https://open.bigmodel.cn/api/paas/v4\"\n",
    "export ZHIPUAI_API_MODEL=\"glm-4-plus\"\n",
    "export ZHIPUAI_API_KEY=\"api.key\"\n",
    "\n",
    "```\n",
    "\n",
    "- 资源配置\n",
    "```\n",
    "# rerank的模块，需要支持 from sentence_transformers import CrossEncoder\n",
    "export cross_encoder_path=\"/mnt/ceph/develop/jiawei/model_checkpoint/jina-reranker-v2-base-multilingual\"\n",
    "# embedding的模块，需要支持 from sentence_transformers import SentenceTransformer\n",
    "export embed_model_path=\"/mnt/ceph/develop/jiawei/model_checkpoint/m3e-base\"\n",
    "# 任务描述\n",
    "export start_task_context=\"模型蒸馏的是什么？\"\n",
    "# 是否是一个新任务\n",
    "export allow_init=\"true\"\n",
    "```\n",
    "\n",
    "\n",
    "导入环境后，请使用如下脚本`test_task/glm/main.py`运行你需要的服务\n",
    "\n",
    "- 推理\n",
    "```\n",
    "python test_task/glm/main.py\n",
    "```\n",
    "> 这个脚本会在执行位置创建本地目录，包含了`storage`中间过程，`vector_store`矢量库\n",
    "\n",
    "> 这个过程会涉及大量的io处理请使用本地磁盘，网络磁盘会影响调度速度\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "### 渲染文档\n",
    "\n",
    "我们也提供了一个默认的文档渲染封装，如果你想渲染其它形式的结构，请读取`storage`中间过程自行编写代码\n",
    "\n",
    "```\n",
    "python test_task/glm/printmd.md\n",
    "```\n",
    "> 脚本会读取`start_task_context`环境变量\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f04853b-302b-435e-b0a3-c0e5ed8d7315",
   "metadata": {},
   "source": [
    "### 任务表格"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be4ee8a0-d50b-4728-8b18-a2d33860d5d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task_step_id</th>\n",
       "      <th>shot_number</th>\n",
       "      <th>scene_number</th>\n",
       "      <th>start_task_context</th>\n",
       "      <th>aemo_representation_context</th>\n",
       "      <th>task_step_name</th>\n",
       "      <th>task_step_description</th>\n",
       "      <th>task_step_level</th>\n",
       "      <th>task_step_question</th>\n",
       "      <th>task_step_question_context</th>\n",
       "      <th>task_step_question_answer</th>\n",
       "      <th>ref_task_step_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6c7d537f-57b5-4e83-ac4f-382b18371d9a</td>\n",
       "      <td>1</td>\n",
       "      <td>story_board0</td>\n",
       "      <td>模型蒸馏的是什么？</td>\n",
       "      <td>模型蒸馏（Model Distillation）是一种在机器学习和深度学习领域中被广泛使用的...</td>\n",
       "      <td>基本概念</td>\n",
       "      <td>模型蒸馏的基本概念，包括教师模型和学生模型的定义。</td>\n",
       "      <td>0</td>\n",
       "      <td>模型蒸馏中的教师模型和学生模型分别是什么？</td>\n",
       "      <td>[{'ref_id': '454846021212093966', 'chunk_id': ...</td>\n",
       "      <td>在知识蒸馏中，损失函数的设计是关键，它直接影响学生模型的学习效果。通常，损失函数由两部分组成...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00dd8782-ed06-4ca0-9336-1ead3c76e363</td>\n",
       "      <td>2</td>\n",
       "      <td>story_board1</td>\n",
       "      <td>模型蒸馏的是什么？</td>\n",
       "      <td>模型蒸馏（Model Distillation）是一种在机器学习和深度学习领域中被广泛使用的...</td>\n",
       "      <td>教师模型</td>\n",
       "      <td>一个已经训练好的、性能优异但可能计算复杂度高的模型。</td>\n",
       "      <td>0&gt;1</td>\n",
       "      <td>教师模型的训练数据是否需要与学生模型的训练数据完全一致？</td>\n",
       "      <td>[{'ref_id': '454846021397691930', 'chunk_id': ...</td>\n",
       "      <td>在语音识别任务中，硬蒸馏被用于利用大量无标签语音数据，提高学生模型的识别准确率。例如，在RN...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>efcde363-e9f2-4634-8a9f-27c169d028c6</td>\n",
       "      <td>3</td>\n",
       "      <td>story_board2</td>\n",
       "      <td>模型蒸馏的是什么？</td>\n",
       "      <td>模型蒸馏（Model Distillation）是一种在机器学习和深度学习领域中被广泛使用的...</td>\n",
       "      <td>学生模型</td>\n",
       "      <td>一个较小、计算效率更高的模型，目标是学习教师模型的知识。</td>\n",
       "      <td>0&gt;2</td>\n",
       "      <td>学生模型如何确保在学习教师模型知识的同时保持自身的计算效率？</td>\n",
       "      <td>[{'ref_id': '455037545741550556', 'chunk_id': ...</td>\n",
       "      <td>在知识蒸馏中，学生模型的计算效率不仅依赖于上述方法，还可以通过引入更多的优化策略来进一步提升...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4e16c975-1df8-47b9-8f06-ea5fdcda90cf</td>\n",
       "      <td>4</td>\n",
       "      <td>story_board3</td>\n",
       "      <td>模型蒸馏的是什么？</td>\n",
       "      <td>模型蒸馏（Model Distillation）是一种在机器学习和深度学习领域中被广泛使用的...</td>\n",
       "      <td>蒸馏过程</td>\n",
       "      <td>模型蒸馏的具体过程，包括训练教师模型和知识转移。</td>\n",
       "      <td>1</td>\n",
       "      <td>在模型蒸馏过程中，如何有效地将教师模型的知识转移到学生模型中？</td>\n",
       "      <td>[{'ref_id': '454919276633279012', 'chunk_id': ...</td>\n",
       "      <td>在模型蒸馏过程中，有效地将教师模型的知识转移到学生模型中是关键。以下是一些行之有效的方法：\\...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>80e623b5-e6b0-46ce-a1df-f286d7c78fa6</td>\n",
       "      <td>5</td>\n",
       "      <td>story_board4</td>\n",
       "      <td>模型蒸馏的是什么？</td>\n",
       "      <td>模型蒸馏（Model Distillation）是一种在机器学习和深度学习领域中被广泛使用的...</td>\n",
       "      <td>训练教师模型</td>\n",
       "      <td>首先，使用大量数据训练一个高性能的教师模型。</td>\n",
       "      <td>1&gt;1</td>\n",
       "      <td>在训练教师模型时，如何确保所使用的大量数据能够充分覆盖任务所需的各类特征，以保证教师模型的高性能？</td>\n",
       "      <td>[{'ref_id': '454895484123495166', 'chunk_id': ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ef80b69f-0ebd-4fc5-aa96-7b567492cce2</td>\n",
       "      <td>6</td>\n",
       "      <td>story_board5</td>\n",
       "      <td>模型蒸馏的是什么？</td>\n",
       "      <td>模型蒸馏（Model Distillation）是一种在机器学习和深度学习领域中被广泛使用的...</td>\n",
       "      <td>知识转移</td>\n",
       "      <td>通过某种方式将教师模型的知识转移到学生模型上。常见的做法是使用教师模型的输出（如softma...</td>\n",
       "      <td>1&gt;2</td>\n",
       "      <td>在模型蒸馏中，如何有效地将教师模型的知识转移到学生模型上？</td>\n",
       "      <td>[{'ref_id': '454919276633279012', 'chunk_id': ...</td>\n",
       "      <td>在特征蒸馏中，选择教师模型的中间层特征时，通常会考虑这些特征在不同抽象层次上的表达能力。例如...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>063b04b2-85dd-415d-b945-b0395bfbabac</td>\n",
       "      <td>7</td>\n",
       "      <td>story_board6</td>\n",
       "      <td>模型蒸馏的是什么？</td>\n",
       "      <td>模型蒸馏（Model Distillation）是一种在机器学习和深度学习领域中被广泛使用的...</td>\n",
       "      <td>关键技术</td>\n",
       "      <td>模型蒸馏中的关键技术，包括软标签、温度调节和损失函数。</td>\n",
       "      <td>2</td>\n",
       "      <td>在模型蒸馏中，如何通过软标签、温度调节和损失函数等关键技术实现知识的有效转移？</td>\n",
       "      <td>[{'ref_id': '454846429879674798', 'chunk_id': ...</td>\n",
       "      <td>在模型蒸馏中，除了上述技术外，还可以通过引入注意力机制来进一步提升知识转移的效果。注意力机制...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>508a66a9-f2e5-4e43-8e01-496b1d156f74</td>\n",
       "      <td>8</td>\n",
       "      <td>story_board7</td>\n",
       "      <td>模型蒸馏的是什么？</td>\n",
       "      <td>模型蒸馏（Model Distillation）是一种在机器学习和深度学习领域中被广泛使用的...</td>\n",
       "      <td>软标签</td>\n",
       "      <td>教师模型的输出（通常是概率分布）作为软标签，用于指导学生模型的训练。这些软标签包含了比硬标签...</td>\n",
       "      <td>2&gt;1</td>\n",
       "      <td>在模型蒸馏中，软标签具体是如何帮助学生模型更好地学习教师模型的知识的？</td>\n",
       "      <td>[{'ref_id': '454845564881216834', 'chunk_id': ...</td>\n",
       "      <td>在图像分类任务中，教师模型对一张猫的图片可能输出一个接近90%的概率属于猫类，而硬标签则只是...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7b6c512c-4878-43ea-bce1-b3d3906c8c68</td>\n",
       "      <td>9</td>\n",
       "      <td>story_board8</td>\n",
       "      <td>模型蒸馏的是什么？</td>\n",
       "      <td>模型蒸馏（Model Distillation）是一种在机器学习和深度学习领域中被广泛使用的...</td>\n",
       "      <td>温度调节</td>\n",
       "      <td>在softmax层引入一个温度参数（Temperature），使得输出分布更加平滑，从而更容...</td>\n",
       "      <td>2&gt;2</td>\n",
       "      <td>在模型蒸馏中，温度调节如何影响知识转移的效果？</td>\n",
       "      <td>[{'ref_id': '454846429944424370', 'chunk_id': ...</td>\n",
       "      <td>在模型蒸馏中，温度调节不仅影响教师模型的输出分布，还与其他技术如软标签和损失函数协同工作，以...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>275fc0ad-6585-43ed-b463-8cf7d64f17ed</td>\n",
       "      <td>10</td>\n",
       "      <td>story_board9</td>\n",
       "      <td>模型蒸馏的是什么？</td>\n",
       "      <td>模型蒸馏（Model Distillation）是一种在机器学习和深度学习领域中被广泛使用的...</td>\n",
       "      <td>损失函数</td>\n",
       "      <td>结合软标签和硬标签的损失函数，通常包括两部分：一部分是学生模型输出与教师模型输出的差异，另一...</td>\n",
       "      <td>2&gt;3</td>\n",
       "      <td>在模型蒸馏中，如何设计一个结合软标签和硬标签的损失函数，以平衡学生模型输出与教师模型输出的差...</td>\n",
       "      <td>[{'ref_id': '454846109840627474', 'chunk_id': ...</td>\n",
       "      <td>在模型蒸馏中，除了温度参数和损失函数的设计，还可以通过其他技术进一步优化知识转移的效果。例如...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>8c337324-8c7d-42e4-be74-bdf5ed270288</td>\n",
       "      <td>11</td>\n",
       "      <td>story_board10</td>\n",
       "      <td>模型蒸馏的是什么？</td>\n",
       "      <td>模型蒸馏（Model Distillation）是一种在机器学习和深度学习领域中被广泛使用的...</td>\n",
       "      <td>应用场景</td>\n",
       "      <td>模型蒸馏的应用场景，包括模型压缩、知识迁移和提高泛化能力。</td>\n",
       "      <td>3</td>\n",
       "      <td>模型蒸馏的应用场景中，如何在实际应用中平衡模型压缩、知识迁移和提高泛化能力之间的关系？</td>\n",
       "      <td>[{'ref_id': '454846429944424370', 'chunk_id': ...</td>\n",
       "      <td>在模型压缩中，剪枝技术通过移除神经网络中不重要的权重、神经元或通道来减少模型的复杂度。量化技...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>f294540f-722a-460b-b3bc-f10227978cec</td>\n",
       "      <td>12</td>\n",
       "      <td>story_board11</td>\n",
       "      <td>模型蒸馏的是什么？</td>\n",
       "      <td>模型蒸馏（Model Distillation）是一种在机器学习和深度学习领域中被广泛使用的...</td>\n",
       "      <td>模型压缩</td>\n",
       "      <td>将大型模型压缩成小型模型，便于部署在资源受限的设备上。</td>\n",
       "      <td>3&gt;1</td>\n",
       "      <td>在模型压缩过程中，如何选择合适的学生模型架构以确保在资源受限的设备上高效运行，同时保持与教师...</td>\n",
       "      <td>[{'ref_id': '455037545741550556', 'chunk_id': ...</td>\n",
       "      <td>在模型压缩过程中，选择合适的学生模型架构以确保在资源受限的设备上高效运行，同时保持与教师模型...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>02abe5b1-3c00-445f-b8af-a4f967e45a92</td>\n",
       "      <td>13</td>\n",
       "      <td>story_board12</td>\n",
       "      <td>模型蒸馏的是什么？</td>\n",
       "      <td>模型蒸馏（Model Distillation）是一种在机器学习和深度学习领域中被广泛使用的...</td>\n",
       "      <td>知识迁移</td>\n",
       "      <td>在不同任务或领域之间迁移知识。</td>\n",
       "      <td>3&gt;2</td>\n",
       "      <td>在模型蒸馏中，如何确保学生模型在不同任务或领域之间有效地迁移教师模型的知识？</td>\n",
       "      <td>[{'ref_id': '454919276633279012', 'chunk_id': ...</td>\n",
       "      <td>在处理跨领域数据迁移时，领域自适应技术可以通过最小化源域和目标域之间的分布差异来优化知识迁移...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>824f9802-393d-49ce-bd9b-469d25386813</td>\n",
       "      <td>14</td>\n",
       "      <td>story_board13</td>\n",
       "      <td>模型蒸馏的是什么？</td>\n",
       "      <td>模型蒸馏（Model Distillation）是一种在机器学习和深度学习领域中被广泛使用的...</td>\n",
       "      <td>提高泛化能力</td>\n",
       "      <td>通过学习教师模型的知识，学生模型可能在某些情况下具有更好的泛化能力。</td>\n",
       "      <td>3&gt;3</td>\n",
       "      <td>学生模型通过学习教师模型的知识，如何在某些情况下实现更好的泛化能力？</td>\n",
       "      <td>[{'ref_id': '454845516226498482', 'chunk_id': ...</td>\n",
       "      <td>分数/总分: 9/10</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4db44f90-796c-4d8b-9171-4ddb6421aa05</td>\n",
       "      <td>15</td>\n",
       "      <td>story_board14</td>\n",
       "      <td>模型蒸馏的是什么？</td>\n",
       "      <td>模型蒸馏（Model Distillation）是一种在机器学习和深度学习领域中被广泛使用的...</td>\n",
       "      <td>优势与局限性</td>\n",
       "      <td>模型蒸馏的优势和局限性。</td>\n",
       "      <td>4</td>\n",
       "      <td>模型蒸馏有哪些主要优势和局限性？</td>\n",
       "      <td>[{'ref_id': '454849635198899072', 'chunk_id': ...</td>\n",
       "      <td>分数/总分</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>95a17065-6829-4c26-9e21-47ffa5cce59f</td>\n",
       "      <td>16</td>\n",
       "      <td>story_board15</td>\n",
       "      <td>模型蒸馏的是什么？</td>\n",
       "      <td>模型蒸馏（Model Distillation）是一种在机器学习和深度学习领域中被广泛使用的...</td>\n",
       "      <td>优势</td>\n",
       "      <td>计算效率高，学生模型更小，计算和存储需求更低；性能保持，在许多情况下，学生模型能够保持与教师...</td>\n",
       "      <td>4&gt;1</td>\n",
       "      <td>在模型蒸馏中，软标签和温度调节技术是如何帮助学生模型在保持高性能的同时，实现计算效率的提升的？</td>\n",
       "      <td>[{'ref_id': '454846109840627474', 'chunk_id': ...</td>\n",
       "      <td>在移动设备上进行实时图像分类时，学生模型可以在保持较高准确性的同时，显著减少计算时间和能耗。...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>fc0ddb45-3e91-455f-83cc-ec0ce08227c1</td>\n",
       "      <td>17</td>\n",
       "      <td>story_board16</td>\n",
       "      <td>模型蒸馏的是什么？</td>\n",
       "      <td>模型蒸馏（Model Distillation）是一种在机器学习和深度学习领域中被广泛使用的...</td>\n",
       "      <td>局限性</td>\n",
       "      <td>性能损失，在某些复杂任务中，学生模型的性能可能不如教师模型；训练复杂，需要先训练一个高性能的...</td>\n",
       "      <td>4&gt;2</td>\n",
       "      <td>如何理解模型蒸馏中学生模型的性能损失以及训练过程的复杂性？</td>\n",
       "      <td>[{'ref_id': '454846021212093966', 'chunk_id': ...</td>\n",
       "      <td>在具体的任务中，学生模型的性能损失可以通过量化来进一步说明。例如，在图像分类任务中，教师模型...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>fc5791d1-27d2-481d-99cb-7031f548fbf6</td>\n",
       "      <td>18</td>\n",
       "      <td>story_board17</td>\n",
       "      <td>模型蒸馏的是什么？</td>\n",
       "      <td>模型蒸馏（Model Distillation）是一种在机器学习和深度学习领域中被广泛使用的...</td>\n",
       "      <td>最新进展</td>\n",
       "      <td>模型蒸馏的最新进展，包括多教师蒸馏、自蒸馏和跨模态蒸馏。</td>\n",
       "      <td>5</td>\n",
       "      <td>模型蒸馏的最新进展中，多教师蒸馏、自蒸馏和跨模态蒸馏各自面临哪些主要的技术挑战和未来发展方向？</td>\n",
       "      <td>[{'ref_id': '454847000032419890', 'chunk_id': ...</td>\n",
       "      <td>在实际应用中，多教师蒸馏已经在图像分类和自然语言处理任务中取得了显著的效果。例如，在Imag...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>9611456a-e799-41b6-8ae8-fb0db3136ebe</td>\n",
       "      <td>19</td>\n",
       "      <td>story_board18</td>\n",
       "      <td>模型蒸馏的是什么？</td>\n",
       "      <td>模型蒸馏（Model Distillation）是一种在机器学习和深度学习领域中被广泛使用的...</td>\n",
       "      <td>多教师蒸馏</td>\n",
       "      <td>使用多个教师模型共同指导一个学生模型。</td>\n",
       "      <td>5&gt;1</td>\n",
       "      <td>多教师蒸馏中，如何整合多个教师模型的输出以指导学生模型的训练？</td>\n",
       "      <td>[{'ref_id': '454984267297727980', 'chunk_id': ...</td>\n",
       "      <td>在多教师蒸馏中，教师模型之间的知识冲突是一个常见的挑战。为了解决这一问题，可以采用知识共享技...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1d249791-159d-4edc-8b67-fdba1bb0b39e</td>\n",
       "      <td>20</td>\n",
       "      <td>story_board19</td>\n",
       "      <td>模型蒸馏的是什么？</td>\n",
       "      <td>模型蒸馏（Model Distillation）是一种在机器学习和深度学习领域中被广泛使用的...</td>\n",
       "      <td>自蒸馏</td>\n",
       "      <td>模型自身作为教师模型进行蒸馏。</td>\n",
       "      <td>5&gt;2</td>\n",
       "      <td>自蒸馏中，模型自身作为教师模型进行蒸馏时，如何确保知识的有效转移和性能的提升？</td>\n",
       "      <td>[{'ref_id': '454984176802988974', 'chunk_id': ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>195a6dee-ae40-4bcb-8382-e8a40b227cbc</td>\n",
       "      <td>21</td>\n",
       "      <td>story_board20</td>\n",
       "      <td>模型蒸馏的是什么？</td>\n",
       "      <td>模型蒸馏（Model Distillation）是一种在机器学习和深度学习领域中被广泛使用的...</td>\n",
       "      <td>跨模态蒸馏</td>\n",
       "      <td>在不同模态（如图像和文本）之间进行知识蒸馏。</td>\n",
       "      <td>5&gt;3</td>\n",
       "      <td>在跨模态蒸馏中，如何有效地将图像和文本之间的知识进行迁移？</td>\n",
       "      <td>[{'ref_id': '454846524788653356', 'chunk_id': ...</td>\n",
       "      <td>在跨模态蒸馏中，动态权重分配策略的具体实现可以通过引入注意力机制来进一步优化。例如，在视觉问...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>b438eb5d-3e98-4b91-8ce4-061e0f785681</td>\n",
       "      <td>22</td>\n",
       "      <td>story_board21</td>\n",
       "      <td>模型蒸馏的是什么？</td>\n",
       "      <td>模型蒸馏（Model Distillation）是一种在机器学习和深度学习领域中被广泛使用的...</td>\n",
       "      <td>未来研究方向</td>\n",
       "      <td>模型蒸馏的未来研究方向，包括更有效的知识转移方法、自适应蒸馏和泛化能力提升。</td>\n",
       "      <td>6</td>\n",
       "      <td>模型蒸馏的未来研究方向中，如何实现更有效的知识转移方法、自适应蒸馏和泛化能力提升？</td>\n",
       "      <td>[{'ref_id': '454847000032419890', 'chunk_id': ...</td>\n",
       "      <td>在未来的研究中，还可以进一步探索如何将多粒度知识融合与动态知识转移结合，以更全面地提升学生模...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>698e4964-272b-48e6-99d4-7beca68703a0</td>\n",
       "      <td>23</td>\n",
       "      <td>story_board22</td>\n",
       "      <td>模型蒸馏的是什么？</td>\n",
       "      <td>模型蒸馏（Model Distillation）是一种在机器学习和深度学习领域中被广泛使用的...</td>\n",
       "      <td>更有效的知识转移方法</td>\n",
       "      <td>探索更有效的知识表示和转移方法。</td>\n",
       "      <td>6&gt;1</td>\n",
       "      <td>在模型蒸馏中，如何探索更有效的知识表示和转移方法以提高学生模型的学习效果？</td>\n",
       "      <td>[{'ref_id': '454845510461164774', 'chunk_id': ...</td>\n",
       "      <td>分数/总分: 9/10</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>f70c75ba-ce85-405d-8c9e-869a95298e6a</td>\n",
       "      <td>24</td>\n",
       "      <td>story_board23</td>\n",
       "      <td>模型蒸馏的是什么？</td>\n",
       "      <td>模型蒸馏（Model Distillation）是一种在机器学习和深度学习领域中被广泛使用的...</td>\n",
       "      <td>自适应蒸馏</td>\n",
       "      <td>根据任务和数据自适应调整蒸馏过程。</td>\n",
       "      <td>6&gt;2</td>\n",
       "      <td>在自适应蒸馏中，如何根据具体的任务需求和数据特点动态调整蒸馏过程以优化学生模型的性能？</td>\n",
       "      <td>[{'ref_id': '454849635198899072', 'chunk_id': ...</td>\n",
       "      <td>在自适应蒸馏中，根据具体的任务需求和数据特点动态调整蒸馏过程以优化学生模型的性能，可以通过以...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>9c332766-5a7c-4366-a986-0dc0e638624b</td>\n",
       "      <td>25</td>\n",
       "      <td>story_board24</td>\n",
       "      <td>模型蒸馏的是什么？</td>\n",
       "      <td>模型蒸馏（Model Distillation）是一种在机器学习和深度学习领域中被广泛使用的...</td>\n",
       "      <td>泛化能力提升</td>\n",
       "      <td>研究如何通过蒸馏提高模型在未见过的数据上的泛化能力。</td>\n",
       "      <td>6&gt;3</td>\n",
       "      <td>在模型蒸馏中，如何通过蒸馏提高模型在未见过的数据上的泛化能力？</td>\n",
       "      <td>[{'ref_id': '454849635198899072', 'chunk_id': ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>ff2674ca-68b1-4ab7-b668-75a86128e62e</td>\n",
       "      <td>26</td>\n",
       "      <td>story_board25</td>\n",
       "      <td>模型蒸馏的是什么？</td>\n",
       "      <td>模型蒸馏（Model Distillation）是一种在机器学习和深度学习领域中被广泛使用的...</td>\n",
       "      <td>总结</td>\n",
       "      <td>模型蒸馏的总结，强调其作为有效的知识迁移技术，实现了模型压缩和性能保持的双重目标，并指出其在...</td>\n",
       "      <td>7</td>\n",
       "      <td>模型蒸馏的总结中提到的“更有效的知识转移方法”具体指哪些技术或策略？</td>\n",
       "      <td>[{'ref_id': '454846969260382800', 'chunk_id': ...</td>\n",
       "      <td>在未来的研究中，自适应蒸馏技术可以通过动态调整蒸馏温度或损失权重来优化知识转移。例如，在训练...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            task_step_id  shot_number   scene_number  \\\n",
       "0   6c7d537f-57b5-4e83-ac4f-382b18371d9a            1   story_board0   \n",
       "1   00dd8782-ed06-4ca0-9336-1ead3c76e363            2   story_board1   \n",
       "2   efcde363-e9f2-4634-8a9f-27c169d028c6            3   story_board2   \n",
       "3   4e16c975-1df8-47b9-8f06-ea5fdcda90cf            4   story_board3   \n",
       "4   80e623b5-e6b0-46ce-a1df-f286d7c78fa6            5   story_board4   \n",
       "5   ef80b69f-0ebd-4fc5-aa96-7b567492cce2            6   story_board5   \n",
       "6   063b04b2-85dd-415d-b945-b0395bfbabac            7   story_board6   \n",
       "7   508a66a9-f2e5-4e43-8e01-496b1d156f74            8   story_board7   \n",
       "8   7b6c512c-4878-43ea-bce1-b3d3906c8c68            9   story_board8   \n",
       "9   275fc0ad-6585-43ed-b463-8cf7d64f17ed           10   story_board9   \n",
       "10  8c337324-8c7d-42e4-be74-bdf5ed270288           11  story_board10   \n",
       "11  f294540f-722a-460b-b3bc-f10227978cec           12  story_board11   \n",
       "12  02abe5b1-3c00-445f-b8af-a4f967e45a92           13  story_board12   \n",
       "13  824f9802-393d-49ce-bd9b-469d25386813           14  story_board13   \n",
       "14  4db44f90-796c-4d8b-9171-4ddb6421aa05           15  story_board14   \n",
       "15  95a17065-6829-4c26-9e21-47ffa5cce59f           16  story_board15   \n",
       "16  fc0ddb45-3e91-455f-83cc-ec0ce08227c1           17  story_board16   \n",
       "17  fc5791d1-27d2-481d-99cb-7031f548fbf6           18  story_board17   \n",
       "18  9611456a-e799-41b6-8ae8-fb0db3136ebe           19  story_board18   \n",
       "19  1d249791-159d-4edc-8b67-fdba1bb0b39e           20  story_board19   \n",
       "20  195a6dee-ae40-4bcb-8382-e8a40b227cbc           21  story_board20   \n",
       "21  b438eb5d-3e98-4b91-8ce4-061e0f785681           22  story_board21   \n",
       "22  698e4964-272b-48e6-99d4-7beca68703a0           23  story_board22   \n",
       "23  f70c75ba-ce85-405d-8c9e-869a95298e6a           24  story_board23   \n",
       "24  9c332766-5a7c-4366-a986-0dc0e638624b           25  story_board24   \n",
       "25  ff2674ca-68b1-4ab7-b668-75a86128e62e           26  story_board25   \n",
       "\n",
       "   start_task_context                        aemo_representation_context  \\\n",
       "0           模型蒸馏的是什么？  模型蒸馏（Model Distillation）是一种在机器学习和深度学习领域中被广泛使用的...   \n",
       "1           模型蒸馏的是什么？  模型蒸馏（Model Distillation）是一种在机器学习和深度学习领域中被广泛使用的...   \n",
       "2           模型蒸馏的是什么？  模型蒸馏（Model Distillation）是一种在机器学习和深度学习领域中被广泛使用的...   \n",
       "3           模型蒸馏的是什么？  模型蒸馏（Model Distillation）是一种在机器学习和深度学习领域中被广泛使用的...   \n",
       "4           模型蒸馏的是什么？  模型蒸馏（Model Distillation）是一种在机器学习和深度学习领域中被广泛使用的...   \n",
       "5           模型蒸馏的是什么？  模型蒸馏（Model Distillation）是一种在机器学习和深度学习领域中被广泛使用的...   \n",
       "6           模型蒸馏的是什么？  模型蒸馏（Model Distillation）是一种在机器学习和深度学习领域中被广泛使用的...   \n",
       "7           模型蒸馏的是什么？  模型蒸馏（Model Distillation）是一种在机器学习和深度学习领域中被广泛使用的...   \n",
       "8           模型蒸馏的是什么？  模型蒸馏（Model Distillation）是一种在机器学习和深度学习领域中被广泛使用的...   \n",
       "9           模型蒸馏的是什么？  模型蒸馏（Model Distillation）是一种在机器学习和深度学习领域中被广泛使用的...   \n",
       "10          模型蒸馏的是什么？  模型蒸馏（Model Distillation）是一种在机器学习和深度学习领域中被广泛使用的...   \n",
       "11          模型蒸馏的是什么？  模型蒸馏（Model Distillation）是一种在机器学习和深度学习领域中被广泛使用的...   \n",
       "12          模型蒸馏的是什么？  模型蒸馏（Model Distillation）是一种在机器学习和深度学习领域中被广泛使用的...   \n",
       "13          模型蒸馏的是什么？  模型蒸馏（Model Distillation）是一种在机器学习和深度学习领域中被广泛使用的...   \n",
       "14          模型蒸馏的是什么？  模型蒸馏（Model Distillation）是一种在机器学习和深度学习领域中被广泛使用的...   \n",
       "15          模型蒸馏的是什么？  模型蒸馏（Model Distillation）是一种在机器学习和深度学习领域中被广泛使用的...   \n",
       "16          模型蒸馏的是什么？  模型蒸馏（Model Distillation）是一种在机器学习和深度学习领域中被广泛使用的...   \n",
       "17          模型蒸馏的是什么？  模型蒸馏（Model Distillation）是一种在机器学习和深度学习领域中被广泛使用的...   \n",
       "18          模型蒸馏的是什么？  模型蒸馏（Model Distillation）是一种在机器学习和深度学习领域中被广泛使用的...   \n",
       "19          模型蒸馏的是什么？  模型蒸馏（Model Distillation）是一种在机器学习和深度学习领域中被广泛使用的...   \n",
       "20          模型蒸馏的是什么？  模型蒸馏（Model Distillation）是一种在机器学习和深度学习领域中被广泛使用的...   \n",
       "21          模型蒸馏的是什么？  模型蒸馏（Model Distillation）是一种在机器学习和深度学习领域中被广泛使用的...   \n",
       "22          模型蒸馏的是什么？  模型蒸馏（Model Distillation）是一种在机器学习和深度学习领域中被广泛使用的...   \n",
       "23          模型蒸馏的是什么？  模型蒸馏（Model Distillation）是一种在机器学习和深度学习领域中被广泛使用的...   \n",
       "24          模型蒸馏的是什么？  模型蒸馏（Model Distillation）是一种在机器学习和深度学习领域中被广泛使用的...   \n",
       "25          模型蒸馏的是什么？  模型蒸馏（Model Distillation）是一种在机器学习和深度学习领域中被广泛使用的...   \n",
       "\n",
       "   task_step_name                              task_step_description  \\\n",
       "0            基本概念                          模型蒸馏的基本概念，包括教师模型和学生模型的定义。   \n",
       "1            教师模型                         一个已经训练好的、性能优异但可能计算复杂度高的模型。   \n",
       "2            学生模型                       一个较小、计算效率更高的模型，目标是学习教师模型的知识。   \n",
       "3            蒸馏过程                           模型蒸馏的具体过程，包括训练教师模型和知识转移。   \n",
       "4          训练教师模型                             首先，使用大量数据训练一个高性能的教师模型。   \n",
       "5            知识转移  通过某种方式将教师模型的知识转移到学生模型上。常见的做法是使用教师模型的输出（如softma...   \n",
       "6            关键技术                        模型蒸馏中的关键技术，包括软标签、温度调节和损失函数。   \n",
       "7             软标签  教师模型的输出（通常是概率分布）作为软标签，用于指导学生模型的训练。这些软标签包含了比硬标签...   \n",
       "8            温度调节  在softmax层引入一个温度参数（Temperature），使得输出分布更加平滑，从而更容...   \n",
       "9            损失函数  结合软标签和硬标签的损失函数，通常包括两部分：一部分是学生模型输出与教师模型输出的差异，另一...   \n",
       "10           应用场景                      模型蒸馏的应用场景，包括模型压缩、知识迁移和提高泛化能力。   \n",
       "11           模型压缩                        将大型模型压缩成小型模型，便于部署在资源受限的设备上。   \n",
       "12           知识迁移                                    在不同任务或领域之间迁移知识。   \n",
       "13         提高泛化能力                 通过学习教师模型的知识，学生模型可能在某些情况下具有更好的泛化能力。   \n",
       "14         优势与局限性                                       模型蒸馏的优势和局限性。   \n",
       "15             优势  计算效率高，学生模型更小，计算和存储需求更低；性能保持，在许多情况下，学生模型能够保持与教师...   \n",
       "16            局限性  性能损失，在某些复杂任务中，学生模型的性能可能不如教师模型；训练复杂，需要先训练一个高性能的...   \n",
       "17           最新进展                       模型蒸馏的最新进展，包括多教师蒸馏、自蒸馏和跨模态蒸馏。   \n",
       "18          多教师蒸馏                                使用多个教师模型共同指导一个学生模型。   \n",
       "19            自蒸馏                                    模型自身作为教师模型进行蒸馏。   \n",
       "20          跨模态蒸馏                             在不同模态（如图像和文本）之间进行知识蒸馏。   \n",
       "21         未来研究方向             模型蒸馏的未来研究方向，包括更有效的知识转移方法、自适应蒸馏和泛化能力提升。   \n",
       "22     更有效的知识转移方法                                   探索更有效的知识表示和转移方法。   \n",
       "23          自适应蒸馏                                  根据任务和数据自适应调整蒸馏过程。   \n",
       "24         泛化能力提升                         研究如何通过蒸馏提高模型在未见过的数据上的泛化能力。   \n",
       "25             总结  模型蒸馏的总结，强调其作为有效的知识迁移技术，实现了模型压缩和性能保持的双重目标，并指出其在...   \n",
       "\n",
       "   task_step_level                                 task_step_question  \\\n",
       "0                0                              模型蒸馏中的教师模型和学生模型分别是什么？   \n",
       "1              0>1                       教师模型的训练数据是否需要与学生模型的训练数据完全一致？   \n",
       "2              0>2                     学生模型如何确保在学习教师模型知识的同时保持自身的计算效率？   \n",
       "3                1                    在模型蒸馏过程中，如何有效地将教师模型的知识转移到学生模型中？   \n",
       "4              1>1  在训练教师模型时，如何确保所使用的大量数据能够充分覆盖任务所需的各类特征，以保证教师模型的高性能？   \n",
       "5              1>2                      在模型蒸馏中，如何有效地将教师模型的知识转移到学生模型上？   \n",
       "6                2            在模型蒸馏中，如何通过软标签、温度调节和损失函数等关键技术实现知识的有效转移？   \n",
       "7              2>1                在模型蒸馏中，软标签具体是如何帮助学生模型更好地学习教师模型的知识的？   \n",
       "8              2>2                            在模型蒸馏中，温度调节如何影响知识转移的效果？   \n",
       "9              2>3  在模型蒸馏中，如何设计一个结合软标签和硬标签的损失函数，以平衡学生模型输出与教师模型输出的差...   \n",
       "10               3        模型蒸馏的应用场景中，如何在实际应用中平衡模型压缩、知识迁移和提高泛化能力之间的关系？   \n",
       "11             3>1  在模型压缩过程中，如何选择合适的学生模型架构以确保在资源受限的设备上高效运行，同时保持与教师...   \n",
       "12             3>2             在模型蒸馏中，如何确保学生模型在不同任务或领域之间有效地迁移教师模型的知识？   \n",
       "13             3>3                 学生模型通过学习教师模型的知识，如何在某些情况下实现更好的泛化能力？   \n",
       "14               4                                   模型蒸馏有哪些主要优势和局限性？   \n",
       "15             4>1    在模型蒸馏中，软标签和温度调节技术是如何帮助学生模型在保持高性能的同时，实现计算效率的提升的？   \n",
       "16             4>2                      如何理解模型蒸馏中学生模型的性能损失以及训练过程的复杂性？   \n",
       "17               5    模型蒸馏的最新进展中，多教师蒸馏、自蒸馏和跨模态蒸馏各自面临哪些主要的技术挑战和未来发展方向？   \n",
       "18             5>1                    多教师蒸馏中，如何整合多个教师模型的输出以指导学生模型的训练？   \n",
       "19             5>2            自蒸馏中，模型自身作为教师模型进行蒸馏时，如何确保知识的有效转移和性能的提升？   \n",
       "20             5>3                      在跨模态蒸馏中，如何有效地将图像和文本之间的知识进行迁移？   \n",
       "21               6          模型蒸馏的未来研究方向中，如何实现更有效的知识转移方法、自适应蒸馏和泛化能力提升？   \n",
       "22             6>1              在模型蒸馏中，如何探索更有效的知识表示和转移方法以提高学生模型的学习效果？   \n",
       "23             6>2        在自适应蒸馏中，如何根据具体的任务需求和数据特点动态调整蒸馏过程以优化学生模型的性能？   \n",
       "24             6>3                    在模型蒸馏中，如何通过蒸馏提高模型在未见过的数据上的泛化能力？   \n",
       "25               7                 模型蒸馏的总结中提到的“更有效的知识转移方法”具体指哪些技术或策略？   \n",
       "\n",
       "                           task_step_question_context  \\\n",
       "0   [{'ref_id': '454846021212093966', 'chunk_id': ...   \n",
       "1   [{'ref_id': '454846021397691930', 'chunk_id': ...   \n",
       "2   [{'ref_id': '455037545741550556', 'chunk_id': ...   \n",
       "3   [{'ref_id': '454919276633279012', 'chunk_id': ...   \n",
       "4   [{'ref_id': '454895484123495166', 'chunk_id': ...   \n",
       "5   [{'ref_id': '454919276633279012', 'chunk_id': ...   \n",
       "6   [{'ref_id': '454846429879674798', 'chunk_id': ...   \n",
       "7   [{'ref_id': '454845564881216834', 'chunk_id': ...   \n",
       "8   [{'ref_id': '454846429944424370', 'chunk_id': ...   \n",
       "9   [{'ref_id': '454846109840627474', 'chunk_id': ...   \n",
       "10  [{'ref_id': '454846429944424370', 'chunk_id': ...   \n",
       "11  [{'ref_id': '455037545741550556', 'chunk_id': ...   \n",
       "12  [{'ref_id': '454919276633279012', 'chunk_id': ...   \n",
       "13  [{'ref_id': '454845516226498482', 'chunk_id': ...   \n",
       "14  [{'ref_id': '454849635198899072', 'chunk_id': ...   \n",
       "15  [{'ref_id': '454846109840627474', 'chunk_id': ...   \n",
       "16  [{'ref_id': '454846021212093966', 'chunk_id': ...   \n",
       "17  [{'ref_id': '454847000032419890', 'chunk_id': ...   \n",
       "18  [{'ref_id': '454984267297727980', 'chunk_id': ...   \n",
       "19  [{'ref_id': '454984176802988974', 'chunk_id': ...   \n",
       "20  [{'ref_id': '454846524788653356', 'chunk_id': ...   \n",
       "21  [{'ref_id': '454847000032419890', 'chunk_id': ...   \n",
       "22  [{'ref_id': '454845510461164774', 'chunk_id': ...   \n",
       "23  [{'ref_id': '454849635198899072', 'chunk_id': ...   \n",
       "24  [{'ref_id': '454849635198899072', 'chunk_id': ...   \n",
       "25  [{'ref_id': '454846969260382800', 'chunk_id': ...   \n",
       "\n",
       "                            task_step_question_answer ref_task_step_id  \n",
       "0   在知识蒸馏中，损失函数的设计是关键，它直接影响学生模型的学习效果。通常，损失函数由两部分组成...                   \n",
       "1   在语音识别任务中，硬蒸馏被用于利用大量无标签语音数据，提高学生模型的识别准确率。例如，在RN...                   \n",
       "2   在知识蒸馏中，学生模型的计算效率不仅依赖于上述方法，还可以通过引入更多的优化策略来进一步提升...                   \n",
       "3   在模型蒸馏过程中，有效地将教师模型的知识转移到学生模型中是关键。以下是一些行之有效的方法：\\...                   \n",
       "4                                                                       \n",
       "5   在特征蒸馏中，选择教师模型的中间层特征时，通常会考虑这些特征在不同抽象层次上的表达能力。例如...                   \n",
       "6   在模型蒸馏中，除了上述技术外，还可以通过引入注意力机制来进一步提升知识转移的效果。注意力机制...                   \n",
       "7   在图像分类任务中，教师模型对一张猫的图片可能输出一个接近90%的概率属于猫类，而硬标签则只是...                   \n",
       "8   在模型蒸馏中，温度调节不仅影响教师模型的输出分布，还与其他技术如软标签和损失函数协同工作，以...                   \n",
       "9   在模型蒸馏中，除了温度参数和损失函数的设计，还可以通过其他技术进一步优化知识转移的效果。例如...                   \n",
       "10  在模型压缩中，剪枝技术通过移除神经网络中不重要的权重、神经元或通道来减少模型的复杂度。量化技...                   \n",
       "11  在模型压缩过程中，选择合适的学生模型架构以确保在资源受限的设备上高效运行，同时保持与教师模型...                   \n",
       "12  在处理跨领域数据迁移时，领域自适应技术可以通过最小化源域和目标域之间的分布差异来优化知识迁移...                   \n",
       "13                                        分数/总分: 9/10                   \n",
       "14                                              分数/总分                   \n",
       "15  在移动设备上进行实时图像分类时，学生模型可以在保持较高准确性的同时，显著减少计算时间和能耗。...                   \n",
       "16  在具体的任务中，学生模型的性能损失可以通过量化来进一步说明。例如，在图像分类任务中，教师模型...                   \n",
       "17  在实际应用中，多教师蒸馏已经在图像分类和自然语言处理任务中取得了显著的效果。例如，在Imag...                   \n",
       "18  在多教师蒸馏中，教师模型之间的知识冲突是一个常见的挑战。为了解决这一问题，可以采用知识共享技...                   \n",
       "19                                                                      \n",
       "20  在跨模态蒸馏中，动态权重分配策略的具体实现可以通过引入注意力机制来进一步优化。例如，在视觉问...                   \n",
       "21  在未来的研究中，还可以进一步探索如何将多粒度知识融合与动态知识转移结合，以更全面地提升学生模...                   \n",
       "22                                        分数/总分: 9/10                   \n",
       "23  在自适应蒸馏中，根据具体的任务需求和数据特点动态调整蒸馏过程以优化学生模型的性能，可以通过以...                   \n",
       "24                                                                      \n",
       "25  在未来的研究中，自适应蒸馏技术可以通过动态调整蒸馏温度或损失权重来优化知识转移。例如，在训练...                   "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dreamsboard.engine.storage.task_step_store.simple_task_step_store import SimpleTaskStepStore\n",
    "\n",
    "from dreamsboard.dreams.task_step_to_question_chain.weaviate.prepare_load import get_query_hash\n",
    "import os\n",
    "from dreamsboard.document_loaders.structured_storyboard_loader import StructuredStoryboard\n",
    "start_task_context=\"模型蒸馏的是什么？\"\n",
    "base_path = f'./{get_query_hash(start_task_context)}/'\n",
    "store_load = SimpleTaskStepStore.from_persist_dir(persist_dir=f'./{base_path}/storage')\n",
    " \n",
    "structured_storyboard = StructuredStoryboard(json_data=[step.__dict__ for step in list(store_load.task_step_all.values())])\n",
    "\n",
    "structured_storyboard.parse_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee88719-d7d8-4a99-be5c-45003880166b",
   "metadata": {},
   "source": [
    "### 渲染效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4c7d06a-858a-48c9-80d5-f7dedeb20220",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "# 模型蒸馏的是什么？ \n",
       "\n",
       "\n",
       "### 基本概念 [task_id](6c7d537f-57b5-4e83-ac4f-382b18371d9a)<sup>0</sup>\n",
       "\n",
       "在知识蒸馏中，损失函数的设计是关键，它直接影响学生模型的学习效果。通常，损失函数由两部分组成：第一部分是学生模型输出与教师模型输出的差异，通常使用KL散度来衡量；第二部分是学生模型输出与真实标签的差异，通常使用交叉熵损失来衡量。通过结合这两部分损失，学生模型不仅能够学习到教师模型的“暗知识”，还能够保持对真实标签的预测能力。这种双重损失的设计使得学生模型在保持高效推理的同时，能够达到接近教师模型的性能。此外，知识蒸馏还可以通过调整温度参数来进一步优化软标签的分布，使得学生模型能够更好地捕捉到教师模型中的复杂模式。\n",
       "\n",
       "教师模型 [task_id](00dd8782-ed06-4ca0-9336-1ead3c76e363)<sup>0>1</sup> 在语音识别任务中，硬蒸馏被用于利用大量无标签语音数据，提高学生模型的识别准确率。例如，在RNN-T语音识别中，教师模型可以使用少量有标签数据进行训练，然后生成无标签数据的伪标签，学生模型再使用这些伪标签进行训练。在文本分类任务中，硬蒸馏被用于利用无标签文本数据，提高学生模型的分类性能。例如，在情感分析任务中，教师模型可以使用有标签数据进行训练，然后生成无标签文本的伪标签，学生模型再使用这些伪标签进行训练。此外，硬蒸馏还可以与数据增强技术结合使用，通过生成更多的无标签数据，提高硬蒸馏的效果。正则化技术也可以用于防止学生模型过拟合伪标签，提高模型的泛化能力。\n",
       "\n",
       "学生模型 [task_id](efcde363-e9f2-4634-8a9f-27c169d028c6)<sup>0>2</sup> 在知识蒸馏中，学生模型的计算效率不仅依赖于上述方法，还可以通过引入更多的优化策略来进一步提升。例如，在自蒸馏方法中，可以通过引入多教师模型策略，利用多个教师模型的知识来指导学生模型的学习，从而进一步提升学生模型的性能。在蒸馏损失函数的调整中，可以通过引入更多的正则化项，如L2正则化或稀疏正则化，来进一步优化模型的泛化能力。在轻量化网络架构的设计中，可以通过引入更多的结构优化策略，如通道剪枝或层剪枝，来进一步降低模型的计算复杂度。在参数共享机制中，可以通过引入更多的共享策略，如跨任务共享或跨层共享，来进一步提升模型的效率。在低秩近似方法中，可以通过引入更多的分解策略，如奇异值分解或非负矩阵分解，来进一步降低模型的参数量。在剪枝操作中，可以通过引入更多的剪枝策略，如结构化剪枝或非结构化剪枝，来进一步提升模型的计算效率。在量化方法中，可以通过引入更多的量化策略，如混合精度量化或动态量化，来进一步提升模型的存储和计算效率。在近似计算方法中，可以通过引入更多的近似策略，如多项式近似或分段线性近似，来进一步提升模型的计算效率。在条件计算中，可以通过引入更多的条件策略，如基于样本复杂度的条件计算或基于模型置信度的条件计算，来进一步提升模型的计算效率。在多出口结构中，可以通过引入更多的出口策略，如基于任务难度的多出口或基于模型性能的多出口，来进一步提升模型的计算效率。\n",
       "\n",
       "### 蒸馏过程 [task_id](4e16c975-1df8-47b9-8f06-ea5fdcda90cf)<sup>1</sup>\n",
       "\n",
       "在模型蒸馏过程中，有效地将教师模型的知识转移到学生模型中是关键。以下是一些行之有效的方法：\n",
       "\n",
       "### 一、基本方法\n",
       "**1. 直接模仿教师模型的输出**\n",
       "   - 教师模型在训练过程中会产生预测输出。对于分类任务来说，这个输出是一个概率分布，通常被称为软标签（soft labels）。软标签包含了比硬标签（one-hot编码的真值标签）更多的信息，因为它展示了教师模型对每个类别的不确定性。学生模型可以通过最小化其预测概率分布与教师模型软标签之间的差异来学习。\n",
       "   - 例如，在图像分类任务中，教师模型对一张猫的图片可能输出一个接近90%的概率属于猫类，而硬标签则只是100%属于猫类。学生模型学习软标签可以更好地理解不同类别之间的相似性和差异性。\n",
       "**2. 使用中间层特征**\n",
       "   - 除了模仿教师模型的输出，还可以利用教师模型中间层的特征。中间层特征反映了教师模型在不同抽象层次上对输入数据的理解。学生模型可以被训练来模仿这些中间层特征，从而学习到教师模型的内部表征。\n",
       "   - 例如，在卷积神经网络（CNN）中，教师模型的某些中间卷积层可能提取了高级语义特征，如物体的形状或纹理。学生模型通过学习这些特征，可以更好地捕捉数据的复杂结构。\n",
       "\n",
       "### 二、高级方法\n",
       "**1. 自蒸馏（Self-Distillation）**\n",
       "   - 在自蒸馏中，学生模型实际上是教师模型的一个简化版本。教师模型首先在完整的数据集上进行训练，然后学生模型利用教师模型的预测（软标签）来进行训练。这种方法可以让学生模型从自身更复杂的版本中学习，进一步优化性能。\n",
       "   - 例如，一个大型的深度神经网络可以作为教师模型，而一个较小的、浅层的网络作为学生模型。学生模型通过学习教师模型的预测结果，能够继承其大部分知识，同时保持较低的计算复杂度。\n",
       "**2. 多教师知识蒸馏（Multi-Teacher Knowledge Distillation）**\n",
       "   - 这种方法使用多个教师模型来指导学生模型的学习。每个教师模型可以提供不同的视角或专业知识，学生模型通过整合多个教师的预测来获得更全面的知识。\n",
       "   - 例如，在自然语言处理任务中，可以使用多个针对不同语言风格或领域的教师模型，学生模型通过学习这些教师模型的输出，能够更好地理解和处理多样化的语言输入。\n",
       "**3. 元学习（Meta-Learning）与知识蒸馏的结合**\n",
       "   - 元学习技术可以用于优化知识蒸馏的过程。例如，可以通过元学习来动态调整蒸馏过程中的超参数，如温度参数（\n",
       "\n",
       "训练教师模型 [task_id](80e623b5-e6b0-46ce-a1df-f286d7c78fa6)<sup>1>1</sup>\n",
       "\n",
       "知识转移 [task_id](ef80b69f-0ebd-4fc5-aa96-7b567492cce2)<sup>1>2</sup> 在特征蒸馏中，选择教师模型的中间层特征时，通常会考虑这些特征在不同抽象层次上的表达能力。例如，在卷积神经网络中，浅层卷积层通常捕捉到的是低级特征，如边缘和纹理，而深层卷积层则捕捉到的是高级语义特征，如物体的形状和结构。通过选择不同层次的中间层特征，学生模型可以更全面地学习教师模型的知识。\n",
       "\n",
       "在注意力蒸馏中，教师模型的注意力机制通常是通过多头注意力机制实现的。每个注意力头可以捕捉到输入数据的不同方面，学生模型通过模仿这些注意力头，可以更全面地理解输入数据。例如，在视觉问答任务中，教师模型的注意力机制可以帮助学生模型更准确地定位图像中的关键区域，从而提高问答的准确性。\n",
       "\n",
       "此外，特征蒸馏和注意力蒸馏还可以结合使用，以进一步提升学生模型的性能。例如，在自然语言处理任务中，可以同时使用特征蒸馏和注意力蒸馏，使学生模型既能够学习到教师模型的中间层特征，又能够模仿教师模型的注意力机制。实验结果表明，在GLUE基准测试中，结合使用特征蒸馏和注意力蒸馏能够将学生模型的性能提升2%。\n",
       "\n",
       "### 关键技术 [task_id](063b04b2-85dd-415d-b945-b0395bfbabac)<sup>2</sup>\n",
       "\n",
       "在模型蒸馏中，除了上述技术外，还可以通过引入注意力机制来进一步提升知识转移的效果。注意力机制可以帮助学生模型更好地捕捉教师模型中的关键信息。例如，在图像分类任务中，教师模型可能会对某些特定区域给予更高的注意力，学生模型可以通过学习这些注意力权重，更准确地定位和分类目标物体。此外，注意力机制还可以用于多教师知识蒸馏中，通过动态调整不同教师模型的权重，使学生模型能够更有效地整合多个教师模型的知识。这种方法特别适用于复杂任务，如目标检测和语义分割，其中不同教师模型可能在不同区域或特征上表现出色。通过引入注意力机制，学生模型可以更灵活地学习和整合教师模型的知识，从而在复杂任务中实现更高的性能。\n",
       "\n",
       "软标签 [task_id](508a66a9-f2e5-4e43-8e01-496b1d156f74)<sup>2>1</sup> 在图像分类任务中，教师模型对一张猫的图片可能输出一个接近90%的概率属于猫类，而硬标签则只是100%属于猫类。学生模型学习软标签可以更好地理解不同类别之间的相似性和差异性。在自然语言处理任务中，软标签可以帮助学生模型理解不同词汇或短语之间的语义相似性，从而提高模型的泛化能力。在某些情况下，软标签可能过于平滑，导致学生模型学习到的信息不够明确。可以通过调整温度参数来优化软标签的分布，以克服这一问题。计算软标签需要额外的计算资源，特别是在处理大规模数据集时，可能会增加训练时间和计算成本。\n",
       "\n",
       "温度调节 [task_id](7b6c512c-4878-43ea-bce1-b3d3906c8c68)<sup>2>2</sup> 在模型蒸馏中，温度调节不仅影响教师模型的输出分布，还与其他技术如软标签和损失函数协同工作，以进一步提高知识转移的效果。软标签通过提供更丰富的类别间关系信息，帮助学生模型更好地理解数据的复杂性。损失函数的设计则确保学生模型在模仿教师模型的同时，保持对真实标签的预测能力。通过结合这些技术，模型蒸馏能够在保持高效推理的同时，达到接近教师模型的性能。此外，温度调节的优化策略如自适应温度调节，可以根据训练过程中的表现动态调整温度参数，从而进一步提高学生模型的学习效果。在实际应用中，温度调节的策略需要根据具体任务和数据集的特点进行灵活调整，以达到最佳的知识转移效果。\n",
       "\n",
       "损失函数 [task_id](275fc0ad-6585-43ed-b463-8cf7d64f17ed)<sup>2>3</sup> 在模型蒸馏中，除了温度参数和损失函数的设计，还可以通过其他技术进一步优化知识转移的效果。例如，使用自适应学习率来动态调整训练过程中的学习率，以加速收敛并提高模型的泛化能力。此外，正则化技术如L2正则化或Dropout也可以应用于学生模型的训练中，以防止过拟合并提高模型的鲁棒性。在某些复杂的任务中，还可以结合元学习技术，通过动态调整蒸馏过程中的超参数，如温度参数 \\(T\\) 和损失函数权重 \\(\\lambda\\)，以优化知识转移的效果。这些技术的结合使用可以进一步提升学生模型的性能，使其在保持高效推理的同时，能够达到接近教师模型的性能。\n",
       "\n",
       "### 应用场景 [task_id](8c337324-8c7d-42e4-be74-bdf5ed270288)<sup>3</sup>\n",
       "\n",
       "在模型压缩中，剪枝技术通过移除神经网络中不重要的权重、神经元或通道来减少模型的复杂度。量化技术则将模型的参数从高精度转换为低精度，从而减少模型的存储和计算需求。知识蒸馏通过将一个大型教师模型的知识转移到一个小型学生模型中，使学生模型在保持较高准确性的同时具有更小的复杂度。在知识迁移中，输出蒸馏通过学生模型模仿教师模型的输出分布来实现知识转移。特征蒸馏则通过学生模型模仿教师模型的中间层特征图来实现知识转移。关系蒸馏通过学生模型学习教师模型中特征之间的关系来实现知识转移。在提高泛化能力中，动态调整技术根据输入数据的复杂度，动态调整模型的计算路径，以提高计算效率。多出口结构则在模型中设计多个输出层，根据中间层的输出结果提前终止计算。\n",
       "\n",
       "模型压缩 [task_id](f294540f-722a-460b-b3bc-f10227978cec)<sup>3>1</sup> 在模型压缩过程中，选择合适的学生模型架构以确保在资源受限的设备上高效运行，同时保持与教师模型相当的性能，需要综合考虑以下几个方面：\n",
       "\n",
       "### 1. **明确目标设备的资源限制**\n",
       "- **计算能力**：了解目标设备的处理器类型（如 CPU、GPU 或专用加速器）及其计算能力，以便选择计算量适中的模型架构。\n",
       "- **内存容量**：确定设备的可用内存，避免选择过大导致无法加载的模型。\n",
       "- **能耗约束**：对于移动或边缘设备，低能耗是关键。优先考虑轻量级且运算效率高的架构。\n",
       "\n",
       "### 2. **分析教师模型的特性**\n",
       "- **架构复杂度**：教师模型通常参数量大、结构复杂。学生模型需在保持关键特征的基础上，简化结构。\n",
       "- **性能瓶颈**：分析教师模型的计算瓶颈（如某些层或模块），针对性地优化学生模型架构，避免保留不必要的复杂性。\n",
       "\n",
       "### 3. **选择合适的学生模型架构**\n",
       "- **轻量级架构**：选用MobileNet、ShuffleNet等专门为移动设备设计的架构，这些架构在保持一定精度的同时，大幅减少计算量和参数量。\n",
       "- **结构相似性**：学生模型的结构与教师模型保持一定的相似性，有助于知识的有效迁移。例如，可采用剪枝、量化或低秩分解等方式，从教师模型中提取关键信息，构建更紧凑的结构。\n",
       "- **模块化设计**：采用模块化架构，可根据实际需求灵活调整模型大小和复杂度。例如，通过堆叠或裁剪模块，实现对模型容量的精细控制，以适应不同资源限制。\n",
       "\n",
       "### 4. **优化方法与策略**\n",
       "- **知识蒸馏**：利用知识蒸馏技术，将教师模型的知识（如软标签、中间层特征等）传递给学生模型。可采用多阶段蒸馏或自适应蒸馏等策略，提升学生模型的性能。\n",
       "- **参数共享**：对于某些任务，可考虑参数共享策略，如在自然语言处理中，共享词嵌入层或Transformer层的参数，减少模型的存储和计算成本。\n",
       "- **超参数调优**：对学习率、批大小、蒸馏温度等超参数进行精细调优，以达到最佳的性能与资源利用平衡。\n",
       "\n",
       "### 5. **实验验证与迭代优化**\n",
       "- **性能评估**：在资源受限的设备上，对候选学生模型进行性能评估，包括准确率、延迟、资源消耗等指标。\n",
       "- **迭代优化**：根据评估结果，不断调整学生模型的架构和训练策略。例如，通过网络架构搜索（NAS）自动寻找最优架构，或结合剪枝和量化技术进一步压缩模型。\n",
       "\n",
       "通过以上方法，可以在资源受限的设备上选择并优化学生模型架构，使其在保持与教师模型相当性能的同时，实现高效的运行。\n",
       "\n",
       "知识迁移 [task_id](02abe5b1-3c00-445f-b8af-a4f967e45a92)<sup>3>2</sup> 在处理跨领域数据迁移时，领域自适应技术可以通过最小化源域和目标域之间的分布差异来优化知识迁移效果。例如，在计算机视觉任务中，可以通过对抗训练或特征对齐技术来减少不同领域之间的特征分布差异，从而提高学生模型在新领域中的表现。此外，分布式训练可以通过将计算任务分配到多个GPU或节点上来加速训练过程，而混合精度训练则通过使用低精度浮点数来减少内存占用和计算开销，从而在保持模型性能的同时提高训练效率。数据增强技术不仅可以通过随机裁剪和旋转来增加数据的多样性，还可以通过颜色抖动、噪声添加等方式来模拟不同的光照和噪声条件，从而进一步提高模型的鲁棒性和泛化能力。\n",
       "\n",
       "提高泛化能力 [task_id](824f9802-393d-49ce-bd9b-469d25386813)<sup>3>3</sup> 分数/总分: 9/10\n",
       "\n",
       "### 优势与局限性 [task_id](4db44f90-796c-4d8b-9171-4ddb6421aa05)<sup>4</sup>\n",
       "\n",
       "分数/总分\n",
       "\n",
       "优势 [task_id](95a17065-6829-4c26-9e21-47ffa5cce59f)<sup>4>1</sup> 在移动设备上进行实时图像分类时，学生模型可以在保持较高准确性的同时，显著减少计算时间和能耗。例如，在ImageNet数据集上，蒸馏后的学生模型在保持90%以上准确率的情况下，推理时间从教师模型的300ms降低到了100ms，能耗减少了60%。此外，软标签的使用可以帮助学生模型在未见过的数据上表现更好，从而提高其泛化能力。在自然语言处理任务中，多教师蒸馏可以通过结合多个教师模型的优点，进一步提高学生模型的性能。自蒸馏方法则可以通过让学生模型从自身更复杂的版本中学习，进一步优化其性能。跨模态蒸馏方法可以帮助学生模型更好地理解多模态数据，从而提高其在复杂任务中的表现。通过自适应蒸馏方法，可以根据任务和数据的特点动态调整蒸馏过程，进一步提高模型的知识转移效果。\n",
       "\n",
       "局限性 [task_id](fc0ddb45-3e91-455f-83cc-ec0ce08227c1)<sup>4>2</sup> 在具体的任务中，学生模型的性能损失可以通过量化来进一步说明。例如，在图像分类任务中，教师模型的准确率可能达到95%，而学生模型的准确率可能只有90%，这表明学生模型在压缩过程中损失了5%的性能。这种性能损失在复杂任务中尤为明显，如自然语言处理中的机器翻译任务，教师模型可能能够捕捉到复杂的语法结构和语义关系，而学生模型由于参数限制，可能无法完全复现这些复杂特征，导致翻译质量下降。此外，知识压缩的难度在具体任务中也有所体现。例如，在目标检测任务中，教师模型可能通过多层卷积网络学习到物体的精确边界和复杂背景，而学生模型由于网络深度和宽度的限制，可能无法准确捕捉这些细节，导致检测精度下降。数据分布差异的影响在跨领域任务中尤为显著。例如，在医学图像分析中，教师模型可能在特定类型的医学图像上表现良好，而学生模型在面对不同类型的医学图像时，由于数据分布差异，可能无法有效学习到教师模型的知识，导致性能下降。训练复杂性方面，训练一个大型教师模型可能需要数千个GPU小时，而蒸馏过程本身也需要数百个GPU小时，这显著增加了整体的训练时间和计算成本。为了缓解这些问题，可以采用多教师蒸馏技术，通过整合多个教师模型的知识来减少性能损失。例如，在语音识别任务中，多个教师模型可以提供不同的语音特征，学生模型通过学习这些特征，能够更好地适应不同的语音输入。自蒸馏技术也可以通过让学生模型从自身更复杂的版本中学习，进一步优化性能。分布式训练则可以通过将计算任务分配到多个GPU或节点上来加速训练过程，从而减少训练时间。\n",
       "\n",
       "### 最新进展 [task_id](fc5791d1-27d2-481d-99cb-7031f548fbf6)<sup>5</sup>\n",
       "\n",
       "在实际应用中，多教师蒸馏已经在图像分类和自然语言处理任务中取得了显著的效果。例如，在ImageNet数据集上，通过动态权重分配策略，学生模型的准确率提升了2%，同时减少了30%的计算资源消耗。未来，可以进一步探索如何将多教师蒸馏与元学习结合，以动态调整蒸馏过程中的超参数，从而进一步提升学生模型的性能。通过以上改进，回答不仅提供了技术细节，还增加了实际案例和未来研究方向，使得内容更加丰富和有建设性。\n",
       "\n",
       "多教师蒸馏 [task_id](9611456a-e799-41b6-8ae8-fb0db3136ebe)<sup>5>1</sup> 在多教师蒸馏中，教师模型之间的知识冲突是一个常见的挑战。为了解决这一问题，可以采用知识共享技术，通过共享部分网络参数或设计跨模态注意力机制来实现知识共享，从而减少知识冲突的影响。此外，计算资源的限制也是多教师蒸馏面临的一个重要问题。通过轻量化技术，如模型压缩或知识共享，可以有效减少计算资源的需求，提高蒸馏效率。在实际应用中，多教师蒸馏已经在视觉问答、图像描述生成等任务中取得了显著的效果。例如，在视觉问答任务中，通过跨模态特征对齐和多模态融合技术，问答系统的性能得到了显著提升。未来，更有效的动态权重分配策略和轻量化技术的优化将是多教师蒸馏的重要研究方向。\n",
       "\n",
       "自蒸馏 [task_id](1d249791-159d-4edc-8b67-fdba1bb0b39e)<sup>5>2</sup>\n",
       "\n",
       "跨模态蒸馏 [task_id](195a6dee-ae40-4bcb-8382-e8a40b227cbc)<sup>5>3</sup> 在跨模态蒸馏中，动态权重分配策略的具体实现可以通过引入注意力机制来进一步优化。例如，在视觉问答任务中，教师模型的注意力机制可以帮助学生模型更准确地定位图像中的关键区域，从而提高问答的准确性。通过动态调整不同教师模型的注意力权重，学生模型可以更灵活地学习和整合教师模型的知识，从而在复杂任务中实现更高的性能。此外，轻量化技术的优化策略如自适应模型压缩，可以根据训练过程中的表现动态调整模型的复杂度，从而进一步提高学生模型的学习效果。在实际应用中，这些策略需要根据具体任务和数据集的特点进行灵活调整，以达到最佳的知识转移效果。\n",
       "\n",
       "### 未来研究方向 [task_id](b438eb5d-3e98-4b91-8ce4-061e0f785681)<sup>6</sup>\n",
       "\n",
       "在未来的研究中，还可以进一步探索如何将多粒度知识融合与动态知识转移结合，以更全面地提升学生模型的学习效果。例如，在图像分类任务中，可以通过多头注意力机制动态调整不同层知识的转移权重，具体可以通过计算每一层的特征重要性得分，并使用这些得分来加权不同层的知识转移。此外，自适应蒸馏中的自适应样本选择和自适应知识源融合可以结合使用，例如在自然语言处理任务中，通过MAML算法动态选择最具信息量的样本，具体可以通过元学习的方式，根据样本的梯度信息来选择最具挑战性的样本，并根据任务需求自动融合多个教师模型的知识，例如通过加权平均或注意力机制来融合多个教师模型的输出。在泛化能力提升方面，可以进一步研究如何将数据增强与噪声注入技术结合正则化与约束，例如在图像分类任务中，通过在输入数据中添加高斯噪声并结合Dropout和BatchNorm，具体可以通过动态调整噪声强度和Dropout率，使学生模型在保持高效推理的同时，能够更好地适应不同的数据分布。跨域泛化蒸馏可以结合迁移学习技术，例如在医学图像分析任务中，通过迁移学习技术将教师模型的知识有效地迁移到目标领域，具体可以通过对抗训练或特征对齐来减少源域和目标域之间的分布差异，从而提高学生模型在新领域中的泛化能力。\n",
       "\n",
       "更有效的知识转移方法 [task_id](698e4964-272b-48e6-99d4-7beca68703a0)<sup>6>1</sup> 分数/总分: 9/10\n",
       "\n",
       "自适应蒸馏 [task_id](f70c75ba-ce85-405d-8c9e-869a95298e6a)<sup>6>2</sup> 在自适应蒸馏中，根据具体的任务需求和数据特点动态调整蒸馏过程以优化学生模型的性能，可以通过以下几种方法实现：\n",
       "\n",
       "### 1. **动态调整蒸馏温度（Temperature）**\n",
       "蒸馏温度是知识蒸馏中的一个重要超参数，它控制了教师模型输出的软化程度。较高的温度可以使教师模型的输出更加平滑，从而帮助学生模型更好地学习软目标。根据任务需求和数据特点，可以动态调整温度以优化性能。\n",
       "\n",
       "- **方法**：\n",
       "  - **自适应温度调整**：根据训练进度和学生模型的性能动态调整温度。例如，在训练初期使用较高的温度以帮助学生模型快速学习软目标，而在训练后期逐渐降低温度以强化硬目标的学习。\n",
       "  - **样本自适应温度**：根据每个样本的难度动态调整温度。对于较难的样本，使用较高的温度以提供更多的软目标信息；对于较简单的样本，使用较低的温度以强化硬目标学习。\n",
       "\n",
       "### 2. **动态调整蒸馏损失权重**\n",
       "在知识蒸馏中，通常会结合蒸馏损失（KL散度）和分类损失（交叉熵）来训练学生模型。根据任务需求和数据特点，可以动态调整这两部分损失的权重以优化学生模型的性能。\n",
       "\n",
       "- **方法**：\n",
       "  - **固定比例调整**：根据预实验结果设定一个固定的比例范围，例如在训练初期增加蒸馏损失的权重，而在训练后期增加分类损失的权重。\n",
       "  - **自适应权重调整**：根据学生模型的性能动态调整权重。例如，当学生模型在验证集上的性能提升时，逐渐增加分类损失的权重；当性能下降时，增加蒸馏损失的权重。\n",
       "\n",
       "### 3. **选择合适的教师模型**\n",
       "教师模型的选择对知识蒸馏的效果至关重要。根据任务需求和数据特点，可以选择不同结构和复杂度的教师模型。\n",
       "\n",
       "- **方法**：\n",
       "  - **任务特定教师模型**：根据具体任务选择或训练教师模型。例如，在图像分类\n",
       "\n",
       "泛化能力提升 [task_id](9c332766-5a7c-4366-a986-0dc0e638624b)<sup>6>3</sup>\n",
       "\n",
       "### 总结 [task_id](ff2674ca-68b1-4ab7-b668-75a86128e62e)<sup>7</sup>\n",
       "\n",
       "在未来的研究中，自适应蒸馏技术可以通过动态调整蒸馏温度或损失权重来优化知识转移。例如，在训练初期使用较高的温度以帮助学生模型快速学习软目标，而在训练后期逐渐降低温度以强化硬目标的学习。此外，多教师蒸馏可以通过整合多个教师模型的知识来减少性能损失，特别是在语音识别任务中，多个教师模型可以提供不同的语音特征，学生模型通过学习这些特征，能够更好地适应不同的语音输入。自蒸馏技术也可以通过让学生模型从自身更复杂的版本中学习，进一步优化性能。分布式训练则可以通过将计算任务分配到多个GPU或节点上来加速训练过程，从而减少训练时间。\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "# References  \n",
       "\n",
       "[0] Bayes Conditional Distribution Estimation for Knowledge Distillation Based on Conditional Mutual Information. ,chunk_id:454846021212093966 \n",
       "\r\n",
       "[0] On-Device Next-Item Recommendation with Self-Supervised Knowledge Distillation ,chunk_id:455037545741550556 \n",
       "\r\n",
       "[0] Adaptive Label Smoothing with Self-Knowledge in Natural Language Generation ,chunk_id:454919276633279012 \n",
       "\r\n",
       "[0>1] Bayes Conditional Distribution Estimation for Knowledge Distillation Based on Conditional Mutual Information. ,chunk_id:454846021397691930 \n",
       "\r\n",
       "[0>1] Learning Differentially Private Diffusion Models Via Stochastic Adversarial Distillation ,chunk_id:454846524788653356 \n",
       "\r\n",
       "[0>1] Bayes Conditional Distribution Estimation for Knowledge Distillation Based on Conditional Mutual Information. ,chunk_id:454846021212093966 \n",
       "\r\n",
       "[0>2] On-Device Next-Item Recommendation with Self-Supervised Knowledge Distillation ,chunk_id:455037545741550556 \n",
       "\r\n",
       "[0>2] Adaptive Label Smoothing with Self-Knowledge in Natural Language Generation ,chunk_id:454919276633279012 \n",
       "\r\n",
       "[0>2] Bayes Conditional Distribution Estimation for Knowledge Distillation Based on Conditional Mutual Information. ,chunk_id:454846021212093966 \n",
       "\r\n",
       "[1] Adaptive Label Smoothing with Self-Knowledge in Natural Language Generation ,chunk_id:454919276633279012 \n",
       "\r\n",
       "[1] On-Device Next-Item Recommendation with Self-Supervised Knowledge Distillation ,chunk_id:455037545741550556 \n",
       "\r\n",
       "[1] Learning to Explore Distillability and Sparsability: A Joint Framework for Model Compression ,chunk_id:454845510461164774 \n",
       "\r\n",
       "[1>1] AuG-KD: Anchor-Based Mixup Generation for Out-of-Domain Knowledge Distillation. ,chunk_id:454895484123495166 \n",
       "\r\n",
       "[1>1] Learning Differentially Private Diffusion Models Via Stochastic Adversarial Distillation ,chunk_id:454846524788653356 \n",
       "\r\n",
       "[1>1] Select and Distill: Selective Dual-Teacher Knowledge Transfer for Continual Learning on Vision-Language Models ,chunk_id:454846250864671532 \n",
       "\r\n",
       "[1>2] Adaptive Label Smoothing with Self-Knowledge in Natural Language Generation ,chunk_id:454919276633279012 \n",
       "\r\n",
       "[1>2] Learning to Explore Distillability and Sparsability: A Joint Framework for Model Compression ,chunk_id:454845510461164774 \n",
       "\r\n",
       "[1>2] On-Device Next-Item Recommendation with Self-Supervised Knowledge Distillation ,chunk_id:455037545741550556 \n",
       "\r\n",
       "[2] Encapsulating Knowledge in One Prompt ,chunk_id:454846429879674798 \n",
       "\r\n",
       "[2] Uncertainty Calibration with Energy Based Instance-wise Scaling in the Wild Dataset ,chunk_id:454846435071437024 \n",
       "\r\n",
       "[2] SPTNet: an Efficient Alternative Framework for Generalized Category Discovery with Spatial Prompt Tuning. ,chunk_id:454895454428868532 \n",
       "\r\n",
       "[2>1] Learning from Biased Soft Labels. ,chunk_id:454845564881216834 \n",
       "\r\n",
       "[2>1] Learning from Biased Soft Labels. ,chunk_id:454845564898518340 \n",
       "\r\n",
       "[2>1] When Object Detection Meets Knowledge Distillation: A Survey ,chunk_id:454845516142612392 \n",
       "\r\n",
       "[2>2] Encapsulating Knowledge in One Prompt ,chunk_id:454846429944424370 \n",
       "\r\n",
       "[2>2] Adaptive Label Smoothing with Self-Knowledge in Natural Language Generation ,chunk_id:454919276633279012 \n",
       "\r\n",
       "[2>2] Encapsulating Knowledge in One Prompt ,chunk_id:454846429879674798 \n",
       "\r\n",
       "[2>3] On the Fairness Impacts of Private Ensembles Models ,chunk_id:454846109840627474 \n",
       "\r\n",
       "[2>3] Learning from Biased Soft Labels. ,chunk_id:454845564972442954 \n",
       "\r\n",
       "[2>3] Bayes Conditional Distribution Estimation for Knowledge Distillation Based on Conditional Mutual Information. ,chunk_id:454846021298077204 \n",
       "\r\n",
       "[3] Encapsulating Knowledge in One Prompt ,chunk_id:454846429944424370 \n",
       "\r\n",
       "[3] Encapsulating Knowledge in One Prompt ,chunk_id:454846429879674798 \n",
       "\r\n",
       "[3] On \"scientific Debt\" in NLP: A Case for More Rigour in Language Model Pre-Training Research. ,chunk_id:454847903962379542 \n",
       "\r\n",
       "[3>1] On-Device Next-Item Recommendation with Self-Supervised Knowledge Distillation ,chunk_id:455037545741550556 \n",
       "\r\n",
       "[3>1] Learning to Explore Distillability and Sparsability: A Joint Framework for Model Compression ,chunk_id:454845510602198264 \n",
       "\r\n",
       "[3>1] Differentially Private Model Compression ,chunk_id:454965269723354462 \n",
       "\r\n",
       "[3>2] Adaptive Label Smoothing with Self-Knowledge in Natural Language Generation ,chunk_id:454919276633279012 \n",
       "\r\n",
       "[3>2] On-Device Next-Item Recommendation with Self-Supervised Knowledge Distillation ,chunk_id:455037545741550556 \n",
       "\r\n",
       "[3>2] Bayes Conditional Distribution Estimation for Knowledge Distillation Based on Conditional Mutual Information. ,chunk_id:454846021212093966 \n",
       "\r\n",
       "[3>3] When Object Detection Meets Knowledge Distillation: A Survey ,chunk_id:454845516226498482 \n",
       "\r\n",
       "[3>3] Adaptive Label Smoothing with Self-Knowledge in Natural Language Generation ,chunk_id:454919276688853548 \n",
       "\r\n",
       "[3>3] An Unsupervised Multiple-Task and Multiple-Teacher Model for Cross-lingual Named Entity Recognition ,chunk_id:454898870705195248 \n",
       "\r\n",
       "[4] Rethinking the Spatial Inconsistency in Classifier-Free Diffusion Guidance ,chunk_id:454849635198899072 \n",
       "\r\n",
       "[4] Normalization Techniques in Training DNNs: Methodology, Analysis and Application ,chunk_id:454845536381442298 \n",
       "\r\n",
       "[4] Massively Scaling Heteroscedastic Classifiers ,chunk_id:454848201295552756 \n",
       "\r\n",
       "[4>1] On the Fairness Impacts of Private Ensembles Models ,chunk_id:454846109840627474 \n",
       "\r\n",
       "[4>1] How to Distill Your BERT: an Empirical Study on the Impact of Weight Initialisation and Distillation Objectives ,chunk_id:454847845063601170 \n",
       "\r\n",
       "[4>1] Bayes Conditional Distribution Estimation for Knowledge Distillation Based on Conditional Mutual Information. ,chunk_id:454846021212093966 \n",
       "\r\n",
       "[4>2] Bayes Conditional Distribution Estimation for Knowledge Distillation Based on Conditional Mutual Information. ,chunk_id:454846021212093966 \n",
       "\r\n",
       "[4>2] Data Distribution Distilled Generative Model for Generalized Zero-Shot Recognition ,chunk_id:454847039370799022 \n",
       "\r\n",
       "[4>2] Self-Adapting Large Visual-Language Models to Edge Devices Across Visual Modalities ,chunk_id:454846239382763602 \n",
       "\r\n",
       "[5] Vector Field Oriented Diffusion Model for Crystal Material Generation ,chunk_id:454847000032419890 \n",
       "\r\n",
       "[5] Normalization Techniques in Training DNNs: Methodology, Analysis and Application ,chunk_id:454845536381442298 \n",
       "\r\n",
       "[5] Vector Field Oriented Diffusion Model for Crystal Material Generation ,chunk_id:454846999994409008 \n",
       "\r\n",
       "[5>1] Learning Generalizable Models for Vehicle Routing Problems via Knowledge   Distillation ,chunk_id:454984267297727980 \n",
       "\r\n",
       "[5>1] Improving Machine Reading Comprehension with Contextualized Commonsense Knowledge ,chunk_id:454896044923158948 \n",
       "\r\n",
       "[5>1] Bayes Conditional Distribution Estimation for Knowledge Distillation Based on Conditional Mutual Information. ,chunk_id:454846021212093966 \n",
       "\r\n",
       "[5>2] Sample Efficiency Matters: A Benchmark for Practical Molecular Optimization. ,chunk_id:454984176802988974 \n",
       "\r\n",
       "[5>2] Rethinking the Spatial Inconsistency in Classifier-Free Diffusion Guidance ,chunk_id:454849635198899072 \n",
       "\r\n",
       "[5>2] Vector Field Oriented Diffusion Model for Crystal Material Generation ,chunk_id:454847000032419890 \n",
       "\r\n",
       "[5>3] Learning Differentially Private Diffusion Models Via Stochastic Adversarial Distillation ,chunk_id:454846524788653356 \n",
       "\r\n",
       "[5>3] DistilPose: Tokenized Pose Regression with Heatmap Distillation ,chunk_id:454847316713920294 \n",
       "\r\n",
       "[5>3] One is All: Bridging the Gap Between Neural Radiance Fields Architectures with Progressive Volume Distillation ,chunk_id:454845570435524492 \n",
       "\r\n",
       "[6] Vector Field Oriented Diffusion Model for Crystal Material Generation ,chunk_id:454847000032419890 \n",
       "\r\n",
       "[6] Encapsulating Knowledge in One Prompt ,chunk_id:454846429944424370 \n",
       "\r\n",
       "[6] Vector Field Oriented Diffusion Model for Crystal Material Generation ,chunk_id:454846999994409008 \n",
       "\r\n",
       "[6>1] Learning to Explore Distillability and Sparsability: A Joint Framework for Model Compression ,chunk_id:454845510461164774 \n",
       "\r\n",
       "[6>1] Encapsulating Knowledge in One Prompt ,chunk_id:454846429944424370 \n",
       "\r\n",
       "[6>1] Low-Rank Knowledge Decomposition for Medical Foundation Models ,chunk_id:454849596909883040 \n",
       "\r\n",
       "[6>2] Rethinking the Spatial Inconsistency in Classifier-Free Diffusion Guidance ,chunk_id:454849635198899072 \n",
       "\r\n",
       "[6>2] On-Device Next-Item Recommendation with Self-Supervised Knowledge Distillation ,chunk_id:455037545741550556 \n",
       "\r\n",
       "[6>2] Logit Standardization in Knowledge Distillation ,chunk_id:454849384740221868 \n",
       "\r\n",
       "[6>3] Rethinking the Spatial Inconsistency in Classifier-Free Diffusion Guidance ,chunk_id:454849635198899072 \n",
       "\r\n",
       "[6>3] Normalization Techniques in Training DNNs: Methodology, Analysis and Application ,chunk_id:454845536381442298 \n",
       "\r\n",
       "[6>3] Diffusion-GAN: Training GANs with Diffusion ,chunk_id:454848005305156698 \n",
       "\r\n",
       "[7] Transfer and Alignment Network for Generalized Category Discovery ,chunk_id:454846969260382800 \n",
       "\r\n",
       "[7] BT^2: Backward-compatible Training with Basis Transformation ,chunk_id:454848385319860832 \n",
       "\r\n",
       "[7] Learning to Explore Distillability and Sparsability: A Joint Framework for Model Compression ,chunk_id:454845510461164774 \n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from dreamsboard.dreams.task_step_md.base import TaskStepMD\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    " \n",
    "task_step_store = SimpleTaskStepStore.from_persist_dir(f'./{base_path}/storage')\n",
    "task_step_md = TaskStepMD(task_step_store)\n",
    "md_text =   task_step_md.format_md()\n",
    "\n",
    "display(Markdown(md_text.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd9c103-b9fb-459b-9589-2920b90e00fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dreams] *",
   "language": "python",
   "name": "conda-env-dreams-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
