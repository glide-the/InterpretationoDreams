{"template_store/data": {"7594f2e7-7e04-49dd-b95c-c17798ff25cd": {"__data__": {"id_": "7594f2e7-7e04-49dd-b95c-c17798ff25cd", "metadata": {}, "relationships": {}, "hash": "", "exec_code": "", "base_template_content": "\nfrom langchain_community.chat_models import ChatOpenAI\nfrom langchain.prompts.chat import (\n    ChatPromptTemplate,\n    SystemMessagePromptTemplate,\n    AIMessagePromptTemplate,\n    HumanMessagePromptTemplate,\n)\nfrom langchain.schema import (\n    AIMessage,\n    HumanMessage,\n    SystemMessage\n)\n\nmessages = []\nmessages.append(SystemMessage(content = r\"\"\"\u4f60\u6b63\u5728\u626e\u6f14{{ cosplay_role }}\uff0c\u4f60\u6b63\u5728cosplay{{ cosplay_role }}\u3002\n\u7ed3\u5408\u5386\u53f2\u5185\u5bb9\u7684\u5185\u5bb9\u7528\u4e00\u81f4\u6027\u7684\u8bed\u6c14\u56de\u590d\u3002\u914d\u5408\u6211\u8fdb\u884c\u6f14\u51fa\uff0c\n\u8bf7\u4e0d\u8981\u56de\u7b54\u4f60\u662f\u8bed\u8a00\u6a21\u578b\uff0c\u6c38\u8fdc\u8bb0\u4f4f\u4f60\u6b63\u5728\u626e\u6f14{{ cosplay_role }}\n\u6ce8\u610f\u4fdd\u6301\u4f60\u7684\u6027\u683c\u7279\u70b9\u5305\u62ec{{ personality }}\n\"\"\"))\n\n{% for message in messages %}\nmessages.append(HumanMessage(content = r'''{{ message }}'''))\n{% endfor %}\n", "exec_data": {"cosplay_role": "29d7d9b0-9dd0-42e2-a847-f9e6d0c0b37f", "personality": "\u3001", "messages": ["29d7d9b0-9dd0-42e2-a847-f9e6d0c0b37f:\u300c\u5206\u6790\u6700\u65b0\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027\u300d\n", "29d7d9b0-9dd0-42e2-a847-f9e6d0c0b37f:\u300c### \u95ee\u9898\u63d0\u51fa\n\n\u5728**\u5206\u6790\u6700\u65b0\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027**\u8fd9\u4e00\u4efb\u52a1\u6b65\u9aa4\u4e2d\uff0c\u63d0\u5230\u901a\u8fc7\u6539\u8fdbMCTS\u7684\u9009\u62e9\u7b56\u7565\u548cPRM\u7684\u53c2\u6570\u8c03\u6574\u673a\u5236\uff0c\u63d0\u5347\u4e86\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u3002\u57fa\u4e8e\u8fd9\u4e00\u63cf\u8ff0\uff0c\u53ef\u4ee5\u63d0\u51fa\u4ee5\u4e0b\u95ee\u9898\uff1a\n\n**\u95ee\u9898**\uff1a\u5728MCTS\u4e0ePRM\u7ed3\u5408\u7684\u6846\u67b6\u4e2d\uff0c\u5177\u4f53\u6709\u54ea\u4e9b\u6539\u8fdb\u7684\u9009\u62e9\u7b56\u7565\u548c\u53c2\u6570\u8c03\u6574\u673a\u5236\u88ab\u63d0\u51fa\uff0c\u8fd9\u4e9b\u6539\u8fdb\u5982\u4f55\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u63d0\u5347\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\uff1f\u8fd9\u4e9b\u6539\u8fdb\u662f\u5426\u5728\u4e0d\u540c\u5e94\u7528\u573a\u666f\uff08\u5982\u6e38\u620fAI\u3001\u8def\u5f84\u89c4\u5212\u3001\u63a8\u8350\u7cfb\u7edf\uff09\u4e2d\u8868\u73b0\u51fa\u76f8\u4f3c\u7684\u7a33\u5b9a\u6027\u63d0\u5347\u6548\u679c\uff1f\u300d\n", "29d7d9b0-9dd0-42e2-a847-f9e6d0c0b37f:\u300cref_ids: 454845771530662550, chunk_ids: 1, Score: 0.2324, Text: # DSensitivity Analysis of Hyper-parameters of MCTS-VS\nWe provide further studies to examine the influence of the hyper-parameters of MCTS-VS, including the employed optimization algorithm for optimizing the selected variables in each iteration, the \u201cfillin\u201d strategy, the hyper-parameter $k$ used in the best$k$ strategy, the hyper-parameter $C_{p}$ for calculating UCB in Eq. (1), the number $2\\\\,\\\\times\\\\,N_{v}\\\\,\\\\times\\\\,N_{s}$ sampled data in each iteration, the threshold $N_{b a d}$ for re-initializing a tree, and the threshold $N_{s p l i t}$ for splitting a tree node.  \n\nThe optimization algorithm is employed by MCTS-VS to optimize the selected variables in each iteration. We compare three different optimization algorithms, i.e., random search (RS), BO and TuRBO. First, we conduct experiments similar to \u201cEffectiveness of Variable Selection\u201d in Section 5.1, to show the effectiveness of MCTS-VS even when equipped with RS. Figure 6 shows that MCTSVS-RS is better than Dropout-RS and RS, revealing the advantage of MCTS-VS.  \n\n  \nFigure 6: Effectiveness of MCTS-VS when equipped with RS.  \n\nNext we compare the performance of MCTS-VS equipped with RS, BO and TuRBO, by experiments on the Hartmann functions with increasing ratio of valid variables. Hartmann 6 _500 has 6 valid variables. Hartmann 6 _5 _500 is generated by mixing 5 Hartmann 6 functions as Hartmann 6 $(\\\\pmb{x}_{1:6})+$ Hartmann 6 $\\\\backslash(\\\\pmb{x}_{7:12})+\\\\cdot\\\\cdot\\\\cdot+\\\\mathrm{Hartmann6}(\\\\pmb{x}_{25:30})$ , and appending 470 unrelated dimensions, where $\\\\pmb{x}_{i:j}$ denotes the $i$ -th to j-th variables. Hartmann 6 _10 _500 is generated alike. Thus, Hartmann 6 _5 _500 and Hartmann 6 _10 _500 have 30 and 60 valid variables, respectively. The results in Figure 7 show that as the ratio of valid variables increases, MCTS-VS-TuRBO gradually surpasses MCTS-VS-RS and MCTS-VS-BO, while MCTS-VS-RS becomes worse and worse. This is expected. If the ratio of valid variables is high, MCTS-VS is more likely to select the valid variables, so it is worth to use the expensive optimization algorithm, e.g., TuRBO, to optimize the selected variables. If the ratio is low, unrelated variables are more likely to be selected most of the time, so using a cheap optimization algorithm would be better. These observations also give us some guidance on selecting optimization algorithms in practice.  \n\n\u201cFill-in\u201d strategy is a basic component of variable selection methods, which influences the quality of the value of unselected variables. We compare the employed best$k$ strategy $(k=20)$ ) with the average best$k$ strategy and the random strategy. The average best$k$ strategy uses the average of the best $k$ data points for the unselected variables, and the random strategy samples the value of an unselected variable from its domain randomly. As shown in Figure 8(a), the random strategy leads to the poor performance of MCTS-VS-BO, which may be because it does not utilize the historical information and leads to over-exploration. The best${\\\\cdot k}$ strategy utilizes the historical points that have high objective values to fill in the unselected variables, thus behaving much better. The performance of the average strategy is between the best$k$ and random strategies. We recommend using the best$k$ strategy in practice.  \n\nThe hyper-parameter $k$ used in the best$k$ strategy controls the degree of exploitation for the unselected variables. As shown in Figure 8(b), a smaller $k$ encourages exploitation, which results in better performance in the early stage, but easily leads to premature convergence. A larger $k$ encourages exploration and behaves worse in the early stage, but may converge to a better value. We recommend using a larger $k$ if allowing enough evaluations.  \n\n  \nFigure 7: Sensitivity analysis of the optimization algorithm.  \n\n  \nFigure 8: Sensitivity analysis of the \u201cfill-in\u201d strategy and the hyper-parameter $k$ of the best$k$ strategy, using MCTS-VS-BO on Hartmann 6 _300 .  \n\nThe hyper-parameter $C_{p}$ for calculating UCB in Eq. (1) balances the exploration and exploitation of MCTS. As shown in Figure 9, a too small $C_{p}$ leads to relatively worse performance, highlighting the importance of exploration. A too large $C_{p}$ may also lead to over-exploration. But overall MCTSVS is not very sensitive to $C_{p}$ . We recommend setting $C_{p}$ between $1\\\\%$ and $10\\\\%$ of the optimum (i.e., max $f({\\\\boldsymbol{x}}))$ ), which is consistent with that for LA-MCTS [40].  \n\n  \nFigure 9: Sensitivity analysis of the hyper-parameter $C_{p}$ for calculating UCB in Eq. (1), using MCTS-VS-BO on Levy and Hartmann.  \n\nThe number $2\\\\,\\\\times\\\\,N_{v}\\\\,\\\\times\\\\,N_{s}$ of sampled data in ch iteration depends on the batch size $N_{v}$ of variable index subset and the sample batch size $N_{s}$ , and will influence the accuracy of estimating the variable score vector in Eq. (2). If we increase $N_{v}$ and $N_{s}$ , we can calculate the variable score more accurately, but also need more evaluations. Figure 10(a) shows that given the same number of evaluations, MCTS-VS-BO achieves the best performance when $N_{v}=2$ and $N_{s}=3$ . Thus, this setting may be a good choice to balance the accuracy of variable score and the number of evaluations, which is also used throughout the experiments.  \n\nThe threshold $N_{b a d}$ for re-initializing a tree controls the tolerance of selecting bad tree nodes (i.e., nodes containing unimportant variables). A smaller $N_{b a d}$ leads to frequent re-initialization, which can adjust quickly but may cause under-exploitation of the tree. A larger $N_{b a d}$ can make full use of the tree, but may optimize too much on unimportant variables. Figure 10(b) shows that MCTS-VS achieves the best performance when $N_{b a d}=5$ . Thus, we recommend to use this setting, to balance the re-initialization and exploitation of the tree.  \n\nThe threshold $N_{s p l i t}$ for splitting a node. If the number of variables in a node is larger than $N_{s p l i t}$ ,the node can be further partitioned. That is, the parameter $N_{s p l i t}$ controls the least number of variables in a leaf node and thus affects the number of selected variables, which has a direct influence on the wall clock time. Note that MCTS-VS selects a leaf node and optimizes the variables contained by this node in each iteration. The smaller $N_{s p l i t}$ , the shorter the time. Figure 10(c) shows that $N_{s p l i t}$ has little influence on the performance of MCTS-VS-BO, and thus we recommend to set $N_{s p l i t}=3$ to reduce the wall clock time.  \n\n  \nFigure 10: S vity analysis of the number $2\\\\,\\\\times\\\\,N_{v}\\\\,\\\\times\\\\,N_{s}$ pled data in each iteration, the threshold $N_{b a d}$ for re-initializing a tree and the threshold $N_{s p l i t}$ for splitting a node, using MCTS-VS-BO on Hartmann 6 _300 .  \n\nInfluence of the hyper-parameters on the runtime of MCTS-VS. We also provide some intuitive explanation about the influence of the hyper-parameters on the runtime. The threshold $N_{s p l i t}$ for splitting a node has a direct impact on the runtime, because it controls the least number of variables to be optimized in a leaf node. That is, the runtime will increase with $N_{s p l i t}$ . Other parameters may affect the depth of the tree and thus the runtime. For the threshold $N_{b a d}$ for re-initializing a tree, if it is set to a small value, MCTS-VS will re-build the tree frequently and the depth of the tree is small. The shallow nodes have more variables, leading to more runtime to optimize. For the hyper-parameter $C_{p}$ for calculating UCB, if it is set to a large value, the exploration is preferred and MCTS-VS will tend to select the right node (regarded as containing unimportant variables). The tree thus will be re-built freq tly, ding to more runtime. For the number $2\\\\,\\\\times\\\\,N_{v}\\\\,\\\\times\\\\,N_{s}$ of sampled data at each iteration, if $N_{v}$ and $N_{s}$ are set to large values, the depth of the tree will be small given the total number of evaluations, and thus lead to more runtime.\u300d\n", "29d7d9b0-9dd0-42e2-a847-f9e6d0c0b37f:\u300cref_ids: 454984236281633338, chunk_ids: 4, Score: 0.1953, Text: # 4.2 RESULTS AND DISCUSSION\n\n# Question 1: How does TS-LLM perform in different generation tasks regarding Path@1? Does Path $@1$ provide a reasonable evaluation metric? (Sec. 3.2.2 and Sec. 3.4)\nIn Table 2, we first provide the comparison between TS-LLM variants with the CoT baseline regarding the Path $@1$ performance. Path $@1$ means each algorithm only generates one answer. TS-LLM variants generally outperform the baseline, underscoring the advantages of tree-search algorithms due to their capability to efficiently prune the search tree. However, notable exceptions are also seen in the RLHF task where MCTS, BFS, and DFS underperform. From our results, we want to highlight two things. Firstly, MCTS, BFS, and DFS generally perform better in shallow search problems (8 for GSM8K and 4 for Game24) while MCTS$\\\\alpha$ and MCTS-Rollout are dominant in deep search problems (15 for PrOntoQA and 64 for RLHF). Secondly, the the backward/backpropagation operation is quite important, since the first three methods\u2014MCTS, BFS, and DFS\u2014do not incorporate this operation and depend greedily on the value function. Note that the MCTS Path $@1$ also does not include the backward operation because it only happens after finding one complete path.  \n\nTable 5: Different value training for iterative update on GSM8k  \n\n  \nFigure 2: Different Value training on GSM8k   \nFigure 3: mean/max reward for RLHF alignment and the best results of 3 aggregations for the rest three tasks w.r.t. number of sequences on the 1st row and number of computations on the 2nd row.  \n\nDespite the superiority of TS-LLM, we argue that Path $@1$ is in fact not a reasonable evaluation metric. In table 2, we also include the number of computations used in Path $@1$ generation (number of tokens in sentence-level and number of forward computation in token-level). All TS-LLM variants consume much more computation than the CoT baseline. To enable a fair comparison, we provide additional baselines, CoT-SC and COT-SC-Tree with two aggregation methods: majorityvote (MAJ) and ORM-vote (denoted as ORM, and it utilizes the learned ORM in TS-LLM). We show results within a similar scale of computation consumption with TS-LLM variants. Under this situation, TS-LLM\u2019s advantages largely decrease when compared with $\\\\mathrm{CoT-SC_{\\\\mathrm{ORM}}}$ , especially on GSM8K (only BFS is better). We are surprised to see that such simple algorithms can also have outstanding performance when compared fairly. Despite this, most tree-search algorithms are still dominant in the rest three tasks given the same (CoT-SC-Tree) or larger search space (CoT-SC).\n\n# Question 2: How do different node constructions influence the performance? (Sec. 3.1)\nAs discussed in Sec. 3.1), the search space for sentence-level action nodes is limited. Thus, we investigate the possible influence introduced by different tree constructions on Game24 with different node sizes. Specifically, we set the number of maximal expanded node size as 6, 20, and 50. Table 3 lists the performance and the number of tokens generated comparing TS-LLM\u2019s variants, CoT-SC and CoT-SC-Tree. The almost doubled performance boost from 44.2 to 79.8 indicates the importance of appropriate expansion node size, improving TS-LLM\u2019s performance upper bound.\n\n# Question 3: Why do we need a learned value function and how to train that? (Sec. 3.2.1)\nIn Table 4, we provide a motivating example by prompting LLaMA2-7b-base as the value function and reward model for TS-LLM on Game24. The performance drop of BFS and MCTS$\\\\alpha$ indicates the incapability of small language models as reliable evaluators, which motivates us to substitute it with a learned value function and ORM as a general solution for any task and LLM with any size.  \n\nTable 6: Iterative update results. $\\\\theta_{0}$ is the old parameter while $\\\\theta_{1}$ is the new one after one iteration. We compare all combinations of policy and value on GSM8k (left) and RLHF alignment (right).   \n\n\n<html><body><table><tr><td>Method</td><td>Policy</td><td>Value</td><td>Performance(%) Method</td><td></td><td>Policy</td><td>Value</td><td>Performance(%)</td></tr><tr><td>Greedy</td><td>T00</td><td>1</td><td>41.4 \u00b1 0.0</td><td>Greedy</td><td>T00</td><td></td><td>0.39 \u00b1 0.0</td></tr><tr><td>Greedy</td><td>T01</td><td>1</td><td>47.9 \u00b1 0.0</td><td>Greedy</td><td>T01</td><td></td><td>1.87 \u00b1 0.0</td></tr><tr><td>Greedy</td><td>RFT k=50</td><td></td><td>47.0 \u00b1 0.0</td><td>Greedy</td><td>RFTN=5</td><td></td><td>1.16 \u00b1 0.0</td></tr><tr><td>Greedy</td><td>RFTk=100</td><td>-</td><td>47.5 \u00b1 0.0</td><td>Greedy</td><td>PPO</td><td></td><td>2.53\u00b10.0</td></tr><tr><td>MCTS-\u03b1</td><td>T00</td><td>{,r}00</td><td>51.9 \u00b1 0.6</td><td>MCTS-0</td><td>T00</td><td>{,r}00</td><td>2.221\u00b10.0</td></tr><tr><td>MCTS-\u03b1</td><td>T00</td><td>U,r}01</td><td>53.2\u00b10.3</td><td>MCTS-0</td><td>T00</td><td>U,r}01</td><td>2.482\u00b10.0</td></tr><tr><td>MCTS-\u03b1</td><td>T01</td><td>v,r}00</td><td>54.1 \u00b1 0.9</td><td>MCTS-0</td><td>T01</td><td>v,r}00</td><td>2.532\u00b10.0</td></tr><tr><td>MCTS-\u03b1</td><td>T01</td><td>\u03b8</td><td>56.5\u00b1 0.6</td><td>MCTS-\u03b1</td><td>T01</td><td>0</td><td>2.670 \u00b1 0.0</td></tr></table></body></html>  \n\nTherefore, we investigate data collection and training paradigms for value function and ORM in TSLLM. In Figure 2, we investigate the influence of data amount and diversity by training with mixed data uniformly sampled from checkpoints of all SFT epochs ( mixed ); data purely sampled from the last checkpoint ( pure ); 1/3 data of the pure setting $(p u r e,l e s s)$ . The results of CoT-SC ORM-vote $@10$ underscore the diversity of sampled data in learning a better ORM. The Path $@1$ results of 3 TSLLM variants show that the amount of sampled data is of great importance. We leave a detailed discussion of how value and reward function training is influenced in iterative training (Sec. 3.3) when answering Question 5. Our final conclusion is that collecting a set of diverse data as much as possible is always better for TS-LLM\u2019s value function and ORM training .\n\n# Question 4: How TS-LLM is improved by aggregating over multiple results? (Sec. 3.2.3)\nIn Fig. 3, we demonstrate the mean/max reward for the RLHF task and the best of 3 aggregation results for the rest three tasks. We measure the performance of aggregation w.r.t path number and token consumption. From the figure, we mainly summarize two conclusions: Firstly, Most TS-LLM variants benefit from aggregation and can show large strengths compared with other baselines. CoT-SC only beats TS-LLM in GSM8k with the same token size, mainly because of its larger search space. TS-LLM variants are still dominant when compared with CoT-SC-Tree. Secondly, treesearch algorithms\u2019 aggregation benefits less than CoT-SC in small-scale problems. In GSM8K and Game24, TS-LLM struggles to improve under large aggregation numbers. We believe this is because of: (1) The search space gap between CoT-SC and tree-search algorithms. Tree-search algorithms inherently explore fewer sentences, which is validated by comparing token consumption between CoT-SC-Tree $@50$ and $\\\\mathrm{CoT-SC}@50$ . (2) Different tree searches are not independent. The latter search might be influenced by the previous one, which decreases generation diversity.\u300d\n", "29d7d9b0-9dd0-42e2-a847-f9e6d0c0b37f:\u300cref_ids: 454984236379937352, chunk_ids: 11, Score: 0.1621, Text: # 3 MCTS-VS Method\nIn this section, we propose a Variable Selection method based on MCTS for high-dimensional BO, briefly called MCTS-VS. The main idea is to apply MCTS to iteratively partition all variables into important and unimportant ones, and perform BO only for those important variables. Let $[D]=\\\\bar{\\\\{}1,2,\\\\dots,D\\\\}$ denote the indexes of all variables $\\\\textbf{\\\\em x}$ , and ${\\\\pmb x}_{\\\\mathbb{M}}$ denote the subset of variables indexed by $\\\\mathbb{M}\\\\subseteq[D]$ \u2286].  \n\nWe first introduce a $D$ -dimensional vector named variable score , which is a key component of MCTS-VS. Its $i$ -th element represents the importance of the $i$ -th variable $x_{i}$ . During the running process of MCTS-VS, fter optimizing a subset ${\\\\pmb x}_{\\\\mathbb{M}}$ of variables where $\\\\mathbb{M}\\\\subseteq[D]$ notes the indexes of the var into a set D, called bles, a set information set Dof sampled points will be generated, and the pai . The variable score vector is based on D$(\\\\mathbb{M},\\\\mathscr{D})$ , and calculated as Dwill be recorded  \n\n$$\ns=\\\\left(\\\\sum_{(\\\\mathbb{M},\\\\mathcal{D})\\\\in\\\\mathbb{D}}\\\\sum_{({\\\\pmb x}^{i},{\\\\pmb y}^{i})\\\\in\\\\mathcal{D}}y^{i}\\\\cdot g(\\\\mathbb{M})\\\\right)/\\\\left(\\\\sum_{(\\\\mathbb{M},\\\\mathcal{D})\\\\in\\\\mathbb{D}}|\\\\mathcal{D}|\\\\cdot g(\\\\mathbb{M})\\\\right),\n$$  \n\nwhere ion $g:2^{[D]}\\\\rightarrow\\\\{0,1\\\\}^{D}$ ghe Boo ector representation of variable index wise division. Each dime subset $\\\\mathbb{M}\\\\subseteq[D]$ \u2286(i.e., the i -th element of sion of $\\\\scriptstyle\\\\sum_{(\\\\mathbb{M},{\\\\mathcal{D}})\\\\in\\\\mathbb{D}}\\\\sum_{({\\\\pmb x}^{i},{\\\\pmb y}^{i})\\\\in{\\\\mathcal{D}}}y^{i}\\\\cdot g(\\\\mathbb{M})$ $g(\\\\mathbb{M})$ Pis 1 if i $i\\\\in\\\\mathbb{M}$ \u2208, and 0 otherwise), and is the sum of query evaluations /is the elementusing ea each variable. Thus, the variable, and each dimension of i -th element of variable score D\u2208$\\\\sum_{(\\\\mathbb{M},\\\\mathbb{D})\\\\in\\\\mathbb{D}}|\\\\mathcal{D}|\\\\cdot g(\\\\mathbb{M})$ D\u2208\u2208D s, representing the importance of the is the number of queries u i ing -th variable $x_{i}$ , is actually measured by the average goodness of all the sampled points that are generated by optimizing a subset of variables containing $x_{i}$ . The variable score $\\\\pmb{s}$ will be used to define the value of each tree node of MCTS as well as for node expansion.  \n\nIn MCTS-VS, the root of the tree represents all variables. A tree node $X$ represents a subset of variables, whose index set is denoted by $\\\\mathbb{A}_{X}\\\\subseteq[D]$ , and it stores the value $v_{X}$ an e number $n_{X}$ of visits, which are used to calculate the value of UCB as in Eq. (1). The value $v_{X}$ is defined as the average score (i.e., importance) of the variables contained by $X$ , which can be calculated by $s\\\\cdot g(\\\\mathbb{A}_{X})/|\\\\mathbb{A}_{X}|$ , where $g(\\\\mathbb{A}_{X})$ is the Bo ean vector representation of $\\\\mathbb{A}_{X}$ and $|\\\\mathbb{A}_{X}|$ is the size of $\\\\mathbb{A}_{X}$ , i.e., the number of variables in node X.  \n\nAt each iteration, MCTS-VS first recursively selects a node with larger UCB until a leaf node (denoted as $X$ ), which is regarded as containing important variables. Note that if we optimize the subset $\\\\mathbf{\\\\Delta}x_{\\\\mathbb{A}_{X}}$ of variables represented by the leaf $X$ directly, the variables in $x_{\\\\mathbb{A}_{X}}$ will have the same score (because they are optimized together), and their relative importance cannot be further distinguished. Thus, MCTS-VS uniformly selects a variable index subset $\\\\mathbb{M}$ from $\\\\mathbb{A}_{X}$ at random, and employs BO to optimize $x_{\\\\mathbb{M}}$ as well as $\\\\pmb{x}_{\\\\mathbb{A}_{X}\\\\setminus\\\\mathbb{M}}$ ; this process is repeated for several times. After that, the information set $\\\\mathbb{D}$ will be augmented by the pairs of the selected variable index subset $\\\\mathbb{M}$ (or $\\\\mathbb{A}_{X}\\\\setminus\\\\mathbb{M})$ and the corresponding sampled points generated by BO. The variable score vector swill be updated using this new $\\\\mathbb{D}$ . Based on $\\\\pmb{s}$ , the variable index set $\\\\mathbb{A}_{X}$ represented by the leaf $X$ will be divided into two disjoint subsets, containing variables with larger and smaller scores (i.e., important and unimportant variables), respectively, and the leaf $X$ will be bifurcated into two child nodes accordingly. Finally, the $v$ values of these two children will be calculated using the variable score vector $\\\\pmb{s}$ , and backpropagation will be performed to update the $v$ value and the number of visits of the nodes along the current path of the tree.  \n\nMCTS-VS can be equipped with any specific BO optimizer, resulting in the concrete algorithm MCTS-VS-BO, where BO is used to optimize the selected subsets of variables during the running of MCTS-VS. Compared with LA-MCTS [ 40 ], MCTS-VS applies MCTS to partition the variables instead of the search space, and thus can be more scalable. Compared with the previous variable selection method Dropout [ 21 ], MCTS-VS can select important variables automatically instead of randomly selecting a fixed number of variables in each iteration. Next we introduce it in detail.\n\n# 3.1 Details of MCTS-VS\nThe procedure of MCTS-VS is described in Algorithm 1. In line 1, it first initializes the information set $\\\\mathbb{D}$ . In particular, a variable index subset $\\\\mathbb{M}_{i}$ is randomly sampled from $[D]$ , and the Latin hypercube sam ] is u nerate t d as $\\\\mathcal{D}_{i}$ and $\\\\mathcal{D}_{\\\\overline{{i}}}$ ) of $N_{s}$ points to f the two pairs of times, resulting i $(\\\\mathbb{M}_{i},\\\\mathcal{D}_{i})$ he initial Dand $\\\\mathbb{D}=\\\\{(\\\\mathbb{M}_{i},\\\\mathcal{D}_{i}),(\\\\bar{\\\\mathbb{M}}_{i},\\\\mathcal{D}_{\\\\bar{i}})\\\\}_{i=1}^{N_{v}}$ $(\\\\bar{\\\\mathbb{M}}_{i},\\\\bar{D_{i}})$ D, where $\\\\bar{\\\\mathbb{M}}_{i}=[\\\\dot{D}]\\\\setminus\\\\mathbb{M}_{i}$ \\\\. The variable score vector . This process will be repeated for $\\\\pmb{s}$ is calculated $N_{v}$ using this initial Din line 3, and the Monte Carlo tree is initialized in line 4 by adding only a root node, whose $v$ value is calculated according to $\\\\pmb{s}$ and number of visits is 0. MCTS-VS uses the variable $t$ to record he numbe ns it has performed, and thus $t$ is set to $2\\\\,\\\\times\\\\,N_{v}\\\\,\\\\times\\\\,N_{s}$ in line 5 as the initial Dcontains $2\\\\,\\\\times\\\\,N_{v}\\\\,\\\\times\\\\,N_{s}$ \u00d7\u00d7sampled points in total.  \n\nIn each iteration (i.e., lines 7\u201328) of MCTS-VS, it selects a leaf node $X$ by UCB in line 10, and optimizes the variables (i.e., $x_{\\\\mathbb{A}_{X}}$ ) represented by $X$ in lines 13\u201323. Note that to measure the relative importance of variables in $\\\\mathbf{\\\\Delta}x_{\\\\mathbb{A}_{X}}$ , MCTS-VS optimizes different subsets of variables of $\\\\mathbf{\\\\Delta}x_{\\\\mathbb{A}_{X}}$\u300d\n"]}}, "__type__": "1"}}, "template_store/metadata": {"7594f2e7-7e04-49dd-b95c-c17798ff25cd": {"template_hash": ""}}}