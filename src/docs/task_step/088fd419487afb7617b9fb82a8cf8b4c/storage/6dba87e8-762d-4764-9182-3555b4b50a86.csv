角色,内容,分镜
6dba87e8-762d-4764-9182-3555b4b50a86,探讨计算模型在不同数据集与应用场景下的适用性与泛化能力,3
6dba87e8-762d-4764-9182-3555b4b50a86,"### 问题提出

在探讨MCTS+PRM框架在不同数据集与应用场景下的适用性与泛化能力时，如何有效评估该框架在跨领域应用中的性能表现，特别是在处理多模态数据时，如何确保模型在不同数据源之间的泛化能力？此外，在动态环境下，如何调整MCTS+PRM框架以适应不断变化的应用场景，并保持其决策的稳定性和高效性？",3
6dba87e8-762d-4764-9182-3555b4b50a86,"ref_ids: 454847538467043982, chunk_ids: 7, Score: 0.2256, Text: # 5. Conclusion
In this work, we proposed a meta learning-based framework (MCRES) to improve the generalization performance of RES models, especially when handling novel compositions of learned concepts. By constructing a virtual training set and multiple virtual testing sets w.r.t. various levels of novel compositions and then optimizing the model via meta optimization, our framework can effectively improve model generalization performance. Extensive experiments show that our framework achieves superior performance on widely used benchmarks. Moreover, our framework is flexible, and can be seamlessly applied on various models with different architectures to enhance their performance.  

Acknowledgement. This work is supported by MOE AcRF Tier 2 (Proposal ID: T2EP20222-0035), National Research Foundation Singapore under its AI Singapore Programme (AISG-100E-2020-065), and SUTD SKI Project (SKI 2021 02 06).",3
6dba87e8-762d-4764-9182-3555b4b50a86,"ref_ids: 454984236281633338, chunk_ids: 4, Score: 0.2178, Text: # 5 Experiment
To examine the performance of MCTS-VS, we conduct experiments on different tasks, including synthetic functions, NAS-bench problems and MuJoCo locomotion tasks, to compare MCTS-VS with other black-box optimization methods. For MCTS-VS, we use the same hyper-parameters except $C_{p}$ , which is used for calculating UCB in Eq. (1). For Dropout and embedding-based methods, we set the parameter $d$ to the number of valid dimensions for synthetic functions, and a reasonable value for real-world problems. The hyper-parameters of the same components of different methods are set to the same. We use five identical random seeds (2021–2025) for all problems and methods. More details about the settings can be found in Appendix C. Our code is available at https://github.com/lamda-bbo/MCTS-VS .",3
6dba87e8-762d-4764-9182-3555b4b50a86,"ref_ids: 454984236293691964, chunk_ids: 5, Score: 0.1768, Text: # DSensitivity Analysis of Hyper-parameters of MCTS-VS
We provide further studies to examine the influence of the hyper-parameters of MCTS-VS, including the employed optimization algorithm for optimizing the selected variables in each iteration, the “fillin” strategy, the hyper-parameter $k$ used in the best$k$ strategy, the hyper-parameter $C_{p}$ for calculating UCB in Eq. (1), the number $2\\,\\times\\,N_{v}\\,\\times\\,N_{s}$ sampled data in each iteration, the threshold $N_{b a d}$ for re-initializing a tree, and the threshold $N_{s p l i t}$ for splitting a tree node.  

The optimization algorithm is employed by MCTS-VS to optimize the selected variables in each iteration. We compare three different optimization algorithms, i.e., random search (RS), BO and TuRBO. First, we conduct experiments similar to “Effectiveness of Variable Selection” in Section 5.1, to show the effectiveness of MCTS-VS even when equipped with RS. Figure 6 shows that MCTSVS-RS is better than Dropout-RS and RS, revealing the advantage of MCTS-VS.  

  
Figure 6: Effectiveness of MCTS-VS when equipped with RS.  

Next we compare the performance of MCTS-VS equipped with RS, BO and TuRBO, by experiments on the Hartmann functions with increasing ratio of valid variables. Hartmann 6 _500 has 6 valid variables. Hartmann 6 _5 _500 is generated by mixing 5 Hartmann 6 functions as Hartmann 6 $(\\pmb{x}_{1:6})+$ Hartmann 6 $\\backslash(\\pmb{x}_{7:12})+\\cdot\\cdot\\cdot+\\mathrm{Hartmann6}(\\pmb{x}_{25:30})$ , and appending 470 unrelated dimensions, where $\\pmb{x}_{i:j}$ denotes the $i$ -th to j-th variables. Hartmann 6 _10 _500 is generated alike. Thus, Hartmann 6 _5 _500 and Hartmann 6 _10 _500 have 30 and 60 valid variables, respectively. The results in Figure 7 show that as the ratio of valid variables increases, MCTS-VS-TuRBO gradually surpasses MCTS-VS-RS and MCTS-VS-BO, while MCTS-VS-RS becomes worse and worse. This is expected. If the ratio of valid variables is high, MCTS-VS is more likely to select the valid variables, so it is worth to use the expensive optimization algorithm, e.g., TuRBO, to optimize the selected variables. If the ratio is low, unrelated variables are more likely to be selected most of the time, so using a cheap optimization algorithm would be better. These observations also give us some guidance on selecting optimization algorithms in practice.  

“Fill-in” strategy is a basic component of variable selection methods, which influences the quality of the value of unselected variables. We compare the employed best$k$ strategy $(k=20)$ ) with the average best$k$ strategy and the random strategy. The average best$k$ strategy uses the average of the best $k$ data points for the unselected variables, and the random strategy samples the value of an unselected variable from its domain randomly. As shown in Figure 8(a), the random strategy leads to the poor performance of MCTS-VS-BO, which may be because it does not utilize the historical information and leads to over-exploration. The best${\\cdot k}$ strategy utilizes the historical points that have high objective values to fill in the unselected variables, thus behaving much better. The performance of the average strategy is between the best$k$ and random strategies. We recommend using the best$k$ strategy in practice.  

The hyper-parameter $k$ used in the best$k$ strategy controls the degree of exploitation for the unselected variables. As shown in Figure 8(b), a smaller $k$ encourages exploitation, which results in better performance in the early stage, but easily leads to premature convergence. A larger $k$ encourages exploration and behaves worse in the early stage, but may converge to a better value. We recommend using a larger $k$ if allowing enough evaluations.  

  
Figure 7: Sensitivity analysis of the optimization algorithm.  

  
Figure 8: Sensitivity analysis of the “fill-in” strategy and the hyper-parameter $k$ of the best$k$ strategy, using MCTS-VS-BO on Hartmann 6 _300 .  

The hyper-parameter $C_{p}$ for calculating UCB in Eq. (1) balances the exploration and exploitation of MCTS. As shown in Figure 9, a too small $C_{p}$ leads to relatively worse performance, highlighting the importance of exploration. A too large $C_{p}$ may also lead to over-exploration. But overall MCTSVS is not very sensitive to $C_{p}$ . We recommend setting $C_{p}$ between $1\\%$ and $10\\%$ of the optimum (i.e., max $f({\\boldsymbol{x}}))$ ), which is consistent with that for LA-MCTS [40].  

  
Figure 9: Sensitivity analysis of the hyper-parameter $C_{p}$ for calculating UCB in Eq. (1), using MCTS-VS-BO on Levy and Hartmann.  

The number $2\\,\\times\\,N_{v}\\,\\times\\,N_{s}$ of sampled data in ch iteration depends on the batch size $N_{v}$ of variable index subset and the sample batch size $N_{s}$ , and will influence the accuracy of estimating the variable score vector in Eq. (2). If we increase $N_{v}$ and $N_{s}$ , we can calculate the variable score more accurately, but also need more evaluations. Figure 10(a) shows that given the same number of evaluations, MCTS-VS-BO achieves the best performance when $N_{v}=2$ and $N_{s}=3$ . Thus, this setting may be a good choice to balance the accuracy of variable score and the number of evaluations, which is also used throughout the experiments.  

The threshold $N_{b a d}$ for re-initializing a tree controls the tolerance of selecting bad tree nodes (i.e., nodes containing unimportant variables). A smaller $N_{b a d}$ leads to frequent re-initialization, which can adjust quickly but may cause under-exploitation of the tree. A larger $N_{b a d}$ can make full use of the tree, but may optimize too much on unimportant variables. Figure 10(b) shows that MCTS-VS achieves the best performance when $N_{b a d}=5$ . Thus, we recommend to use this setting, to balance the re-initialization and exploitation of the tree.  

The threshold $N_{s p l i t}$ for splitting a node. If the number of variables in a node is larger than $N_{s p l i t}$ ,the node can be further partitioned. That is, the parameter $N_{s p l i t}$ controls the least number of variables in a leaf node and thus affects the number of selected variables, which has a direct influence on the wall clock time. Note that MCTS-VS selects a leaf node and optimizes the variables contained by this node in each iteration. The smaller $N_{s p l i t}$ , the shorter the time. Figure 10(c) shows that $N_{s p l i t}$ has little influence on the performance of MCTS-VS-BO, and thus we recommend to set $N_{s p l i t}=3$ to reduce the wall clock time.  

  
Figure 10: S vity analysis of the number $2\\,\\times\\,N_{v}\\,\\times\\,N_{s}$ pled data in each iteration, the threshold $N_{b a d}$ for re-initializing a tree and the threshold $N_{s p l i t}$ for splitting a node, using MCTS-VS-BO on Hartmann 6 _300 .  

Influence of the hyper-parameters on the runtime of MCTS-VS. We also provide some intuitive explanation about the influence of the hyper-parameters on the runtime. The threshold $N_{s p l i t}$ for splitting a node has a direct impact on the runtime, because it controls the least number of variables to be optimized in a leaf node. That is, the runtime will increase with $N_{s p l i t}$ . Other parameters may affect the depth of the tree and thus the runtime. For the threshold $N_{b a d}$ for re-initializing a tree, if it is set to a small value, MCTS-VS will re-build the tree frequently and the depth of the tree is small. The shallow nodes have more variables, leading to more runtime to optimize. For the hyper-parameter $C_{p}$ for calculating UCB, if it is set to a large value, the exploration is preferred and MCTS-VS will tend to select the right node (regarded as containing unimportant variables). The tree thus will be re-built freq tly, ding to more runtime. For the number $2\\,\\times\\,N_{v}\\,\\times\\,N_{s}$ of sampled data at each iteration, if $N_{v}$ and $N_{s}$ are set to large values, the depth of the tree will be small given the total number of evaluations, and thus lead to more runtime.",3
