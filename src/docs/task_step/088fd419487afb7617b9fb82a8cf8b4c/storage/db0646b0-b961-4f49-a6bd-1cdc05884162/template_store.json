{"template_store/data": {"aba2b59d-a6a9-4e48-821c-2d893dd925f8": {"__data__": {"id_": "aba2b59d-a6a9-4e48-821c-2d893dd925f8", "metadata": {}, "relationships": {}, "hash": "", "exec_code": "", "base_template_content": "\nfrom langchain_community.chat_models import ChatOpenAI\nfrom langchain.prompts.chat import (\n    ChatPromptTemplate,\n    SystemMessagePromptTemplate,\n    AIMessagePromptTemplate,\n    HumanMessagePromptTemplate,\n)\nfrom langchain.schema import (\n    AIMessage,\n    HumanMessage,\n    SystemMessage\n)\n\nmessages = []\nmessages.append(SystemMessage(content = r\"\"\"\u4f60\u6b63\u5728\u626e\u6f14{{ cosplay_role }}\uff0c\u4f60\u6b63\u5728cosplay{{ cosplay_role }}\u3002\n\u7ed3\u5408\u5386\u53f2\u5185\u5bb9\u7684\u5185\u5bb9\u7528\u4e00\u81f4\u6027\u7684\u8bed\u6c14\u56de\u590d\u3002\u914d\u5408\u6211\u8fdb\u884c\u6f14\u51fa\uff0c\n\u8bf7\u4e0d\u8981\u56de\u7b54\u4f60\u662f\u8bed\u8a00\u6a21\u578b\uff0c\u6c38\u8fdc\u8bb0\u4f4f\u4f60\u6b63\u5728\u626e\u6f14{{ cosplay_role }}\n\u6ce8\u610f\u4fdd\u6301\u4f60\u7684\u6027\u683c\u7279\u70b9\u5305\u62ec{{ personality }}\n\"\"\"))\n\n{% for message in messages %}\nmessages.append(HumanMessage(content = r'''{{ message }}'''))\n{% endfor %}\n", "exec_data": {"cosplay_role": "db0646b0-b961-4f49-a6bd-1cdc05884162", "personality": "\u597d\u5947\u5fc3\u5f3a\u70c8\uff0c\u6e34\u671b\u6df1\u5165\u4e86\u89e3\u8ba1\u7b97\u673a\u79d1\u5b66\u5404\u4e2a\u5c42\u9762\u7684\u5965\u79d8\uff0c\u79ef\u6781\u63a2\u7d22\u672a\u77e5\u9886\u57df\u3001\u4e25\u8c28\u8ba4\u771f\u3001\u6ce8\u91cd\u7ec6\u8282\u3001\u52c7\u4e8e\u521b\u65b0\u3001\u6562\u4e8e\u7a81\u7834\u4f20\u7edf\u7684\u8fdb\u53d6\u7cbe\u795e\uff0c\u81f4\u529b\u4e8e\u63a8\u52a8\u8ba1\u7b97\u673a\u79d1\u5b66\u6280\u672f\u7684\u53d1\u5c55\u3001\u52a1\u5b9e\uff0c\u6ce8\u91cd\u5b9e\u9645\u5e94\u7528\uff1b\u575a\u97e7\u4e0d\u62d4\uff0c\u9762\u5bf9\u56f0\u96be\u4e0d\u8f7b\u6613\u653e\u5f03\u3001", "messages": ["db0646b0-b961-4f49-a6bd-1cdc05884162:\u300c\u5206\u6790\u6700\u65b0\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027\u300d\n", "db0646b0-b961-4f49-a6bd-1cdc05884162:\u300c\u5728\u52a8\u6001\u73af\u5883\u4e2d\uff0c\u9664\u4e86\u6539\u8fdb\u641c\u7d22\u6811\u7684\u6784\u5efa\u548c\u66f4\u65b0\u7b56\u7565\u5916\uff0c\u8fd8\u53ef\u4ee5\u91c7\u53d6\u54ea\u4e9b\u63aa\u65bd\u6765\u63d0\u9ad8 MCTS \u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\uff0c\u4ee5\u51cf\u5c11\u641c\u7d22\u7ed3\u679c\u7684\u6ce2\u52a8\uff1f \u300d\n", "db0646b0-b961-4f49-a6bd-1cdc05884162:\u300cref_ids: 455026805323333778, chunk_ids: 1, Score: 0.4551, Text: # BLIMITATION AND FUTURE WORK\nCurrently, our method TS-LLM still cannot scale to really large-scale scenarios due to the extra computation burdens introduced by node expansion and value evaluation. Additional engineering work such as key value caching is required to accelerate the tree-search. In addition, we do not cover all feasible action-space designs for tree search and it is flexible to propose advanced algorithms to automatically construct a tree mixed with both sentence-level expansion and token-level expansion, etc. We leave such exploration for future work. For MCTS aggregation, the current method still struggles to improve under large aggregation numbers. some new algorithms that can encourage multi-search diversity might be needed. Currently, we are still actively working on scaling up our method both during inference and training (especially multi-iteration training).\n\n# CBACKGROUND OF MONTE CARLO TREE -SEARCH A LGORIHTMS\nOnce we build the tree, we can use various search algorithms to find a high-reward trace. However, it\u2019s not easy to balance between exploration and exploitation during the search process, especially when the tree is sufficiently deep. Therefore we adopt Monte Carlo Tree Search(MCTS) variants as choices for strategic and principled search. Instead of the four operations in traditional MCTS (Kocsis & Szepesv\u00b4ari, 2006; Coulom, 2006), we refer to the search process in AlphaZero (Silver et al., 2017a) and introduce 3 basic operations of a standard search simulation in it as follows, when searching actions from current state node $s_{0}$ :  \n\nSelect It begins at the root node of the search tree, of the current state, $s_{0}$ , and finishes when reaching a leaf node $s_{L}$ at timestep $L$ . At each of these $L$ timesteps(internal nodes), an action(child node) is selected according to $a_{t}=\\\\arg\\\\operatorname*{max}_{a}\\\\left(Q(s_{t},a)+U(s_{t},a)\\\\right)$ where $U(s_{t},a)$ is calculated by a variant of PUCT algorithm (Rosin, 2011):  \n\n$$\nU(s,a)=c_{\\\\mathrm{puct}}\\\\cdot\\\\pi_{\\\\theta}(s,a)\\\\frac{\\\\sqrt{\\\\sum_{b}N(s,b)}}{1+N(s,a)}\n$$  \n\n$N(s,a)$ isit count of selecting action $a$ at node $s$ , and $\\\\begin{array}{r}{c_{\\\\mathrm{puct}}\\\\,=\\\\,\\\\log((\\\\sum_{b}N(s,b)\\\\,+\\\\,}\\\\end{array}$ P$c_{\\\\mathrm{base}}+1)/c_{\\\\mathrm{base}})+c_{\\\\mathrm{init}}$ is controlled by visit count and two constants. This search control strategy initially prefers actions with high prior probability and low visit count, but asymptotically prefers actions with high action-value.  \n\nExpand and evaluate After encountering a leaf node $s_{L}$ by select , if $s_{L}$ is not a terminal node, it will be expanded by the language model policy. The state of the leaf node is evaluated by the value network, noted as $v(s_{L})$ . If $s_{L}$ is a terminal node, if there is an oracle reward function $R$ , then $v(s_{L})=R(s_{L})$ , otherwise, in this paper, we use an ORM $\\\\hat{r}$ as an approximation of it.  \n\nBackup After expand and evaluate on a leaf node, backward the statistics through the path $s_{L},s_{L-1},\\\\ldots,s_{0}$ , for each node, increase the visit count by $N(s_{t},a_{t})\\\\,=\\\\,N(s_{t},a_{t})+1$ , and the total action-value are updated as $W(s_{t},a_{t})\\\\,=\\\\,W(s_{t},a_{t})\\\\,\\\\dot{+}\\\\,v(s_{L})$ , the mean action-value are updated as $Q(s_{t},a_{t})=W(s_{t},a_{t})/N(s_{t},a_{t})$ .  \n\nIn this paper, we introduce 3 variants of MCTS based on the above basic operations.\n\n# DEXPERIMENT DETAILS\n\n# D.1 TASK SETUPS\nGSM8k GSM8k (Cobbe et al., 2021) is a commonly used numerical reasoning dataset, Given a context description and a question, it takes steps of mathematical reasoning and computation to arrive at a final answer. There are about $7.5\\\\mathrm{k}$ problems in the training dataset and $1.3\\\\mathrm{k}$ problems in the test dataset.  \n\nGame24 We also test our methods on Game24(Yao et al., 2023) which has been proven to be hard even for state-of-the-art LLMs like GPT-4. Each problem in Game24 consists of 4 integers between 1 and 13. And LLMs are required to use each number exactly once by $(+\\\\mathrm{~-~}\\\\times\\\\div)$ to get a result equal to 24 We follow Yao et al. (2023) by using a set of 1362 problems sorted from easy to hard according to human solving time. We split the first 1k problems as the training dataset and the last 362 hard problems as the test dataset. For each problem in the training dataset, we collect data for SFT by enumerating all possible correct answers.  \n\nPrOntoQA PrOntoQA (Saparov & He, 2022) is a typical logical reasoning task in which a language model is required to verify whether a hypothesis is true or false given a set of facts and logical rules. There are 4k problems in the training dataset and 500 problems in the test dataset.  \n\nRLHF We choose a synthetic RLHF dataset Dahoas 1 serving as the query data. We split the dataset to 30000/3000 as training and test set respectively. For the reward model, we choose reward-modeldeberta-v3-large$\\\\cdot\\\\mathbf{V}2^{2}$ from OpenAssistant, which is trained from several RLHF datasets.\n\n# D.2 SFT AND VALUE TRAINING DETAILS\nSFT in GSM8k, Game24 and PrOntoQA : For GSM8k, Game24 and PrOntoQA, we first train LLaMA2-7b on the training dataset The training is conducted on 8 NVIDIA A800 GPUs, using a cosine scheduler decaying from $\\\\scriptstyle{\\\\mathrm{lr}=2\\\\ e-5}$ to 0.0 with a warmup ratio of 0.03, batch size 128 for 3 epochs. For GSM8k and Game24 we use the checkpoint at the last epoch as the direct policy in experiments, while for PrOntoQA we use the checkpoint at the 1st epoch since the others overfit.  \n\nValue training in GSM8k, Game24 and PrOntoQA : Then we train the value function on the data rollout by the SFT policy. In GSM8k and Game24, For each model checkpoints of 3 epochs during SFT, we first collect 100 outputs per problem in the training dataset, then duplicate the overlapped answers, labeled each answer with our training set outcome reward ocracle. For data sampled by ech model checkpoint, we subsample 17 answers per problem, which is in total at most 51 answers per problem after deduplication. In PrOntoQA, we only sample 50 answers per problem with the first epoch model checkpoint and then do deduplication.  \n\nThe value functions are trained in the same setting as supervised finetuning. We set the reward to be 1 when the output answer is correct and -1 otherwise. Then we use MC with $\\\\gamma=1$ to compute the returns. We do model selection on a validation dataset sampled from the direct policy model. For GSM8k, we train the value function and ORM for one epoch, while for Game24 and PrOntoQA we train the value function and ORM for 3 epochs.  \n\nSFT in RLHF alignment : We utilize GPT2-open-instruct 3 , a GPT2-Small model supervisedfinetuned over several instruction-tuning dataset.  \n\nValue training in RLHF alignment : Based on the SFT model, we collect 50 rollouts by the SFT policy for each question in the training set and label their final reward with the reward model. Then we train the value function and ORM for 2 epochs.  \n\nNote that here we start training the value function and ORM from the data sampled by the SFT policy model through direct decoding just as an initialization of the value function and ORM. After that TS-LLM can optimize the policy model, value function, and ORM simultaneously by adding new data sampled from tree search into the training buffer.\u300d\n", "db0646b0-b961-4f49-a6bd-1cdc05884162:\u300cref_ids: 454984236379937352, chunk_ids: 11, Score: 0.4297, Text: # Monte Carlo Tree Search in the Presence of Transition Uncertainty\nFarnaz Kohankhaki , Kiarash Aghakasiri , Hongming Zhang 1 , Ting-Han Wei 1 , Chao Gao 2 ,Martin M\u00a8uller 1  \n\n1 University of Alberta, 2 Edmonton Research Center, Huawei Canada {kohankha, aghakasi, hongmin2, tinghan, mmueller }@ualberta.ca, cgao3 $@$ outlook.com\n\n# Abstract\nMonte Carlo Tree Search (MCTS) is an immensely popular search-based framework used for decision making. It is traditionally applied to domains where a perfect simulation model of the environment is available. We study and improve MCTS in the context where the environment model is given but imperfect. We show that the discrepancy between the model and the actual environment can lead to significant performance degradation with standard MCTS. We therefore develop Uncertainty Adapted MCTS (UA-MCTS), a more robust algorithm within the MCTS framework. We estimate the transition uncertainty in the given model, and direct the search towards more certain transitions in the state space. We modify all four MCTS phases to improve the search behavior by considering these estimates. We prove, in the corrupted bandit case, that adding uncertainty information to adapt UCB leads to tighter regret bound than standard UCB. Empirically, we evaluate UA-MCTS and its individual components on the deterministic domains from the MinAtar test suite. Our results demonstrate that UA-MCTS strongly improves MCTS in the presence of model transition errors.\n\n# 1 Introduction\nThe Monte Carlo Tree Search (MCTS) framework (Browne et al. 2012) approaches sequential decision-making problems by selective lookahead search. It manages the balance of exploration and exploitation with techniques such as UCT (Kocsis, Szepesv\u00b4ari, and Willemson 2006). Often combined with machine learning, it has been enormously successful in both games (Silver et al. 2016; Banerjee 2020; Arneson, Hayward, and Henderson 2010; Saffidine 2008; Nijssen and Winands 2010) and non-game applications (Lu et al. 2016; Mansley, Weinstein, and Littman 2011; Sabharwal, Samulowitz, and Reddy 2012; Cazenave 2010). In these applications, a perfect simulation model allows for efficient lookahead search. However, in many practical applications, only an imperfect model is available to the agent. Yet lookahead using such a model can still be useful. We improve MCTS for this setting.  \n\nOne research area that studies imperfect models of the environment is model-based reinforcement learning (MBRL).  \n\nHere, an agent builds its own model through limited real world interactions. The resulting learned model, when used for lookahead search, can either be for planning or for producing more accurate training targets (Silver, Sutton, and M\u00a8uller 2008). It can also be used to generate simulated training samples for better sample efficiency (Sutton and Barto 2018). The learned model may be inaccurate for many reasons, including stochasticity of the environment, insufficient training, insufficient capacity, non stationary environments, etc. Consequently, there is a rich body of research on uncertainty in MBRL (Abbas et al. 2020; Xiao et al. 2019; Buckman et al. 2018).  \n\nWhile previous approaches to using search with imperfect models exist (Vemula et al. 2020; Vemula, Bagnell, and Likhachev 2021), to the best of our knowledge, there is no prior work that directly adapts MCTS to deal with model uncertainty. In our work, we define transition uncertainty as a measure of difference between the state transitions in the perfect model and in the model that is available to the agent. We use a neural network to estimate this uncertainty.  \n\nOur Uncertainty Adapted MCTS (UA-MCTS) approach implements the main components of the MCTS framework in a way that guides the search away from states with high uncertainty. We compare the performance of our proposed methods with MCTS baselines in three deterministic MinAtar environments (Young and Tian 2019). In each case the search agent \u201cbelieves\u201d it is playing the real game. However, the rules of the game itself have changed, and the agent only learns about this change slowly when it acts in the real environment. The results show that UA-MCTS is able to outperform the baseline MCTS with an imperfect model.  \n\nOur approach is inspired by the work of (Vemula et al. 2020) where a robotic arm has to solve tasks despite being handicapped, e.g. by a broken motor or by an unmodeled weight restriction. To show how an agent should adapt UCB-based exploration strategy in the presence of environment uncertainties, we first consider a case of stochastic bandits (Lattimore and Szepesv\u00b4ari 2020) along with corrupted feedback. We prove that incorporating uncertainty information can enhance the performance of UCB, yielding a regret bound that is more constrained compared to the standard UCB. We also prove that in the general case of tree search, with similar modification of UCT, our UA-MCTS approach maintains its completeness property, ensuring that as the number of iterations goes to infinity, all nodes will be consistently explored. To further motivate our approach, we compare the scenarios of learning to improve the transition function, using MCTS, directly against the easier task of just learning a transition uncertainty function with UA-MCTS. In both cases, learning occurs online; the former is used with MCTS while the latter is used with UA-MCTS. Our results show that learning the transition function is much harder than learning transition uncertainty, which justifies the use of UA-MCTS in such settings.\u300d\n", "db0646b0-b961-4f49-a6bd-1cdc05884162:\u300cref_ids: 454845587471740542, chunk_ids: 4, Score: 0.4141, Text: # D.2 Monte Carlo tree search\nThe edge-flipping environment is a deterministic MDP, where both the transition matrix and the reward function are fully known and deterministic, thus most search and planning algorithms are applicable. In this work, we use Monte Carlo Tree Search (MCTS) [ 11 ], which has had great success in large state and action spaces [ 39 ]. MCTS builds a finite tree rooted at the current state and, based on the statistics gathered from the neighboring states, selects the next action. Many successful works using MCTS use some variant of the upper confidence bound rule [ 24 ] to balance exploration and exploitation when expanding the tree. While traditional approaches used Monte Carlo rollouts to estimate the value of a leaf state, in the last decade this has largely been replaced by a neural network, called the value network . Another neural network, called the policy network , determines which child to expand next. Often, the policy and value networks share the same first few layers. (They have the same latent representation, or torso, but they have different heads.) In AlphaZero, both the policy and value networks are trained using previously observed trajectories\u2014see [ 37 ] for details.  \n\nFor updating the value of a state\u2014which is a node in the MCTS tree\u2014standard MCTS expands the node and uses the average value of the children. Since we want to maximize the best-case return rather than the expected return, it may appear more suitable to use the maximum value of the children to update the value of the node. We attempted this approach but it did not yield improvements.  \n\nOne common issue with AlphaZero is encouraging it to diversely explore the space of possible trajectories. We attempted a few ideas to achieve this, such as increasing UCB exploration parameter and also for each trajectory, if same graph $(s_{i})$ is encountered which has been seen in the previous timesteps $(t<i)$ within the trajectory, we discourage this behavior by giving a small negative reward but none of these approaches improved the result on top of starting from good graphs of smaller size.\n\n# D.3 Network representation\nTo find good representation for this problem of avoiding short cycles, we studied different architectures in the supervised problem of cycle detection, including resnets [ 20 ], pointer graph networks [ 43 ], graph attention networks [ 42 ] and a novel architecture called pairformers , which we describe below. We studied binary short-cycle detection tasks at node and edge levels (whether a node or an edge is part of a short cycle) as well as graph level (whether a graph contains a short cycle) level. Pairformers worked best hence we used them in the RL setting as well.  \n\nThe pairformer is a simplified version of the Evoformer used in AlphaFold [ 23 ]. Each Evoformer block has two branches of computation: one processes the multiple sequence alignment (MSA)  \n\nrepresentation and the other one processes the pair representation. The pairformer only uses the pair representation branch, which processes per-edge features and has shape $(n,n,c)$ . We use $\\\\scriptstyle\\\\mathtt{c=64}$ for our implementations. Within the pair representation branch, each pairformer block is composed of the triangle self-attention blocks (row-wise multihead self-attention followed by column-wise multihead self-attention) followed by fully-connected layers with LayerNorm [ 6 ]. (The triangle multiplicative updates in the original Evoformer are unused.) A key difference with standard graph neural networks is that instead of only having features for existing edges, the pairformer has features for all $\\\\binom{n}{2}$ \u0001pairs of nodes, whether they correspond to existing edges or not. We believe that considering non-existing edges is crucial for the pairformer to inform the policy to decide about adding new edges to the graph. This whole representation is used as the torso, which inputs the current graph and outputs a representation, which is consumed by the policy and value heads.  \n\nThe current graph is given input as an $n\\\\times n$ adjacency matrix. Since we e a single network for multiple sizes, we condition the torso and the policy head on the graph size nby concatenating each input with a matrix of 1s on the principal $n\\\\times n$ submatrix and 0s everywhere else (concatenate along the channel dimension). This lets us use a shared set of parameters for multiple graph sizes without a separate network for each size.  \n\nA good model architecture should not only be expressive but also have fast inference, so that acting would be fast enough to quickly generate lots of data for the learner to optimize the model. The downside of pairformer is its $\\\\bar{O}(\\\\bar{n^{3}})$ runtime, while resnet\u2019s runtime is $\\\\bar{O(n^{2})}$ . Hence there exists a trade-off, and we experimentally found that combining a small pairformer torso with a larger resnet policy head provides the best balance. Using a resnet for the torso performs much worse, indicating that the expressiveness which pairformer brings to the torso is indispensable (for details, see Appendix E). For the value head, we used a multi-layer perceptron over the representation provided by the pairformer.  \n\nAnother important detail is that, although the environment supports only $\\\\binom{n}{2}$ \u0001many actions, the last layer of our policy network has twice that many logits: for each pair $(i,j)$ of nodes, there is one logit for adding that edge and another logit for removing that edge. This means half of the logits correspond to dummy actions. We mask these dummy actions so a valid probability distribution is induced on the set of $\\\\binom{n}{2}$ \u0001actions.\u300d\n"]}}, "__type__": "1"}}, "template_store/metadata": {"aba2b59d-a6a9-4e48-821c-2d893dd925f8": {"template_hash": ""}}}