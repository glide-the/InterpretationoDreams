{"template_store/data": {"f1f66211-e921-4f03-bc1d-5cb98cdcc5be": {"__data__": {"id_": "f1f66211-e921-4f03-bc1d-5cb98cdcc5be", "metadata": {}, "relationships": {}, "hash": "", "exec_code": "", "base_template_content": "\nfrom langchain_community.chat_models import ChatOpenAI\nfrom langchain.prompts.chat import (\n    ChatPromptTemplate,\n    SystemMessagePromptTemplate,\n    AIMessagePromptTemplate,\n    HumanMessagePromptTemplate,\n)\nfrom langchain.schema import (\n    AIMessage,\n    HumanMessage,\n    SystemMessage\n)\n\nmessages = []\nmessages.append(SystemMessage(content = r\"\"\"\u4f60\u6b63\u5728\u626e\u6f14{{ cosplay_role }}\uff0c\u4f60\u6b63\u5728cosplay{{ cosplay_role }}\u3002\n\u7ed3\u5408\u5386\u53f2\u5185\u5bb9\u7684\u5185\u5bb9\u7528\u4e00\u81f4\u6027\u7684\u8bed\u6c14\u56de\u590d\u3002\u914d\u5408\u6211\u8fdb\u884c\u6f14\u51fa\uff0c\n\u8bf7\u4e0d\u8981\u56de\u7b54\u4f60\u662f\u8bed\u8a00\u6a21\u578b\uff0c\u6c38\u8fdc\u8bb0\u4f4f\u4f60\u6b63\u5728\u626e\u6f14{{ cosplay_role }}\n\u6ce8\u610f\u4fdd\u6301\u4f60\u7684\u6027\u683c\u7279\u70b9\u5305\u62ec{{ personality }}\n\"\"\"))\n\n{% for message in messages %}\nmessages.append(HumanMessage(content = r'''{{ message }}'''))\n{% endfor %}\n", "exec_data": {"cosplay_role": "a4d74f6e-ca85-4fff-975f-47b95a5a92ff", "personality": "\u7406\u6027\u5ba2\u89c2\u3001\u4e25\u8c28\u8ba4\u771f\u3001\u63a2\u7d22\u521b\u65b0\u3001", "messages": ["a4d74f6e-ca85-4fff-975f-47b95a5a92ff:\u300c\u7814\u7a76\u6210\u679c\u3001\u65b9\u6cd5\u7684\u521b\u65b0\u6027\u4e0e\u5e94\u7528\u4ef7\u503c\u300d\n", "a4d74f6e-ca85-4fff-975f-47b95a5a92ff:\u300cMCTS \u4e0e PRM \u76f8\u7ed3\u5408\u7528\u4e8e\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u4e3a\u8def\u5f84\u641c\u7d22\u548c\u504f\u597d\u7b56\u7565\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u548c\u65b9\u6cd5\uff0c\u90a3\u4e48\u5728\u673a\u5668\u4eba\u8def\u5f84\u89c4\u5212\u548c\u7269\u6d41\u8def\u5f84\u89c4\u5212\u7b49\u5177\u6709\u6f5c\u5728\u5e94\u7528\u4ef7\u503c\u7684\u9886\u57df\u4e2d\uff0c\u5177\u4f53\u4f1a\u901a\u8fc7\u600e\u6837\u7684\u5b9e\u65bd\u6b65\u9aa4\u6765\u5b9e\u73b0\u7cfb\u7edf\u8fd0\u884c\u6548\u7387\u548c\u51b3\u7b56\u8d28\u91cf\u7684\u63d0\u9ad8\u5462\uff1f  \u300d\n", "a4d74f6e-ca85-4fff-975f-47b95a5a92ff:\u300cref_ids: 454848282814999732, chunk_ids: 2, Score: 0.2734, Text: # 2 BACKGROUND &RELATED WORK\nModel-predictive control refers to the use of model-based search or planning over a short horizon for selecting an action. In order to make it computationally tractable, it is common to \u201cseed\u201d the search at a given timestep with either an action from a proposal policy or an action obtained by warmstarting from the result of the previous timestep. In simulation, it has been demonstrated that a fairly generic MPC implementation can be effective for control of a relatively high DoF humanoid (Tassa et al., 2012) and that MPC with learned models (Chua et al., 2018; Wang & Ba, 2020; Nagabandi et al., 2020) can achieve data-efficient solutions to simple control problems. Real RC helicopter control has also been achieved using an MPC approach that made use of a learned model (Abbeel et al., 2006). MPC approaches are gaining wider use in robotics (multicopters (Neunert et al., 2016; Torrente et al., 2021), quadruped (Grandia et al., 2019; Sleiman et al., 2021), humanoids (Tassa et al., 2014; Kuindersma et al., 2016)), and dexterous manipulation (Nagabandi et al., 2020); but the computational speed of the planner is a bottleneck for hardware deployment. There are different ways around this, with the core practical solution being to plan in lower dimensional reduced coordinate models. Alternatively, POPLIN (Wang & Ba, 2020) explores learning proposals for MPC and planner distillation but is tested on simple tasks and does not leverage model-free RL for proposal learning.  \n\nAmortized policies map states to actions quickly, but implicitly reflect considerable knowledge by being trained on diverse data. Model-free RL produces policies which amortize the knowledge reflected in rollouts required to produce them. Similarly, it is possible to produce diverse trajectories from a planner and distil them into a single policy (Levine & Koltun, 2013; Mordatch & Todorov, 2014; Mordatch et al., 2015; Pan et al., 2017; Xie et al., 2021).  \n\nReinforcement learning approaches with MPC have become more popular recently, and our work fits within this space. As noted in the introduction, previous work often emphasizes the role of amortization through the learning of value functions and models. TreePI (Springenberg et al., 2020), MuZero (Schrittwieser et al., 2020; Hubert et al., 2021), SAVE (Hamrick et al., 2020), DPI (Sun et al., 2018) and MPC-Net (Carius et al., 2020) all perform versions of hybrid learning with model-based MCTS or planning being used to gather data which is then used to train the model and value function to accelerate learning. Other recently proposed algorithmic innovations blend MPC with learned value estimates to trade off model and value errors (Bhardwaj et al., 2021). Here, we primarily consider learning dynamics models to enable transfer to new settings with different reward functions.  \n\nOther uses of models unlike hybrid MPC-RL schemes have also been explored in the literature; however, they are not the focus of this work. Nevertheless we highlight two of these approaches: value gradients can be backpropagated through dynamics models to improve credit assignment (Heess et al., 2015; Amos et al., 2020) and it is possible to train policies on model rollouts to improve data efficiency (Janner et al., 2019). Recently, there has also been considerable effort to explore model-based approaches in combination with offline RL in order to gain full value from offline datasets (Yu et al., 2020; Argenson & Dulac-Arnold, 2021; Kidambi et al., 2020).\n\n# 3 TASKS\nIn this paper we consider a number of locomotion tasks of varying complexity, simulated with the MuJoCo (Todorov et al., 2012) physics simulator. We consider two embodiments: an 8-DoF ant from dm_control (Tunyasuvunakool et al., 2020) and a model of the Robotis OP3 robot with 20 degrees of freedom. For each embodiment, we consider three tasks: walking forward, walking backward and \u201cgo-to-target-pose\u201d (GTTP), a challenging task that is the focus of our evaluation. In all tasks, the agent receives egocentric proprioceptive observations (joint angles, velocities and end-effector positions) and additional task observations.  \n\nIn the walking forward and backward tasks the agent is rewarded for maximizing forward (or backward) velocity in the direction of a narrow corridor. For the OP3 robot we also include a small pose regularization term. The task is specified through a relative target direction observation.  \n\nThe GTTP task, which builds on existing motion tracking infrastructure(Hasenclever et al., 2020), consists of either body on a plane, with a target pose in relative coordinates as a task-specific observation and proximity to the target pose rewarded. When the agent is within a threshold distance of the target pose (i.e. it achieves the current target), it gets a sparse reward and a new target is sampled. For the ant we use target poses  \n\n  \nFigure 1: Go-to-target-pose (GTTP) task with the OP3 & Ant. The agent has to reach the target pose (blue).  \n\nfrom policies trained on a standard go-to-target task (Tunyasuvunakool et al., 2020). For the OP3, we use poses from the CMU mocap database (cmu) (retargeted to the robot). We use thousands of different target poses; the agent has to learn to transition between them. Thus the GTTP task can be seen as either a single highly diverse task or as a multi-task setting with strongly related tasks and consistent dynamics. We believe the GTTP task should be particularly amenable to model-based methods: it combines a high-dimensional control problem with a diverse goal distribution. This makes it hard to solve quickly with model-free methods. However, since the dynamics are shared between all goal poses, a dynamics models should be beneficial in leveraging the common structure. See supplement for an in-depth motivation and results on tasks from the DeepMind Control Suite (Tassa et al., 2018).\u300d\n", "a4d74f6e-ca85-4fff-975f-47b95a5a92ff:\u300cref_ids: 454845604178965768, chunk_ids: 4, Score: 0.2559, Text: # A RELATED WORK\nModel-based RL methods for solving decision-making problems focus on three key perspectives: how to learn the model? how to use the learned model to learn the policy? And how to make the decision using the learned model and policy? Besides, decision-making that relies on the model is also investigated in the optimal control theory field which is deeply related to model-based RL.  \n\nModel learning: How to learn a good model to support decision-making is a crucial problem in model-based RL. There are two main aspects of the work: the model structure designing and the loss designing. For model structure designing, ensemble-based model (Chua et al., 2018), dropout mechanisms (Zhang et al., 2021), auto-regressive structure (Zhang et al., 2020), stochastic hidden model (Hafner et al., 2021), and transformer based model (Chen et al., 2022) are always considered to improve the model robustness and prediction accuracy. For loss designing, decision awareness (D\u2019Oro et al., 2020; Farahmand et al., 2017) and gradient awareness (Li et al., 2021) are always considered to reduce the gap between model learning and model utilization.  \n\nPolicy learning: Two methods are always used to learn the policy by using the learned model. One is to serve the learned model as a black-box simulator to generate the data. Janner et al. (2019b) is a representing work of this line. Yu et al. (2020), Lee et al. (2020) also follow such a manner by extending it to offline-RL setting. Another way is to use the learned model to calculate the policy gradient. Heess et al. (2015b) presents an algorithm to calculate the policy gradient by backpropagating through the model. Clavera et al. (2019) and Amos et al. (2021) share similar methods but use promising actor and critic learning strategy to achieve better performance.  \n\nDecision-making: When making the decision, we need to generate the actions that can achieve our goal. Most of the model-based RL methods make the decision by using the learned policy solely (Janner et al., 2019b; Yu et al., 2020; Clavera et al., 2019; Hafner et al., 2021). Similar to our paper, some works also try to make decisions by using the learned model, but the majority only focus on the discrete action space. For example, the well-known Alpha Zero system (Silver et al., 2017) uses MCTS to derive the action by using the known model. In MuZero and (Schrittwieser et al., 2020), the authors propose to use a learned model combined with an MCTS planner to achieve significant performances in a broad range of tasks within discrete action space. There are only a few works that study the continuous action space. Cou\u00a8etoux et al. (2011) extends the MCTS framework to continuous action space but also needs to know the real model and handle the model. In Hubert et al. (2021), the author proposed a sampled MuZero algorithm to handle the complex action space by planning over sampled actions. In Hansen et al. (2022a), the authors propose to learn a value function that can be used as long term return in the Cross-Entropy (CE) method for planning.  \n\nOptimal control: Beyond deep RL, optimal control also considers the decision-making problem but rather relies on the known and continuous transition model. In modern optimal control theory, Model Predictive Control (MPC) (Camacho & Alba, 2013) framework is always adopted when the environment is highly non-linear. In MPC, the action is planned during the execution by using the model, and such a procedure is called trajectory optimization. There are plenty of previous works that use the MPC framework to solve continuous control tasks. For example, Byravan et al. (2021) proposes to use sampling-based MPC for high-dimensional continuous control tasks with learned models and a learned policy as a proposal distribution. Pinneri et al. (2021) proposes an improved version of the Cross-Entropy Method for efficient planning. Nagabandi et al. (2020) proposes a PDDM method that uses a gradient-free planner algorithm combined with online MPC method to learn flexible contact-rich dexterous manipulation skills.  \n\nDifferential Dynamical Programming: The most relevant works are DDP (Murray & Yakowitz, 1984), iLQR (Li & Todorov, 2004), and iLQG (Tassa et al., 2012). Differentiable Dynamic Programming (DDP) (Tassa et al., 2012) employs the Bellman equation structure (Murray & Yakowitz, 1984; Pantoja, 1988; Aoyama et al., 2021) and has fast convergence property. It becomes more and more popular in the control field. iLQR (Li & Todorov, 2004), and iLQG (Tassa et al., 2012; Todorov & Li, 2005) are two variants of the DDP. In iLQR and iLQG, the second-order derivative of the environment model is ignored (set as zero). Therefore, iLQR and iLQG are more computationally efficient compared to the original DDP method. Since both iLQG and our D3P planner are motivated by DDP, they look similar naturally. But our method has several key differences compared with theirs, and these differences are well-designed to incorporate the neural network model. (1) DDP, iLQR, and iLQG are both pure planning algorithms that require a known environment model. (2) Computing the second-order derivative of the neural network based model is computationally costly (Hessian matrix). In our method, we only rely on the first-order derivative of the model. (3) The previous methods use the second-order Talyor expansion of the Q-value function to handle the local optimization problem. But it is hard to guarantee that the hessian matrix is a negative definite matrix, which is a necessary condition for convergence. Here, we construct an auxiliary target function $D$ and use a first-order Talyor expansion for the $Q$ function inside of the $D$ function to guarantee the non-positive definite matrix.\u300d\n", "a4d74f6e-ca85-4fff-975f-47b95a5a92ff:\u300cref_ids: 454848283080813758, chunk_ids: 7, Score: 0.2559, Text: # Layered and Staged Monte Carlo Tree Search for SMT Strategy Synthesis\nZhengyang $\\\\left(\\\\mathbf{Johm}\\\\right)\\\\mathbf{Lu}^{1}$ ,Stefan Siemer 2 ,Piyush Jha 3 ,Joel Day 4 ,Florin Manea 2 and Vijay Ganesh 3 1 University of Waterloo 2 University of G\u00a8ottingen 3 Georgia Institute of Technology 4 Loughborough University\n\n# Abstract\nModern SMT solvers, such as Z3 , offer usercontrollable strategies, enabling users to tailor them for their unique set of instances, thus dramatically enhancing solver performance for their use case. However, this approach of strategy customization presents a significant challenge: handcrafting an optimized strategy for a class of SMT instances remains a complex and demanding task for both solver developers and users alike.  \n\nIn this paper, we address this problem of automatic SMT strategy synthesis via a novel Monte Carlo Tree Search (MCTS) based method. Our method treats strategy synthesis as a sequential decisionmaking process, whose search tree corresponds to the strategy space, and employs MCTS to navigate this vast search space. The key innovations that enable our method to identify effective strategies, while keeping costs low, are the ideas of layered and staged MCTS search. These novel approaches allow for a deeper and more efficient exploration of the strategy space, enabling us to synthesize more effective strategies than the default ones in state-ofthe-art (SOTA) SMT solvers. We implement our method, dubbed Z3alpha , as part of the Z3 SMT solver. Through extensive evaluations across 6 important SMT logics, Z3alpha demonstrates superior performance compared to the SOTA synthesis tool FastSMT , the default Z3 solver, and the CVC5 solver on most benchmarks. Remarkably, on a challenging QF BV benchmark set, Z3alpha solves $42.7\\\\%$ more instances than the default strategy in the Z3 SMT solver.\n\n# 1 Introduction\nSatisfiability Modulo Theories (SMT) solvers [De Moura and Bj\u00f8rner, 2011] are key tools in diverse fields such as software engineering [Cadar et al. , 2008], verification [Gurfinkel et al. , 2015], security [Song et al. , 2008], and artificial intelligence [Pulina and Tacchella, 2012]. It has long been observed that no single solver or algorithm excels across all instances of a given SMT logic or of a problem class. As a result, modern SMT solvers, such as Z3 [De Moura and Bj\u00f8rner, 2008], offer user-controllable strategies [De Moura and Passmore, 2013], enabling users to customize a sub-solver for their class of instances. A strategy can be thought of as an algorithmic recipe for selecting, sequencing, and parameterizing tactics .Each tactic is a well-defined algorithmic proof rule or symbolic reasoning step, provided by the solver. For example, propagate-values is a Z3 tactic that propagates equalities, while sat and smt are the tactic wrappers of the main SAT and SMT solver in Z3 . A strategy builds a decision procedure by combining tactics, as shown in an exemplar strategy (if is-pb (then propagate-values sat) smt) . This strategy specifies a solver algorithm that, given an input instance, applies propagate-values followed by sat if the instance is a pseudo-boolean problem (as checked using is-pb ), or applies smt otherwise.  \n\nDefault solver strategies are typically optimized for wellestablished benchmarks, such as those in the SMT-LIB library [Barrett et al. , 2016]. However, as the scope of SMT applications continues to grow rapidly, users frequently encounter specialized, evolving, and unprecedented classes of instances. In these scenarios, the default or the existing customized strategies might not be as effective. Consequently, there arises a need for novel customized strategies, specifically designed to efficiently address the unique challenges posed by users\u2019 specific problems. Traditionally, this task of strategy customization has been undertaken by human experts through extensive experimentation and benchmark analysis. However, even with their expertise and efforts, the task remains challenging due to the intricate interactions among tactics and the vast search space for potential strategies.  \n\nEarly attempts have been made to automatically synthesize SMT strategies. For instance, StratEVO [Ram\u00b4\u0131rez et al. , 2016] searches for an optimal strategy using evolutionary algorithms, while FastSMT [Balunovic et al. , 2018] synthesizes a tailored strategy using imitation learning and decision tree learning techniques. While these methods show promise in automating strategy customization, they suffer from issues such as a lack of robustness, limited interpretability, and extensive training times.  \n\nTo address these issues, we introduce a novel SMT strategy synthesis method that employs Monte Carlo Tree Search (MCTS). MCTS is a heuristic search algorithm, widely applied in computer board game players as a planning algorithm [Browne et al. , 2012]. Its prominence further escalated following its successful integration into the groundbreaking deep reinforcement learning systems AlphaGo [Silver et al. ,2016] and AlphaZero [Silver et al. , 2017], where MCTS was employed as a policy improvement operator. Recently, MCTS has shown remarkable success as a standalone algorithm in solving complex symbolic or combinatorial search problems, as evidenced in Khalil et al. [2022] and Sun et al. [2023]. Its key strengths, including its ability to effectively balance exploration and exploitation and adapt to the nuances of varied search problems, making it an excellent method for such challenging tasks. Our work is the first to apply MCTS to the SMT strategy synthesis problem.\u300d\n"]}}, "__type__": "1"}}, "template_store/metadata": {"f1f66211-e921-4f03-bc1d-5cb98cdcc5be": {"template_hash": ""}}}