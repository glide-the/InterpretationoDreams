角色,内容,分镜
03a69f17-fbfb-4e1c-93cb-b83ae52d8393,研究论文中采用的主要框架在不同任务中的应用与变体,1
03a69f17-fbfb-4e1c-93cb-b83ae52d8393,在路径搜索任务和偏好策略模型微调任务中，MCTS和PRM作为核心框架各自发挥了怎样的作用，以及针对它们的改进形成了哪些适用于不同任务的变体框架？  ,1
03a69f17-fbfb-4e1c-93cb-b83ae52d8393,"ref_ids: 454845766417543398, chunk_ids: 4, Score: 0.2266, Text: # 4.2 RESULTS AND DISCUSSION

# Question 1: How does TS-LLM perform in different generation tasks regarding Path@1? Does Path $@1$ provide a reasonable evaluation metric? (Sec. 3.2.2 and Sec. 3.4)
In Table 2, we first provide the comparison between TS-LLM variants with the CoT baseline regarding the Path $@1$ performance. Path $@1$ means each algorithm only generates one answer. TS-LLM variants generally outperform the baseline, underscoring the advantages of tree-search algorithms due to their capability to efficiently prune the search tree. However, notable exceptions are also seen in the RLHF task where MCTS, BFS, and DFS underperform. From our results, we want to highlight two things. Firstly, MCTS, BFS, and DFS generally perform better in shallow search problems (8 for GSM8K and 4 for Game24) while MCTS$\\alpha$ and MCTS-Rollout are dominant in deep search problems (15 for PrOntoQA and 64 for RLHF). Secondly, the the backward/backpropagation operation is quite important, since the first three methods—MCTS, BFS, and DFS—do not incorporate this operation and depend greedily on the value function. Note that the MCTS Path $@1$ also does not include the backward operation because it only happens after finding one complete path.  

Table 5: Different value training for iterative update on GSM8k  

  
Figure 2: Different Value training on GSM8k   
Figure 3: mean/max reward for RLHF alignment and the best results of 3 aggregations for the rest three tasks w.r.t. number of sequences on the 1st row and number of computations on the 2nd row.  

Despite the superiority of TS-LLM, we argue that Path $@1$ is in fact not a reasonable evaluation metric. In table 2, we also include the number of computations used in Path $@1$ generation (number of tokens in sentence-level and number of forward computation in token-level). All TS-LLM variants consume much more computation than the CoT baseline. To enable a fair comparison, we provide additional baselines, CoT-SC and COT-SC-Tree with two aggregation methods: majorityvote (MAJ) and ORM-vote (denoted as ORM, and it utilizes the learned ORM in TS-LLM). We show results within a similar scale of computation consumption with TS-LLM variants. Under this situation, TS-LLM’s advantages largely decrease when compared with $\\mathrm{CoT-SC_{\\mathrm{ORM}}}$ , especially on GSM8K (only BFS is better). We are surprised to see that such simple algorithms can also have outstanding performance when compared fairly. Despite this, most tree-search algorithms are still dominant in the rest three tasks given the same (CoT-SC-Tree) or larger search space (CoT-SC).

# Question 2: How do different node constructions influence the performance? (Sec. 3.1)
As discussed in Sec. 3.1), the search space for sentence-level action nodes is limited. Thus, we investigate the possible influence introduced by different tree constructions on Game24 with different node sizes. Specifically, we set the number of maximal expanded node size as 6, 20, and 50. Table 3 lists the performance and the number of tokens generated comparing TS-LLM’s variants, CoT-SC and CoT-SC-Tree. The almost doubled performance boost from 44.2 to 79.8 indicates the importance of appropriate expansion node size, improving TS-LLM’s performance upper bound.

# Question 3: Why do we need a learned value function and how to train that? (Sec. 3.2.1)
In Table 4, we provide a motivating example by prompting LLaMA2-7b-base as the value function and reward model for TS-LLM on Game24. The performance drop of BFS and MCTS$\\alpha$ indicates the incapability of small language models as reliable evaluators, which motivates us to substitute it with a learned value function and ORM as a general solution for any task and LLM with any size.  

Table 6: Iterative update results. $\\theta_{0}$ is the old parameter while $\\theta_{1}$ is the new one after one iteration. We compare all combinations of policy and value on GSM8k (left) and RLHF alignment (right).   


<html><body><table><tr><td>Method</td><td>Policy</td><td>Value</td><td>Performance(%) Method</td><td></td><td>Policy</td><td>Value</td><td>Performance(%)</td></tr><tr><td>Greedy</td><td>T00</td><td>1</td><td>41.4 ± 0.0</td><td>Greedy</td><td>T00</td><td></td><td>0.39 ± 0.0</td></tr><tr><td>Greedy</td><td>T01</td><td>1</td><td>47.9 ± 0.0</td><td>Greedy</td><td>T01</td><td></td><td>1.87 ± 0.0</td></tr><tr><td>Greedy</td><td>RFT k=50</td><td></td><td>47.0 ± 0.0</td><td>Greedy</td><td>RFTN=5</td><td></td><td>1.16 ± 0.0</td></tr><tr><td>Greedy</td><td>RFTk=100</td><td>-</td><td>47.5 ± 0.0</td><td>Greedy</td><td>PPO</td><td></td><td>2.53±0.0</td></tr><tr><td>MCTS-α</td><td>T00</td><td>{,r}00</td><td>51.9 ± 0.6</td><td>MCTS-0</td><td>T00</td><td>{,r}00</td><td>2.221±0.0</td></tr><tr><td>MCTS-α</td><td>T00</td><td>U,r}01</td><td>53.2±0.3</td><td>MCTS-0</td><td>T00</td><td>U,r}01</td><td>2.482±0.0</td></tr><tr><td>MCTS-α</td><td>T01</td><td>v,r}00</td><td>54.1 ± 0.9</td><td>MCTS-0</td><td>T01</td><td>v,r}00</td><td>2.532±0.0</td></tr><tr><td>MCTS-α</td><td>T01</td><td>θ</td><td>56.5± 0.6</td><td>MCTS-α</td><td>T01</td><td>0</td><td>2.670 ± 0.0</td></tr></table></body></html>  

Therefore, we investigate data collection and training paradigms for value function and ORM in TSLLM. In Figure 2, we investigate the influence of data amount and diversity by training with mixed data uniformly sampled from checkpoints of all SFT epochs ( mixed ); data purely sampled from the last checkpoint ( pure ); 1/3 data of the pure setting $(p u r e,l e s s)$ . The results of CoT-SC ORM-vote $@10$ underscore the diversity of sampled data in learning a better ORM. The Path $@1$ results of 3 TSLLM variants show that the amount of sampled data is of great importance. We leave a detailed discussion of how value and reward function training is influenced in iterative training (Sec. 3.3) when answering Question 5. Our final conclusion is that collecting a set of diverse data as much as possible is always better for TS-LLM’s value function and ORM training .

# Question 4: How TS-LLM is improved by aggregating over multiple results? (Sec. 3.2.3)
In Fig. 3, we demonstrate the mean/max reward for the RLHF task and the best of 3 aggregation results for the rest three tasks. We measure the performance of aggregation w.r.t path number and token consumption. From the figure, we mainly summarize two conclusions: Firstly, Most TS-LLM variants benefit from aggregation and can show large strengths compared with other baselines. CoT-SC only beats TS-LLM in GSM8k with the same token size, mainly because of its larger search space. TS-LLM variants are still dominant when compared with CoT-SC-Tree. Secondly, treesearch algorithms’ aggregation benefits less than CoT-SC in small-scale problems. In GSM8K and Game24, TS-LLM struggles to improve under large aggregation numbers. We believe this is because of: (1) The search space gap between CoT-SC and tree-search algorithms. Tree-search algorithms inherently explore fewer sentences, which is validated by comparing token consumption between CoT-SC-Tree $@50$ and $\\mathrm{CoT-SC}@50$ . (2) Different tree searches are not independent. The latter search might be influenced by the previous one, which decreases generation diversity.",1
03a69f17-fbfb-4e1c-93cb-b83ae52d8393,"ref_ids: 454898862265206278, chunk_ids: 2, Score: 0.1562, Text: # 6 Search as a Meta-Strategy Operator
Many MATAs produce a policy network $\\pmb{p}$ that maps directly from an infostate to a distribution over actions in a forward pass for every player. Recent work has found that leveraging computation at run-time and adding search to $\\textbf{\\emph{p}}$ can improve performance in large EFG domains [Silver et al. , 2018; Brown et al. , 2020; Schmid et al. , 2023]. As a case study for our meta-game evaluation framework, we apply it to investigate the effect of search as a general policy improver.  

Toward that end, we propose a heuristic search method for large EFGs based on information-set MCTS (IS-MCTS) [Cowling et al. , 2012] and Gumbel AlphaZero [Danihelka et al. , 2022]. Alg. 1 presents the procedure, Gumbel IS-MCTS ,in detail. Parameterized by a policy net $\\textbf{\\emph{p}}$ and a value net $\\pmb{v}$ ,Gumbel IS-MCTS conducts multiple passes over the gametree guided by $\\pmb{v}$ and $\\textbf{\\emph{p}}$ at an input infostate $s$ , and outputs an action $a$ for decision-making. We can apply this procedure to a variety of underlying MATAs, as a meta-strategy operator ters like simulation budget). The meta-stra adds run-time search to the output policy : transforming $\\mathcal{M}$ to $\\mathcal{M}^{\\prime}$ (with additional h pgy of $\\mathcal{M}^{\\prime}$ M. Unlike rparamein effect AlphaZero—which uses the same MCTS method for training and run-time, with meta-strategy operators we can explore a variety of MATAs as training-time methods which produce $\\pmb{v}$ and $\\pmb{p}$ for search at test-time [Sokota et al. , 2024] (details in $\\S7.2)$ ).  

<html><body><table><tr><td colspan=""2"">Algorithm 1 Gumbel IS-MCTS</td></tr><tr><td></td><td>1: function Gumbel-Search(s, v,p)</td></tr><tr><td>2:</td><td>(s,a). R(s,a) ← O, C(s,a) ← 0</td></tr><tr><td>3:</td><td>Va E A(s). sample g(a) ~ Gumbel(0),A < A(s)</td></tr><tr><td>4: 5:</td><td>repeat Sample a world state: h ~ Pr(h I s,p)</td></tr><tr><td>6:</td><td>while do</td></tr><tr><td>7:</td><td>if h is terminal then</td></tr><tr><td>8:</td><td></td></tr><tr><td></td><td>r ←payoffs of players Break</td></tr><tr><td>9:</td><td>else if  (h) is chance then</td></tr><tr><td>10:</td><td>a ← sample according to chance</td></tr><tr><td>11:</td><td>else if si(h) not in search tree then</td></tr><tr><td>12:</td><td>Add si(h)to search tree</td></tr><tr><td>13:</td><td>r ← v(si(h)). Break</td></tr><tr><td>14:</td><td>else if s(h) is root node s then</td></tr><tr><td>15:</td><td>a, A ← one step of sequential halving (Alg. 3) based on GS(s, a) and remaining actions in A</td></tr><tr><td>16: 17:</td><td>else Select a according to Eq.(3) in App. B.2.</td></tr><tr><td>18:</td><td>end if</td></tr><tr><td></td><td>h←ha</td></tr><tr><td>19:</td><td>end while</td></tr><tr><td>20:</td><td></td></tr><tr><td>21:</td><td>for (si, a) in this trajectory do Increment R(si, a) by ri, C(si, a) by 1.</td></tr><tr><td>22: 23:</td><td>end for</td></tr><tr><td>24:</td><td>until num_sim simulations done</td></tr><tr><td>25:</td><td>return Action a that remains in A</td></tr><tr><td></td><td></td></tr><tr><td></td><td>26: end function</td></tr></table></body></html>  

Just like MCTS, IS-MCTS incrementally builds and traverses a search tree and aggregates statistics such as visit counts $C(s,a)$ and aggregated values $R(s,a)$ for visited $(s,a)$ pairs. During each simulation of the search (line 5), a world state is sampled from a posterior bel f$\\operatorname*{Pr}(h\\mid s,p)$ assuming the opponents played according to pprior to s. In our test domains, $\\operatorname*{Pr}(h\\mid s,p)$ can be computed exactly via Bayes’s rule, where in larger domains, using particle filtering [Silver and Veness, 2010] or deep generative models $[\\mathrm{Hu}~e t$ al. , 2021; Li et al. , 2023] to approximate $\\operatorname*{Pr}(h\\mid s,p)$ are possible. Further technical details are provided in App. B.  

The key feature of Gumbel IS-MCTS is how it selects actions at the search nodes. At the beginning of the search (line 3), a Gumbel random variable, $g(a)$ , is sampled i.i.d. for each legal action $a$ of the root, for later use in action selection. At the root (line 15), the algorithm treats each legal action as an arm of a stochastic bandit, and uses a sequentialhalving algorithm [Pepels et al. , 2014] (Alg. 3) to distribute the simulation budget. Sequential-halving algorithms usually are designed for minimizing the simple regret [Bubeck et al. , 2009], which is the regret at the last-iteration action recommendation. By contrast, UCB-style algorithms are usually designed for minimizing the accumulated regret during an online learning process. For a game-playing search algorithm, minimizing simple regret makes more sense in terms of producing a single optimal action at a decision point.  

  
Figure 1: Example start of sequential bargaining game instance.  

We assign to each arm $a$ a Gumbel score $G S(s,a)\\;=\\;$ $g(a)+\\log\\!\\mathrm{it}\\,p(s,a)+G(\\hat{q}(s,a))$ . The second term is the logit of $a$ produced by $\\textbf{\\emph{p}}$ , and the third term is a monotone transformation of the action value $\\hat{q}(s,a)$ , which is estimated by $R(s,a),C(s,a)$ , and $\\pmb{v}$ . The intuition is that a high $\\hat{q}(s,a)$ value indicates a direction for policy improvement. Indeed, the improved policy $I m p(p)(s,a)\\triangleq\\operatorname{SoftMax}(\\log\\!\\mathrm{it}\\,p(s,a)+$ $G(\\hat{q}(s,a)))$ provably achieves higher expected values Danihelka et al. [2022, App. B]. The forms of $G$ and $\\hat{q}$ is detailed in App. B.1.  

Adding Gumbel noise $g(a)$ implements the “Gumbel topK-trick”: deterministically selecting the top $K$ actions according to $G S(s,a)$ is equivalent to sampling $K$ actions from $I m p(\\pmb{p})(s,a)$ without replacement [Huijben et al. , 2022]. The Gumbel score induces a low-variance non-deterministic action selection of the root node during the sequential halving process, which encourages exploration while distributing the simulation budget toward actions likely to yield higher expected values.  

At a non-root node (line 17), an action is selected to minimize the discrepancy between $I m p(p)$ and the produced visited frequency (details in App. B.2). At the end of the search, Gumbel IS-MCTS outputs the action $a$ that survives the sequential halving procedure.",1
03a69f17-fbfb-4e1c-93cb-b83ae52d8393,"ref_ids: 454984282436545430, chunk_ids: 1, Score: 0.1514, Text: # 4.4 Finding Explanations with MCTS
We use the MCTS framework established in Silver et al. (2016 ), but we modify the search tree and the reward function to suite our purposes (see Figure 2 ). Each node in the tree represents an explanation $\\mathbf{E}=\\{\\mathbf{p}_{1},\\mathbf{p}_{2},\\ldots,\\mathbf{p}_{k}\\}$ . The root of the tree represents the whole text piece as a single phrase, Table 1: Performances of stress classifiers on the test set of D READDIT . While non-neural classifiers could not surpass $72\\%$ accuracy, the MentalRoBERTa FT model fine-tuned on the D READDIT train set yielded $82\\%$ accuracy. Here, the superscript FT denotes that the model was fine-tuned.  

<html><body><table><tr><td>Model</td><td>Precision</td><td>Recall</td><td>F-1</td><td>Accuracy</td></tr><tr><td>BernoulliNB</td><td>0.69</td><td>0.84</td><td>0.75</td><td>0.72</td></tr><tr><td>MultinomialNB</td><td>0.68</td><td>0.87</td><td>0.76</td><td>0.72</td></tr><tr><td>SVM</td><td>0.71</td><td>0.77</td><td>0.74</td><td>0.72</td></tr><tr><td>MLP</td><td>0.71</td><td>0.74</td><td>0.73</td><td>0.71</td></tr><tr><td>MentalRoBERTa FI</td><td>0.78</td><td>0.90</td><td>0.84</td><td>0.82</td></tr></table></body></html>  

i.e., $\\mathbf{E}_{r o o t}\\,=\\,\\{\\mathbf{t}\\}$ . When the search is at a given node in the tree, there are two options for expanding the next node: (i) remove the first or last token in any phrase, as long as the shortened phrase still contains at least $N_{l e n g t h}$ tokens, or (ii) remove a token in the middle of a phrase, thus breaking it into two phrases, as long as both resulting phrases have at least $N_{l e n g t h}$ tokens and the total number of phrases does not exceed $N_{p h r a s e s}$ .  

The search continues to expand nodes in the tree until either the current node cannot be expanded using either of the two rules above or the explanation at the current node contains too few tokens, i.e., $r(\\mathbf{E})\\leq r_{m i n}$ . This node serves as a leaf node and is given a reward equal to  

$$
R(\\mathbf{E})=S(\\mathbf{E})+I\\cdot\\alpha\\cdot H(\\mathbf{E})
$$  

some $I\\in\\{-1,+1\\}$ and $\\alpha\\geq0$ . We use $I=$ $+1$ to select for high entropy (context-independent) explanations and $I=-1$ to select for low entropy (context-dependent) explanations. This reward is propagated back to all the nodes on the path from the root to this leaf node according to the update rules from Silver et al. (2016 ). After the search is complete, the best explanation $\\hat{\\bf E}$ is selected as  

$$
\\begin{array}{r}{\\hat{\\mathbf{E}}=\\underset{\\mathbf{E}}{\\mathrm{argmax}}\\;R(\\mathbf{E})\\;\\;\\mathrm{s.t.}\\;r(\\mathbf{E})\\leq r_{m a x},}\\end{array}
$$  

which means $\\hat{{\\bf E}}$ is the explanation in the search tree that maximizes the reward while satisfying the condition on the maximum proportion of tokens. The other interpretability conditions are guaranteed by the rules of the search tree expansion.

# 5 Experiments
All of our experiments were run on the D READDIT dataset. We report results of our stress and context classification models and share findings of our MCTS explanation algorithm.  

<html><body><table><tr><td>Model</td><td>Precision</td><td>Recall</td><td>F-1</td><td>Accuracy</td></tr><tr><td>BernoulliNB</td><td>0.81</td><td>0.75</td><td>0.76</td><td>0.80</td></tr><tr><td>MultinomialNB</td><td>0.77</td><td>0.75</td><td>0.75</td><td>0.79</td></tr><tr><td>SVM</td><td>0.76</td><td>0.72</td><td>0.74</td><td>0.76</td></tr><tr><td>MLP</td><td>0.78</td><td>0.78</td><td>0.78</td><td>0.79</td></tr><tr><td>MentalRoBERTa F1</td><td>0.85</td><td>0.86</td><td>0.86</td><td>0.87</td></tr></table></body></html>  

Table 2: Performances of context classifiers. We restricted our focus to three subreddits: “anxiety,” “assistance,” “relationships.” The fine-tuned MentalRoBERTa FT model yielded the best results with $87\\%$ accuracy.

# 5.1 Classification
As Table 1 illustrates, basic stress classification models, such as Naive Bayes classifiers, SVMs, and MLPs, performed reasonably on the test set of D READDIT . The MentalRoBERTa FT model for stress fine-tuned on the training set of D READDIT for five epochs, however, was able to outperform all the other models, achieving an accuracy score of $82\\%$ and demonstrating the efficacy of the pretraining on mental health data 3 . Our results on the stress classification task are consistent with those of Turcan and McKeown (2019 ). Table 2 reports the performance of various models on the multiclass subreddit category classification. Here, we limited our attention to three categories, namely “anxiety,” “assistance,” and “relationships.” The Reddit posts in these categories embody various distinct everyday, financial, and interpersonal stress factors, but at the same time, they seem to have common (context-independent) stress elements. In this context classification task, all models were able to go beyond the $75\\%$ accuracy level, but MentalRoBERTa FT yielded the highest accuracy.

# 5.2 Explainability
We demonstrate our MCTS approach to explainability using the same three categories as above. We use stress and context classification models implemented with Multinomial NB, MLP, and MentalRoBERTa FT .For each of these models, we apply MCTS to identify explanations for each of the 166 test texts that is labeled as stressed and belongs to one of our three categories. We use the interpretability conditions $N_{p h r a s e s}\\,=\\,3$ ,$N_{l e n g t h}=5$ ,$r_{m i n}=0.2$ , and $r_{m a x}=0.5$ for all experiments 4 , and we use $\\alpha\\,=\\,10$ except where otherwise noted.  

<html><body><table><tr><td></td><td></td><td>Original</td><td>Dependent</td><td>Independent</td></tr><tr><td rowspan=""2"">MNB</td><td>S</td><td>0.850±0.317</td><td>0.706±0.190</td><td>0.617±0.124</td></tr><tr><td>E</td><td>0.047 ± 0.140</td><td>0.274 ± 0.181</td><td>0.942 ± 0.086</td></tr><tr><td rowspan=""2"">MLP</td><td>S</td><td>0.725±0.383</td><td>0.512±0.194</td><td>0.546±0.145</td></tr><tr><td>E</td><td>0.214 ± 0.274</td><td>0.766 ± 0.163</td><td>1.067±0.022</td></tr><tr><td rowspan=""2"">MRB</td><td>S</td><td>0.878±0.324</td><td>0.830±0.220</td><td>0.430±0.273</td></tr><tr><td>E</td><td>0.042 ± 0.124</td><td>0.019 ± 0.018</td><td>0.640 ± 0.171</td></tr></table></body></html>  

Table 3: Stress (S) and context entropy (E) for original text, context-dependent explanation, and contextindependent explanation for the Multinomial Naive Bayes (MNB), Multilayer Perceptron (MLP), and Mental RoBERTa (MRB) models. Results were generated through MCTS with stress and context entropy averaged over the test set. The Wilcoxon signed rank test (Wilcoxon ,1945 ) between dependent and independent entropy is $p<0.0001$ for all models, indicating a very significant difference as desired.  

  
  
Figure 3: Histogram of stress scores for the original text and for the context-dependent and contextindependent explanations extracted by our MCTS algorithm using an MLP model. Although stress is often higher in the original text than in the extracted explanations, the explanations still maintain a meaningful amount of stress.  

We quantitatively evaluate the explanations produced by MCTS. In Table 3 , we show the average stress and context entropy scores of the original text and of the context-dependent and contextindependent explanations. Our method is able to maintain a reasonably high and consistent level of stress across the explanations while modulating the context entropy appropriately for the two different types of explanations. This indicates that our approach can identify both context-dependent and context-independent sources of stress.  

Figures 3 and 4 further illustrate this result for the MLP model by showing the full distribution of stress and context entropy scores across the test examples. Figures 5 ,6 , and 7 in the Appendix show the stress and context entropy distributions for all three models and for different values of $\\alpha$ .Lower $\\alpha$ increases stress but decreases the difference in entropy between the two types of explanations while higher $\\alpha$ decreases stress but increases the difference in entropy. This shows the flexibility of MCTS to select different types of explanations without retraining the classifiers.  

  
Context Entropy for Original Text and Explanations $(\\alpha=10.0)$   
Figure 4: Histogram of context entropy for the original text and for the context-dependent and contextindependent explanations extracted by our MCTS algorithm using an MLP model. The context-independent explanations clearly have much higher context entropy than the context-dependent explanations as desired.  

Furthermore, we qualitatively demonstrate our approach. Tables 4 ,5 , and 6 in the Appendix show examples from each of the three subreddits that illustrate how our method captures different underlying sources of stress in an interpretable manner.",1
