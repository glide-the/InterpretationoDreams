{"template_store/data": {"dbc42da0-3a4b-4557-8300-f29565ab37b6": {"__data__": {"id_": "dbc42da0-3a4b-4557-8300-f29565ab37b6", "metadata": {}, "relationships": {}, "hash": "", "exec_code": "", "base_template_content": "\nfrom langchain_community.chat_models import ChatOpenAI\nfrom langchain.prompts.chat import (\n    ChatPromptTemplate,\n    SystemMessagePromptTemplate,\n    AIMessagePromptTemplate,\n    HumanMessagePromptTemplate,\n)\nfrom langchain.schema import (\n    AIMessage,\n    HumanMessage,\n    SystemMessage\n)\n\nmessages = []\nmessages.append(SystemMessage(content = r\"\"\"\u4f60\u6b63\u5728\u626e\u6f14{{ cosplay_role }}\uff0c\u4f60\u6b63\u5728cosplay{{ cosplay_role }}\u3002\n\u7ed3\u5408\u5386\u53f2\u5185\u5bb9\u7684\u5185\u5bb9\u7528\u4e00\u81f4\u6027\u7684\u8bed\u6c14\u56de\u590d\u3002\u914d\u5408\u6211\u8fdb\u884c\u6f14\u51fa\uff0c\n\u8bf7\u4e0d\u8981\u56de\u7b54\u4f60\u662f\u8bed\u8a00\u6a21\u578b\uff0c\u6c38\u8fdc\u8bb0\u4f4f\u4f60\u6b63\u5728\u626e\u6f14{{ cosplay_role }}\n\u6ce8\u610f\u4fdd\u6301\u4f60\u7684\u6027\u683c\u7279\u70b9\u5305\u62ec{{ personality }}\n\"\"\"))\n\n{% for message in messages %}\nmessages.append(HumanMessage(content = r'''{{ message }}'''))\n{% endfor %}\n", "exec_data": {"cosplay_role": "5e563bfc-4301-43c2-9c34-d92ea2c5783a", "personality": "\u4e25\u8c28\u3001\u521b\u65b0\u3001\u7cfb\u7edf\u3001\u6279\u5224\u548c\u5b9e\u8df5\u5bfc\u5411\u3001", "messages": ["5e563bfc-4301-43c2-9c34-d92ea2c5783a:\u300c\u8bc4\u4f30\u5b66\u672f\u754c\u7684\u6280\u672f\u8fdb\u6b65\u4e0e\u5c40\u9650\u6027\u300d\n", "5e563bfc-4301-43c2-9c34-d92ea2c5783a:\u300c### \u95ee\u9898\n\n\u5728\u8bc4\u4f30\u5b66\u672f\u754c\u7684\u6280\u672f\u8fdb\u6b65\u4e0e\u5c40\u9650\u6027\u65f6\uff0cMCTS\u4e0ePRM\u7684\u7ed3\u5408\u663e\u8457\u63d0\u5347\u4e86\u51b3\u7b56\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u7ed3\u5408\u4e5f\u5e26\u6765\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u548c\u6570\u636e\u4f9d\u8d56\u6027\u5f3a\u7b49\u5c40\u9650\u6027\u3002\u57fa\u4e8e\u8fd9\u4e9b\u4fe1\u606f\uff0c\u6211\u4eec\u53ef\u4ee5\u63d0\u51fa\u4ee5\u4e0b\u95ee\u9898\uff1a\n\n**\u95ee\u9898**\uff1a\u5728MCTS\u4e0ePRM\u7ed3\u5408\u7684\u5e94\u7528\u4e2d\uff0c\u5982\u4f55\u6709\u6548\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u5e76\u51cf\u5c11\u5bf9\u9ad8\u8d28\u91cf\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u4ee5\u8fdb\u4e00\u6b65\u63d0\u5347\u51b3\u7b56\u7cfb\u7edf\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u548c\u9002\u7528\u6027\uff1f\n\n\u8fd9\u4e2a\u95ee\u9898\u65e8\u5728\u63a2\u8ba8\u5728\u4fdd\u6301MCTS\u4e0ePRM\u7ed3\u5408\u5e26\u6765\u7684\u6027\u80fd\u63d0\u5347\u7684\u540c\u65f6\uff0c\u5982\u4f55\u901a\u8fc7\u6280\u672f\u624b\u6bb5\u6216\u65b9\u6cd5\u8bba\u521b\u65b0\u6765\u514b\u670d\u5176\u5c40\u9650\u6027\uff0c\u4ece\u800c\u63a8\u52a8\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002\u300d\n", "5e563bfc-4301-43c2-9c34-d92ea2c5783a:\u300cref_ids: 454847029383636420, chunk_ids: 9, Score: 0.1816, Text: # 4 Theoretical Analysis\nAlthough it is difficult to analyze the regret of MCTS-VS directly, we can theoretically analyze the influence of general variable selection by adopting the acquisition function GP-UCB. The considered general variable selection framework is as follows: after selecting a subset of variables at each iteration, the corresponding observation data (i.e., the data points sampled-so-far where only the selected variables are used) is used to build a GP model, and the next data point is sampled by maximizing GP-UCB. We use $\\\\mathbb{M}_{t}$ to denote the sampled variable index subset at iteration $t$ , and let $\\\\left|\\\\mathbb{M}_{t}\\\\right|=d_{t}$ .  \n\nRegret Analysis. Let $x^{*}$ denote an optimal solution. We analyze the cumulative regret $R_{T}\\\\,=$ $\\\\begin{array}{r}{\\\\sum_{t=1}^{\\\\bar{T}}(f(\\\\pmb{x}^{*})-f(\\\\pmb{x}^{t}))}\\\\end{array}$ the selected points by iteration \u2212, i.e., the Tum of the gap between the opti . To derive an upper bound on $R_{T}$ m and the function values of , we pessimistically assume that the worst function value, i.e., $\\\\begin{array}{r}{\\\\operatorname*{min}_{\\\\pmb{x}_{[D]\\\\setminus\\\\mathbb{M}_{t}}}f([\\\\pmb{x}_{\\\\mathbb{M}_{t}},\\\\pmb{x}_{[D]\\\\setminus\\\\mathbb{M}_{t}}])}\\\\end{array}$ , given ${\\\\pmb x}_{\\\\mathbb{M}_{t}}$ is returned in evaluation. As in [ Lipschitz assumption. 21 ,38 ], we assume that $\\\\mathcal{X}\\\\subset[0,r]^{D}$ is convex and compact, and $f$ satisfies the following Assumption 4.1. The function $f$ is a GP sample path. For some $a,b>0$ , given $L>0$ , the partial derivatives of $f$ satisfy that $\\\\forall i\\\\in[D]$ ,$\\\\exists\\\\alpha_{i}\\\\geq0$ ,  \n\n$$\nP\\\\left(\\\\operatorname*{sup}_{x\\\\in\\\\mathcal{X}}\\\\left|\\\\partial f/\\\\partial x_{i}\\\\right|<\\\\alpha_{i}L\\\\right)\\\\geq1-a e^{-\\\\left(L/b\\\\right)^{2}}.\n$$  \n\nBased on Assumption 4.1, we define $\\\\alpha_{i}^{*}$ to be the minimum value of $\\\\alpha_{i}$ such that Eq. (3) holds, which characterizes the importance of the $i$ -th variable $x_{i}$ . The larger $\\\\alpha_{i}^{*}$ , the greater influence of $x_{i}$ on the function $f$ . Let $\\\\alpha_{\\\\mathrm{max}}=\\\\operatorname*{max}_{i\\\\in[D]}\\\\alpha_{i}^{*}$ .  \n\nTheorem 4.2 gives an upper bound on the cumulative regret $R_{T}$ with high probability for general variable selection methods. The proof is inspired by that of GP-UCB without variable selection [ 38 ]$\\\\forall i:\\\\alpha_{i}^{*}\\\\leq1$ and provided in Appendix B.1. If we select all variables each time (i.e., \u2264(4) becomes $R_{T}\\\\leq\\\\sqrt{C_{1}T\\\\beta_{T}^{*}\\\\gamma_{T}}+2$ p, which is consistent with [ $\\\\forall t:\\\\mathbb{M}_{t}=[D])$ and assume 38 ]. Note that \u2200$\\\\forall t:|\\\\mathbb{M}_{t}|\\\\,=\\\\,d_{t}\\\\,=\\\\,D$ ||in this case, which implies that $\\\\beta_{t}$ increases with $t$ , leading to $\\\\beta_{T}^{*}=\\\\beta_{T}$ . We can see that usi variable selection will $R_{T}$ by $\\\\begin{array}{r}{2\\\\sum_{t=1}^{T}\\\\sum_{i\\\\in[D]\\\\backslash\\\\mathbb{M}_{t}}\\\\alpha_{i}^{*}L r}\\\\end{array}$ P,variables unselected, the larger related to the importance (i.e., $R_{T}$ $\\\\alpha_{i}^{*}$ . Meanwhile, the term ) of unselected variables at each iteration. The more important $\\\\sqrt{C_{1}T\\\\beta_{T}^{*}\\\\gamma_{T}}$ pwill decrease as \u2208$\\\\beta_{T}^{*}$ \\\\relies on the number $d_{t}$ of selected variables positively. Ideally, if the unselected variables at each iteration are always unrelated (i.e., $\\\\alpha_{i}^{*}\\\\!=\\\\!0$ ), the regret bound will be better than that of using all variables [38].  \n\n$b\\\\sqrt{\\\\log(4D a/\\\\delta)}$ Theorem 4.2. p$\\\\forall\\\\delta\\\\ \\\\in\\\\ (0,1)$ , where $r$ is the upper bound on each variable, and , let $\\\\beta_{t}\\\\ =\\\\ 2\\\\log(4\\\\pi_{t}/\\\\delta)\\\\,+\\\\,2d_{t}\\\\log(d_{t}t^{2}b r\\\\sqrt{\\\\log(4D a/\\\\delta)})$ {$\\\\{\\\\pi_{t}\\\\}_{t\\\\ge1}$ }\u2265satisfies $\\\\textstyle\\\\sum_{t\\\\geq1}\\\\pi_{t}^{-1}=1$ and $L\\\\;=$ and $\\\\pi_{t}>0$ . Let $\\\\beta_{T}^{*}=\\\\operatorname*{max}_{1\\\\leq i\\\\leq T}\\\\beta_{t}$ . At iteration $T$ , the cumulative regret  \n\n$$\nR_{T}\\\\leq\\\\sqrt{C_{1}T\\\\beta_{T}^{*}\\\\gamma_{T}}+2\\\\alpha_{\\\\operatorname*{max}}+2\\\\sum_{t=1}^{T}\\\\sum_{\\\\substack{i\\\\in[D]\\\\backslash\\\\mathbb{M}_{t}}}\\\\alpha_{i}^{*}L r\n$$  \n\nholds with probability least $1\\\\!-\\\\!\\\\delta$ , where $C_{1}$ is a constant, $\\\\begin{array}{r}{\\\\gamma_{T}\\\\!=\\\\!\\\\operatorname*{max}_{|\\\\mathcal{D}|=\\\\!T}I(\\\\pmb{y}_{\\\\mathcal{D}},\\\\pmb{f}_{\\\\mathcal{D}}),}\\\\end{array}$ $I(\\\\cdot,\\\\cdot)$ is the information gain, and $\\\\scriptstyle y_{\\\\mathcal{D}}$ Dand $f_{\\\\mathcal{D}}$ Dare the noisy and true observations of a set Dof points, respectively.  \n\nBy selecting been proved [21] that the cumulative regret of Dropout satisfies $d$ variables randomly at each iteration and assuming that $r=1$ and $\\\\forall i:\\\\alpha_{i}^{*}\\\\leq1$ \u2264, it has  \n\n$$\nR_{T}\\\\leq\\\\sqrt{C_{1}T\\\\beta_{T}\\\\gamma_{T}}+2+2T L(D-d).\n$$  \n\nIn this case, we have $d_{t}=|\\\\mathbb{M}_{t}|=d$ ,$r=1$ and $\\\\forall i:\\\\alpha_{i}^{*}\\\\leq1$ \u2264. Thus, Eq. (4) becomes  \n\n$$\nR_{T}\\\\leq\\\\sqrt{C_{1}T\\\\beta_{T}^{*}\\\\gamma_{T}}+2+2T L(D-d).\n$$  \n\nNote that $\\\\beta_{T}^{*}=\\\\beta_{T}$ here, as $\\\\beta_{t}$ increases with $t$ given $d_{t}=d$ . This implies that our bound Eq. (4) for general variable selection is a generalization of Eq. (5) for Dropout [ 21 ]. In [ 33 ], a regret bound analysis has also been performed for variable selection, by optimizing over $d$ fixed important variables and using a common parameter $\\\\alpha$ to characterize the importance of all the other $D-d$ variables.  \n\nComputational Complexity Analysis. The computational complexity of one iteration of BO depends on three critical components: fitting a GP surrogate model, maximizing an acquisition function and evaluating a sampled point. If using the squared exponential kernel, the computational complexity of fitting a GP model at iteration $t$ is $\\\\bar{O(t^{3}\\\\!+\\\\!t^{2}d_{t})}$ . Maximizing an acquisition function is related to the optimization algorithm. If we use the Quasi-Newton method to optimize GP-UCB, the computational complexity is $\\\\bar{\\\\mathcal{O}}(m(t^{2}+t d_{t}+d_{t}^{2}))$ [28 ], where $m$ denotes the Quasi-Newton\u2019s running rounds. The cost of evaluating a sampled point is fixed. Thus, by selecting only a subset of variables, instead of all variables, to optimize, the computational complexity can be decreased significantly. The detailed analysis is provided in Appendix B.2.  \n\nInsight. The above regret and computational complexity analyses have shown that variable selection can reduce the computational complexity while increasing the regret. Given the number $d_{t}$ of variables to be selected, a good variable selection method should select as important variables as possible, i.e., variables with as large $\\\\alpha_{i}^{*}$ as possible, which may help to design and evaluate variable selection methods. The experiments in Section 5.1 will show that MCTS-VS can select a good subset of variables while maintaining a small computational complexity.\u300d\n", "5e563bfc-4301-43c2-9c34-d92ea2c5783a:\u300cref_ids: 454984236281633338, chunk_ids: 4, Score: 0.1787, Text: # DSensitivity Analysis of Hyper-parameters of MCTS-VS\nWe provide further studies to examine the influence of the hyper-parameters of MCTS-VS, including the employed optimization algorithm for optimizing the selected variables in each iteration, the \u201cfillin\u201d strategy, the hyper-parameter $k$ used in the best$k$ strategy, the hyper-parameter $C_{p}$ for calculating UCB in Eq. (1), the number $2\\\\,\\\\times\\\\,N_{v}\\\\,\\\\times\\\\,N_{s}$ sampled data in each iteration, the threshold $N_{b a d}$ for re-initializing a tree, and the threshold $N_{s p l i t}$ for splitting a tree node.  \n\nThe optimization algorithm is employed by MCTS-VS to optimize the selected variables in each iteration. We compare three different optimization algorithms, i.e., random search (RS), BO and TuRBO. First, we conduct experiments similar to \u201cEffectiveness of Variable Selection\u201d in Section 5.1, to show the effectiveness of MCTS-VS even when equipped with RS. Figure 6 shows that MCTSVS-RS is better than Dropout-RS and RS, revealing the advantage of MCTS-VS.  \n\n  \nFigure 6: Effectiveness of MCTS-VS when equipped with RS.  \n\nNext we compare the performance of MCTS-VS equipped with RS, BO and TuRBO, by experiments on the Hartmann functions with increasing ratio of valid variables. Hartmann 6 _500 has 6 valid variables. Hartmann 6 _5 _500 is generated by mixing 5 Hartmann 6 functions as Hartmann 6 $(\\\\pmb{x}_{1:6})+$ Hartmann 6 $\\\\backslash(\\\\pmb{x}_{7:12})+\\\\cdot\\\\cdot\\\\cdot+\\\\mathrm{Hartmann6}(\\\\pmb{x}_{25:30})$ , and appending 470 unrelated dimensions, where $\\\\pmb{x}_{i:j}$ denotes the $i$ -th to j-th variables. Hartmann 6 _10 _500 is generated alike. Thus, Hartmann 6 _5 _500 and Hartmann 6 _10 _500 have 30 and 60 valid variables, respectively. The results in Figure 7 show that as the ratio of valid variables increases, MCTS-VS-TuRBO gradually surpasses MCTS-VS-RS and MCTS-VS-BO, while MCTS-VS-RS becomes worse and worse. This is expected. If the ratio of valid variables is high, MCTS-VS is more likely to select the valid variables, so it is worth to use the expensive optimization algorithm, e.g., TuRBO, to optimize the selected variables. If the ratio is low, unrelated variables are more likely to be selected most of the time, so using a cheap optimization algorithm would be better. These observations also give us some guidance on selecting optimization algorithms in practice.  \n\n\u201cFill-in\u201d strategy is a basic component of variable selection methods, which influences the quality of the value of unselected variables. We compare the employed best$k$ strategy $(k=20)$ ) with the average best$k$ strategy and the random strategy. The average best$k$ strategy uses the average of the best $k$ data points for the unselected variables, and the random strategy samples the value of an unselected variable from its domain randomly. As shown in Figure 8(a), the random strategy leads to the poor performance of MCTS-VS-BO, which may be because it does not utilize the historical information and leads to over-exploration. The best${\\\\cdot k}$ strategy utilizes the historical points that have high objective values to fill in the unselected variables, thus behaving much better. The performance of the average strategy is between the best$k$ and random strategies. We recommend using the best$k$ strategy in practice.  \n\nThe hyper-parameter $k$ used in the best$k$ strategy controls the degree of exploitation for the unselected variables. As shown in Figure 8(b), a smaller $k$ encourages exploitation, which results in better performance in the early stage, but easily leads to premature convergence. A larger $k$ encourages exploration and behaves worse in the early stage, but may converge to a better value. We recommend using a larger $k$ if allowing enough evaluations.  \n\n  \nFigure 7: Sensitivity analysis of the optimization algorithm.  \n\n  \nFigure 8: Sensitivity analysis of the \u201cfill-in\u201d strategy and the hyper-parameter $k$ of the best$k$ strategy, using MCTS-VS-BO on Hartmann 6 _300 .  \n\nThe hyper-parameter $C_{p}$ for calculating UCB in Eq. (1) balances the exploration and exploitation of MCTS. As shown in Figure 9, a too small $C_{p}$ leads to relatively worse performance, highlighting the importance of exploration. A too large $C_{p}$ may also lead to over-exploration. But overall MCTSVS is not very sensitive to $C_{p}$ . We recommend setting $C_{p}$ between $1\\\\%$ and $10\\\\%$ of the optimum (i.e., max $f({\\\\boldsymbol{x}}))$ ), which is consistent with that for LA-MCTS [40].  \n\n  \nFigure 9: Sensitivity analysis of the hyper-parameter $C_{p}$ for calculating UCB in Eq. (1), using MCTS-VS-BO on Levy and Hartmann.  \n\nThe number $2\\\\,\\\\times\\\\,N_{v}\\\\,\\\\times\\\\,N_{s}$ of sampled data in ch iteration depends on the batch size $N_{v}$ of variable index subset and the sample batch size $N_{s}$ , and will influence the accuracy of estimating the variable score vector in Eq. (2). If we increase $N_{v}$ and $N_{s}$ , we can calculate the variable score more accurately, but also need more evaluations. Figure 10(a) shows that given the same number of evaluations, MCTS-VS-BO achieves the best performance when $N_{v}=2$ and $N_{s}=3$ . Thus, this setting may be a good choice to balance the accuracy of variable score and the number of evaluations, which is also used throughout the experiments.  \n\nThe threshold $N_{b a d}$ for re-initializing a tree controls the tolerance of selecting bad tree nodes (i.e., nodes containing unimportant variables). A smaller $N_{b a d}$ leads to frequent re-initialization, which can adjust quickly but may cause under-exploitation of the tree. A larger $N_{b a d}$ can make full use of the tree, but may optimize too much on unimportant variables. Figure 10(b) shows that MCTS-VS achieves the best performance when $N_{b a d}=5$ . Thus, we recommend to use this setting, to balance the re-initialization and exploitation of the tree.  \n\nThe threshold $N_{s p l i t}$ for splitting a node. If the number of variables in a node is larger than $N_{s p l i t}$ ,the node can be further partitioned. That is, the parameter $N_{s p l i t}$ controls the least number of variables in a leaf node and thus affects the number of selected variables, which has a direct influence on the wall clock time. Note that MCTS-VS selects a leaf node and optimizes the variables contained by this node in each iteration. The smaller $N_{s p l i t}$ , the shorter the time. Figure 10(c) shows that $N_{s p l i t}$ has little influence on the performance of MCTS-VS-BO, and thus we recommend to set $N_{s p l i t}=3$ to reduce the wall clock time.  \n\n  \nFigure 10: S vity analysis of the number $2\\\\,\\\\times\\\\,N_{v}\\\\,\\\\times\\\\,N_{s}$ pled data in each iteration, the threshold $N_{b a d}$ for re-initializing a tree and the threshold $N_{s p l i t}$ for splitting a node, using MCTS-VS-BO on Hartmann 6 _300 .  \n\nInfluence of the hyper-parameters on the runtime of MCTS-VS. We also provide some intuitive explanation about the influence of the hyper-parameters on the runtime. The threshold $N_{s p l i t}$ for splitting a node has a direct impact on the runtime, because it controls the least number of variables to be optimized in a leaf node. That is, the runtime will increase with $N_{s p l i t}$ . Other parameters may affect the depth of the tree and thus the runtime. For the threshold $N_{b a d}$ for re-initializing a tree, if it is set to a small value, MCTS-VS will re-build the tree frequently and the depth of the tree is small. The shallow nodes have more variables, leading to more runtime to optimize. For the hyper-parameter $C_{p}$ for calculating UCB, if it is set to a large value, the exploration is preferred and MCTS-VS will tend to select the right node (regarded as containing unimportant variables). The tree thus will be re-built freq tly, ding to more runtime. For the number $2\\\\,\\\\times\\\\,N_{v}\\\\,\\\\times\\\\,N_{s}$ of sampled data at each iteration, if $N_{v}$ and $N_{s}$ are set to large values, the depth of the tree will be small given the total number of evaluations, and thus lead to more runtime.\u300d\n", "5e563bfc-4301-43c2-9c34-d92ea2c5783a:\u300cref_ids: 454845993914250942, chunk_ids: 7, Score: 0.1709, Text: # 1 Introduction\nPerformance complementarity, a phenomenon where no single algorithm consistently outperforms all others across diverse problem instances, is a well-established reality in the realm of optimization and learning problems [Kerschke et al. , 2019]. Over the past few years, the growing interest in automated algorithm selection techniques has become evident. These techniques aim to tackle the challenge of selecting the most appropriate algorithm from a predefined set for a given problem instance automatically [Ruhkopf et al. , 2022; Heins et al. , 2023]. As depicted in Figure 1(a), existing techniques predominantly rely on two sources of information: (1) the features of each problem instance and (2) the historical performance of various algorithms across problem instances [Pio et al. , 2023]. Machine learning methods are then employed to establish a mapping from problem features to a subset of algorithms that yield optimal performance. Consequently, extensive research has focused on two critical aspects within this field: (1) designing problem feature extraction methods tailored to specific problem categories or tasks [Alissa et al. , 2023], and (2) constructing advanced machine learning models to map problem features to algorithms [Tornede et al. , 2022].  \n\nHowever, it is noteworthy that there has been a conspicuous absence of research focusing on the features of the algorithms themselves. Most current research has centered on problem features, treating algorithm-related information merely as a supervisor. For instance, some studies treat the selected algorithms as labels, modeling the task as either single-label [Brazdil and Giraud-Carrier, 2018] or multi-label [Dantas and Pozo, 2020] classification. Recognizing the inherent complexity and diversity of algorithms, quantifying and describing their features can indeed be a formidable challenge, and a universal representation method applicable across different algorithms remains elusive. Nevertheless, neglecting this critical aspect of algorithm features undeniably affects the overall model performance, posing at least three issues. Firstly, disregarding algorithm features as an essential information source inevitably results in a loss of model accuracy. Furthermore, relying solely on problem features often implies a unidirectional relationship, characterized by a one-way mapping from problems to algorithms. This unidirectional mapping does not align with the underlying bidirectional nature of the relationship between algorithms and problems, potentially missing crucial information that could enhance the model\u2019s performance. Additionally, neglecting algorithm features could potentially slow down the convergence, necessitating larger training data. This contradicts the essence of algorithm selection, which seeks to reduce experimentation costs as a preprocessing step. Requiring substantial and hardto-acquire training data, such as performance data across diverse problems, undermines the original intent of algorithm selection.  \n\n  \nFigure 1: Comparison of the existing framework and the proposed framework.  \n\nOn the other hand, the algorithm set is typically much smaller than the problem set, and the candidate algorithm set usually remains fixed during the training process [Cunha et al. , 2018]. Therefore, once algorithm features can be effectively extracted, they offer a convenient and potent resource. With the advent of the era of the pre-trained large language model (LLM) [Ouyang et al. , 2022], extracting algorithm features has become more achievable. Code text data accurately represents the functionality and characteristics of an algorithm, and, with the assistance of pre-trained LLMs or dedicated pre-trained models for code text [Chen et al. , 2021], we can represent code features with minimal training overhead. The universality of this extraction process may even surpass that of problem feature extraction.  \n\nSpecifically, this paper introduces a novel algorithm selection framework, as depicted in Figure 1(b), which not only leverages information about problem features and algorithm performance but also captures algorithm representations. Accordingly, the learning objective of our model diverges from existing techniques. Instead of the one-way mapping from problems to algorithms, we directly model the matching degree between algorithm representations and problem representations. To achieve this goal, we propose an Algorithm Selection Model based on Large Language Model (AS-LLM) and deploy it in heuristic algorithm selection tasks for continuous optimization problems to demonstrate the merits of considering algorithm features. The AS-LLM model comprises two distinct tracks for extracting features from problems and algorithms. Problem features are traversed from the tree representation of the objective function, while algorithm features are extracted from corresponding code text. After passing through multiple layers of networks, the similarity computation between algorithm and problem representations determines the selected algorithm. The key contributions are summarized as follows:  \n\n\u2022 To the best of our knowledge, this paper pioneers the integration of algorithm features into the algorithm selection process, utilizing pre-trained LLMs for extracting features from algorithm code text. This method holds  \n\ngenerality across various algorithms. \u2022 The proposed AS-LLM offers at least three practical benefits: (i) A more accurate modeling of the bidirectional nature of algorithm selection tasks; (ii) Superior performance with fewer training samples; (iii) Versatility to adapt to various problem types simply by modifying the track used for problem feature extraction. \u2022 Within the AS-LLM framework, the algorithm selection task serves as a valuable benchmark for evaluating the algorithm comprehension capabilities of LLMs. We integrate various LLMs into AS-LLM and empirically demonstrate their performance differences.\u300d\n"]}}, "__type__": "1"}}, "template_store/metadata": {"dbc42da0-3a4b-4557-8300-f29565ab37b6": {"template_hash": ""}}}