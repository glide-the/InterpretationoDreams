角色,内容,分镜
edaf9c7c-601d-4068-b117-92e5eaa45af5,泛化能力,3>2
edaf9c7c-601d-4068-b117-92e5eaa45af5,"### 问题提出

在MCTS与PRM结合的框架中，迁移学习被用来增强模型在不同领域间的泛化能力。然而，迁移学习的有效性往往依赖于源领域和目标领域之间的相似性。基于此，提出以下问题：

**问题**：在MCTS+PRM框架中，如何量化源领域和目标领域之间的相似性，以确保迁移学习的有效性？具体来说，有哪些指标或方法可以用来评估两个领域之间的相似性，从而优化迁移学习策略，提升模型在目标领域的泛化能力？

### 问题背景

迁移学习是一种通过将在一个领域（源领域）学到的知识应用到另一个领域（目标领域）的技术。在MCTS+PRM框架中，迁移学习被用来增强模型在不同领域间的泛化能力。然而，迁移学习的有效性往往依赖于源领域和目标领域之间的相似性。如果两个领域之间的差异过大，迁移学习可能会导致负迁移，即模型在目标领域的性能下降。

### 问题分析

为了确保迁移学习的有效性，需要量化源领域和目标领域之间的相似性。具体来说，可以考虑以下几个方面：

1. **特征相似性**：源领域和目标领域的特征空间是否相似？例如，在推荐系统中，用户的行为特征是否在不同领域间保持一致？
2. **任务相似性**：源领域和目标领域的任务是否相似？例如，在游戏AI和路径规划中，决策过程是否具有相似的结构？
3. **数据分布相似性**：源领域和目标领域的数据分布是否相似？例如，在推荐系统中，用户偏好分布是否在不同领域间保持一致？

### 可能的解决方案

1. **特征相似性度量**：可以使用余弦相似度、欧氏距离等度量方法来评估源领域和目标领域特征空间的相似性。
2. **任务相似性度量**：可以通过任务的结构相似性、决策过程的复杂性等指标来评估源领域和目标领域任务的相似性。
3. **数据分布相似性度量**：可以使用KL散度、JS散度等度量方法来评估源领域和目标领域数据分布的相似性。

### 结论

通过量化源领域和目标领域之间的相似性，可以优化迁移学习策略，提升MCTS+PRM框架在目标领域的泛化能力。这需要综合考虑特征相似性、任务相似性和数据分布相似性等多个方面，并选择合适的度量方法进行评估。",3>2
edaf9c7c-601d-4068-b117-92e5eaa45af5,"ref_ids: 454848214017925842, chunk_ids: 2, Score: 0.2754, Text: # C.2 Compared Methods
We compare DRIP with various methods from related research fields. As discussed in the paper, MDRAU task has not been studied well in previous literature, and many existing methods cannot be directly applied. Therefore, we have modified the original methods to perform MDRAU, and the modified versions are annotated with the suffix $\\cdot_{+},$ . The following are our competing methods.  

•BPRMF (Rendle et al. 2009) is a representative method for implicit feedback. It learns pair-wise ranking in the latent space.   
•MMOE (Ma et al. 2018) utilizes multiple experts that share user embeddings over multiple domains, and it adopts a domain-specific gating module to effectively capture inter-domain relationships.   
•PLE (Tang et al. 2020) also utilizes multiple experts, but it separates domain-specific and domain-shared experts for more balanced training.  

Note that the above methods learn global user embeddings that are shared across all domains, so that the global user embeddings can be directly used for recommendation in any target domain.  

EMCDR and PTUPCDR learn a mapping function for each source-target domain pair. Due to a large number of seen-unseen domain combinations, it is infeasible to directly apply them to MDRAU. For this reason, we tailor their learning task for a many-to-one mapping. That is, for each target domain, we learn a mapping function that takes concatenated domain-specific user embeddings as input 5 and predicts user preference in the target domain.  

•$\\mathbf{EMCDR+}$ (Man et al. 2017) is a representative CDR method based on the embedding-and-mapping approach for unseen domain recommendation (or cold-start recommendation). It trains a mapping function to predict user embedding in the target domain from the source domain. The $\\cdot+\\rangle$ version (i.e., EMCDR+) is modified according to the above description.  

•PTUPCDR $^{+}$ (Zhu et al. 2022) improves $\\operatorname{EMCDR+}$ by employing a meta-network as its personalized mapping function. It trains the mapping function to predict ratings in the target domain.  

•CAT-ART $^+$ (Li et al. 2023) generates shared user embeddings that summarize user preferences from all interacted domains and exploits the shared embeddings to improve the recommendation quality in the target domain. As user embeddings do not exist for unseen domains, we use the reconstructed user embedding from the autoencoder module used to generate the shared embedding.  

•UniCDR (Cao et al. 2023) is the state-of-the-art CDR method that can handle various CDR scenarios. It models domain-specific and domain-shared user embeddings separately and transfers the knowledge from other domains based on the domain-shared embeddings.  

  
Figure 4: Sensitivity analysis of DRIP.  

All the compared methods except BPRMF were proposed to recommend items for a single target domain. To evaluate such single target methods in the MDRAU-MT, the recommendations for each unseen domain need to be integrated over multiple unseen domains. For effective integration, we generate a unified recommendation list by using the scores normalized within each domain.

# C.3 Implementation Details
We use Xeon Gold 6226R CPU and A5000 24GB GPU on Ubuntu 20.04 LTS for experiments. We implement DRIP and other competing methods with PyTorch (Paszke et al. 2019) and use Adam optimizer (Kingma and Ba 2014) with $\\beta_{1}~~=~~0.9,\\beta_{2}~~=~~0.999$ . For domainspecific encoders, we use BPR (Rendle et al. 2009) with embedding size 64. We ran five times for each experiment. We tune all hyperparameters by grid search using the validation set. The learning rates are searched in the range of $\\{0.0001,0.0005,0.001,0.005,0.01\\}$ .The weight decay is searched in the range of $\\{0.0001,0.0005,0.001,0.005,0.01\\}$ .For MMOE and PLE, the number of experts is searched in the range of $\\{2,3,5,8,10\\}$ . For the mapping function in EMCDR, we use multi-layer perception (MLP) with one-hidden layer: $[64\\,\\times\\,4\\,\\rightarrow\\,2\\,\\times\\,64\\,\\times\\,4\\,\\rightarrow\\,64\\,\\times\\,4]$ and tanh activation (Man et al. 2017; Kang et al. 2019). For CAT-ART and UniCDR, we follow the recommended values from the public implementation and from the original papers (Li et al. 2023; Cao et al. 2023). For DRIP, the number of heads, the number of layers, and random masking probabilities $\\{0,1,0.2,0.3,0.4,0.5,0.6,0.7,\\dot{0}.9\\}$ results are reported in Sec. D. For the masking process, {$\\{2^{0},2^{1},2^{2},2^{3}\\}$ },respectively. ,$\\{1,2,3,4\\}$ The ,we introduce a selection probability $\\epsilon_{i}$ in epoch $i$ . When $\\epsilon_{i}=1$ , we use only random masking, while when $\\epsilon_{i}=0$ ,we only use adaptive masking. We linearly decrease the $\\epsilon_{i}=\\operatorname*{max}(0.5,1-0.002i)$ in training phase.

# DSensitivity Analysis
We provide a sensitivity analysis to guide the hyperparameter selection of DRIP. In Fig 4, we report the recommendation accuracy $(\\mathbf{R}@20)$ with varying random masking probabilities, the number of heads, and the number of layers for MDRAU-MT on P1. Similar tendencies are observed in the other scenario. We first observe that the masking probability has a small impact within the range of [0.1, 0.7]. However, the performance drastically drops when the masking probability exceeds 0.7. If the masking probability is too high, the model cannot obtain sufficient information from other domains, leading to degraded performance. Also, we observe stable performances with varying numbers of heads and layers of the multi-domain encoder. DRIP achieves a slightly improved performance with more number of heads, and a slightly degraded performance when the number of layers exceeds 3.",3>2
edaf9c7c-601d-4068-b117-92e5eaa45af5,"ref_ids: 454845771530662550, chunk_ids: 1, Score: 0.2178, Text: # 2 RELATED WORK
LLMs for reasoning. For LLMs, reasoning typically involves decomposing complex inputs into sequential intermediate steps before producing a final answer ( Cobbe et al. ,2021 ), commonly demonstrated with Chain-of-Thought (CoT) prompting ( Wei et al. ,2022 ) and its variants ( Wei et al. ,2022 ;Kojima et al. ,2022 ;Wang et al. ,2022 ). However, these methods, which create chains autoregressively in a single step, often suffer from error propagation as the number of steps increases (Guo et al. ,2018 ;Chen et al. ,2022b ), in which errors tend to compound. Various advancements have aimed to mitigate this issue; some approaches, such as Self-Consistency ( Wang et al. ,2022 ), employ majority voting over sampled chains, while others focus on multi-step decomposition, such as least-to-most prompting ( Zhou et al. ,2022 ), or use of external tools such as a scratchpad ( Nye et al. ,2021 ) or compiler ( Gao et al. ,2022 ). More recently, CoT has been improved with search algorithms ( Yao et al. ,2023a ;Hao et al. ,2023 ;Besta et al. ,2023 ) that can sample trajectories more effectively. Tree-of-thought (ToT) prompting ( Yao et al. ,2023a ) uses DFS or BFS-based search guided by an LM-generated heuristic while Reasoning via Planning (RAP) ( Hao et al. ,2023 ) uses MCTS with rollouts simulated by the LM. Despite using a search algorithm, these frameworks rely solely on the internal knowledge of the LM and cannot adapt to external inputs that could enhance the reasoning process.  

  
Figure 2: An overview of the differences between LATS and recently proposed LM search algorithms ToT ( Yao et al. ,2023a ) and RAP ( Hao et al. ,2023 ). LATS leverages environmental feedback and self-reflection to further adapt search and improve performance.  

LLMs for decision-making. The strong reasoning and common-sense abilities of LLMs have also been adapted for decision-making tasks as a policy model in interactive environments. In the realm of robotics LLMs have been employed as high-level controllers of control policies ( Ahn et al. ,2022 ;Huang et al. ,2022 ;Driess et al. ,2023 ). Similar work ( Baker et al. ,2022 ;Wang et al. ,2023 ;Zhu et al. ,2023 ) has also adapted LLM agents to complex multimodal games such as Minecraft ( Guss et al. ,2019 ;Fan et al. ,2022 ). LLMs are particularly useful in text-based environments ( Liu et al. ,2018 ;Shridhar et al. ,2020 ;Liu et al. ,2023 ), where acting-based prompting techniques such as ReAct ( Yao et al. ,2023b ) have seen success. Similar to CoT, ReAct is limited by its simplicity and cannot effectively adapt to environment conditions. Many extensions have been proposed to address this, including Self-refine ( Madaan et al. ,2023 ) and Reflexion ( Shinn et al. ,2023 ;Yao et al. ,2023c ), which uses self-reflection to enhance reasoning and decision-making, and AdaPlanner ( Sun et al. ,2023 ), which incorporates both positive and negative environmental feedback. However these methods focus on refining an individual plan or trajectory and do not consider alternative choices at each step. Alternatively to pure decision-making environments, the reasoning and practical abilities of LLMs have been enhanced by access to external tools, such as APIs, search engines, calculators, or other models ( Schick et al. ,2023 ;Shen et al. ,2023 ;Sur´ıs et al. ,2023 ). Contrary to reasoningbased approaches, these methods have not been improved with planning, limiting their effectiveness.  

Tree-based search. Tree-based search, where multiple branches of outcomes are explored during search, is widely used in many planning algorithms ( Swiechowski et al. ,2023 ;LaValle et al. ,2001 )and Reinforcement Learning (RL) ( Hafner et al. ,2019 ;Du et al. ,2023 ;Wu et al. ,2023 ) algorithms for its good exploration-exploitation trade-off. Though tree-based search requires an environment model that can expand from arbitrary state ( Vodopivec et al. ,2017 ), which often requires extra training in RL ( Hafner et al. ,2023 ), such problem does not exist for LM tasks as we can conveniently backup to any state by setting the input to be the context and corresponding previous output by the LM. Thus, we work on the tree-based framework and use MCTS ( Swiechowski et al. ,2023 ) to fully release the potential of LMs, while avoiding the cost of training a value function over language descriptions by leveraging the in-context learning ( Brown et al. ,2020 ) abilities of LLMs.

# 3 PRELIMINARIES
Before describing LATS, we first define our problem and outline a few established methods that leverage large language models for reasoning or decision-making. In LM reasoning or decision making, we are given an input $x$ in natural language and a pretrained language model $p_{\\theta}(x)$ parameterized by $\\theta$ ; our goal is to generate a final outp $y\\sim p_{\\theta}(x)$ corresponding to the answer (reasoning) or completes the task (decision-making). Both xand $y$ are language sequences , which are comprised of a list of tokens (the basic elements of natural language, often words), denoted as $x=(x[1],\\dots,x[n])$ and $y=(y[1],\\dots,y[n])$ . The LM decodes text autoregressively, i.e., without other inputs, the probability for an LM to generate a sequence $x$ is given by $\\begin{array}{r}{\\bar{p}_{\\theta}(x)=\\dot{\\prod}_{i=1}^{n}p_{\\theta}(x[i]|x[1\\dots i-1])}\\end{array}$ |−. Usually, to improve the LM, prompts are provided along with the input x, which are specific instructions or few-shot input-output examples. We denote the generic process where an input $x$ is transformed into an output $y$ by LM: $y\\sim p_{\\theta}(y|\\mathsf{p r o m p t}_{I O}(x))$ , where prompt $\\ _{I O}(x)$ denotes the input $x$ along with the prompts.  

Chain-of-thought $(\\mathbf{CoT})$ Prompting (Wei et al. ,2022 ) was introduced to cater to scenarios where direct mapping from $x$ to $y$ is intricate, such as when $x$ is from a mathematical query or challenging question. This method hinges on creating thoughts $z_{1},\\ldots,z_{n}$ that act as stepping stones between $x$ and $y$ ; each thought $z_{i}$ is a language sequence. To employ CoT prompting, thoughts are extracted sequentially as $z_{i}^{-}\\!\\sim p_{\\theta}^{C o T}(z_{i}|x,\\overline{{z_{1}}}\\overline{{{...}}}{-1})^{\\!2}$ |··· −, with the final output being $y\\stackrel{\\cdot}{\\sim}p_{\\theta}^{\\overleftarrow{C}o T}(y|\\breve{x},z_{1\\cdots n})$ |··· .  

Tree-of-thought (ToT) Prompting (Yao et al. ,2023a ) extends CoT prompting by exploring multiple reasoning paths over thoughts. It frames problems as a search over a tree where each node $\\boldsymbol{s}^{\\bar{}}=\\left[\\boldsymbol{x},\\boldsymbol{z}_{1\\cdot i}\\right]$ represents a partial solution state comprising the original input $x$ and thought sequence $z_{1\\cdots i}$ liberate search algorithms like breadth-first or depth-first search are used to systematically explore . Thoughts $z_{i}$ are generated by proposal or sampling with CoT $z_{i}\\stackrel{\\triangledown}{\\sim}p_{\\theta}^{C o T}(z_{i}|x,\\breve{z_{1}}...i-1)$ |··· −. Dethe tree, guided by heuristics based on language model evaluations $V(s)$ of each state.  

Reasoning via Planning (RAP) ( Hao et al. ,2023 ) is similar to ToT, except that MCTS is used over DFS or BFS. Heuristics are designed from an LM, such as the likelihood or confidence of an action, and the LM is used as a world model to predict subsequent states during the simulation step.  

ReAct (Yao et al. ,2023b ) extends language models to tasks where the mapping from $x$ to $y$ is enhanced by or requires interactions with an external environment, such as a game or API. This techique constructs an action ace $\\hat{A}=A\\cup Z$ that adds permissible actions $a$ to the reasoning traces $z$ from CoT. Observations ofrom the environment are used to improve both reasoning and acting. To solve problems with ReAct, after each observation, actions are generated from $p_{\\theta}$ sequentially as $a_{i}\\sim p_{\\theta}^{R e A c t}(a_{i}|\\boldsymbol{x},o_{1\\cdots i-1},a_{1\\cdots i-1})$ |··· −··· −, with the final output being $y\\stackrel{<}{\\sim}p_{\\theta}^{R e A c t}(y\\mid x,\\stackrel{<}{o_{1}...n},a_{1}...n)$ |··· ··· .  

While the previously described prompting techniques improve LM performance on reasoning tasks, they falter on difficult tasks that involve multifaceted decision-making due to several shortcomings: 1) Flexibility : Base prompting methods (CoT or ReAct) autoregressively sample from the LM, neglecting potential alternative continuations from specific states. 2) Sensibility : Reasoning-based methods (CoT, RAP, or ToT) rely solely on the internal representations of the LM and cannot consider external observations. This dependency risks fact hallucination and error propagation while setting a performance ceiling. 3) Adaptability : Current planning frameworks (RAP or ToT) use simple search algorithms such as BFS or cannot leverage environmental feedback to improve planning. Additionally, the agent is static and cannot reuse previous experience or learn from trial and error. While RAP also adopts MCTS, it is constrained to tasks where the LM can become a world model and accurately predict states. These shortcomings limit the ability of LMs to be deployed as general problem-solving agents and form the motivation for LATS.  

4 UNIFYING PLANNING , R EASONING ,AND A CTING",3>2
edaf9c7c-601d-4068-b117-92e5eaa45af5,"ref_ids: 454984221666055348, chunk_ids: 5, Score: 0.1934, Text: # 6 Conclusion
LLM-based RL algorithms have shown generalization across multiple tasks and games. We argue that this ability comes from implicit memory that fits a large number of parameters to the training data, which is inefficient in terms of model size. In contrast, we propose a new approach inspired by the concept of “working memory” called Decision Transformers with Memory (DT-Mem), which stores training experience explicitly in a content-addressable matrix module for later retrieval and use. Evaluation demonstrates that DT-Mem achieves better generalization on Atari games with only $10\\%$ of the model parameters compared to the state-of-the-art method. Furthermore, we demonstrate that fine-tuning DT-Memwith a small amount of data can produce state-of-the-art results on both Atari games and the Meta-World environment, when compared to MDT [22], PDT [37], and HDT [38].  

Limitations The first limitation of our work is the sample efficiency of memory fine-tuning. The $10\\%$ fine-tuning dataset is still sizeable, and we plan to explore more sample-efficient methods in the future. We could for instance consider a setting with more tasks, each one with less data so that the inter-task generalization would be even more crucial to its performance. Additionally, this work does not propose a control strategy for collecting data on a new task. For future work, we plan to investigate online data collection methods, which includes the design and learning of exploration strategies for an efficient fine-tuning on new tasks. Finally, the approach has been intuitively motivated, but it would be valuable to have a theoretical grounding that would show the structural limits of large models and how equipping them with a memory component overcomes them.  

Societal Impact We do not foresee any significant societal impact resulting from our proposed method. The current algorithm is not designed to interact with humans, nor any realistic environment yet. If one chooses to extend our methods to such situations, caution should be exercised to ensure that any safety and ethical concerns are appropriately addressed. As our work is categorized in the offline-RL domain, it is feasible to supplement its training with a dataset that aligns with human intents and values. However, one must be wary that the way our architecture generalizes across tasks is still not well understood and as a consequence we cannot guarantee the generalization of its desirable features: performance, robustness, fairness, etc. By working towards methods that improve the computational efficiency of large models, we contribute to increase their access and reduce their ecological impact.



# A Implementation Details

# A.1 DT-Mem network architecture
Table 3 summarizes the different model configurations used for evaluation. In this section, we describe these model configurations in detail. While Table 3 provides a summary, we will also provide additional information here. DT-Mem, PDT and HDT are all share the same transformer architectures. However, for task-adaptation, HDT utilizes a pre-trained $2.3\\mathbf{M}$ hyper-network, while DT-Mem introduces 147K LoRA parameters. To compare with MDT, we use the same parameter size as reported in [22].  

Table 3: Detailed Model Sizes   


<html><body><table><tr><td>Model</td><td>Layers</td><td>Hidden size (d)</td><td>Heads</td><td>Params</td><td>Memory Size</td><td>Memory Module Params</td></tr><tr><td>HDT</td><td>4</td><td>512</td><td>8</td><td>13M</td><td>N.A.</td><td>N.A.</td></tr><tr><td>MDT-200M</td><td>10</td><td>1280</td><td>20</td><td>200M</td><td>N.A.</td><td>N.A.</td></tr><tr><td>DT-Mem</td><td>4</td><td>512</td><td>8</td><td>13M</td><td>559K</td><td>7M</td></tr></table></body></html>

# A.2 Hyper-parameters
In this section, we will delve into the specifics of the model parameters. Understanding these parameters is key to understanding the workings of the model. It is worth noting that the source code for this model is publicly available at https://github.com/luciferkonn/DT_Mem/tree/main .This allows for a deeper understanding of the model’s inner workings and may facilitate the replication of its results.  

Table 4: Hyperparameters for DT-Mem training   


<html><body><table><tr><td>Hyperparameters</td><td>Value</td></tr><tr><td>K (length of context)</td><td>28</td></tr><tr><td>dropoutrate</td><td>0.1</td></tr><tr><td>maximum epochs</td><td>1000</td></tr><tr><td>steps for each epoch</td><td>1000</td></tr><tr><td>optimizer learning rate</td><td>1e-4</td></tr><tr><td>weight decay</td><td>1e-4</td></tr><tr><td>gradient norm clip</td><td>1.</td></tr><tr><td>data points for each dataset</td><td>500,000</td></tr><tr><td>batch size</td><td>64</td></tr><tr><td>memory slots</td><td>1290</td></tr><tr><td>activation</td><td>GELU</td></tr><tr><td>optimizer</td><td>Adamw</td></tr><tr><td>scheduler</td><td>LambdaLR</td></tr></table></body></html>

# A.3 Training and fine-tuning algorithm
In this section, we present the pre-training DT-Memin Appendix A.3 and fine-tuning DT-Mem with LoRA in Appendix 5.5.  

We pre-train DT-Mem on multiple offline datasets. Each gradient update of the DT-Memmodel considers information from each training task.  

We fine-tune the memory module to adapt to each downstream task. To achieve this, we fix the pre-trained DT-Mem model parameters and add additional LoRA parameters for the memory module feed-forward neural networks. The fine-tune dataset is used to update these LoRA parameters only.

# Algorithm 1 Pre-train DT-Mem
1: for T episodes do   
2: 3: 4: for Sample trajectories Split trajectories into different segments with length K and calculate return-to-go in the Task $\\mathcal{T}_{i}\\in T^{t r a i n}\\;.$ do $\\tau=(s_{0},a_{0},r_{0},\\cdot\\cdot\\cdot\\,,s_{H},a_{H},r_{H})$ m the dataset $\\mathcal{D}_{i}$ .  
input sequence.   
5: Given $\\hat{\\tau}_{t+1:t+K}$ , compute the sequence embedding $e_{s e q}$ .  
6: Update the working memory and retrieve the relative information as $E_{o u t}$   
7: Given $E_{o u t}$ , predict actions $\\tilde{a}_{t}$ , reward $\\tilde{r}_{t}$ , and return-to-go ${\\tilde{R}}_{t}$ .  
8: Compute the loss according to Eqn. 1.   
9: Update all modules parameters.   
10: end for   
11: end for  

Algorithm 2 Fine-tuning DT-Mem  

$\\hat{B}^{q},\\hat{B}^{k},\\hat{B}^{v},\\hat{A}^{q},\\hat{A}^{k},\\hat{A}^{v},B^{q},A^{q},B^{k},A^{k}$ Require: Fine-tuning dataset $\\mathcal{T}^{i}~\\in~T^{t e s t}$ .dataset $\\mathcal{D}^{i}$ for $\\mathcal{T}^{i}$ .Initialize LoRA parameters 1: for T steps do   
2: Split trajectories into different segments with length $\\mathbf{K}$ and calculate return-to-go in the input sequence.   
3: Given $\\hat{\\tau}_{t+1:t+K}$ , compute the sequence embedding $e_{s e q}$ .  
4: Update working memory using $\\hat{Q}\\,=\\,M(\\hat{W}^{q}+\\hat{B}^{q}\\bar{A}^{q})$ ,$\\hat{K}\\,=\\,M(\\hat{W}^{k}\\,+\\,\\hat{B}^{k}\\hat{A}^{k}),\\hat{V}\\,=$ $M(\\hat{W}^{v}+\\hat{B}^{v}\\hat{A}^{v})$ ,$Q=M(W^{q}+B^{q}A^{q}),K=M(W^{k}+B^{k}A^{k})$   
5: Retrieve the relative information as $E_{o u t}$   
6: Given $E_{o u t}$ , predict actions $\\tilde{a}_{t}$ , reward $\\tilde{r}_{t}$ , and return-to-go ${\\tilde{R}}_{t}$ .  
7: Compute the loss according to Eqn. 1.   
8: Update LoRA parameters only.   
9: end for",3>2
