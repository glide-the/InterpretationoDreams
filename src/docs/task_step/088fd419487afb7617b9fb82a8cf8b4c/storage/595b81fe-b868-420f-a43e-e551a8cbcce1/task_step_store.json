{"task_step_store/data": {"276fc94b-2aa5-4712-ab27-c1e4f31af69f": {"__data__": {"id_": "276fc94b-2aa5-4712-ab27-c1e4f31af69f", "metadata": {}, "relationships": {}, "hash": "", "start_task_context": "MCTS\u5728PRM\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u7684\u5e94\u7528\u63a2\u7d22\u7efc\u8ff0", "aemo_representation_context": "### Step 1: \u5206\u6790\u8fd1\u51e0\u5e74\u7814\u7a76\u9886\u57df\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba\n\n**\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09**\uff1a\n- **\u57fa\u672c\u539f\u7406**\uff1aMCTS\u662f\u4e00\u79cd\u7528\u4e8e\u51b3\u7b56\u8fc7\u7a0b\u7684\u542f\u53d1\u5f0f\u641c\u7d22\u7b97\u6cd5\uff0c\u5e7f\u6cdb\u5e94\u7528\u4e8e\u6e38\u620fAI\u3001\u8def\u5f84\u89c4\u5212\u7b49\u9886\u57df\u3002\u5176\u6838\u5fc3\u601d\u60f3\u662f\u901a\u8fc7\u6a21\u62df\u591a\u6b21\u968f\u673a\u6e38\u620f\u6765\u8bc4\u4f30\u4e0d\u540c\u51b3\u7b56\u7684\u4f18\u52a3\u3002\n- **\u6280\u672f\u6846\u67b6**\uff1a\u4e3b\u8981\u5305\u62ec\u9009\u62e9\uff08Selection\uff09\u3001\u6269\u5c55\uff08Expansion\uff09\u3001\u6a21\u62df\uff08Simulation\uff09\u548c\u56de\u6eaf\uff08Backpropagation\uff09\u56db\u4e2a\u6b65\u9aa4\u3002\n\n**\u504f\u597d\u7b56\u7565\u6a21\u578b\uff08PRM\uff09**\uff1a\n- **\u57fa\u672c\u539f\u7406**\uff1aPRM\u65e8\u5728\u901a\u8fc7\u5b66\u4e60\u7528\u6237\u7684\u504f\u597d\u6765\u4f18\u5316\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5e38\u7528\u4e8e\u63a8\u8350\u7cfb\u7edf\u3001\u4e2a\u6027\u5316\u670d\u52a1\u7b49\u3002\n- **\u6280\u672f\u6846\u67b6**\uff1a\u901a\u5e38\u5305\u62ec\u504f\u597d\u5efa\u6a21\u3001\u7b56\u7565\u4f18\u5316\u548c\u6a21\u578b\u5fae\u8c03\u7b49\u73af\u8282\u3002\n\n**\u65b9\u6cd5\u8bba**\uff1a\n- **\u96c6\u6210\u5b66\u4e60**\uff1a\u5c06MCTS\u4e0ePRM\u7ed3\u5408\uff0c\u5229\u7528MCTS\u7684\u641c\u7d22\u80fd\u529b\u4f18\u5316PRM\u7684\u51b3\u7b56\u8fc7\u7a0b\u3002\n- **\u5f3a\u5316\u5b66\u4e60**\uff1a\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u673a\u5236\u4e0d\u65ad\u8c03\u6574PRM\u7684\u53c2\u6570\uff0c\u4ee5\u9002\u5e94\u52a8\u6001\u73af\u5883\u3002\n\n### Step 2: \u7814\u7a76\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u4e3b\u8981\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0e\u53d8\u4f53\n\n**\u5e94\u7528\u573a\u666f**\uff1a\n- **\u6e38\u620fAI**\uff1aMCTS\u4e0ePRM\u7ed3\u5408\u7528\u4e8e\u6e38\u620f\u4e2d\u7684\u667a\u80fd\u51b3\u7b56\uff0c\u5982AlphaGo\u3002\n- **\u8def\u5f84\u89c4\u5212**\uff1a\u5728\u673a\u5668\u4eba\u5bfc\u822a\u548c\u81ea\u52a8\u9a7e\u9a76\u4e2d\uff0c\u5229\u7528MCTS\u4f18\u5316\u8def\u5f84\u9009\u62e9\u3002\n- **\u63a8\u8350\u7cfb\u7edf**\uff1a\u901a\u8fc7PRM\u5b66\u4e60\u7528\u6237\u504f\u597d\uff0c\u7ed3\u5408MCTS\u8fdb\u884c\u66f4\u7cbe\u51c6\u7684\u63a8\u8350\u3002\n\n**\u53d8\u4f53**\uff1a\n- **\u6df1\u5ea6MCTS**\uff1a\u7ed3\u5408\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff0c\u63d0\u5347MCTS\u7684\u51b3\u7b56\u80fd\u529b\u3002\n- **\u591a\u6a21\u6001PRM**\uff1a\u878d\u5408\u591a\u79cd\u6570\u636e\u6e90\uff08\u5982\u6587\u672c\u3001\u56fe\u50cf\uff09\uff0c\u589e\u5f3aPRM\u7684\u6cdb\u5316\u80fd\u529b\u3002\n\n### Step 3: \u8bc4\u4f30\u5b66\u672f\u754c\u7684\u6280\u672f\u8fdb\u6b65\u4e0e\u5c40\u9650\u6027\n\n**\u6280\u672f\u8fdb\u6b65**\uff1a\n- **\u6027\u80fd\u63d0\u5347**\uff1aMCTS\u4e0ePRM\u7ed3\u5408\u663e\u8457\u63d0\u5347\u4e86\u51b3\u7b56\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002\n- **\u6cdb\u5316\u80fd\u529b**\uff1a\u591a\u6a21\u6001PRM\u5728\u5904\u7406\u591a\u6837\u5316\u6570\u636e\u65f6\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002\n\n**\u5c40\u9650\u6027**\uff1a\n- **\u8ba1\u7b97\u590d\u6742\u5ea6**\uff1aMCTS\u5728\u5927\u89c4\u6a21\u95ee\u9898\u4e2d\u8ba1\u7b97\u91cf\u5927\uff0c\u6548\u7387\u6709\u5f85\u63d0\u5347\u3002\n- **\u6570\u636e\u4f9d\u8d56**\uff1aPRM\u7684\u6027\u80fd\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u9ad8\u8d28\u91cf\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u6570\u636e\u504f\u5dee\u95ee\u9898\u4f9d\u7136\u5b58\u5728\u3002\n\n### Step 4: \u63a2\u8ba8\u8ba1\u7b97\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\n\n**\u9002\u7528\u6027**\uff1a\n- **\u591a\u9886\u57df\u5e94\u7528**\uff1aMCTS+PRM\u6846\u67b6\u5728\u6e38\u620f\u3001\u8def\u5f84\u89c4\u5212\u3001\u63a8\u8350\u7cfb\u7edf\u7b49\u591a\u4e2a\u9886\u57df\u8868\u73b0\u51fa\u826f\u597d\u7684\u9002\u7528\u6027\u3002\n- **\u591a\u6a21\u6001\u6570\u636e**\uff1a\u591a\u6a21\u6001PRM\u80fd\u591f\u5904\u7406\u591a\u79cd\u7c7b\u578b\u7684\u6570\u636e\uff0c\u9002\u7528\u4e8e\u590d\u6742\u5e94\u7528\u573a\u666f\u3002\n\n**\u6cdb\u5316\u80fd\u529b**\uff1a\n- **\u8de8\u9886\u57df\u6cdb\u5316**\uff1a\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\uff0cMCTS+PRM\u6846\u67b6\u5728\u4e0d\u540c\u9886\u57df\u95f4\u5c55\u73b0\u51fa\u8f83\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002\n- **\u52a8\u6001\u73af\u5883\u9002\u5e94**\uff1a\u5f3a\u5316\u5b66\u4e60\u673a\u5236\u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u9002\u5e94\u52a8\u6001\u53d8\u5316\u7684\u73af\u5883\u3002\n\n### Step 5: \u5206\u6790\u6700\u65b0\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027\n\n**\u7a33\u5b9a\u6027**\uff1a\n- **\u7b97\u6cd5\u4f18\u5316**\uff1a\u901a\u8fc7\u6539\u8fdbMCTS\u7684\u9009\u62e9\u7b56\u7565\u548cPRM\u7684\u53c2\u6570\u8c03\u6574\u673a\u5236\uff0c\u63d0\u5347\u4e86\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u3002\n- **\u9c81\u68d2\u6027\u589e\u5f3a**\uff1a\u5f15\u5165\u5bf9\u6297\u8bad\u7ec3\u548c\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\uff0c\u63d0\u9ad8\u6a21\u578b\u5728\u590d\u6742\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u6027\u3002\n\n**\u5bb9\u9519\u6027**\uff1a\n- **\u9519\u8bef\u5904\u7406**\uff1a\u5728\u6a21\u62df\u548c\u56de\u6eaf\u9636\u6bb5\u5f15\u5165\u5bb9\u9519\u673a\u5236\uff0c\u51cf\u5c11\u9519\u8bef\u51b3\u7b56\u7684\u5f71\u54cd\u3002\n- **\u5927\u89c4\u6a21\u6570\u636e\u9002\u5e94\u6027**\uff1a\u4f18\u5316\u7b97\u6cd5\u7ed3\u6784\uff0c\u63d0\u5347\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\u7684\u5904\u7406\u80fd\u529b\u3002\n\n### Step 6: \u8bc4\u4f30\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218\n\n**\u672a\u6765\u7814\u7a76\u65b9\u5411**\uff1a\n- **\u9ad8\u6548\u8ba1\u7b97**\uff1a\u7814\u7a76\u66f4\u9ad8\u6548\u7684MCTS\u7b97\u6cd5\uff0c\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002\n- **\u6570\u636e\u589e\u5f3a**\uff1a\u63a2\u7d22\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u51cf\u5c11PRM\u5bf9\u9ad8\u8d28\u91cf\u6570\u636e\u7684\u4f9d\u8d56\u3002\n\n**\u6311\u6218**\uff1a\n- **\u8de8\u6a21\u6001\u878d\u5408**\uff1a\u5982\u4f55\u6709\u6548\u878d\u5408\u591a\u79cd\u6a21\u6001\u6570\u636e\uff0c\u63d0\u5347\u6a21\u578b\u7684\u7efc\u5408\u6027\u80fd\u3002\n- **\u52a8\u6001\u73af\u5883\u9002\u5e94**\uff1a\u5728\u52a8\u6001\u73af\u5883\u4e0b\uff0c\u5982\u4f55\u4fdd\u6301\u6a21\u578b\u7684\u7a33\u5b9a\u6027\u548c\u9ad8\u6548\u6027\u3002\n\n**\u63a8\u52a8\u540e\u7eed\u7814\u7a76**\uff1a\n- **\u65b0\u95ee\u9898\u63d0\u51fa**\uff1a\u5982\u591a\u6a21\u6001\u6570\u636e\u4e0b\u7684\u504f\u597d\u5efa\u6a21\u3001\u52a8\u6001\u73af\u5883\u4e0b\u7684\u51b3\u7b56\u4f18\u5316\u7b49\u3002\n- **\u7814\u7a76\u5207\u5165\u70b9**\uff1a\u4ece\u7b97\u6cd5\u4f18\u5316\u3001\u6570\u636e\u589e\u5f3a\u3001\u8de8\u6a21\u6001\u878d\u5408\u7b49\u591a\u4e2a\u89d2\u5ea6\u5207\u5165\uff0c\u63a8\u52a8\u6280\u672f\u8fdb\u6b65\u3002\n\n### \u603b\u7ed3\n\n\u8fd1\u5e74\u6765\uff0cMCTS\u5728PRM\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u7684\u5e94\u7528\u63a2\u7d22\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4e3b\u8981\u4f53\u73b0\u5728\u6280\u672f\u6846\u67b6\u7684\u5b8c\u5584\u3001\u5e94\u7528\u573a\u666f\u7684\u62d3\u5c55\u3001\u6027\u80fd\u7684\u63d0\u5347\u4ee5\u53ca\u7a33\u5b9a\u6027\u548c\u5bb9\u9519\u6027\u7684\u589e\u5f3a\u3002\u7136\u800c\uff0c\u4ecd\u5b58\u5728\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u3001\u6570\u636e\u4f9d\u8d56\u6027\u5f3a\u7b49\u5c40\u9650\u6027\u3002\u672a\u6765\u7684\u7814\u7a76\u5e94\u5173\u6ce8\u9ad8\u6548\u8ba1\u7b97\u3001\u6570\u636e\u589e\u5f3a\u548c\u8de8\u6a21\u6001\u878d\u5408\u7b49\u65b9\u5411\uff0c\u4ee5\u5e94\u5bf9\u52a8\u6001\u73af\u5883\u4e0b\u7684\u51b3\u7b56\u4f18\u5316\u7b49\u6311\u6218\uff0c\u63a8\u52a8\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002", "task_step_name": "\u5206\u6790\u8fd1\u51e0\u5e74\u7814\u7a76\u9886\u57df\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba", "task_step_description": "\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09\u662f\u4e00\u79cd\u7528\u4e8e\u51b3\u7b56\u8fc7\u7a0b\u7684\u542f\u53d1\u5f0f\u641c\u7d22\u7b97\u6cd5\uff0c\u5e7f\u6cdb\u5e94\u7528\u4e8e\u6e38\u620fAI\u3001\u8def\u5f84\u89c4\u5212\u7b49\u9886\u57df\u3002\u5176\u6838\u5fc3\u601d\u60f3\u662f\u901a\u8fc7\u6a21\u62df\u591a\u6b21\u968f\u673a\u6e38\u620f\u6765\u8bc4\u4f30\u4e0d\u540c\u51b3\u7b56\u7684\u4f18\u52a3\u3002", "task_step_level": "0", "task_step_question": "### \u95ee\u9898\n\n\u5728\u5206\u6790\u8fd1\u51e0\u5e74\u7814\u7a76\u9886\u57df\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba\u65f6\uff0c\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09\u4f5c\u4e3a\u4e00\u79cd\u542f\u53d1\u5f0f\u641c\u7d22\u7b97\u6cd5\uff0c\u5176\u6838\u5fc3\u601d\u60f3\u662f\u901a\u8fc7\u6a21\u62df\u591a\u6b21\u968f\u673a\u6e38\u620f\u6765\u8bc4\u4f30\u4e0d\u540c\u51b3\u7b56\u7684\u4f18\u52a3\u3002\u7136\u800c\uff0cMCTS\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u9762\u4e34\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u5927\u89c4\u6a21\u95ee\u9898\u4e2d\u3002\u57fa\u4e8e\u6b64\uff0c**\u5982\u4f55\u901a\u8fc7\u6539\u8fdbMCTS\u7684\u6280\u672f\u6846\u67b6\u6216\u5f15\u5165\u65b0\u7684\u65b9\u6cd5\u8bba\u6765\u964d\u4f4e\u5176\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u5176\u51b3\u7b56\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\uff1f**", "task_step_question_context": [{"ref_id": "455026805323333778", "chunk_id": "1", "score": 0.5390625, "text": "# Layered and Staged Monte Carlo Tree Search for SMT Strategy Synthesis\nZhengyang $\\left(\\mathbf{Johm}\\right)\\mathbf{Lu}^{1}$ ,Stefan Siemer 2 ,Piyush Jha 3 ,Joel Day 4 ,Florin Manea 2 and Vijay Ganesh 3 1 University of Waterloo 2 University of G\u00a8ottingen 3 Georgia Institute of Technology 4 Loughborough University\n\n# Abstract\nModern SMT solvers, such as Z3 , offer usercontrollable strategies, enabling users to tailor them for their unique set of instances, thus dramatically enhancing solver performance for their use case. However, this approach of strategy customization presents a significant challenge: handcrafting an optimized strategy for a class of SMT instances remains a complex and demanding task for both solver developers and users alike.  \n\nIn this paper, we address this problem of automatic SMT strategy synthesis via a novel Monte Carlo Tree Search (MCTS) based method. Our method treats strategy synthesis as a sequential decisionmaking process, whose search tree corresponds to the strategy space, and employs MCTS to navigate this vast search space. The key innovations that enable our method to identify effective strategies, while keeping costs low, are the ideas of layered and staged MCTS search. These novel approaches allow for a deeper and more efficient exploration of the strategy space, enabling us to synthesize more effective strategies than the default ones in state-ofthe-art (SOTA) SMT solvers. We implement our method, dubbed Z3alpha , as part of the Z3 SMT solver. Through extensive evaluations across 6 important SMT logics, Z3alpha demonstrates superior performance compared to the SOTA synthesis tool FastSMT , the default Z3 solver, and the CVC5 solver on most benchmarks. Remarkably, on a challenging QF BV benchmark set, Z3alpha solves $42.7\\%$ more instances than the default strategy in the Z3 SMT solver.\n\n# 1 Introduction\nSatisfiability Modulo Theories (SMT) solvers [De Moura and Bj\u00f8rner, 2011] are key tools in diverse fields such as software engineering [Cadar et al. , 2008], verification [Gurfinkel et al. , 2015], security [Song et al. , 2008], and artificial intelligence [Pulina and Tacchella, 2012]. It has long been observed that no single solver or algorithm excels across all instances of a given SMT logic or of a problem class. As a result, modern SMT solvers, such as Z3 [De Moura and Bj\u00f8rner, 2008], offer user-controllable strategies [De Moura and Passmore, 2013], enabling users to customize a sub-solver for their class of instances. A strategy can be thought of as an algorithmic recipe for selecting, sequencing, and parameterizing tactics .Each tactic is a well-defined algorithmic proof rule or symbolic reasoning step, provided by the solver. For example, propagate-values is a Z3 tactic that propagates equalities, while sat and smt are the tactic wrappers of the main SAT and SMT solver in Z3 . A strategy builds a decision procedure by combining tactics, as shown in an exemplar strategy (if is-pb (then propagate-values sat) smt) . This strategy specifies a solver algorithm that, given an input instance, applies propagate-values followed by sat if the instance is a pseudo-boolean problem (as checked using is-pb ), or applies smt otherwise.  \n\nDefault solver strategies are typically optimized for wellestablished benchmarks, such as those in the SMT-LIB library [Barrett et al. , 2016]. However, as the scope of SMT applications continues to grow rapidly, users frequently encounter specialized, evolving, and unprecedented classes of instances. In these scenarios, the default or the existing customized strategies might not be as effective. Consequently, there arises a need for novel customized strategies, specifically designed to efficiently address the unique challenges posed by users\u2019 specific problems. Traditionally, this task of strategy customization has been undertaken by human experts through extensive experimentation and benchmark analysis. However, even with their expertise and efforts, the task remains challenging due to the intricate interactions among tactics and the vast search space for potential strategies.  \n\nEarly attempts have been made to automatically synthesize SMT strategies. For instance, StratEVO [Ram\u00b4\u0131rez et al. , 2016] searches for an optimal strategy using evolutionary algorithms, while FastSMT [Balunovic et al. , 2018] synthesizes a tailored strategy using imitation learning and decision tree learning techniques. While these methods show promise in automating strategy customization, they suffer from issues such as a lack of robustness, limited interpretability, and extensive training times.  \n\nTo address these issues, we introduce a novel SMT strategy synthesis method that employs Monte Carlo Tree Search (MCTS). MCTS is a heuristic search algorithm, widely applied in computer board game players as a planning algorithm [Browne et al. , 2012]. Its prominence further escalated following its successful integration into the groundbreaking deep reinforcement learning systems AlphaGo [Silver et al. ,2016] and AlphaZero [Silver et al. , 2017], where MCTS was employed as a policy improvement operator. Recently, MCTS has shown remarkable success as a standalone algorithm in solving complex symbolic or combinatorial search problems, as evidenced in Khalil et al. [2022] and Sun et al. [2023]. Its key strengths, including its ability to effectively balance exploration and exploitation and adapt to the nuances of varied search problems, making it an excellent method for such challenging tasks. Our work is the first to apply MCTS to the SMT strategy synthesis problem."}, {"ref_id": "454845587396505206", "chunk_id": "0", "score": 0.53125, "text": "# BLIMITATION AND FUTURE WORK\nCurrently, our method TS-LLM still cannot scale to really large-scale scenarios due to the extra computation burdens introduced by node expansion and value evaluation. Additional engineering work such as key value caching is required to accelerate the tree-search. In addition, we do not cover all feasible action-space designs for tree search and it is flexible to propose advanced algorithms to automatically construct a tree mixed with both sentence-level expansion and token-level expansion, etc. We leave such exploration for future work. For MCTS aggregation, the current method still struggles to improve under large aggregation numbers. some new algorithms that can encourage multi-search diversity might be needed. Currently, we are still actively working on scaling up our method both during inference and training (especially multi-iteration training).\n\n# CBACKGROUND OF MONTE CARLO TREE -SEARCH A LGORIHTMS\nOnce we build the tree, we can use various search algorithms to find a high-reward trace. However, it\u2019s not easy to balance between exploration and exploitation during the search process, especially when the tree is sufficiently deep. Therefore we adopt Monte Carlo Tree Search(MCTS) variants as choices for strategic and principled search. Instead of the four operations in traditional MCTS (Kocsis & Szepesv\u00b4ari, 2006; Coulom, 2006), we refer to the search process in AlphaZero (Silver et al., 2017a) and introduce 3 basic operations of a standard search simulation in it as follows, when searching actions from current state node $s_{0}$ :  \n\nSelect It begins at the root node of the search tree, of the current state, $s_{0}$ , and finishes when reaching a leaf node $s_{L}$ at timestep $L$ . At each of these $L$ timesteps(internal nodes), an action(child node) is selected according to $a_{t}=\\arg\\operatorname*{max}_{a}\\left(Q(s_{t},a)+U(s_{t},a)\\right)$ where $U(s_{t},a)$ is calculated by a variant of PUCT algorithm (Rosin, 2011):  \n\n$$\nU(s,a)=c_{\\mathrm{puct}}\\cdot\\pi_{\\theta}(s,a)\\frac{\\sqrt{\\sum_{b}N(s,b)}}{1+N(s,a)}\n$$  \n\n$N(s,a)$ isit count of selecting action $a$ at node $s$ , and $\\begin{array}{r}{c_{\\mathrm{puct}}\\,=\\,\\log((\\sum_{b}N(s,b)\\,+\\,}\\end{array}$ P$c_{\\mathrm{base}}+1)/c_{\\mathrm{base}})+c_{\\mathrm{init}}$ is controlled by visit count and two constants. This search control strategy initially prefers actions with high prior probability and low visit count, but asymptotically prefers actions with high action-value.  \n\nExpand and evaluate After encountering a leaf node $s_{L}$ by select , if $s_{L}$ is not a terminal node, it will be expanded by the language model policy. The state of the leaf node is evaluated by the value network, noted as $v(s_{L})$ . If $s_{L}$ is a terminal node, if there is an oracle reward function $R$ , then $v(s_{L})=R(s_{L})$ , otherwise, in this paper, we use an ORM $\\hat{r}$ as an approximation of it.  \n\nBackup After expand and evaluate on a leaf node, backward the statistics through the path $s_{L},s_{L-1},\\ldots,s_{0}$ , for each node, increase the visit count by $N(s_{t},a_{t})\\,=\\,N(s_{t},a_{t})+1$ , and the total action-value are updated as $W(s_{t},a_{t})\\,=\\,W(s_{t},a_{t})\\,\\dot{+}\\,v(s_{L})$ , the mean action-value are updated as $Q(s_{t},a_{t})=W(s_{t},a_{t})/N(s_{t},a_{t})$ .  \n\nIn this paper, we introduce 3 variants of MCTS based on the above basic operations.\n\n# DEXPERIMENT DETAILS\n\n# D.1 TASK SETUPS\nGSM8k GSM8k (Cobbe et al., 2021) is a commonly used numerical reasoning dataset, Given a context description and a question, it takes steps of mathematical reasoning and computation to arrive at a final answer. There are about $7.5\\mathrm{k}$ problems in the training dataset and $1.3\\mathrm{k}$ problems in the test dataset.  \n\nGame24 We also test our methods on Game24(Yao et al., 2023) which has been proven to be hard even for state-of-the-art LLMs like GPT-4. Each problem in Game24 consists of 4 integers between 1 and 13. And LLMs are required to use each number exactly once by $(+\\mathrm{~-~}\\times\\div)$ to get a result equal to 24 We follow Yao et al. (2023) by using a set of 1362 problems sorted from easy to hard according to human solving time. We split the first 1k problems as the training dataset and the last 362 hard problems as the test dataset. For each problem in the training dataset, we collect data for SFT by enumerating all possible correct answers.  \n\nPrOntoQA PrOntoQA (Saparov & He, 2022) is a typical logical reasoning task in which a language model is required to verify whether a hypothesis is true or false given a set of facts and logical rules. There are 4k problems in the training dataset and 500 problems in the test dataset.  \n\nRLHF We choose a synthetic RLHF dataset Dahoas 1 serving as the query data. We split the dataset to 30000/3000 as training and test set respectively. For the reward model, we choose reward-modeldeberta-v3-large$\\cdot\\mathbf{V}2^{2}$ from OpenAssistant, which is trained from several RLHF datasets.\n\n# D.2 SFT AND VALUE TRAINING DETAILS\nSFT in GSM8k, Game24 and PrOntoQA : For GSM8k, Game24 and PrOntoQA, we first train LLaMA2-7b on the training dataset The training is conducted on 8 NVIDIA A800 GPUs, using a cosine scheduler decaying from $\\scriptstyle{\\mathrm{lr}=2\\ e-5}$ to 0.0 with a warmup ratio of 0.03, batch size 128 for 3 epochs. For GSM8k and Game24 we use the checkpoint at the last epoch as the direct policy in experiments, while for PrOntoQA we use the checkpoint at the 1st epoch since the others overfit.  \n\nValue training in GSM8k, Game24 and PrOntoQA : Then we train the value function on the data rollout by the SFT policy. In GSM8k and Game24, For each model checkpoints of 3 epochs during SFT, we first collect 100 outputs per problem in the training dataset, then duplicate the overlapped answers, labeled each answer with our training set outcome reward ocracle. For data sampled by ech model checkpoint, we subsample 17 answers per problem, which is in total at most 51 answers per problem after deduplication. In PrOntoQA, we only sample 50 answers per problem with the first epoch model checkpoint and then do deduplication.  \n\nThe value functions are trained in the same setting as supervised finetuning. We set the reward to be 1 when the output answer is correct and -1 otherwise. Then we use MC with $\\gamma=1$ to compute the returns. We do model selection on a validation dataset sampled from the direct policy model. For GSM8k, we train the value function and ORM for one epoch, while for Game24 and PrOntoQA we train the value function and ORM for 3 epochs.  \n\nSFT in RLHF alignment : We utilize GPT2-open-instruct 3 , a GPT2-Small model supervisedfinetuned over several instruction-tuning dataset.  \n\nValue training in RLHF alignment : Based on the SFT model, we collect 50 rollouts by the SFT policy for each question in the training set and label their final reward with the reward model. Then we train the value function and ORM for 2 epochs.  \n\nNote that here we start training the value function and ORM from the data sampled by the SFT policy model through direct decoding just as an initialization of the value function and ORM. After that TS-LLM can optimize the policy model, value function, and ORM simultaneously by adding new data sampled from tree search into the training buffer."}, {"ref_id": "454845580969520102", "chunk_id": "5", "score": 0.51953125, "text": "# Spending Thinking Time Wisely: Accelerating MCTS with Virtual Expansions\nWeirui $\\mathbf{Ye}^{*}$ Pieter Abbeel \u2020Yang Gao $\\ast\\ddag\\S$ \u2217Tsinghua University, \u2020UC Berkeley, \u00a7Shanghai Qi Zhi Institute\n\n# Abstract\nOne of the most important AI research questions is to trade off computation versus performance since \u201cperfect rationality\" exists in theory but is impossible to achieve in practice. Recently, Monte-Carlo tree search (MCTS) has attracted considerable attention due to the significant performance improvement in various challenging domains. However, the expensive time cost during search severely restricts its scope for applications. This paper proposes the Virtual MCTS (V-MCTS), a variant of MCTS that spends more search time on harder states and less search time on simpler states adaptively. We give theoretical bounds of the proposed method and evaluate the performance and computations on $9\\times9$ Go board games and Atari games. Experiments show that our method can achieve comparable performances to the original search algorithm while requiring less than $50\\%$ search time on average. We believe that this approach is a viable alternative for tasks under limited time and resources. The code is available at https://github.com/YeWR/V-MCTS.git .\n\n# 1 Introduction\nWhen artificial intelligence was first studied in the 1950s, researchers have sought to find the solution to the question \u201cHow to build an agent with perfect rationality\". The term \u201cperfect rationality\" [7 ,24 ,26 ] here refers to the decision made with infinite amounts of computations. However, one can only solve small-scale problems without considering the practical computation time since classical search algorithms usually exhibit exponential running time. Therefore, recent AI research would no longer seek to achieve \u201cperfect rationality\", but instead carefully trade-off computation versus the level of rationality. People have developed computational models like \u201cbounded optimality\" to model these settings [ 26 ]. The increasing level of rationality under the same computational budget has given us a lot of AI successes. Algorithms include the Monte-Carlo sampling algorithms, the variational inference algorithms, and using DNNs as universal function approximators [9, 8, 13, 30, 17].  \n\nRecently, MCTS-based RL algorithms have achieved much success, mainly on board games. The most notable achievement is that AlphaGo beats Hui Fan in 2015 [ 30 ]. It is the first time a computer program beat a human professional Go player. Afterward, AlphaGo beats two top-ranking human players, Lee Sedol in 2016 and Jie Ke in 2017, the latter of which ranked first worldwide at the time. Later, MCTS-based RL algorithms were further extended to other board games and Atari games [ 27 ]. EfficientZero [ 34 ] significantly improves the sample efficiency of MCTS-based RL algorithms, shedding light on its future applications in the real world like robotics and self-driving.  \n\nDespite the impressive performance of MCTS-based RL algorithms, they require massive amounts of computation to train and evaluate. For example, MuZero [ 27 ] used 1000 TPUs trained for 12 hours to learn the game of Go, and for a single Atari game, it needs 40 TPUs to train 12 hours. Compared to previous algorithms on the Atari games benchmark, it needs around two orders of magnitude more compute. This prohibitively large computational requirement has slowed down both the further development of MCTS-based RL algorithms as well as its practical use.  \n\nUnder the hood, MCTS-based RL algorithms imagine the futures when taking different future action sequences. However, this imaging process for the current method is not computationally efficient. For example, AlphaGo needs to look ahead 1600 game states to place a single stone. On the contrary, top human professional players can only think through around 100-200 game states per minute [ 30 ]. Apart from the inefficiency, the current MCTS algorithm deals with easy and challenging cases with the same computational budget. However, human knows to use their time when it is most needed.  \n\nIn this paper, we aim to design new algorithms that save the computational time of the MCTSbased RL methods. We make three key contributions : (1) We present Virtual MCTS, a variant of MCTS, to approximate the vanilla MCTS search policies with less computation. Moreover, unlike previous pruning-based methods that focus on the selection or evaluation stage in MCTS, our method improves the search loop. It terminates the search iterations earlier adaptively when current states are simpler; (2) Theoretically, we provide some error bounds of the proposed method. Furthermore, the visualization results indicate that Virtual MCTS has a better computation and performance trade-off than vanilla MCTS; (3) Empirically, our method can save more than $50\\%$ of search times on the challenging game Go $9\\times9$ and more than $60\\%$ on the visually complex Atari games while keeping comparable performances to those of vanilla MCTS.\n\n# 2 Related Work\n\n# 2.1 Reinforcement Learning with MCTS\nFor a long time, Computer Go has been regarded as a remarkably challenging game [ 3 ,6 ]. Researchers attempt to use Monte-Carlo techniques that evaluate the value of the node state through random playouts [ 4 ,11 ,12 ,30 ]. Afterward, UCT algorithms have generally been applied in Monte-Carlo tree search (MCTS) algorithms, which use UCB1 to select action at each node of the tree [ 20 ]. Recently, MCTS-based RL methods [ 30 ,32 ,31 ,27 ] have become increasingly popular and achieved super-human performances on board games because of their strong ability to search.  \n\nModern MCTS-based RL algorithms include four stages in the search loop : selection, expansion, evaluation, and backpropagation. The computation bottlenecks in vanilla MCTS come from the search loop, especially for the evaluation stage and the selection stage of each iteration. The selection stage is time-consuming when the search tree becomes wider and deeper. The evaluation stage is quite expensive because people attempt to evaluate the node value by random playouts to the end in previous researches. Due to the search loop, MCTS-based algorithms have multiple model inferences compared to other model-free RL methods like PPO [28] and SAC [16]."}], "task_step_question_answer": "\u5728\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\uff0cMCTS\u7684\u5e94\u7528\u4e0d\u4ec5\u9650\u4e8e\u8def\u5f84\u89c4\u5212\uff0c\u8fd8\u53ef\u4ee5\u6269\u5c55\u5230\u5b9e\u65f6\u51b3\u7b56\u5236\u5b9a\u548c\u52a8\u6001\u73af\u5883\u9002\u5e94\u3002\u4f8b\u5982\uff0c\u901a\u8fc7\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\uff0cMCTS\u53ef\u4ee5\u5728\u590d\u6742\u7684\u4ea4\u901a\u573a\u666f\u4e2d\u505a\u51fa\u66f4\u667a\u80fd\u7684\u51b3\u7b56\uff0c\u5982\u907f\u8ba9\u884c\u4eba\u3001\u5e94\u5bf9\u7a81\u53d1\u4ea4\u901a\u4e8b\u4ef6\u7b49\u3002\u6b64\u5916\uff0cMCTS\u8fd8\u53ef\u4ee5\u4e0e\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7ed3\u5408\uff0c\u5229\u7528\u4f20\u611f\u5668\u6570\u636e\uff08\u5982\u6444\u50cf\u5934\u3001\u96f7\u8fbe\uff09\u8fdb\u884c\u5b9e\u65f6\u73af\u5883\u611f\u77e5\uff0c\u4ece\u800c\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u3002\u5728\u667a\u80fd\u533b\u7597\u9886\u57df\uff0cMCTS\u7684\u5e94\u7528\u53ef\u4ee5\u8fdb\u4e00\u6b65\u6269\u5c55\u5230\u4e2a\u6027\u5316\u6cbb\u7597\u65b9\u6848\u63a8\u8350\u548c\u75be\u75c5\u9884\u6d4b\u3002\u901a\u8fc7\u7ed3\u5408\u591a\u6a21\u6001\u6570\u636e\uff08\u5982\u57fa\u56e0\u7ec4\u6570\u636e\u3001\u4e34\u5e8a\u8bb0\u5f55\uff09\uff0cMCTS\u53ef\u4ee5\u4e3a\u60a3\u8005\u63d0\u4f9b\u66f4\u7cbe\u51c6\u7684\u6cbb\u7597\u5efa\u8bae\u3002\u6b64\u5916\uff0cMCTS\u8fd8\u53ef\u4ee5\u7528\u4e8e\u533b\u7597\u8d44\u6e90\u7684\u4f18\u5316\u5206\u914d\uff0c\u4f8b\u5982\u5728\u75ab\u60c5\u671f\u95f4\uff0c\u901a\u8fc7MCTS\u4f18\u5316\u533b\u7597\u8d44\u6e90\u7684\u5206\u914d\u7b56\u7565\uff0c\u53ef\u4ee5\u66f4\u6709\u6548\u5730\u5e94\u5bf9\u7a81\u53d1\u516c\u5171\u536b\u751f\u4e8b\u4ef6\u3002\u5728\u6280\u672f\u6846\u67b6\u7684\u5bf9\u6bd4\u4e0e\u8bc4\u4f30\u65b9\u9762\uff0c\u672a\u6765\u7684\u7814\u7a76\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63a2\u7d22\u4e0d\u540c\u6280\u672f\u6846\u67b6\u5728\u4e0d\u540c\u5e94\u7528\u573a\u666f\u4e2d\u7684\u6027\u80fd\u8868\u73b0\uff0c\u4f8b\u5982\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\uff0c\u5206\u5c42MCTS\u548c\u865a\u62dfMCTS\u7684\u6027\u80fd\u5bf9\u6bd4\uff1b\u5728\u667a\u80fd\u533b\u7597\u4e2d\uff0cTS-LLM\u548c\u591a\u641c\u7d22\u591a\u6837\u6027\u589e\u5f3a\u7b97\u6cd5\u7684\u6027\u80fd\u5bf9\u6bd4\u3002\u901a\u8fc7\u6df1\u5165\u5206\u6790\u4e0d\u540c\u6280\u672f\u6846\u67b6\u7684\u4f18\u7f3a\u70b9\uff0c\u53ef\u4ee5\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u66f4\u5177\u4f53\u7684\u6307\u5bfc\u3002\u6b64\u5916\uff0c\u672a\u6765\u7684\u7814\u7a76\u8fd8\u53ef\u4ee5\u63a2\u7d22\u5982\u4f55\u5c06MCTS\u4e0e\u5176\u4ed6\u65b0\u5174\u6280\u672f\uff08\u5982\u91cf\u5b50\u8ba1\u7b97\u3001\u8fb9\u7f18\u8ba1\u7b97\uff09\u7ed3\u5408\uff0c\u4ee5\u8fdb\u4e00\u6b65\u63d0\u5347\u5176\u8ba1\u7b97\u6548\u7387\u548c\u6269\u5c55\u6027\u3002\u4f8b\u5982\uff0c\u91cf\u5b50\u8ba1\u7b97\u53ef\u4ee5\u4e3aMCTS\u63d0\u4f9b\u66f4\u5f3a\u5927\u7684\u8ba1\u7b97\u80fd\u529b\uff0c\u4ece\u800c\u89e3\u51b3\u66f4\u590d\u6742\u7684\u51b3\u7b56\u95ee\u9898\uff1b\u8fb9\u7f18\u8ba1\u7b97\u53ef\u4ee5\u5c06MCTS\u7684\u8ba1\u7b97\u4efb\u52a1\u5206\u5e03\u5230\u8fb9\u7f18\u8bbe\u5907\u4e0a\uff0c\u4ece\u800c\u51cf\u5c11\u6570\u636e\u4f20\u8f93\u5ef6\u8fdf\uff0c\u63d0\u5347\u5b9e\u65f6\u51b3\u7b56\u7684\u6548\u7387\u3002\u901a\u8fc7\u4e0d\u65ad\u63a2\u7d22\u548c\u521b\u65b0\uff0cMCTS\u5728\u672a\u6765\u6709\u671b\u5728\u66f4\u591a\u9886\u57df\u53d1\u6325\u91cd\u8981\u4f5c\u7528\uff0c\u4e3a\u590d\u6742\u51b3\u7b56\u95ee\u9898\u63d0\u4f9b\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "ref_task_step_id": ""}, "__type__": "task_step_node"}, "3fc9a567-9ad0-439c-bd1f-3954fb72323f": {"__data__": {"id_": "3fc9a567-9ad0-439c-bd1f-3954fb72323f", "metadata": {}, "relationships": {}, "hash": "", "start_task_context": "MCTS\u5728PRM\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u7684\u5e94\u7528\u63a2\u7d22\u7efc\u8ff0", "aemo_representation_context": "### Step 1: \u5206\u6790\u8fd1\u51e0\u5e74\u7814\u7a76\u9886\u57df\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba\n\n**\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09**\uff1a\n- **\u57fa\u672c\u539f\u7406**\uff1aMCTS\u662f\u4e00\u79cd\u7528\u4e8e\u51b3\u7b56\u8fc7\u7a0b\u7684\u542f\u53d1\u5f0f\u641c\u7d22\u7b97\u6cd5\uff0c\u5e7f\u6cdb\u5e94\u7528\u4e8e\u6e38\u620fAI\u3001\u8def\u5f84\u89c4\u5212\u7b49\u9886\u57df\u3002\u5176\u6838\u5fc3\u601d\u60f3\u662f\u901a\u8fc7\u6a21\u62df\u591a\u6b21\u968f\u673a\u6e38\u620f\u6765\u8bc4\u4f30\u4e0d\u540c\u51b3\u7b56\u7684\u4f18\u52a3\u3002\n- **\u6280\u672f\u6846\u67b6**\uff1a\u4e3b\u8981\u5305\u62ec\u9009\u62e9\uff08Selection\uff09\u3001\u6269\u5c55\uff08Expansion\uff09\u3001\u6a21\u62df\uff08Simulation\uff09\u548c\u56de\u6eaf\uff08Backpropagation\uff09\u56db\u4e2a\u6b65\u9aa4\u3002\n\n**\u504f\u597d\u7b56\u7565\u6a21\u578b\uff08PRM\uff09**\uff1a\n- **\u57fa\u672c\u539f\u7406**\uff1aPRM\u65e8\u5728\u901a\u8fc7\u5b66\u4e60\u7528\u6237\u7684\u504f\u597d\u6765\u4f18\u5316\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5e38\u7528\u4e8e\u63a8\u8350\u7cfb\u7edf\u3001\u4e2a\u6027\u5316\u670d\u52a1\u7b49\u3002\n- **\u6280\u672f\u6846\u67b6**\uff1a\u901a\u5e38\u5305\u62ec\u504f\u597d\u5efa\u6a21\u3001\u7b56\u7565\u4f18\u5316\u548c\u6a21\u578b\u5fae\u8c03\u7b49\u73af\u8282\u3002\n\n**\u65b9\u6cd5\u8bba**\uff1a\n- **\u96c6\u6210\u5b66\u4e60**\uff1a\u5c06MCTS\u4e0ePRM\u7ed3\u5408\uff0c\u5229\u7528MCTS\u7684\u641c\u7d22\u80fd\u529b\u4f18\u5316PRM\u7684\u51b3\u7b56\u8fc7\u7a0b\u3002\n- **\u5f3a\u5316\u5b66\u4e60**\uff1a\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u673a\u5236\u4e0d\u65ad\u8c03\u6574PRM\u7684\u53c2\u6570\uff0c\u4ee5\u9002\u5e94\u52a8\u6001\u73af\u5883\u3002\n\n### Step 2: \u7814\u7a76\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u4e3b\u8981\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0e\u53d8\u4f53\n\n**\u5e94\u7528\u573a\u666f**\uff1a\n- **\u6e38\u620fAI**\uff1aMCTS\u4e0ePRM\u7ed3\u5408\u7528\u4e8e\u6e38\u620f\u4e2d\u7684\u667a\u80fd\u51b3\u7b56\uff0c\u5982AlphaGo\u3002\n- **\u8def\u5f84\u89c4\u5212**\uff1a\u5728\u673a\u5668\u4eba\u5bfc\u822a\u548c\u81ea\u52a8\u9a7e\u9a76\u4e2d\uff0c\u5229\u7528MCTS\u4f18\u5316\u8def\u5f84\u9009\u62e9\u3002\n- **\u63a8\u8350\u7cfb\u7edf**\uff1a\u901a\u8fc7PRM\u5b66\u4e60\u7528\u6237\u504f\u597d\uff0c\u7ed3\u5408MCTS\u8fdb\u884c\u66f4\u7cbe\u51c6\u7684\u63a8\u8350\u3002\n\n**\u53d8\u4f53**\uff1a\n- **\u6df1\u5ea6MCTS**\uff1a\u7ed3\u5408\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff0c\u63d0\u5347MCTS\u7684\u51b3\u7b56\u80fd\u529b\u3002\n- **\u591a\u6a21\u6001PRM**\uff1a\u878d\u5408\u591a\u79cd\u6570\u636e\u6e90\uff08\u5982\u6587\u672c\u3001\u56fe\u50cf\uff09\uff0c\u589e\u5f3aPRM\u7684\u6cdb\u5316\u80fd\u529b\u3002\n\n### Step 3: \u8bc4\u4f30\u5b66\u672f\u754c\u7684\u6280\u672f\u8fdb\u6b65\u4e0e\u5c40\u9650\u6027\n\n**\u6280\u672f\u8fdb\u6b65**\uff1a\n- **\u6027\u80fd\u63d0\u5347**\uff1aMCTS\u4e0ePRM\u7ed3\u5408\u663e\u8457\u63d0\u5347\u4e86\u51b3\u7b56\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002\n- **\u6cdb\u5316\u80fd\u529b**\uff1a\u591a\u6a21\u6001PRM\u5728\u5904\u7406\u591a\u6837\u5316\u6570\u636e\u65f6\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002\n\n**\u5c40\u9650\u6027**\uff1a\n- **\u8ba1\u7b97\u590d\u6742\u5ea6**\uff1aMCTS\u5728\u5927\u89c4\u6a21\u95ee\u9898\u4e2d\u8ba1\u7b97\u91cf\u5927\uff0c\u6548\u7387\u6709\u5f85\u63d0\u5347\u3002\n- **\u6570\u636e\u4f9d\u8d56**\uff1aPRM\u7684\u6027\u80fd\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u9ad8\u8d28\u91cf\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u6570\u636e\u504f\u5dee\u95ee\u9898\u4f9d\u7136\u5b58\u5728\u3002\n\n### Step 4: \u63a2\u8ba8\u8ba1\u7b97\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\n\n**\u9002\u7528\u6027**\uff1a\n- **\u591a\u9886\u57df\u5e94\u7528**\uff1aMCTS+PRM\u6846\u67b6\u5728\u6e38\u620f\u3001\u8def\u5f84\u89c4\u5212\u3001\u63a8\u8350\u7cfb\u7edf\u7b49\u591a\u4e2a\u9886\u57df\u8868\u73b0\u51fa\u826f\u597d\u7684\u9002\u7528\u6027\u3002\n- **\u591a\u6a21\u6001\u6570\u636e**\uff1a\u591a\u6a21\u6001PRM\u80fd\u591f\u5904\u7406\u591a\u79cd\u7c7b\u578b\u7684\u6570\u636e\uff0c\u9002\u7528\u4e8e\u590d\u6742\u5e94\u7528\u573a\u666f\u3002\n\n**\u6cdb\u5316\u80fd\u529b**\uff1a\n- **\u8de8\u9886\u57df\u6cdb\u5316**\uff1a\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\uff0cMCTS+PRM\u6846\u67b6\u5728\u4e0d\u540c\u9886\u57df\u95f4\u5c55\u73b0\u51fa\u8f83\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002\n- **\u52a8\u6001\u73af\u5883\u9002\u5e94**\uff1a\u5f3a\u5316\u5b66\u4e60\u673a\u5236\u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u9002\u5e94\u52a8\u6001\u53d8\u5316\u7684\u73af\u5883\u3002\n\n### Step 5: \u5206\u6790\u6700\u65b0\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027\n\n**\u7a33\u5b9a\u6027**\uff1a\n- **\u7b97\u6cd5\u4f18\u5316**\uff1a\u901a\u8fc7\u6539\u8fdbMCTS\u7684\u9009\u62e9\u7b56\u7565\u548cPRM\u7684\u53c2\u6570\u8c03\u6574\u673a\u5236\uff0c\u63d0\u5347\u4e86\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u3002\n- **\u9c81\u68d2\u6027\u589e\u5f3a**\uff1a\u5f15\u5165\u5bf9\u6297\u8bad\u7ec3\u548c\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\uff0c\u63d0\u9ad8\u6a21\u578b\u5728\u590d\u6742\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u6027\u3002\n\n**\u5bb9\u9519\u6027**\uff1a\n- **\u9519\u8bef\u5904\u7406**\uff1a\u5728\u6a21\u62df\u548c\u56de\u6eaf\u9636\u6bb5\u5f15\u5165\u5bb9\u9519\u673a\u5236\uff0c\u51cf\u5c11\u9519\u8bef\u51b3\u7b56\u7684\u5f71\u54cd\u3002\n- **\u5927\u89c4\u6a21\u6570\u636e\u9002\u5e94\u6027**\uff1a\u4f18\u5316\u7b97\u6cd5\u7ed3\u6784\uff0c\u63d0\u5347\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\u7684\u5904\u7406\u80fd\u529b\u3002\n\n### Step 6: \u8bc4\u4f30\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218\n\n**\u672a\u6765\u7814\u7a76\u65b9\u5411**\uff1a\n- **\u9ad8\u6548\u8ba1\u7b97**\uff1a\u7814\u7a76\u66f4\u9ad8\u6548\u7684MCTS\u7b97\u6cd5\uff0c\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002\n- **\u6570\u636e\u589e\u5f3a**\uff1a\u63a2\u7d22\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u51cf\u5c11PRM\u5bf9\u9ad8\u8d28\u91cf\u6570\u636e\u7684\u4f9d\u8d56\u3002\n\n**\u6311\u6218**\uff1a\n- **\u8de8\u6a21\u6001\u878d\u5408**\uff1a\u5982\u4f55\u6709\u6548\u878d\u5408\u591a\u79cd\u6a21\u6001\u6570\u636e\uff0c\u63d0\u5347\u6a21\u578b\u7684\u7efc\u5408\u6027\u80fd\u3002\n- **\u52a8\u6001\u73af\u5883\u9002\u5e94**\uff1a\u5728\u52a8\u6001\u73af\u5883\u4e0b\uff0c\u5982\u4f55\u4fdd\u6301\u6a21\u578b\u7684\u7a33\u5b9a\u6027\u548c\u9ad8\u6548\u6027\u3002\n\n**\u63a8\u52a8\u540e\u7eed\u7814\u7a76**\uff1a\n- **\u65b0\u95ee\u9898\u63d0\u51fa**\uff1a\u5982\u591a\u6a21\u6001\u6570\u636e\u4e0b\u7684\u504f\u597d\u5efa\u6a21\u3001\u52a8\u6001\u73af\u5883\u4e0b\u7684\u51b3\u7b56\u4f18\u5316\u7b49\u3002\n- **\u7814\u7a76\u5207\u5165\u70b9**\uff1a\u4ece\u7b97\u6cd5\u4f18\u5316\u3001\u6570\u636e\u589e\u5f3a\u3001\u8de8\u6a21\u6001\u878d\u5408\u7b49\u591a\u4e2a\u89d2\u5ea6\u5207\u5165\uff0c\u63a8\u52a8\u6280\u672f\u8fdb\u6b65\u3002\n\n### \u603b\u7ed3\n\n\u8fd1\u5e74\u6765\uff0cMCTS\u5728PRM\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u7684\u5e94\u7528\u63a2\u7d22\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4e3b\u8981\u4f53\u73b0\u5728\u6280\u672f\u6846\u67b6\u7684\u5b8c\u5584\u3001\u5e94\u7528\u573a\u666f\u7684\u62d3\u5c55\u3001\u6027\u80fd\u7684\u63d0\u5347\u4ee5\u53ca\u7a33\u5b9a\u6027\u548c\u5bb9\u9519\u6027\u7684\u589e\u5f3a\u3002\u7136\u800c\uff0c\u4ecd\u5b58\u5728\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u3001\u6570\u636e\u4f9d\u8d56\u6027\u5f3a\u7b49\u5c40\u9650\u6027\u3002\u672a\u6765\u7684\u7814\u7a76\u5e94\u5173\u6ce8\u9ad8\u6548\u8ba1\u7b97\u3001\u6570\u636e\u589e\u5f3a\u548c\u8de8\u6a21\u6001\u878d\u5408\u7b49\u65b9\u5411\uff0c\u4ee5\u5e94\u5bf9\u52a8\u6001\u73af\u5883\u4e0b\u7684\u51b3\u7b56\u4f18\u5316\u7b49\u6311\u6218\uff0c\u63a8\u52a8\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002", "task_step_name": "\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09", "task_step_description": "MCTS\u662f\u4e00\u79cd\u7528\u4e8e\u51b3\u7b56\u8fc7\u7a0b\u7684\u542f\u53d1\u5f0f\u641c\u7d22\u7b97\u6cd5\uff0c\u5e7f\u6cdb\u5e94\u7528\u4e8e\u6e38\u620fAI\u3001\u8def\u5f84\u89c4\u5212\u7b49\u9886\u57df\u3002\u5176\u6838\u5fc3\u601d\u60f3\u662f\u901a\u8fc7\u6a21\u62df\u591a\u6b21\u968f\u673a\u6e38\u620f\u6765\u8bc4\u4f30\u4e0d\u540c\u51b3\u7b56\u7684\u4f18\u52a3\u3002", "task_step_level": "0>1", "task_step_question": "### \u95ee\u9898\u63d0\u51fa\n\n\u5728\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09\u7684\u5e94\u7528\u4e2d\uff0c\u7279\u522b\u662f\u5728\u4e0e\u504f\u597d\u7b56\u7565\u6a21\u578b\uff08PRM\uff09\u7ed3\u5408\u7684\u573a\u666f\u4e0b\uff0c\u5982\u4f55\u6709\u6548\u964d\u4f4eMCTS\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u5176\u5728\u590d\u6742\u51b3\u7b56\u4efb\u52a1\u4e2d\u7684\u9ad8\u6548\u6027\u548c\u51c6\u786e\u6027\uff1f\u5177\u4f53\u6765\u8bf4\uff0c\u6709\u54ea\u4e9b\u7b97\u6cd5\u4f18\u5316\u6216\u6280\u672f\u6539\u8fdb\u53ef\u4ee5\u663e\u8457\u51cf\u5c11MCTS\u5728\u5927\u89c4\u6a21\u95ee\u9898\u4e2d\u7684\u8ba1\u7b97\u91cf\uff0c\u800c\u4e0d\u727a\u7272\u5176\u51b3\u7b56\u8d28\u91cf\uff1f\u6b64\u5916\uff0c\u8fd9\u4e9b\u4f18\u5316\u65b9\u6cd5\u5728\u4e0d\u540c\u5e94\u7528\u573a\u666f\uff08\u5982\u6e38\u620fAI\u3001\u8def\u5f84\u89c4\u5212\u3001\u63a8\u8350\u7cfb\u7edf\uff09\u4e2d\u7684\u9002\u7528\u6027\u548c\u6548\u679c\u5982\u4f55\uff1f", "task_step_question_context": [{"ref_id": "454845766462370026", "chunk_id": "6", "score": 0.63671875, "text": "# Monte Carlo Tree Search in the Presence of Transition Uncertainty\nFarnaz Kohankhaki , Kiarash Aghakasiri , Hongming Zhang 1 , Ting-Han Wei 1 , Chao Gao 2 ,Martin M\u00a8uller 1  \n\n1 University of Alberta, 2 Edmonton Research Center, Huawei Canada {kohankha, aghakasi, hongmin2, tinghan, mmueller }@ualberta.ca, cgao3 $@$ outlook.com\n\n# Abstract\nMonte Carlo Tree Search (MCTS) is an immensely popular search-based framework used for decision making. It is traditionally applied to domains where a perfect simulation model of the environment is available. We study and improve MCTS in the context where the environment model is given but imperfect. We show that the discrepancy between the model and the actual environment can lead to significant performance degradation with standard MCTS. We therefore develop Uncertainty Adapted MCTS (UA-MCTS), a more robust algorithm within the MCTS framework. We estimate the transition uncertainty in the given model, and direct the search towards more certain transitions in the state space. We modify all four MCTS phases to improve the search behavior by considering these estimates. We prove, in the corrupted bandit case, that adding uncertainty information to adapt UCB leads to tighter regret bound than standard UCB. Empirically, we evaluate UA-MCTS and its individual components on the deterministic domains from the MinAtar test suite. Our results demonstrate that UA-MCTS strongly improves MCTS in the presence of model transition errors.\n\n# 1 Introduction\nThe Monte Carlo Tree Search (MCTS) framework (Browne et al. 2012) approaches sequential decision-making problems by selective lookahead search. It manages the balance of exploration and exploitation with techniques such as UCT (Kocsis, Szepesv\u00b4ari, and Willemson 2006). Often combined with machine learning, it has been enormously successful in both games (Silver et al. 2016; Banerjee 2020; Arneson, Hayward, and Henderson 2010; Saffidine 2008; Nijssen and Winands 2010) and non-game applications (Lu et al. 2016; Mansley, Weinstein, and Littman 2011; Sabharwal, Samulowitz, and Reddy 2012; Cazenave 2010). In these applications, a perfect simulation model allows for efficient lookahead search. However, in many practical applications, only an imperfect model is available to the agent. Yet lookahead using such a model can still be useful. We improve MCTS for this setting.  \n\nOne research area that studies imperfect models of the environment is model-based reinforcement learning (MBRL).  \n\nHere, an agent builds its own model through limited real world interactions. The resulting learned model, when used for lookahead search, can either be for planning or for producing more accurate training targets (Silver, Sutton, and M\u00a8uller 2008). It can also be used to generate simulated training samples for better sample efficiency (Sutton and Barto 2018). The learned model may be inaccurate for many reasons, including stochasticity of the environment, insufficient training, insufficient capacity, non stationary environments, etc. Consequently, there is a rich body of research on uncertainty in MBRL (Abbas et al. 2020; Xiao et al. 2019; Buckman et al. 2018).  \n\nWhile previous approaches to using search with imperfect models exist (Vemula et al. 2020; Vemula, Bagnell, and Likhachev 2021), to the best of our knowledge, there is no prior work that directly adapts MCTS to deal with model uncertainty. In our work, we define transition uncertainty as a measure of difference between the state transitions in the perfect model and in the model that is available to the agent. We use a neural network to estimate this uncertainty.  \n\nOur Uncertainty Adapted MCTS (UA-MCTS) approach implements the main components of the MCTS framework in a way that guides the search away from states with high uncertainty. We compare the performance of our proposed methods with MCTS baselines in three deterministic MinAtar environments (Young and Tian 2019). In each case the search agent \u201cbelieves\u201d it is playing the real game. However, the rules of the game itself have changed, and the agent only learns about this change slowly when it acts in the real environment. The results show that UA-MCTS is able to outperform the baseline MCTS with an imperfect model.  \n\nOur approach is inspired by the work of (Vemula et al. 2020) where a robotic arm has to solve tasks despite being handicapped, e.g. by a broken motor or by an unmodeled weight restriction. To show how an agent should adapt UCB-based exploration strategy in the presence of environment uncertainties, we first consider a case of stochastic bandits (Lattimore and Szepesv\u00b4ari 2020) along with corrupted feedback. We prove that incorporating uncertainty information can enhance the performance of UCB, yielding a regret bound that is more constrained compared to the standard UCB. We also prove that in the general case of tree search, with similar modification of UCT, our UA-MCTS approach maintains its completeness property, ensuring that as the number of iterations goes to infinity, all nodes will be consistently explored. To further motivate our approach, we compare the scenarios of learning to improve the transition function, using MCTS, directly against the easier task of just learning a transition uncertainty function with UA-MCTS. In both cases, learning occurs online; the former is used with MCTS while the latter is used with UA-MCTS. Our results show that learning the transition function is much harder than learning transition uncertainty, which justifies the use of UA-MCTS in such settings."}, {"ref_id": "454846996555341700", "chunk_id": "2", "score": 0.56640625, "text": "# 2.2 Acceleration of MCTS\nMCTS-based methods have proved their strong capability of solving complex games or tasks. However, the high computational cost of MCTS hinders its application to some real-time and more general scenarios. Therefore, numerous works are devoted to accelerating MCTS. For example, to make the selection stage more effective, some heuristic pruning methods [ 14 ,33 ,29 ,1 ,2 ] aim to reduce the width and depth of the search tree with some heuristic functions. Furthermore, for more efficient evaluations, Lorentz [ 22 ] proposed early playout termination of MCTS (MCTS-EPT) to stop the random playouts early and use an evaluation function to assess win or loss. Moreover, Hsueh et al. [18 ] applied MCTS-EPT to the Chinese dark chess and proved its effectiveness. Afterward, similar ideas have been applied in the evaluation stage of AlphaGoZero [ 32 ] and later MCTS-based methods [31 ,27 ,34 ]. They evaluate the $Q$ -values through a learnable evaluation network instead of running playouts to the end. Grill et al. [15 ] propose a novel regularized policy optimization method based on AlphaZero to decrease the search budget of MCTS, which is from the optimization perspective. Danihelka et al. [10 ] propose a policy improvement algorithm based on sampling actions without replacement, named Gumbel trick to achieve better performance when planning with few simulations. However, these methods mentioned above focus on the specific stage of the search iteration or reduce the total budget through pruning and optimization methods, which are orthogonal to us. And few works targets at the search loop. Lan et al. [21 ] propose DS-MCTS, which defines the uncertainty of MCTS and approximates it by extra DNNs with specific features for board games in training. During the evaluation, DS-MCTS will check periodically and stop the search if the state is certain.\n\n# 3 Background\nThe AlphaGo series of work [ 30 ,32 ,31 ,27 ] are all MCTS-based reinforcement learning algorithms. Those algorithms assume the environment transition dynamics are known or learn the environment dynamics. Based on the dynamics, they use the Monte-Carlo tree search (MCTS) as the policy improvement operator. I.e., taking in the current policy, MCTS returns a better policy with the search algorithm. The systematic search allows the MCTS-based RL algorithm to quickly improve the policy and perform much better in the setting where heavy reasoning is required.\n\n# 3.1 MCTS\nThis part briefly introduces the MCTS method implemented in reinforcement learning applications. As mentioned in the related works, modern MCTS-based RL algorithms include four stages in the search loop, namely selection, expansion, evaluation, and backpropagation.  \n\nMCTS takes in the current states and generates a policy after the search loop of $N$ iterations. Here $N$ is a constant number of iterations set by the designer, regarded as the total budget. In the selection stage of each iteration, an action will be selected by maximizing over UCB. Specifically, AlphaZero [31 ] and MuZero [ 27 ] are developed based on a variant of UCB, P-UCT [ 25 ] and have achieved great success on board games and Atari games. The formula of P-UCT is the Eq (1):  \n\n$$\na^{k}=\\arg\\operatorname*{max}_{a\\in\\mathcal{A}}Q(s,a)+P(s,a)\\frac{\\sqrt{\\sum_{b\\in\\mathcal{A}}N(s,b)}}{1+N(s,a)}(c_{1}+\\log((\\sum_{b\\in\\mathcal{A}}N(s,b)+c_{2}+1)/c_{2})),\n$$  \n\nwhere $k$ is the index of iteration, $\\boldsymbol{\\mathcal{A}}$ is the acti $Q(s,a)$ is the estimated Q-value, $P(s,a)$ is the policy prior obtained from neural networks, $N(s,a)$ is the visitations to select the action a from the state $s$ and $c_{1},c_{2}$ are hyper-parameters. The output of MCTS is the visitation of each action of the root node. After $N$ search iterations, the final policy $\\pi(s)$ is defined as the normalized root visitation distribution simplification, we use $\\pi_{N}(s)$ $\\pi_{k}$ in place of , where $\\begin{array}{r}{\\pi_{k}(s,a)=N(s,a)/\\sum_{b\\in\\mathcal{A}}N(s,b)=N(s,a)/k,a\\in\\mathcal{A}}\\end{array}$ $\\pi_{k}(s)$ sometimes. And the detailed procedure of MCTS is \u2208A . For introduced in Appendix. In our method, we propose to approximate the final policy $\\pi_{N}(s)$ with $\\hat{\\pi}_{k}(s)$ ,which we name as a virtual expanded policy, through a new expansion method and a termination rule. In this way, the number of iterations in MCTS can be reduced from $N$ to $k$ .\n\n# 3.2 Computation Requirement\nMost of the computations in MCTS-based RL are in the MCTS procedure. Each action taken by MCTS requires $N$ times neural network evaluations, where $N$ is a constant number of iterations in the search loop. Traditional RL algorithms, such as PPO [ 28 ] or DQN [ 23 ], only need a single neural network evaluation per action. Thus, MCTS-based RL is roughly $N$ times computationally more expensive than traditional RL algorithms. In practice, training a single Atari game needs 12 hours of computation time on 40 TPUs [ 27 ]. The computation need is roughly two orders of magnitude more than traditional RL algorithms [28], although the final performance of MuZero is much better.\n\n# 4 Method\nWe aim to spend more search time on harder states and less on easier states. Intuitively, human knows when to make a quick decision or a slow decision under different circumstances. Unfortunately, this situation-aware behavior is absent in current MCTS algorithms. Therefore, we propose an MCTS variant that terminates the search iteration adaptively. It consists of two components: a novel expansion method named virtual expansion to estimate the final visitation based on the current partial tree; a termination rule that decides when to terminate based on the hardness of the current scenario. And we will display the adaptive mechanism through visualizations in Section 5.5."}, {"ref_id": "455026805307867280", "chunk_id": "0", "score": 0.546875, "text": "# Spending Thinking Time Wisely: Accelerating MCTS with Virtual Expansions\nWeirui $\\mathbf{Ye}^{*}$ Pieter Abbeel \u2020Yang Gao $\\ast\\ddag\\S$ \u2217Tsinghua University, \u2020UC Berkeley, \u00a7Shanghai Qi Zhi Institute\n\n# Abstract\nOne of the most important AI research questions is to trade off computation versus performance since \u201cperfect rationality\" exists in theory but is impossible to achieve in practice. Recently, Monte-Carlo tree search (MCTS) has attracted considerable attention due to the significant performance improvement in various challenging domains. However, the expensive time cost during search severely restricts its scope for applications. This paper proposes the Virtual MCTS (V-MCTS), a variant of MCTS that spends more search time on harder states and less search time on simpler states adaptively. We give theoretical bounds of the proposed method and evaluate the performance and computations on $9\\times9$ Go board games and Atari games. Experiments show that our method can achieve comparable performances to the original search algorithm while requiring less than $50\\%$ search time on average. We believe that this approach is a viable alternative for tasks under limited time and resources. The code is available at https://github.com/YeWR/V-MCTS.git .\n\n# 1 Introduction\nWhen artificial intelligence was first studied in the 1950s, researchers have sought to find the solution to the question \u201cHow to build an agent with perfect rationality\". The term \u201cperfect rationality\" [7 ,24 ,26 ] here refers to the decision made with infinite amounts of computations. However, one can only solve small-scale problems without considering the practical computation time since classical search algorithms usually exhibit exponential running time. Therefore, recent AI research would no longer seek to achieve \u201cperfect rationality\", but instead carefully trade-off computation versus the level of rationality. People have developed computational models like \u201cbounded optimality\" to model these settings [ 26 ]. The increasing level of rationality under the same computational budget has given us a lot of AI successes. Algorithms include the Monte-Carlo sampling algorithms, the variational inference algorithms, and using DNNs as universal function approximators [9, 8, 13, 30, 17].  \n\nRecently, MCTS-based RL algorithms have achieved much success, mainly on board games. The most notable achievement is that AlphaGo beats Hui Fan in 2015 [ 30 ]. It is the first time a computer program beat a human professional Go player. Afterward, AlphaGo beats two top-ranking human players, Lee Sedol in 2016 and Jie Ke in 2017, the latter of which ranked first worldwide at the time. Later, MCTS-based RL algorithms were further extended to other board games and Atari games [ 27 ]. EfficientZero [ 34 ] significantly improves the sample efficiency of MCTS-based RL algorithms, shedding light on its future applications in the real world like robotics and self-driving.  \n\nDespite the impressive performance of MCTS-based RL algorithms, they require massive amounts of computation to train and evaluate. For example, MuZero [ 27 ] used 1000 TPUs trained for 12 hours to learn the game of Go, and for a single Atari game, it needs 40 TPUs to train 12 hours. Compared to previous algorithms on the Atari games benchmark, it needs around two orders of magnitude more compute. This prohibitively large computational requirement has slowed down both the further development of MCTS-based RL algorithms as well as its practical use.  \n\nUnder the hood, MCTS-based RL algorithms imagine the futures when taking different future action sequences. However, this imaging process for the current method is not computationally efficient. For example, AlphaGo needs to look ahead 1600 game states to place a single stone. On the contrary, top human professional players can only think through around 100-200 game states per minute [ 30 ]. Apart from the inefficiency, the current MCTS algorithm deals with easy and challenging cases with the same computational budget. However, human knows to use their time when it is most needed.  \n\nIn this paper, we aim to design new algorithms that save the computational time of the MCTSbased RL methods. We make three key contributions : (1) We present Virtual MCTS, a variant of MCTS, to approximate the vanilla MCTS search policies with less computation. Moreover, unlike previous pruning-based methods that focus on the selection or evaluation stage in MCTS, our method improves the search loop. It terminates the search iterations earlier adaptively when current states are simpler; (2) Theoretically, we provide some error bounds of the proposed method. Furthermore, the visualization results indicate that Virtual MCTS has a better computation and performance trade-off than vanilla MCTS; (3) Empirically, our method can save more than $50\\%$ of search times on the challenging game Go $9\\times9$ and more than $60\\%$ on the visually complex Atari games while keeping comparable performances to those of vanilla MCTS.\n\n# 2 Related Work\n\n# 2.1 Reinforcement Learning with MCTS\nFor a long time, Computer Go has been regarded as a remarkably challenging game [ 3 ,6 ]. Researchers attempt to use Monte-Carlo techniques that evaluate the value of the node state through random playouts [ 4 ,11 ,12 ,30 ]. Afterward, UCT algorithms have generally been applied in Monte-Carlo tree search (MCTS) algorithms, which use UCB1 to select action at each node of the tree [ 20 ]. Recently, MCTS-based RL methods [ 30 ,32 ,31 ,27 ] have become increasingly popular and achieved super-human performances on board games because of their strong ability to search.  \n\nModern MCTS-based RL algorithms include four stages in the search loop : selection, expansion, evaluation, and backpropagation. The computation bottlenecks in vanilla MCTS come from the search loop, especially for the evaluation stage and the selection stage of each iteration. The selection stage is time-consuming when the search tree becomes wider and deeper. The evaluation stage is quite expensive because people attempt to evaluate the node value by random playouts to the end in previous researches. Due to the search loop, MCTS-based algorithms have multiple model inferences compared to other model-free RL methods like PPO [28] and SAC [16]."}], "task_step_question_answer": "\u5728MCTS\u7684\u4f18\u5316\u8fc7\u7a0b\u4e2d\uff0c\u7406\u8bba\u652f\u6301\u662f\u4e0d\u53ef\u6216\u7f3a\u7684\u3002\u4f8b\u5982\uff0cV-MCTS\u63d0\u4f9b\u4e86\u8bef\u5dee\u754c\uff0c\u786e\u4fdd\u5728\u51cf\u5c11\u8ba1\u7b97\u91cf\u7684\u540c\u65f6\uff0c\u6027\u80fd\u635f\u5931\u53ef\u63a7\u3002\u8fd9\u79cd\u7406\u8bba\u652f\u6301\u4e3a\u65b9\u6cd5\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u9760\u6027\u4fdd\u969c\u3002\u6b64\u5916\uff0c\u542f\u53d1\u5f0f\u526a\u679d\u7684\u6536\u655b\u6027\u5206\u6790\u4e5f\u4e3a\u5176\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u7406\u8bba\u4f9d\u636e\u3002\u901a\u8fc7\u7ed3\u5408\u8fd9\u4e9b\u7406\u8bba\u5206\u6790\uff0c\u53ef\u4ee5\u8fdb\u4e00\u6b65\u4f18\u5316MCTS\u7684\u6027\u80fd\uff0c\u5e76\u786e\u4fdd\u5176\u5728\u4e0d\u540c\u5e94\u7528\u573a\u666f\u4e2d\u7684\u7a33\u5b9a\u6027\u548c\u9ad8\u6548\u6027\u3002", "ref_task_step_id": ""}, "__type__": "task_step_node"}, "372a76b7-8c9e-42aa-b783-cf530fb1852d": {"__data__": {"id_": "372a76b7-8c9e-42aa-b783-cf530fb1852d", "metadata": {}, "relationships": {}, "hash": "", "start_task_context": "MCTS\u5728PRM\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u7684\u5e94\u7528\u63a2\u7d22\u7efc\u8ff0", "aemo_representation_context": "### Step 1: \u5206\u6790\u8fd1\u51e0\u5e74\u7814\u7a76\u9886\u57df\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba\n\n**\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09**\uff1a\n- **\u57fa\u672c\u539f\u7406**\uff1aMCTS\u662f\u4e00\u79cd\u7528\u4e8e\u51b3\u7b56\u8fc7\u7a0b\u7684\u542f\u53d1\u5f0f\u641c\u7d22\u7b97\u6cd5\uff0c\u5e7f\u6cdb\u5e94\u7528\u4e8e\u6e38\u620fAI\u3001\u8def\u5f84\u89c4\u5212\u7b49\u9886\u57df\u3002\u5176\u6838\u5fc3\u601d\u60f3\u662f\u901a\u8fc7\u6a21\u62df\u591a\u6b21\u968f\u673a\u6e38\u620f\u6765\u8bc4\u4f30\u4e0d\u540c\u51b3\u7b56\u7684\u4f18\u52a3\u3002\n- **\u6280\u672f\u6846\u67b6**\uff1a\u4e3b\u8981\u5305\u62ec\u9009\u62e9\uff08Selection\uff09\u3001\u6269\u5c55\uff08Expansion\uff09\u3001\u6a21\u62df\uff08Simulation\uff09\u548c\u56de\u6eaf\uff08Backpropagation\uff09\u56db\u4e2a\u6b65\u9aa4\u3002\n\n**\u504f\u597d\u7b56\u7565\u6a21\u578b\uff08PRM\uff09**\uff1a\n- **\u57fa\u672c\u539f\u7406**\uff1aPRM\u65e8\u5728\u901a\u8fc7\u5b66\u4e60\u7528\u6237\u7684\u504f\u597d\u6765\u4f18\u5316\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5e38\u7528\u4e8e\u63a8\u8350\u7cfb\u7edf\u3001\u4e2a\u6027\u5316\u670d\u52a1\u7b49\u3002\n- **\u6280\u672f\u6846\u67b6**\uff1a\u901a\u5e38\u5305\u62ec\u504f\u597d\u5efa\u6a21\u3001\u7b56\u7565\u4f18\u5316\u548c\u6a21\u578b\u5fae\u8c03\u7b49\u73af\u8282\u3002\n\n**\u65b9\u6cd5\u8bba**\uff1a\n- **\u96c6\u6210\u5b66\u4e60**\uff1a\u5c06MCTS\u4e0ePRM\u7ed3\u5408\uff0c\u5229\u7528MCTS\u7684\u641c\u7d22\u80fd\u529b\u4f18\u5316PRM\u7684\u51b3\u7b56\u8fc7\u7a0b\u3002\n- **\u5f3a\u5316\u5b66\u4e60**\uff1a\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u673a\u5236\u4e0d\u65ad\u8c03\u6574PRM\u7684\u53c2\u6570\uff0c\u4ee5\u9002\u5e94\u52a8\u6001\u73af\u5883\u3002\n\n### Step 2: \u7814\u7a76\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u4e3b\u8981\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0e\u53d8\u4f53\n\n**\u5e94\u7528\u573a\u666f**\uff1a\n- **\u6e38\u620fAI**\uff1aMCTS\u4e0ePRM\u7ed3\u5408\u7528\u4e8e\u6e38\u620f\u4e2d\u7684\u667a\u80fd\u51b3\u7b56\uff0c\u5982AlphaGo\u3002\n- **\u8def\u5f84\u89c4\u5212**\uff1a\u5728\u673a\u5668\u4eba\u5bfc\u822a\u548c\u81ea\u52a8\u9a7e\u9a76\u4e2d\uff0c\u5229\u7528MCTS\u4f18\u5316\u8def\u5f84\u9009\u62e9\u3002\n- **\u63a8\u8350\u7cfb\u7edf**\uff1a\u901a\u8fc7PRM\u5b66\u4e60\u7528\u6237\u504f\u597d\uff0c\u7ed3\u5408MCTS\u8fdb\u884c\u66f4\u7cbe\u51c6\u7684\u63a8\u8350\u3002\n\n**\u53d8\u4f53**\uff1a\n- **\u6df1\u5ea6MCTS**\uff1a\u7ed3\u5408\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff0c\u63d0\u5347MCTS\u7684\u51b3\u7b56\u80fd\u529b\u3002\n- **\u591a\u6a21\u6001PRM**\uff1a\u878d\u5408\u591a\u79cd\u6570\u636e\u6e90\uff08\u5982\u6587\u672c\u3001\u56fe\u50cf\uff09\uff0c\u589e\u5f3aPRM\u7684\u6cdb\u5316\u80fd\u529b\u3002\n\n### Step 3: \u8bc4\u4f30\u5b66\u672f\u754c\u7684\u6280\u672f\u8fdb\u6b65\u4e0e\u5c40\u9650\u6027\n\n**\u6280\u672f\u8fdb\u6b65**\uff1a\n- **\u6027\u80fd\u63d0\u5347**\uff1aMCTS\u4e0ePRM\u7ed3\u5408\u663e\u8457\u63d0\u5347\u4e86\u51b3\u7b56\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002\n- **\u6cdb\u5316\u80fd\u529b**\uff1a\u591a\u6a21\u6001PRM\u5728\u5904\u7406\u591a\u6837\u5316\u6570\u636e\u65f6\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002\n\n**\u5c40\u9650\u6027**\uff1a\n- **\u8ba1\u7b97\u590d\u6742\u5ea6**\uff1aMCTS\u5728\u5927\u89c4\u6a21\u95ee\u9898\u4e2d\u8ba1\u7b97\u91cf\u5927\uff0c\u6548\u7387\u6709\u5f85\u63d0\u5347\u3002\n- **\u6570\u636e\u4f9d\u8d56**\uff1aPRM\u7684\u6027\u80fd\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u9ad8\u8d28\u91cf\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u6570\u636e\u504f\u5dee\u95ee\u9898\u4f9d\u7136\u5b58\u5728\u3002\n\n### Step 4: \u63a2\u8ba8\u8ba1\u7b97\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\n\n**\u9002\u7528\u6027**\uff1a\n- **\u591a\u9886\u57df\u5e94\u7528**\uff1aMCTS+PRM\u6846\u67b6\u5728\u6e38\u620f\u3001\u8def\u5f84\u89c4\u5212\u3001\u63a8\u8350\u7cfb\u7edf\u7b49\u591a\u4e2a\u9886\u57df\u8868\u73b0\u51fa\u826f\u597d\u7684\u9002\u7528\u6027\u3002\n- **\u591a\u6a21\u6001\u6570\u636e**\uff1a\u591a\u6a21\u6001PRM\u80fd\u591f\u5904\u7406\u591a\u79cd\u7c7b\u578b\u7684\u6570\u636e\uff0c\u9002\u7528\u4e8e\u590d\u6742\u5e94\u7528\u573a\u666f\u3002\n\n**\u6cdb\u5316\u80fd\u529b**\uff1a\n- **\u8de8\u9886\u57df\u6cdb\u5316**\uff1a\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\uff0cMCTS+PRM\u6846\u67b6\u5728\u4e0d\u540c\u9886\u57df\u95f4\u5c55\u73b0\u51fa\u8f83\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002\n- **\u52a8\u6001\u73af\u5883\u9002\u5e94**\uff1a\u5f3a\u5316\u5b66\u4e60\u673a\u5236\u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u9002\u5e94\u52a8\u6001\u53d8\u5316\u7684\u73af\u5883\u3002\n\n### Step 5: \u5206\u6790\u6700\u65b0\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027\n\n**\u7a33\u5b9a\u6027**\uff1a\n- **\u7b97\u6cd5\u4f18\u5316**\uff1a\u901a\u8fc7\u6539\u8fdbMCTS\u7684\u9009\u62e9\u7b56\u7565\u548cPRM\u7684\u53c2\u6570\u8c03\u6574\u673a\u5236\uff0c\u63d0\u5347\u4e86\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u3002\n- **\u9c81\u68d2\u6027\u589e\u5f3a**\uff1a\u5f15\u5165\u5bf9\u6297\u8bad\u7ec3\u548c\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\uff0c\u63d0\u9ad8\u6a21\u578b\u5728\u590d\u6742\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u6027\u3002\n\n**\u5bb9\u9519\u6027**\uff1a\n- **\u9519\u8bef\u5904\u7406**\uff1a\u5728\u6a21\u62df\u548c\u56de\u6eaf\u9636\u6bb5\u5f15\u5165\u5bb9\u9519\u673a\u5236\uff0c\u51cf\u5c11\u9519\u8bef\u51b3\u7b56\u7684\u5f71\u54cd\u3002\n- **\u5927\u89c4\u6a21\u6570\u636e\u9002\u5e94\u6027**\uff1a\u4f18\u5316\u7b97\u6cd5\u7ed3\u6784\uff0c\u63d0\u5347\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\u7684\u5904\u7406\u80fd\u529b\u3002\n\n### Step 6: \u8bc4\u4f30\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218\n\n**\u672a\u6765\u7814\u7a76\u65b9\u5411**\uff1a\n- **\u9ad8\u6548\u8ba1\u7b97**\uff1a\u7814\u7a76\u66f4\u9ad8\u6548\u7684MCTS\u7b97\u6cd5\uff0c\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002\n- **\u6570\u636e\u589e\u5f3a**\uff1a\u63a2\u7d22\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u51cf\u5c11PRM\u5bf9\u9ad8\u8d28\u91cf\u6570\u636e\u7684\u4f9d\u8d56\u3002\n\n**\u6311\u6218**\uff1a\n- **\u8de8\u6a21\u6001\u878d\u5408**\uff1a\u5982\u4f55\u6709\u6548\u878d\u5408\u591a\u79cd\u6a21\u6001\u6570\u636e\uff0c\u63d0\u5347\u6a21\u578b\u7684\u7efc\u5408\u6027\u80fd\u3002\n- **\u52a8\u6001\u73af\u5883\u9002\u5e94**\uff1a\u5728\u52a8\u6001\u73af\u5883\u4e0b\uff0c\u5982\u4f55\u4fdd\u6301\u6a21\u578b\u7684\u7a33\u5b9a\u6027\u548c\u9ad8\u6548\u6027\u3002\n\n**\u63a8\u52a8\u540e\u7eed\u7814\u7a76**\uff1a\n- **\u65b0\u95ee\u9898\u63d0\u51fa**\uff1a\u5982\u591a\u6a21\u6001\u6570\u636e\u4e0b\u7684\u504f\u597d\u5efa\u6a21\u3001\u52a8\u6001\u73af\u5883\u4e0b\u7684\u51b3\u7b56\u4f18\u5316\u7b49\u3002\n- **\u7814\u7a76\u5207\u5165\u70b9**\uff1a\u4ece\u7b97\u6cd5\u4f18\u5316\u3001\u6570\u636e\u589e\u5f3a\u3001\u8de8\u6a21\u6001\u878d\u5408\u7b49\u591a\u4e2a\u89d2\u5ea6\u5207\u5165\uff0c\u63a8\u52a8\u6280\u672f\u8fdb\u6b65\u3002\n\n### \u603b\u7ed3\n\n\u8fd1\u5e74\u6765\uff0cMCTS\u5728PRM\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u7684\u5e94\u7528\u63a2\u7d22\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4e3b\u8981\u4f53\u73b0\u5728\u6280\u672f\u6846\u67b6\u7684\u5b8c\u5584\u3001\u5e94\u7528\u573a\u666f\u7684\u62d3\u5c55\u3001\u6027\u80fd\u7684\u63d0\u5347\u4ee5\u53ca\u7a33\u5b9a\u6027\u548c\u5bb9\u9519\u6027\u7684\u589e\u5f3a\u3002\u7136\u800c\uff0c\u4ecd\u5b58\u5728\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u3001\u6570\u636e\u4f9d\u8d56\u6027\u5f3a\u7b49\u5c40\u9650\u6027\u3002\u672a\u6765\u7684\u7814\u7a76\u5e94\u5173\u6ce8\u9ad8\u6548\u8ba1\u7b97\u3001\u6570\u636e\u589e\u5f3a\u548c\u8de8\u6a21\u6001\u878d\u5408\u7b49\u65b9\u5411\uff0c\u4ee5\u5e94\u5bf9\u52a8\u6001\u73af\u5883\u4e0b\u7684\u51b3\u7b56\u4f18\u5316\u7b49\u6311\u6218\uff0c\u63a8\u52a8\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002", "task_step_name": "\u504f\u597d\u7b56\u7565\u6a21\u578b\uff08PRM\uff09", "task_step_description": "PRM\u65e8\u5728\u901a\u8fc7\u5b66\u4e60\u7528\u6237\u7684\u504f\u597d\u6765\u4f18\u5316\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5e38\u7528\u4e8e\u63a8\u8350\u7cfb\u7edf\u3001\u4e2a\u6027\u5316\u670d\u52a1\u7b49\u3002", "task_step_level": "0>2", "task_step_question": "### \u95ee\u9898\u63d0\u51fa\n\n\u5728MCTS\u4e0ePRM\u7ed3\u5408\u7684\u6846\u67b6\u4e2d\uff0c\u5982\u4f55\u6709\u6548\u5229\u7528MCTS\u7684\u641c\u7d22\u80fd\u529b\u6765\u4f18\u5316PRM\u7684\u504f\u597d\u5efa\u6a21\u8fc7\u7a0b\uff0c\u7279\u522b\u662f\u5728\u63a8\u8350\u7cfb\u7edf\u548c\u4e2a\u6027\u5316\u670d\u52a1\u4e2d\uff0c\u5982\u4f55\u786e\u4fdd\u6a21\u578b\u80fd\u591f\u51c6\u786e\u6355\u6349\u7528\u6237\u7684\u52a8\u6001\u504f\u597d\u5e76\u505a\u51fa\u5b9e\u65f6\u51b3\u7b56\uff1f\n\n### \u95ee\u9898\u80cc\u666f\n\nPRM\u901a\u8fc7\u5b66\u4e60\u7528\u6237\u7684\u504f\u597d\u6765\u4f18\u5316\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5e38\u7528\u4e8e\u63a8\u8350\u7cfb\u7edf\u548c\u4e2a\u6027\u5316\u670d\u52a1\u3002\u7136\u800c\uff0c\u7528\u6237\u7684\u504f\u597d\u5f80\u5f80\u662f\u52a8\u6001\u53d8\u5316\u7684\uff0c\u4f20\u7edf\u7684PRM\u6a21\u578b\u53ef\u80fd\u96be\u4ee5\u5b9e\u65f6\u6355\u6349\u8fd9\u4e9b\u53d8\u5316\u3002MCTS\u4f5c\u4e3a\u4e00\u79cd\u542f\u53d1\u5f0f\u641c\u7d22\u7b97\u6cd5\uff0c\u5177\u6709\u5f3a\u5927\u7684\u51b3\u7b56\u4f18\u5316\u80fd\u529b\uff0c\u4f46\u5176\u5728\u5927\u89c4\u6a21\u95ee\u9898\u4e2d\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u8f83\u9ad8\u3002\u56e0\u6b64\uff0c\u5982\u4f55\u5c06MCTS\u4e0ePRM\u6709\u6548\u7ed3\u5408\uff0c\u65e2\u5229\u7528MCTS\u7684\u641c\u7d22\u80fd\u529b\u4f18\u5316PRM\u7684\u51b3\u7b56\u8fc7\u7a0b\uff0c\u53c8\u786e\u4fdd\u6a21\u578b\u80fd\u591f\u5b9e\u65f6\u6355\u6349\u7528\u6237\u7684\u52a8\u6001\u504f\u597d\uff0c\u662f\u4e00\u4e2a\u503c\u5f97\u63a2\u8ba8\u7684\u95ee\u9898\u3002\n\n### \u95ee\u9898\u7ec6\u5316\n\n1. **\u52a8\u6001\u504f\u597d\u6355\u6349**\uff1a\u5728\u63a8\u8350\u7cfb\u7edf\u548c\u4e2a\u6027\u5316\u670d\u52a1\u4e2d\uff0c\u7528\u6237\u7684\u504f\u597d\u53ef\u80fd\u4f1a\u968f\u65f6\u95f4\u3001\u60c5\u5883\u7b49\u56e0\u7d20\u53d1\u751f\u53d8\u5316\u3002\u5982\u4f55\u8bbe\u8ba1MCTS\u4e0ePRM\u7ed3\u5408\u7684\u6846\u67b6\uff0c\u4f7f\u5176\u80fd\u591f\u5b9e\u65f6\u6355\u6349\u8fd9\u4e9b\u52a8\u6001\u53d8\u5316\uff1f\n   \n2. **\u8ba1\u7b97\u6548\u7387\u4f18\u5316**\uff1aMCTS\u5728\u5927\u89c4\u6a21\u95ee\u9898\u4e2d\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u8f83\u9ad8\uff0c\u5982\u4f55\u4f18\u5316MCTS\u7684\u641c\u7d22\u7b56\u7565\uff0c\u4f7f\u5176\u5728PRM\u7684\u504f\u597d\u5efa\u6a21\u8fc7\u7a0b\u4e2d\u4fdd\u6301\u9ad8\u6548\uff1f\n\n3. **\u6a21\u578b\u6cdb\u5316\u80fd\u529b**\uff1a\u5982\u4f55\u786e\u4fddMCTS\u4e0ePRM\u7ed3\u5408\u7684\u6a21\u578b\u5728\u4e0d\u540c\u5e94\u7528\u573a\u666f\u4e0b\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u591a\u6a21\u6001\u6570\u636e\u65f6\uff1f\n\n4. **\u5b9e\u65f6\u51b3\u7b56\u652f\u6301**\uff1a\u5728\u63a8\u8350\u7cfb\u7edf\u548c\u4e2a\u6027\u5316\u670d\u52a1\u4e2d\uff0c\u5982\u4f55\u786e\u4fddMCTS\u4e0ePRM\u7ed3\u5408\u7684\u6a21\u578b\u80fd\u591f\u5feb\u901f\u505a\u51fa\u51b3\u7b56\uff0c\u6ee1\u8db3\u5b9e\u65f6\u6027\u8981\u6c42\uff1f\n\n### \u95ee\u9898\u610f\u4e49\n\n\u901a\u8fc7\u89e3\u51b3\u4e0a\u8ff0\u95ee\u9898\uff0c\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63d0\u5347MCTS\u4e0ePRM\u7ed3\u5408\u6846\u67b6\u5728\u63a8\u8350\u7cfb\u7edf\u548c\u4e2a\u6027\u5316\u670d\u52a1\u4e2d\u7684\u5e94\u7528\u6548\u679c\uff0c\u4f7f\u5176\u80fd\u591f\u66f4\u51c6\u786e\u5730\u6355\u6349\u7528\u6237\u7684\u52a8\u6001\u504f\u597d\uff0c\u5e76\u505a\u51fa\u5b9e\u65f6\u3001\u9ad8\u6548\u7684\u51b3\u7b56\u3002\u8fd9\u4e0d\u4ec5\u6709\u52a9\u4e8e\u63d0\u5347\u7528\u6237\u4f53\u9a8c\uff0c\u8fd8\u80fd\u63a8\u52a8\u76f8\u5173\u9886\u57df\u7684\u6280\u672f\u8fdb\u6b65\u3002", "task_step_question_context": [{"ref_id": "454848282814999732", "chunk_id": "2", "score": 0.3359375, "text": "# 2 RELATED WORK\nThe full version of the related work is in Appendix A, we briefly introduce several highly related works here. In general, model-based RL for solving decision-making problems can be divided into three perspectives: model learning, policy learning, and decision-making. Moreover, optimal control theory also concerns the decision-making problem and is deeply related to model-based RL.  \n\nModel learning: How to learn a good model to support decision-making is crucial in model-based RL. There are two main aspects of the work: the model structure designing (Chua et al., 2018; Zhang  \n\net al., 2021; 2020; Hafner et al., 2021; Chen et al., 2022) and the loss designing (D\u2019Oro et al., 2020;   \nFarahmand et al., 2017; Li et al., 2021).  \n\nPolicy learning: Two methods are always used to learn the policy by using the learned model. One is to serve the learned model as a black-box simulator to generate the data (Janner et al., 2019b; Yu et al., 2020; Lee et al., 2020). Another way is to use the learned model to calculate the policy gradient (Heess et al., 2015b; Clavera et al., 2019; Amos et al., 2021).  \n\nDecision-making: When making the decision, we need to generate the actions that can achieve our goal. Many of the model-based RL methods make the decision by using the learned policy solely (Hafner et al., 2021). Similar to our paper, some works also try to make decisions by using the learned model, but the majority only focus on the discrete action space. The well-known MCTS method achieves a lot of success. For example, the well-known Alpha Zero (Silver et al., 2017), MuZero (Schrittwieser et al., 2020). There are only a few works that study the continuous action space, such as the Continuous UCT (Cou\u00a8etoux et al., 2011), the sampled MuZero (Hubert et al., 2021), the TreePI (Springenberg et al., 2020), and the TD-MPC (Hansen et al., 2022a).  \n\nOptimal control theory: Beyond deep RL, optimal control also considers the decision-making problem but rather relies on the known and continuous transition model. In modern optimal control, Model Predictive Control (MPC) (Camacho & Alba, 2013) framework is always adopted when the environment is highly non-linear. In MPC, the action is planned during the execution by using the model, and such a procedure is called trajectory optimization. Plenty of previous works (Byravan et al., 2021; Chua et al., 2018; Pinneri et al., 2021; Nagabandi et al., 2020) use MPC framework to solve the continuous control tasks, but most of them are based on zero-order or sample-based method to do the planning. The most relevant works are DDP (Murray & Yakowitz, 1984), iLQR (Li & Todorov, 2004), and iLQG (Todorov & Li, 2005; Tassa et al., 2012). We discuss the detailed differences between our method and these methods in Appendix A.  \n\nSince our planning algorithm relies on the learned model and learned policy, we build our algorithm based on these works on model learning and policy learning . Our POMP algorithm tries to solve a more challenging task compared to the related work on decision-making : efficiently optimize the trajectory in continuous action space when the environment model is unknown. Different from our works, the MPC with DDP as trajectory optimizer from optimal control theory requires the known environment model, and also requires the hessian matrix for online optimization from scratch.\n\n# 3 PRELIMINARIES\nReinforcement Le onsider discrete-time Marko Decision Process (M $\\mathcal{M}$ the tuple ($(\\mathcal{X},\\mathcal{A},f,r,\\gamma)$ XA $\\mathcal{X}$ state space, A is the action space, $f\\,:\\,x_{t+1}\\,=$   \n$f(x_{t},a_{t})$ is the transition model, $r:\\mathcal{X}\\times\\mathcal{A}\\to\\mathbb{R}$ X \u00d7 A \u2192 is the reward function, $\\gamma$ is the discount factor. $t$ $\\begin{array}{r}{R_{t}=\\sum_{t^{\\prime}=t}^{\\infty}\\gamma^{t^{\\prime}-t}r_{t^{\\prime}}}\\end{array}$ , and Reinforcement Learn  \n$\\begin{array}{r}{\\operatorname*{max}_{\\theta}J(\\theta)=\\operatorname*{max}_{\\theta}\\mathbb{E}_{\\pi_{\\theta}}R_{t}=\\operatorname*{max}_{\\theta}\\mathbb{E}_{\\pi_{\\theta}}\\Big[\\sum_{t^{\\prime}=t}^{\\infty}\\gamma^{t^{\\prime}-t}r(x_{t^{\\prime}},a_{t^{\\prime}})\\Big].}\\end{array}$ ing (RL) aims to find a policy $\\pi_{\\theta}:\\mathcal{X}\\times\\mathcal{A}\\rightarrow\\mathbb{R}^{+}$ X \u00d7 A \u2192 h P that can maximize the expected return .$J$ . where  \n\n$\\begin{array}{r}{\\operatorname*{max}_{a_{t}}\\mathbb{E}\\Big[r(x_{t},a_{t}|x_{t}\\,=\\,x)+\\gamma V^{*}(x_{t+1})\\Big]}\\end{array}$ value function obeys an important identity known as the Bellman optimality equation Bellman Equation. hWe define the optimal value function i . The idea behind this equation is that if we know the $V^{*}(x)=\\operatorname*{max}\\mathbb{E}[R_{t}|x_{t}=x]$ |. The optimal $V^{*}(x)\\;=\\;$ $r(x_{t},a_{t})$ for any $a_{t}$ and next step value function $V^{*}(x_{t+1})$ for any $s_{t+1}$ , we can recursively select the action $a_{t}$ which m $r(x_{t},a_{t}|x_{t}=x)+\\gamma V^{*}(x_{t+1})$ milarly, we can denote the optimal action-value function $Q^{*}(x,a)\\;=\\;\\operatorname*{max}\\mathbb{E}[R_{t}|x_{t}\\;=\\;x,a_{t}\\;=\\;a]$ |], and it obeys a similar Bellman optimility equation $\\begin{array}{r}{Q^{*}(x,a)=\\operatorname*{max}_{a_{t+1}}\\mathbb{E}\\Big[r(x_{t},a_{t}|x_{t}=x,a_{t}=a)+\\gamma Q^{*}(x_{t+1},a_{t+1})\\Big].}\\end{array}$ .  \n\nModel-based RL. Model-based RL method distinguishes itself from model-free counterparts by using the data to learn a transition model. Following Janner et al. (2019a) and Clavera et al. (2019), we use parametric neural networks to approximate the transition function, reward function, policy function and $\\mathrm{^Q}$ -value function with the following objective function to be optimized  $J_{f}(\\psi)\\,=\\,\\mathbb{E}\\big[\\log f(x_{t+1}|x_{t},a_{t})\\big]$ '', $J_{r}(\\omega)\\,=\\,\\mathbb{E}\\big[\\log r(r_{t}|x_{t},a_{t})\\big]$ '', $\\begin{array}{r}{\\bar{J_{\\pi}}(\\theta)\\,=\\,\\mathbb{E}\\bigl[\\sum_{t=0}^{H-1}\\gamma^{t}r(\\bar{x}_{t},a_{t})\\,+\\,}\\end{array}$ ' P$\\gamma^{H}Q(x_{H},a_{H})]$ 'and $J_{Q}\\,=\\,\\mathbb{E}\\bigl[\\|Q(x_{t},a_{t})-(r+\\tilde{Q}(x_{t+1},a_{t+1}))\\|_{2}\\bigr]$ '\u2225\u2212\u2225', respectively. In ${\\cal J}_{\\pi}(\\theta)$ , we truncate the trajectory in horizon Hto avoid long time model rollout.  \n\nNotations. For one-dimensional state and action case, we denote the partial differentiation of function by using its output with subscripts, e.g. ,$\\begin{array}{r}{r_{x}\\ \\triangleq\\ \\frac{\\partial r(x,a)}{\\partial x},\\ r_{a}\\ \\triangleq\\ \\frac{\\bigtriangleup r(x,a)}{\\partial a},\\ f_{x}\\ \\triangleq\\ \\frac{\\partial f(x,a)}{\\partial x}}\\end{array}$ ,$f_{a}\\triangleq{\\frac{\\partial f(x,a)}{\\partial a}}$ ,$\\begin{array}{r}{Q_{x}\\triangleq\\frac{\\partial Q(x,a)}{\\partial x}}\\end{array}$ and $\\begin{array}{r}{Q_{a}\\triangleq\\frac{\\partial Q(x,a)}{\\partial a}}\\end{array}$ . See Appendix E for the multi-dimension case."}, {"ref_id": "454845581169535994", "chunk_id": "7", "score": 0.306640625, "text": "# 4 Theoretical Analysis\nAlthough it is difficult to analyze the regret of MCTS-VS directly, we can theoretically analyze the influence of general variable selection by adopting the acquisition function GP-UCB. The considered general variable selection framework is as follows: after selecting a subset of variables at each iteration, the corresponding observation data (i.e., the data points sampled-so-far where only the selected variables are used) is used to build a GP model, and the next data point is sampled by maximizing GP-UCB. We use $\\mathbb{M}_{t}$ to denote the sampled variable index subset at iteration $t$ , and let $\\left|\\mathbb{M}_{t}\\right|=d_{t}$ .  \n\nRegret Analysis. Let $x^{*}$ denote an optimal solution. We analyze the cumulative regret $R_{T}\\,=$ $\\begin{array}{r}{\\sum_{t=1}^{\\bar{T}}(f(\\pmb{x}^{*})-f(\\pmb{x}^{t}))}\\end{array}$ the selected points by iteration \u2212, i.e., the Tum of the gap between the opti . To derive an upper bound on $R_{T}$ m and the function values of , we pessimistically assume that the worst function value, i.e., $\\begin{array}{r}{\\operatorname*{min}_{\\pmb{x}_{[D]\\setminus\\mathbb{M}_{t}}}f([\\pmb{x}_{\\mathbb{M}_{t}},\\pmb{x}_{[D]\\setminus\\mathbb{M}_{t}}])}\\end{array}$ , given ${\\pmb x}_{\\mathbb{M}_{t}}$ is returned in evaluation. As in [ Lipschitz assumption. 21 ,38 ], we assume that $\\mathcal{X}\\subset[0,r]^{D}$ is convex and compact, and $f$ satisfies the following Assumption 4.1. The function $f$ is a GP sample path. For some $a,b>0$ , given $L>0$ , the partial derivatives of $f$ satisfy that $\\forall i\\in[D]$ ,$\\exists\\alpha_{i}\\geq0$ ,  \n\n$$\nP\\left(\\operatorname*{sup}_{x\\in\\mathcal{X}}\\left|\\partial f/\\partial x_{i}\\right|<\\alpha_{i}L\\right)\\geq1-a e^{-\\left(L/b\\right)^{2}}.\n$$  \n\nBased on Assumption 4.1, we define $\\alpha_{i}^{*}$ to be the minimum value of $\\alpha_{i}$ such that Eq. (3) holds, which characterizes the importance of the $i$ -th variable $x_{i}$ . The larger $\\alpha_{i}^{*}$ , the greater influence of $x_{i}$ on the function $f$ . Let $\\alpha_{\\mathrm{max}}=\\operatorname*{max}_{i\\in[D]}\\alpha_{i}^{*}$ .  \n\nTheorem 4.2 gives an upper bound on the cumulative regret $R_{T}$ with high probability for general variable selection methods. The proof is inspired by that of GP-UCB without variable selection [ 38 ]$\\forall i:\\alpha_{i}^{*}\\leq1$ and provided in Appendix B.1. If we select all variables each time (i.e., \u2264(4) becomes $R_{T}\\leq\\sqrt{C_{1}T\\beta_{T}^{*}\\gamma_{T}}+2$ p, which is consistent with [ $\\forall t:\\mathbb{M}_{t}=[D])$ and assume 38 ]. Note that \u2200$\\forall t:|\\mathbb{M}_{t}|\\,=\\,d_{t}\\,=\\,D$ ||in this case, which implies that $\\beta_{t}$ increases with $t$ , leading to $\\beta_{T}^{*}=\\beta_{T}$ . We can see that usi variable selection will $R_{T}$ by $\\begin{array}{r}{2\\sum_{t=1}^{T}\\sum_{i\\in[D]\\backslash\\mathbb{M}_{t}}\\alpha_{i}^{*}L r}\\end{array}$ P,variables unselected, the larger related to the importance (i.e., $R_{T}$ $\\alpha_{i}^{*}$ . Meanwhile, the term ) of unselected variables at each iteration. The more important $\\sqrt{C_{1}T\\beta_{T}^{*}\\gamma_{T}}$ pwill decrease as \u2208$\\beta_{T}^{*}$ \\relies on the number $d_{t}$ of selected variables positively. Ideally, if the unselected variables at each iteration are always unrelated (i.e., $\\alpha_{i}^{*}\\!=\\!0$ ), the regret bound will be better than that of using all variables [38].  \n\n$b\\sqrt{\\log(4D a/\\delta)}$ Theorem 4.2. p$\\forall\\delta\\ \\in\\ (0,1)$ , where $r$ is the upper bound on each variable, and , let $\\beta_{t}\\ =\\ 2\\log(4\\pi_{t}/\\delta)\\,+\\,2d_{t}\\log(d_{t}t^{2}b r\\sqrt{\\log(4D a/\\delta)})$ {$\\{\\pi_{t}\\}_{t\\ge1}$ }\u2265satisfies $\\textstyle\\sum_{t\\geq1}\\pi_{t}^{-1}=1$ and $L\\;=$ and $\\pi_{t}>0$ . Let $\\beta_{T}^{*}=\\operatorname*{max}_{1\\leq i\\leq T}\\beta_{t}$ . At iteration $T$ , the cumulative regret  \n\n$$\nR_{T}\\leq\\sqrt{C_{1}T\\beta_{T}^{*}\\gamma_{T}}+2\\alpha_{\\operatorname*{max}}+2\\sum_{t=1}^{T}\\sum_{\\substack{i\\in[D]\\backslash\\mathbb{M}_{t}}}\\alpha_{i}^{*}L r\n$$  \n\nholds with probability least $1\\!-\\!\\delta$ , where $C_{1}$ is a constant, $\\begin{array}{r}{\\gamma_{T}\\!=\\!\\operatorname*{max}_{|\\mathcal{D}|=\\!T}I(\\pmb{y}_{\\mathcal{D}},\\pmb{f}_{\\mathcal{D}}),}\\end{array}$ $I(\\cdot,\\cdot)$ is the information gain, and $\\scriptstyle y_{\\mathcal{D}}$ Dand $f_{\\mathcal{D}}$ Dare the noisy and true observations of a set Dof points, respectively.  \n\nBy selecting been proved [21] that the cumulative regret of Dropout satisfies $d$ variables randomly at each iteration and assuming that $r=1$ and $\\forall i:\\alpha_{i}^{*}\\leq1$ \u2264, it has  \n\n$$\nR_{T}\\leq\\sqrt{C_{1}T\\beta_{T}\\gamma_{T}}+2+2T L(D-d).\n$$  \n\nIn this case, we have $d_{t}=|\\mathbb{M}_{t}|=d$ ,$r=1$ and $\\forall i:\\alpha_{i}^{*}\\leq1$ \u2264. Thus, Eq. (4) becomes  \n\n$$\nR_{T}\\leq\\sqrt{C_{1}T\\beta_{T}^{*}\\gamma_{T}}+2+2T L(D-d).\n$$  \n\nNote that $\\beta_{T}^{*}=\\beta_{T}$ here, as $\\beta_{t}$ increases with $t$ given $d_{t}=d$ . This implies that our bound Eq. (4) for general variable selection is a generalization of Eq. (5) for Dropout [ 21 ]. In [ 33 ], a regret bound analysis has also been performed for variable selection, by optimizing over $d$ fixed important variables and using a common parameter $\\alpha$ to characterize the importance of all the other $D-d$ variables.  \n\nComputational Complexity Analysis. The computational complexity of one iteration of BO depends on three critical components: fitting a GP surrogate model, maximizing an acquisition function and evaluating a sampled point. If using the squared exponential kernel, the computational complexity of fitting a GP model at iteration $t$ is $\\bar{O(t^{3}\\!+\\!t^{2}d_{t})}$ . Maximizing an acquisition function is related to the optimization algorithm. If we use the Quasi-Newton method to optimize GP-UCB, the computational complexity is $\\bar{\\mathcal{O}}(m(t^{2}+t d_{t}+d_{t}^{2}))$ [28 ], where $m$ denotes the Quasi-Newton\u2019s running rounds. The cost of evaluating a sampled point is fixed. Thus, by selecting only a subset of variables, instead of all variables, to optimize, the computational complexity can be decreased significantly. The detailed analysis is provided in Appendix B.2.  \n\nInsight. The above regret and computational complexity analyses have shown that variable selection can reduce the computational complexity while increasing the regret. Given the number $d_{t}$ of variables to be selected, a good variable selection method should select as important variables as possible, i.e., variables with as large $\\alpha_{i}^{*}$ as possible, which may help to design and evaluate variable selection methods. The experiments in Section 5.1 will show that MCTS-VS can select a good subset of variables while maintaining a small computational complexity."}, {"ref_id": "454847097820262972", "chunk_id": "6", "score": 0.298828125, "text": "# 4.1 User Modeling\nWe firstly encode the state $s_{t}$ , which contains all the conversational information of the prior $t\\!-\\!1$ turns. The current state includes six components: $s_{t}\\,=\\,\\{\\dot{u},\\dot{\\mathcal{P}}_{u}^{(t)},\\mathcal{P}_{r e j}^{(t)},\\mathcal{V}_{r e j}^{(t)},\\mathcal{P}_{c a n d}^{(t)},\\mathcal{V}_{c a n d}^{(t)}\\}$ . Previous methods [ 8,13 ,15 ] for MCR only extract the user\u2019s interest from the current state, ignoring the complements of historical interactions to the current user\u2019s preference. To this end, we construct a current graph and a global graph to jointly learn user representations. Moreover, we develop an iterative multi-interest extractor to obtain multiple interests of the user, which will be discussed in subsection 5.1.\n\n# 4.2 Consultation\nOnce the system finishes the user modeling step, it will move to the consultation step, with the purpose to decide whether to ask attribute instances or to recommend items. To make the next action more profitable and recommend successfully with the fewer turns, we employ a reinforcement learning (RL) method based on the extracted multiple interests of the user to learn the policy. The action space includes all candidate items and candidate attribute instances. However, in the real world, the number of items and attribute instances is very large, which severely limits the efficiency of CRS. To improve the efficiency, we sample $K_{v}$ items and $K_{p}$ attribute instances as action space $\\mathcal{A}_{t}$ . We develop a novel dueling Q-network [ 34 ] to calculate the Q-value of each action in $\\mathcal{A}_{t}$ . If CRS decides to ask a question, our method will select $K_{a}$ attribute instances in ${\\mathcal{A}}_{t}$ with the same attribute type to generate attribute type-based multiple choice questions . The user can choose zero (the option \"Others\" as shown in conversation (b) of Figure 1), one, or more attribute instances with the given attribute type. If CRS decides to recommend items, the system will select $K$ items in ${\\mathcal{A}}_{t}$ to recommend. We will discuss the details of sampling strategies and policy learning in subsection 5.2.\n\n# 4.3 Transition\nWhen the user responds to the action of agent, the transition step will be triggered. This step will transition the current state to the next state $s_{t+1}$ . If the user responds to the question, attribute instance sets that the user accepts and rejects in this turn can be defined as $\\mathcal{P}_{c u r\\_a c c}^{(t)}$ and $\\mathcal{P}_{c u r\\_r e j}^{(t)}$ respectively. Some components are updated by $\\mathcal{P}_{c a n d}^{(t+1)}=\\mathcal{P}_{c a n d}^{(t)}-\\overset{-}{\\mathcal{P}}_{c u r\\_r e j}^{(t)}-\\mathcal{P}_{c u r\\_a c c}^{(t)},\\mathcal{P}_{r e j}^{(t+1)}=\\mathcal{P}_{r e j}^{(t)}\\cup\\mathcal{P}_{c u r\\_r e j}^{(t)}$ and $\\mathcal{P}_{u}^{(t+1)}\\;=\\;\\mathcal{P}_{u}^{(t)}\\;\\cup\\;\\mathcal{P}_{c u r\\_a c c}^{(t)}$ . When the user is recommended items, if the set $\\mathcal{V}_{r e c}^{(t)}$ of recommended items are all rejected, the next state can be updated by $\\mathcal{V}_{r e j}^{(t+1)}=\\mathcal{V}_{r e j}^{(t)}\\cup\\mathcal{V}_{r e c}^{(t)}$ . Otherwise, this conversation session ends. Finally, we need to update the candidate item set $\\mathcal{V}_{c a n d}^{(t+1)}$ based on the user\u2019s feedback. Previous works [ 8,15 ]update candidate items based the intersection set strategy, that is, only the items satisfying all the accepted attribute instances in $\\mathcal{P}_{u}^{(t+1)}$ remain, which obviously deviates from the scenario. In fact, the user might not prefer the combination of all attribute instances, but rather part of them. To this end, we propose the attribute instance-based union set strategy to update $\\bar{\\mathcal{V}}_{c a n d}^{(t+\\bar{1})}$ as follows:  \n\n  \nFigure 2: The overview of Multi-Interest Policy Learning (MIPL).  \n\n$$\n\\begin{array}{r}{\\mathcal{V}_{c a n d}^{(t+1)}=\\{v\\vert v\\in\\mathcal{V}_{p_{0}}-\\mathcal{V}_{r e j}^{(t+1)}\\ \\mathrm{~and~}\\,\\mathcal{P}_{v}\\cap\\mathcal{P}_{u}^{(t+1)}\\neq\\varnothing}\\\\ {\\qquad\\qquad\\qquad\\qquad\\qquad\\mathrm{~and~}\\,\\mathcal{P}_{v}\\cap\\mathcal{P}_{r e j}^{(t+1)}=\\varnothing\\}}\\end{array}\n$$  \n\nwhere $\\mathcal{V}_{p_{0}}$ is the item set in which all items are associated to attribute instance $\\scriptstyle{\\mathcal{P}}0$ which initializes the conversation session. In this way, we can get the next state, which will be updated as $s_{t+1}~=$ $\\{\\Bar{u_{*}}^{\\prime}\\mathcal{P}_{u}^{(t+1)},\\mathcal{P}_{r e j}^{(\\Bar{t}+1)},\\mathcal{V}_{r e j}^{(t+1)},\\mathcal{P}_{c a n d}^{(t+1)},\\mathcal{V}_{c a n d}^{(t+1)}\\}$ .\n\n# 4.4 Reward\nIn this work, five kinds of rewards are defined following [ 8,15 ],   \nnamely, (1) $r_{r e c\\_s u c}$ , a strongly positive reward when the recommen  \ndation succeeds, (2) $r_{r e c\\_f a i l}$ , a strongly negative reward when the $r_{a s k\\_s u c}$ , a slightly positive reward when   \nthe user accepts an asked attribute instance, $(4)\\,r_{a s k\\_f a i l}$ , a negative $r_{q u i t}$ , a   \nstrongly negative reward if the session reaches the maximum number   \nof turns. In addition, since our method asks multiple choice ques  \ntions, we design the reward from the user\u2019s feedback on a question   \nin the form of sum as $\\begin{array}{r}{r_{t}=\\sum_{\\mathcal{P}_{c u r_{-}a c c}^{(t)}}r_{a s k_{-}s u c}+\\sum_{\\mathcal{P}_{c u r_{-}r e j}^{(t)}}r_{a s k_{-}r e j}.}\\end{array}$\n\n# 5MULTI-INTEREST POLICY LEARNING\nIn this section, we detail the design of Multi-Interest Policy Learning (MIPL) module. As shown in Figure 2, to obtain more comprehensive user representations, we establish a current graph to capture user current preferences, and a global graph to capture long-term preferences. Based on the learned node representations of the two graphs, we propose an iterative multi-interest extractor to model user\u2019s preferences for different combinations of attribute instances. Moreover, we design a new dueling Q-network [ 34 ] to decide the next action based on the extracted multiple interests."}], "task_step_question_answer": "\u5728\u4e2a\u6027\u5316\u670d\u52a1\u4e2d\uff0c\u901a\u8fc7\u4f18\u5316\u8ba1\u7b97\u6548\u7387\u548c\u589e\u5f3a\u6a21\u578b\u6cdb\u5316\u80fd\u529b\uff0c\u6a21\u578b\u53ef\u4ee5\u66f4\u9ad8\u6548\u5730\u5904\u7406\u591a\u6a21\u6001\u6570\u636e\uff0c\u63d0\u4f9b\u66f4\u4e2a\u6027\u5316\u7684\u670d\u52a1\u4f53\u9a8c\u3002\u4f8b\u5982\uff0c\u5728\u89c6\u9891\u63a8\u8350\u7cfb\u7edf\u4e2d\uff0c\u901a\u8fc7\u7ed3\u5408\u7528\u6237\u7684\u5386\u53f2\u89c2\u770b\u8bb0\u5f55\u548c\u5b9e\u65f6\u53cd\u9988\uff0c\u6a21\u578b\u53ef\u4ee5\u52a8\u6001\u8c03\u6574\u63a8\u8350\u7b56\u7565\uff0c\u786e\u4fdd\u63a8\u8350\u5185\u5bb9\u4e0e\u7528\u6237\u5f53\u524d\u5174\u8da3\u9ad8\u5ea6\u5339\u914d\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u5f15\u5165\u8fc1\u79fb\u5b66\u4e60\uff0c\u6a21\u578b\u53ef\u4ee5\u5728\u4e0d\u540c\u5e94\u7528\u573a\u666f\u4e2d\u5feb\u901f\u9002\u5e94\uff0c\u63d0\u5347\u5176\u6cdb\u5316\u80fd\u529b\u3002\u5728\u5b9e\u65f6\u51b3\u7b56\u652f\u6301\u65b9\u9762\uff0c\u4f7f\u7528\u9ad8\u6548\u7684\u786c\u4ef6\u52a0\u901f\u548c\u5e76\u884c\u8ba1\u7b97\u6280\u672f\uff0c\u6a21\u578b\u53ef\u4ee5\u5728\u6beb\u79d2\u7ea7\u65f6\u95f4\u5185\u751f\u6210\u63a8\u8350\u7ed3\u679c\uff0c\u6ee1\u8db3\u5b9e\u65f6\u6027\u8981\u6c42\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u4f18\u5316\u65b9\u6cd5\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u4ecd\u9700\u8003\u8651\u6570\u636e\u9690\u79c1\u548c\u6a21\u578b\u89e3\u91ca\u6027\u95ee\u9898\uff0c\u4ee5\u786e\u4fdd\u7528\u6237\u4fe1\u4efb\u548c\u5408\u89c4\u6027\u3002", "ref_task_step_id": ""}, "__type__": "task_step_node"}, "d6916989-efb3-47e8-978b-9f9e079e1eaf": {"__data__": {"id_": "d6916989-efb3-47e8-978b-9f9e079e1eaf", "metadata": {}, "relationships": {}, "hash": "", "start_task_context": "MCTS\u5728PRM\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u7684\u5e94\u7528\u63a2\u7d22\u7efc\u8ff0", "aemo_representation_context": "### Step 1: \u5206\u6790\u8fd1\u51e0\u5e74\u7814\u7a76\u9886\u57df\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba\n\n**\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09**\uff1a\n- **\u57fa\u672c\u539f\u7406**\uff1aMCTS\u662f\u4e00\u79cd\u7528\u4e8e\u51b3\u7b56\u8fc7\u7a0b\u7684\u542f\u53d1\u5f0f\u641c\u7d22\u7b97\u6cd5\uff0c\u5e7f\u6cdb\u5e94\u7528\u4e8e\u6e38\u620fAI\u3001\u8def\u5f84\u89c4\u5212\u7b49\u9886\u57df\u3002\u5176\u6838\u5fc3\u601d\u60f3\u662f\u901a\u8fc7\u6a21\u62df\u591a\u6b21\u968f\u673a\u6e38\u620f\u6765\u8bc4\u4f30\u4e0d\u540c\u51b3\u7b56\u7684\u4f18\u52a3\u3002\n- **\u6280\u672f\u6846\u67b6**\uff1a\u4e3b\u8981\u5305\u62ec\u9009\u62e9\uff08Selection\uff09\u3001\u6269\u5c55\uff08Expansion\uff09\u3001\u6a21\u62df\uff08Simulation\uff09\u548c\u56de\u6eaf\uff08Backpropagation\uff09\u56db\u4e2a\u6b65\u9aa4\u3002\n\n**\u504f\u597d\u7b56\u7565\u6a21\u578b\uff08PRM\uff09**\uff1a\n- **\u57fa\u672c\u539f\u7406**\uff1aPRM\u65e8\u5728\u901a\u8fc7\u5b66\u4e60\u7528\u6237\u7684\u504f\u597d\u6765\u4f18\u5316\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5e38\u7528\u4e8e\u63a8\u8350\u7cfb\u7edf\u3001\u4e2a\u6027\u5316\u670d\u52a1\u7b49\u3002\n- **\u6280\u672f\u6846\u67b6**\uff1a\u901a\u5e38\u5305\u62ec\u504f\u597d\u5efa\u6a21\u3001\u7b56\u7565\u4f18\u5316\u548c\u6a21\u578b\u5fae\u8c03\u7b49\u73af\u8282\u3002\n\n**\u65b9\u6cd5\u8bba**\uff1a\n- **\u96c6\u6210\u5b66\u4e60**\uff1a\u5c06MCTS\u4e0ePRM\u7ed3\u5408\uff0c\u5229\u7528MCTS\u7684\u641c\u7d22\u80fd\u529b\u4f18\u5316PRM\u7684\u51b3\u7b56\u8fc7\u7a0b\u3002\n- **\u5f3a\u5316\u5b66\u4e60**\uff1a\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u673a\u5236\u4e0d\u65ad\u8c03\u6574PRM\u7684\u53c2\u6570\uff0c\u4ee5\u9002\u5e94\u52a8\u6001\u73af\u5883\u3002\n\n### Step 2: \u7814\u7a76\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u4e3b\u8981\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0e\u53d8\u4f53\n\n**\u5e94\u7528\u573a\u666f**\uff1a\n- **\u6e38\u620fAI**\uff1aMCTS\u4e0ePRM\u7ed3\u5408\u7528\u4e8e\u6e38\u620f\u4e2d\u7684\u667a\u80fd\u51b3\u7b56\uff0c\u5982AlphaGo\u3002\n- **\u8def\u5f84\u89c4\u5212**\uff1a\u5728\u673a\u5668\u4eba\u5bfc\u822a\u548c\u81ea\u52a8\u9a7e\u9a76\u4e2d\uff0c\u5229\u7528MCTS\u4f18\u5316\u8def\u5f84\u9009\u62e9\u3002\n- **\u63a8\u8350\u7cfb\u7edf**\uff1a\u901a\u8fc7PRM\u5b66\u4e60\u7528\u6237\u504f\u597d\uff0c\u7ed3\u5408MCTS\u8fdb\u884c\u66f4\u7cbe\u51c6\u7684\u63a8\u8350\u3002\n\n**\u53d8\u4f53**\uff1a\n- **\u6df1\u5ea6MCTS**\uff1a\u7ed3\u5408\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff0c\u63d0\u5347MCTS\u7684\u51b3\u7b56\u80fd\u529b\u3002\n- **\u591a\u6a21\u6001PRM**\uff1a\u878d\u5408\u591a\u79cd\u6570\u636e\u6e90\uff08\u5982\u6587\u672c\u3001\u56fe\u50cf\uff09\uff0c\u589e\u5f3aPRM\u7684\u6cdb\u5316\u80fd\u529b\u3002\n\n### Step 3: \u8bc4\u4f30\u5b66\u672f\u754c\u7684\u6280\u672f\u8fdb\u6b65\u4e0e\u5c40\u9650\u6027\n\n**\u6280\u672f\u8fdb\u6b65**\uff1a\n- **\u6027\u80fd\u63d0\u5347**\uff1aMCTS\u4e0ePRM\u7ed3\u5408\u663e\u8457\u63d0\u5347\u4e86\u51b3\u7b56\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002\n- **\u6cdb\u5316\u80fd\u529b**\uff1a\u591a\u6a21\u6001PRM\u5728\u5904\u7406\u591a\u6837\u5316\u6570\u636e\u65f6\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002\n\n**\u5c40\u9650\u6027**\uff1a\n- **\u8ba1\u7b97\u590d\u6742\u5ea6**\uff1aMCTS\u5728\u5927\u89c4\u6a21\u95ee\u9898\u4e2d\u8ba1\u7b97\u91cf\u5927\uff0c\u6548\u7387\u6709\u5f85\u63d0\u5347\u3002\n- **\u6570\u636e\u4f9d\u8d56**\uff1aPRM\u7684\u6027\u80fd\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u9ad8\u8d28\u91cf\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u6570\u636e\u504f\u5dee\u95ee\u9898\u4f9d\u7136\u5b58\u5728\u3002\n\n### Step 4: \u63a2\u8ba8\u8ba1\u7b97\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\n\n**\u9002\u7528\u6027**\uff1a\n- **\u591a\u9886\u57df\u5e94\u7528**\uff1aMCTS+PRM\u6846\u67b6\u5728\u6e38\u620f\u3001\u8def\u5f84\u89c4\u5212\u3001\u63a8\u8350\u7cfb\u7edf\u7b49\u591a\u4e2a\u9886\u57df\u8868\u73b0\u51fa\u826f\u597d\u7684\u9002\u7528\u6027\u3002\n- **\u591a\u6a21\u6001\u6570\u636e**\uff1a\u591a\u6a21\u6001PRM\u80fd\u591f\u5904\u7406\u591a\u79cd\u7c7b\u578b\u7684\u6570\u636e\uff0c\u9002\u7528\u4e8e\u590d\u6742\u5e94\u7528\u573a\u666f\u3002\n\n**\u6cdb\u5316\u80fd\u529b**\uff1a\n- **\u8de8\u9886\u57df\u6cdb\u5316**\uff1a\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\uff0cMCTS+PRM\u6846\u67b6\u5728\u4e0d\u540c\u9886\u57df\u95f4\u5c55\u73b0\u51fa\u8f83\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002\n- **\u52a8\u6001\u73af\u5883\u9002\u5e94**\uff1a\u5f3a\u5316\u5b66\u4e60\u673a\u5236\u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u9002\u5e94\u52a8\u6001\u53d8\u5316\u7684\u73af\u5883\u3002\n\n### Step 5: \u5206\u6790\u6700\u65b0\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027\n\n**\u7a33\u5b9a\u6027**\uff1a\n- **\u7b97\u6cd5\u4f18\u5316**\uff1a\u901a\u8fc7\u6539\u8fdbMCTS\u7684\u9009\u62e9\u7b56\u7565\u548cPRM\u7684\u53c2\u6570\u8c03\u6574\u673a\u5236\uff0c\u63d0\u5347\u4e86\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u3002\n- **\u9c81\u68d2\u6027\u589e\u5f3a**\uff1a\u5f15\u5165\u5bf9\u6297\u8bad\u7ec3\u548c\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\uff0c\u63d0\u9ad8\u6a21\u578b\u5728\u590d\u6742\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u6027\u3002\n\n**\u5bb9\u9519\u6027**\uff1a\n- **\u9519\u8bef\u5904\u7406**\uff1a\u5728\u6a21\u62df\u548c\u56de\u6eaf\u9636\u6bb5\u5f15\u5165\u5bb9\u9519\u673a\u5236\uff0c\u51cf\u5c11\u9519\u8bef\u51b3\u7b56\u7684\u5f71\u54cd\u3002\n- **\u5927\u89c4\u6a21\u6570\u636e\u9002\u5e94\u6027**\uff1a\u4f18\u5316\u7b97\u6cd5\u7ed3\u6784\uff0c\u63d0\u5347\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\u7684\u5904\u7406\u80fd\u529b\u3002\n\n### Step 6: \u8bc4\u4f30\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218\n\n**\u672a\u6765\u7814\u7a76\u65b9\u5411**\uff1a\n- **\u9ad8\u6548\u8ba1\u7b97**\uff1a\u7814\u7a76\u66f4\u9ad8\u6548\u7684MCTS\u7b97\u6cd5\uff0c\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002\n- **\u6570\u636e\u589e\u5f3a**\uff1a\u63a2\u7d22\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u51cf\u5c11PRM\u5bf9\u9ad8\u8d28\u91cf\u6570\u636e\u7684\u4f9d\u8d56\u3002\n\n**\u6311\u6218**\uff1a\n- **\u8de8\u6a21\u6001\u878d\u5408**\uff1a\u5982\u4f55\u6709\u6548\u878d\u5408\u591a\u79cd\u6a21\u6001\u6570\u636e\uff0c\u63d0\u5347\u6a21\u578b\u7684\u7efc\u5408\u6027\u80fd\u3002\n- **\u52a8\u6001\u73af\u5883\u9002\u5e94**\uff1a\u5728\u52a8\u6001\u73af\u5883\u4e0b\uff0c\u5982\u4f55\u4fdd\u6301\u6a21\u578b\u7684\u7a33\u5b9a\u6027\u548c\u9ad8\u6548\u6027\u3002\n\n**\u63a8\u52a8\u540e\u7eed\u7814\u7a76**\uff1a\n- **\u65b0\u95ee\u9898\u63d0\u51fa**\uff1a\u5982\u591a\u6a21\u6001\u6570\u636e\u4e0b\u7684\u504f\u597d\u5efa\u6a21\u3001\u52a8\u6001\u73af\u5883\u4e0b\u7684\u51b3\u7b56\u4f18\u5316\u7b49\u3002\n- **\u7814\u7a76\u5207\u5165\u70b9**\uff1a\u4ece\u7b97\u6cd5\u4f18\u5316\u3001\u6570\u636e\u589e\u5f3a\u3001\u8de8\u6a21\u6001\u878d\u5408\u7b49\u591a\u4e2a\u89d2\u5ea6\u5207\u5165\uff0c\u63a8\u52a8\u6280\u672f\u8fdb\u6b65\u3002\n\n### \u603b\u7ed3\n\n\u8fd1\u5e74\u6765\uff0cMCTS\u5728PRM\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u7684\u5e94\u7528\u63a2\u7d22\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4e3b\u8981\u4f53\u73b0\u5728\u6280\u672f\u6846\u67b6\u7684\u5b8c\u5584\u3001\u5e94\u7528\u573a\u666f\u7684\u62d3\u5c55\u3001\u6027\u80fd\u7684\u63d0\u5347\u4ee5\u53ca\u7a33\u5b9a\u6027\u548c\u5bb9\u9519\u6027\u7684\u589e\u5f3a\u3002\u7136\u800c\uff0c\u4ecd\u5b58\u5728\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u3001\u6570\u636e\u4f9d\u8d56\u6027\u5f3a\u7b49\u5c40\u9650\u6027\u3002\u672a\u6765\u7684\u7814\u7a76\u5e94\u5173\u6ce8\u9ad8\u6548\u8ba1\u7b97\u3001\u6570\u636e\u589e\u5f3a\u548c\u8de8\u6a21\u6001\u878d\u5408\u7b49\u65b9\u5411\uff0c\u4ee5\u5e94\u5bf9\u52a8\u6001\u73af\u5883\u4e0b\u7684\u51b3\u7b56\u4f18\u5316\u7b49\u6311\u6218\uff0c\u63a8\u52a8\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002", "task_step_name": "\u7814\u7a76\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u4e3b\u8981\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0e\u53d8\u4f53", "task_step_description": "MCTS\u4e0ePRM\u7ed3\u5408\u7528\u4e8e\u6e38\u620f\u4e2d\u7684\u667a\u80fd\u51b3\u7b56\uff0c\u5982AlphaGo\u3002", "task_step_level": "1", "task_step_question": "### \u95ee\u9898\u63d0\u51fa\n\n\u5728\u201c\u7814\u7a76\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u4e3b\u8981\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0e\u53d8\u4f53\u201d\u8fd9\u4e00\u6b65\u9aa4\u4e2d\uff0cMCTS\u4e0ePRM\u7ed3\u5408\u7528\u4e8e\u6e38\u620f\u4e2d\u7684\u667a\u80fd\u51b3\u7b56\uff0c\u5982AlphaGo\u3002\u57fa\u4e8e\u8fd9\u4e00\u80cc\u666f\uff0c\u63d0\u51fa\u4ee5\u4e0b\u95ee\u9898\uff1a\n\n**\u95ee\u9898**\uff1a\u5728\u6e38\u620fAI\u9886\u57df\uff0cMCTS\u4e0ePRM\u7ed3\u5408\u7684\u5177\u4f53\u5b9e\u73b0\u65b9\u5f0f\u6709\u54ea\u4e9b\uff1f\u8fd9\u4e9b\u5b9e\u73b0\u65b9\u5f0f\u5728AlphaGo\u7b49\u5b9e\u9645\u5e94\u7528\u4e2d\u5982\u4f55\u4f18\u5316\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5e76\u63d0\u5347\u6e38\u620fAI\u7684\u667a\u80fd\u6c34\u5e73\uff1f\n\n**\u95ee\u9898\u89e3\u6790**\uff1a\n1. **\u5177\u4f53\u5b9e\u73b0\u65b9\u5f0f**\uff1a\u63a2\u8ba8MCTS\u4e0ePRM\u5728\u6e38\u620fAI\u4e2d\u7684\u7ed3\u5408\u65b9\u5f0f\uff0c\u4f8b\u5982\u5982\u4f55\u5229\u7528MCTS\u7684\u641c\u7d22\u80fd\u529b\u4f18\u5316PRM\u7684\u51b3\u7b56\u7b56\u7565\uff0c\u6216\u8005\u5982\u4f55\u901a\u8fc7PRM\u7684\u504f\u597d\u5efa\u6a21\u6307\u5bfcMCTS\u7684\u641c\u7d22\u65b9\u5411\u3002\n2. **\u4f18\u5316\u51b3\u7b56\u8fc7\u7a0b**\uff1a\u5206\u6790\u8fd9\u4e9b\u7ed3\u5408\u65b9\u5f0f\u5728\u5b9e\u9645\u5e94\u7528\uff08\u5982AlphaGo\uff09\u4e2d\u5982\u4f55\u63d0\u5347\u51b3\u7b56\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u4f8b\u5982\u901a\u8fc7\u51cf\u5c11\u641c\u7d22\u7a7a\u95f4\u3001\u63d0\u9ad8\u6a21\u62df\u6548\u7387\u6216\u589e\u5f3a\u7b56\u7565\u7684\u9002\u5e94\u6027\u3002\n3. **\u63d0\u5347\u667a\u80fd\u6c34\u5e73**\uff1a\u8bc4\u4f30\u8fd9\u4e9b\u7ed3\u5408\u65b9\u5f0f\u5bf9\u6e38\u620fAI\u667a\u80fd\u6c34\u5e73\u7684\u63d0\u5347\u6548\u679c\uff0c\u4f8b\u5982\u5728\u590d\u6742\u6e38\u620f\u73af\u5883\u4e2d\u7684\u8868\u73b0\u3001\u5bf9\u52a8\u6001\u53d8\u5316\u7684\u9002\u5e94\u80fd\u529b\u4ee5\u53ca\u5bf9\u591a\u6837\u5316\u7b56\u7565\u7684\u751f\u6210\u80fd\u529b\u3002\n\n\u901a\u8fc7\u56de\u7b54\u8fd9\u4e9b\u95ee\u9898\uff0c\u53ef\u4ee5\u66f4\u6df1\u5165\u5730\u7406\u89e3MCTS\u4e0ePRM\u5728\u6e38\u620fAI\u4e2d\u7684\u5e94\u7528\u4ef7\u503c\uff0c\u5e76\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u53c2\u8003\u3002", "task_step_question_context": [{"ref_id": "455026805323333778", "chunk_id": "1", "score": 0.5859375, "text": "# 2.2 Acceleration of MCTS\nMCTS-based methods have proved their strong capability of solving complex games or tasks. However, the high computational cost of MCTS hinders its application to some real-time and more general scenarios. Therefore, numerous works are devoted to accelerating MCTS. For example, to make the selection stage more effective, some heuristic pruning methods [ 14 ,33 ,29 ,1 ,2 ] aim to reduce the width and depth of the search tree with some heuristic functions. Furthermore, for more efficient evaluations, Lorentz [ 22 ] proposed early playout termination of MCTS (MCTS-EPT) to stop the random playouts early and use an evaluation function to assess win or loss. Moreover, Hsueh et al. [18 ] applied MCTS-EPT to the Chinese dark chess and proved its effectiveness. Afterward, similar ideas have been applied in the evaluation stage of AlphaGoZero [ 32 ] and later MCTS-based methods [31 ,27 ,34 ]. They evaluate the $Q$ -values through a learnable evaluation network instead of running playouts to the end. Grill et al. [15 ] propose a novel regularized policy optimization method based on AlphaZero to decrease the search budget of MCTS, which is from the optimization perspective. Danihelka et al. [10 ] propose a policy improvement algorithm based on sampling actions without replacement, named Gumbel trick to achieve better performance when planning with few simulations. However, these methods mentioned above focus on the specific stage of the search iteration or reduce the total budget through pruning and optimization methods, which are orthogonal to us. And few works targets at the search loop. Lan et al. [21 ] propose DS-MCTS, which defines the uncertainty of MCTS and approximates it by extra DNNs with specific features for board games in training. During the evaluation, DS-MCTS will check periodically and stop the search if the state is certain.\n\n# 3 Background\nThe AlphaGo series of work [ 30 ,32 ,31 ,27 ] are all MCTS-based reinforcement learning algorithms. Those algorithms assume the environment transition dynamics are known or learn the environment dynamics. Based on the dynamics, they use the Monte-Carlo tree search (MCTS) as the policy improvement operator. I.e., taking in the current policy, MCTS returns a better policy with the search algorithm. The systematic search allows the MCTS-based RL algorithm to quickly improve the policy and perform much better in the setting where heavy reasoning is required.\n\n# 3.1 MCTS\nThis part briefly introduces the MCTS method implemented in reinforcement learning applications. As mentioned in the related works, modern MCTS-based RL algorithms include four stages in the search loop, namely selection, expansion, evaluation, and backpropagation.  \n\nMCTS takes in the current states and generates a policy after the search loop of $N$ iterations. Here $N$ is a constant number of iterations set by the designer, regarded as the total budget. In the selection stage of each iteration, an action will be selected by maximizing over UCB. Specifically, AlphaZero [31 ] and MuZero [ 27 ] are developed based on a variant of UCB, P-UCT [ 25 ] and have achieved great success on board games and Atari games. The formula of P-UCT is the Eq (1):  \n\n$$\na^{k}=\\arg\\operatorname*{max}_{a\\in\\mathcal{A}}Q(s,a)+P(s,a)\\frac{\\sqrt{\\sum_{b\\in\\mathcal{A}}N(s,b)}}{1+N(s,a)}(c_{1}+\\log((\\sum_{b\\in\\mathcal{A}}N(s,b)+c_{2}+1)/c_{2})),\n$$  \n\nwhere $k$ is the index of iteration, $\\boldsymbol{\\mathcal{A}}$ is the acti $Q(s,a)$ is the estimated Q-value, $P(s,a)$ is the policy prior obtained from neural networks, $N(s,a)$ is the visitations to select the action a from the state $s$ and $c_{1},c_{2}$ are hyper-parameters. The output of MCTS is the visitation of each action of the root node. After $N$ search iterations, the final policy $\\pi(s)$ is defined as the normalized root visitation distribution simplification, we use $\\pi_{N}(s)$ $\\pi_{k}$ in place of , where $\\begin{array}{r}{\\pi_{k}(s,a)=N(s,a)/\\sum_{b\\in\\mathcal{A}}N(s,b)=N(s,a)/k,a\\in\\mathcal{A}}\\end{array}$ $\\pi_{k}(s)$ sometimes. And the detailed procedure of MCTS is \u2208A . For introduced in Appendix. In our method, we propose to approximate the final policy $\\pi_{N}(s)$ with $\\hat{\\pi}_{k}(s)$ ,which we name as a virtual expanded policy, through a new expansion method and a termination rule. In this way, the number of iterations in MCTS can be reduced from $N$ to $k$ .\n\n# 3.2 Computation Requirement\nMost of the computations in MCTS-based RL are in the MCTS procedure. Each action taken by MCTS requires $N$ times neural network evaluations, where $N$ is a constant number of iterations in the search loop. Traditional RL algorithms, such as PPO [ 28 ] or DQN [ 23 ], only need a single neural network evaluation per action. Thus, MCTS-based RL is roughly $N$ times computationally more expensive than traditional RL algorithms. In practice, training a single Atari game needs 12 hours of computation time on 40 TPUs [ 27 ]. The computation need is roughly two orders of magnitude more than traditional RL algorithms [28], although the final performance of MuZero is much better.\n\n# 4 Method\nWe aim to spend more search time on harder states and less on easier states. Intuitively, human knows when to make a quick decision or a slow decision under different circumstances. Unfortunately, this situation-aware behavior is absent in current MCTS algorithms. Therefore, we propose an MCTS variant that terminates the search iteration adaptively. It consists of two components: a novel expansion method named virtual expansion to estimate the final visitation based on the current partial tree; a termination rule that decides when to terminate based on the hardness of the current scenario. And we will display the adaptive mechanism through visualizations in Section 5.5."}, {"ref_id": "455026805307867280", "chunk_id": "0", "score": 0.51171875, "text": "# Related Works\nTwo streams of research are particularly relevant to our work: learnable (lifelong) MAPF methods and utilizing MCTS for multi-agent systems and MAPF in particular. Next, we review both of these domains.  \n\nLearnable (L)MAPF Solvers Among the recent works dedicated to MAPF, one of the first ones that were specifically dedicated to creating a learning-based MAPF solver was (Sartoretti et al. 2019). A combination of reinforcement learning and learning from expert demonstrations was used to create a learnable policy called Primal, tailored to solve conventional MAPF problems. Later in (Damani et al. 2021), an enhanced version of this solver, Primal2, was introduced. The latter was equipped with special corridor reasoning techniques, aiming at avoiding the deadlocks in narrow corridors, and it supported lifelong MAPF setting (therefore, we choose Primal2 as one of the baselines we compare our method to). Among the other learnable MAPF solvers that use reinforcement learning to obtain a decision-making policy, one can name (Riviere et al. 2020; Wang et al. 2020). The learnable methods introduced in (Li et al. 2020; Ma, Luo, and Ma 2021; Li et al. 2022) add communication capabilities to the agents, i.e., allow the agents to communicate to resolve deadlocks and avoid congestion. In this work, we compare with one of the most recent communication-based methods, i.e., SCRIMP (Wang et al. 2023). However, it is worth noting that our method does not rely on agent communication.  \n\nMCTS for MAPF Initially, Monte Carlo Tree Search (MCTS) algorithms demonstrated their effectiveness in competitive games with complete information, such as chess or Go (Silver et al. 2017). More recent versions of MCTS utilize deep neural networks to approximate the values of game states instead of relying solely on simulations. These approaches have also shown promising results in singleagent scenarios, where agents can learn a model of the environment and play Atari games (Schrittwieser et al. 2020; Ye et al. 2021). Besides gaming, MCTS methods have found applications in other domains, such as matrix multiplication optimization (Fawzi et al. 2022) and theorem proving using the Hyper Tree approach (Lample et al. 2022). Additionally, MCTS techniques have demonstrated applicability in robotics (Best et al. 2019; Dam et al. 2022).  \n\nDespite the growing interest in utilizing MCTS for multiagent tasks, there have been limited applications of MCTS for MAPF. In their work (Zerbel and Yliniemi 2019), the authors propose a multi-agent MCTS for Anonymous MAPF in a grid-world environment. Their environment has a dense reward signal (the agent who reached any goal on the map received a reward and ended the episode), and there are no obstacles, making collision avoidance easier. The authors build a separate tree for each agent using a classical algorithm. They then jointly apply the best actions (forming a plan) from the trees in the simulator to receive true scores of the solution and update the trees on that difference. This approach performs well even with a large number of agents.  \n\nA recent paper (Skrynnik et al. 2021) proposed a more sophisticated approach for multi-agent planning that combines RL and MCTS. The authors suggested a two-part scheme that includes a goal achievement module and a conflict resolution module. The latter was trained using MCTS. The construction of the search tree for each of the agents was also performed independently, and actions for other agents were selected using the currently trained policy. This work used MCTS only during training to train the conflict resolution policy."}, {"ref_id": "454847012122763908", "chunk_id": "1", "score": 0.478515625, "text": "# Spending Thinking Time Wisely: Accelerating MCTS with Virtual Expansions\nWeirui $\\mathbf{Ye}^{*}$ Pieter Abbeel \u2020Yang Gao $\\ast\\ddag\\S$ \u2217Tsinghua University, \u2020UC Berkeley, \u00a7Shanghai Qi Zhi Institute\n\n# Abstract\nOne of the most important AI research questions is to trade off computation versus performance since \u201cperfect rationality\" exists in theory but is impossible to achieve in practice. Recently, Monte-Carlo tree search (MCTS) has attracted considerable attention due to the significant performance improvement in various challenging domains. However, the expensive time cost during search severely restricts its scope for applications. This paper proposes the Virtual MCTS (V-MCTS), a variant of MCTS that spends more search time on harder states and less search time on simpler states adaptively. We give theoretical bounds of the proposed method and evaluate the performance and computations on $9\\times9$ Go board games and Atari games. Experiments show that our method can achieve comparable performances to the original search algorithm while requiring less than $50\\%$ search time on average. We believe that this approach is a viable alternative for tasks under limited time and resources. The code is available at https://github.com/YeWR/V-MCTS.git .\n\n# 1 Introduction\nWhen artificial intelligence was first studied in the 1950s, researchers have sought to find the solution to the question \u201cHow to build an agent with perfect rationality\". The term \u201cperfect rationality\" [7 ,24 ,26 ] here refers to the decision made with infinite amounts of computations. However, one can only solve small-scale problems without considering the practical computation time since classical search algorithms usually exhibit exponential running time. Therefore, recent AI research would no longer seek to achieve \u201cperfect rationality\", but instead carefully trade-off computation versus the level of rationality. People have developed computational models like \u201cbounded optimality\" to model these settings [ 26 ]. The increasing level of rationality under the same computational budget has given us a lot of AI successes. Algorithms include the Monte-Carlo sampling algorithms, the variational inference algorithms, and using DNNs as universal function approximators [9, 8, 13, 30, 17].  \n\nRecently, MCTS-based RL algorithms have achieved much success, mainly on board games. The most notable achievement is that AlphaGo beats Hui Fan in 2015 [ 30 ]. It is the first time a computer program beat a human professional Go player. Afterward, AlphaGo beats two top-ranking human players, Lee Sedol in 2016 and Jie Ke in 2017, the latter of which ranked first worldwide at the time. Later, MCTS-based RL algorithms were further extended to other board games and Atari games [ 27 ]. EfficientZero [ 34 ] significantly improves the sample efficiency of MCTS-based RL algorithms, shedding light on its future applications in the real world like robotics and self-driving.  \n\nDespite the impressive performance of MCTS-based RL algorithms, they require massive amounts of computation to train and evaluate. For example, MuZero [ 27 ] used 1000 TPUs trained for 12 hours to learn the game of Go, and for a single Atari game, it needs 40 TPUs to train 12 hours. Compared to previous algorithms on the Atari games benchmark, it needs around two orders of magnitude more compute. This prohibitively large computational requirement has slowed down both the further development of MCTS-based RL algorithms as well as its practical use.  \n\nUnder the hood, MCTS-based RL algorithms imagine the futures when taking different future action sequences. However, this imaging process for the current method is not computationally efficient. For example, AlphaGo needs to look ahead 1600 game states to place a single stone. On the contrary, top human professional players can only think through around 100-200 game states per minute [ 30 ]. Apart from the inefficiency, the current MCTS algorithm deals with easy and challenging cases with the same computational budget. However, human knows to use their time when it is most needed.  \n\nIn this paper, we aim to design new algorithms that save the computational time of the MCTSbased RL methods. We make three key contributions : (1) We present Virtual MCTS, a variant of MCTS, to approximate the vanilla MCTS search policies with less computation. Moreover, unlike previous pruning-based methods that focus on the selection or evaluation stage in MCTS, our method improves the search loop. It terminates the search iterations earlier adaptively when current states are simpler; (2) Theoretically, we provide some error bounds of the proposed method. Furthermore, the visualization results indicate that Virtual MCTS has a better computation and performance trade-off than vanilla MCTS; (3) Empirically, our method can save more than $50\\%$ of search times on the challenging game Go $9\\times9$ and more than $60\\%$ on the visually complex Atari games while keeping comparable performances to those of vanilla MCTS.\n\n# 2 Related Work\n\n# 2.1 Reinforcement Learning with MCTS\nFor a long time, Computer Go has been regarded as a remarkably challenging game [ 3 ,6 ]. Researchers attempt to use Monte-Carlo techniques that evaluate the value of the node state through random playouts [ 4 ,11 ,12 ,30 ]. Afterward, UCT algorithms have generally been applied in Monte-Carlo tree search (MCTS) algorithms, which use UCB1 to select action at each node of the tree [ 20 ]. Recently, MCTS-based RL methods [ 30 ,32 ,31 ,27 ] have become increasingly popular and achieved super-human performances on board games because of their strong ability to search.  \n\nModern MCTS-based RL algorithms include four stages in the search loop : selection, expansion, evaluation, and backpropagation. The computation bottlenecks in vanilla MCTS come from the search loop, especially for the evaluation stage and the selection stage of each iteration. The selection stage is time-consuming when the search tree becomes wider and deeper. The evaluation stage is quite expensive because people attempt to evaluate the node value by random playouts to the end in previous researches. Due to the search loop, MCTS-based algorithms have multiple model inferences compared to other model-free RL methods like PPO [28] and SAC [16]."}], "task_step_question_answer": "\u5728\u533b\u7597\u8bca\u65ad\u9886\u57df\uff0cMCTS\u4e0ePRM\u7684\u7ed3\u5408\u4e5f\u88ab\u5e7f\u6cdb\u5e94\u7528\u3002PRM\u901a\u8fc7\u5206\u6790\u60a3\u8005\u7684\u75c5\u53f2\u548c\u75c7\u72b6\uff0c\u4e3aMCTS\u63d0\u4f9b\u5148\u9a8c\u4fe1\u606f\uff0c\u5e2e\u52a9\u5176\u5728\u8bca\u65ad\u8fc7\u7a0b\u4e2d\u4f18\u5148\u8003\u8651\u53ef\u80fd\u7684\u75be\u75c5\u3002MCTS\u5219\u901a\u8fc7\u6a21\u62df\u4e0d\u540c\u7684\u8bca\u65ad\u8def\u5f84\u6765\u8bc4\u4f30\u6700\u4f18\u8bca\u65ad\u65b9\u6848\u3002\u8fd9\u79cd\u7ed3\u5408\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u8bca\u65ad\u7684\u51c6\u786e\u6027\uff0c\u8fd8\u901a\u8fc7\u51cf\u5c11\u641c\u7d22\u7a7a\u95f4\u548c\u4f18\u5316\u6a21\u62df\u6548\u7387\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u7684\u8ba1\u7b97\u6548\u7387\u3002\u5728\u91d1\u878d\u9884\u6d4b\u9886\u57df\uff0cMCTS\u4e0ePRM\u7684\u7ed3\u5408\u88ab\u7528\u4e8e\u9884\u6d4b\u5e02\u573a\u8d8b\u52bf\u548c\u6295\u8d44\u51b3\u7b56\u3002PRM\u901a\u8fc7\u5206\u6790\u5386\u53f2\u5e02\u573a\u6570\u636e\u548c\u6295\u8d44\u8005\u884c\u4e3a\uff0c\u4e3aMCTS\u63d0\u4f9b\u5148\u9a8c\u4fe1\u606f\uff0c\u5e2e\u52a9\u5176\u5728\u9884\u6d4b\u8fc7\u7a0b\u4e2d\u4f18\u5148\u8003\u8651\u53ef\u80fd\u7684\u8d8b\u52bf\u3002MCTS\u5219\u901a\u8fc7\u6a21\u62df\u4e0d\u540c\u7684\u6295\u8d44\u8def\u5f84\u6765\u8bc4\u4f30\u6700\u4f18\u6295\u8d44\u7b56\u7565\u3002\u8fd9\u79cd\u7ed3\u5408\u5728\u590d\u6742\u5e02\u573a\u73af\u5883\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "ref_task_step_id": ""}, "__type__": "task_step_node"}, "8a1e6e8e-b71e-46d2-aa53-8f3d04b1d842": {"__data__": {"id_": "8a1e6e8e-b71e-46d2-aa53-8f3d04b1d842", "metadata": {}, "relationships": {}, "hash": "", "start_task_context": "MCTS\u5728PRM\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u7684\u5e94\u7528\u63a2\u7d22\u7efc\u8ff0", "aemo_representation_context": "### Step 1: \u5206\u6790\u8fd1\u51e0\u5e74\u7814\u7a76\u9886\u57df\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba\n\n**\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09**\uff1a\n- **\u57fa\u672c\u539f\u7406**\uff1aMCTS\u662f\u4e00\u79cd\u7528\u4e8e\u51b3\u7b56\u8fc7\u7a0b\u7684\u542f\u53d1\u5f0f\u641c\u7d22\u7b97\u6cd5\uff0c\u5e7f\u6cdb\u5e94\u7528\u4e8e\u6e38\u620fAI\u3001\u8def\u5f84\u89c4\u5212\u7b49\u9886\u57df\u3002\u5176\u6838\u5fc3\u601d\u60f3\u662f\u901a\u8fc7\u6a21\u62df\u591a\u6b21\u968f\u673a\u6e38\u620f\u6765\u8bc4\u4f30\u4e0d\u540c\u51b3\u7b56\u7684\u4f18\u52a3\u3002\n- **\u6280\u672f\u6846\u67b6**\uff1a\u4e3b\u8981\u5305\u62ec\u9009\u62e9\uff08Selection\uff09\u3001\u6269\u5c55\uff08Expansion\uff09\u3001\u6a21\u62df\uff08Simulation\uff09\u548c\u56de\u6eaf\uff08Backpropagation\uff09\u56db\u4e2a\u6b65\u9aa4\u3002\n\n**\u504f\u597d\u7b56\u7565\u6a21\u578b\uff08PRM\uff09**\uff1a\n- **\u57fa\u672c\u539f\u7406**\uff1aPRM\u65e8\u5728\u901a\u8fc7\u5b66\u4e60\u7528\u6237\u7684\u504f\u597d\u6765\u4f18\u5316\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5e38\u7528\u4e8e\u63a8\u8350\u7cfb\u7edf\u3001\u4e2a\u6027\u5316\u670d\u52a1\u7b49\u3002\n- **\u6280\u672f\u6846\u67b6**\uff1a\u901a\u5e38\u5305\u62ec\u504f\u597d\u5efa\u6a21\u3001\u7b56\u7565\u4f18\u5316\u548c\u6a21\u578b\u5fae\u8c03\u7b49\u73af\u8282\u3002\n\n**\u65b9\u6cd5\u8bba**\uff1a\n- **\u96c6\u6210\u5b66\u4e60**\uff1a\u5c06MCTS\u4e0ePRM\u7ed3\u5408\uff0c\u5229\u7528MCTS\u7684\u641c\u7d22\u80fd\u529b\u4f18\u5316PRM\u7684\u51b3\u7b56\u8fc7\u7a0b\u3002\n- **\u5f3a\u5316\u5b66\u4e60**\uff1a\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u673a\u5236\u4e0d\u65ad\u8c03\u6574PRM\u7684\u53c2\u6570\uff0c\u4ee5\u9002\u5e94\u52a8\u6001\u73af\u5883\u3002\n\n### Step 2: \u7814\u7a76\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u4e3b\u8981\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0e\u53d8\u4f53\n\n**\u5e94\u7528\u573a\u666f**\uff1a\n- **\u6e38\u620fAI**\uff1aMCTS\u4e0ePRM\u7ed3\u5408\u7528\u4e8e\u6e38\u620f\u4e2d\u7684\u667a\u80fd\u51b3\u7b56\uff0c\u5982AlphaGo\u3002\n- **\u8def\u5f84\u89c4\u5212**\uff1a\u5728\u673a\u5668\u4eba\u5bfc\u822a\u548c\u81ea\u52a8\u9a7e\u9a76\u4e2d\uff0c\u5229\u7528MCTS\u4f18\u5316\u8def\u5f84\u9009\u62e9\u3002\n- **\u63a8\u8350\u7cfb\u7edf**\uff1a\u901a\u8fc7PRM\u5b66\u4e60\u7528\u6237\u504f\u597d\uff0c\u7ed3\u5408MCTS\u8fdb\u884c\u66f4\u7cbe\u51c6\u7684\u63a8\u8350\u3002\n\n**\u53d8\u4f53**\uff1a\n- **\u6df1\u5ea6MCTS**\uff1a\u7ed3\u5408\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff0c\u63d0\u5347MCTS\u7684\u51b3\u7b56\u80fd\u529b\u3002\n- **\u591a\u6a21\u6001PRM**\uff1a\u878d\u5408\u591a\u79cd\u6570\u636e\u6e90\uff08\u5982\u6587\u672c\u3001\u56fe\u50cf\uff09\uff0c\u589e\u5f3aPRM\u7684\u6cdb\u5316\u80fd\u529b\u3002\n\n### Step 3: \u8bc4\u4f30\u5b66\u672f\u754c\u7684\u6280\u672f\u8fdb\u6b65\u4e0e\u5c40\u9650\u6027\n\n**\u6280\u672f\u8fdb\u6b65**\uff1a\n- **\u6027\u80fd\u63d0\u5347**\uff1aMCTS\u4e0ePRM\u7ed3\u5408\u663e\u8457\u63d0\u5347\u4e86\u51b3\u7b56\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002\n- **\u6cdb\u5316\u80fd\u529b**\uff1a\u591a\u6a21\u6001PRM\u5728\u5904\u7406\u591a\u6837\u5316\u6570\u636e\u65f6\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002\n\n**\u5c40\u9650\u6027**\uff1a\n- **\u8ba1\u7b97\u590d\u6742\u5ea6**\uff1aMCTS\u5728\u5927\u89c4\u6a21\u95ee\u9898\u4e2d\u8ba1\u7b97\u91cf\u5927\uff0c\u6548\u7387\u6709\u5f85\u63d0\u5347\u3002\n- **\u6570\u636e\u4f9d\u8d56**\uff1aPRM\u7684\u6027\u80fd\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u9ad8\u8d28\u91cf\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u6570\u636e\u504f\u5dee\u95ee\u9898\u4f9d\u7136\u5b58\u5728\u3002\n\n### Step 4: \u63a2\u8ba8\u8ba1\u7b97\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\n\n**\u9002\u7528\u6027**\uff1a\n- **\u591a\u9886\u57df\u5e94\u7528**\uff1aMCTS+PRM\u6846\u67b6\u5728\u6e38\u620f\u3001\u8def\u5f84\u89c4\u5212\u3001\u63a8\u8350\u7cfb\u7edf\u7b49\u591a\u4e2a\u9886\u57df\u8868\u73b0\u51fa\u826f\u597d\u7684\u9002\u7528\u6027\u3002\n- **\u591a\u6a21\u6001\u6570\u636e**\uff1a\u591a\u6a21\u6001PRM\u80fd\u591f\u5904\u7406\u591a\u79cd\u7c7b\u578b\u7684\u6570\u636e\uff0c\u9002\u7528\u4e8e\u590d\u6742\u5e94\u7528\u573a\u666f\u3002\n\n**\u6cdb\u5316\u80fd\u529b**\uff1a\n- **\u8de8\u9886\u57df\u6cdb\u5316**\uff1a\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\uff0cMCTS+PRM\u6846\u67b6\u5728\u4e0d\u540c\u9886\u57df\u95f4\u5c55\u73b0\u51fa\u8f83\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002\n- **\u52a8\u6001\u73af\u5883\u9002\u5e94**\uff1a\u5f3a\u5316\u5b66\u4e60\u673a\u5236\u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u9002\u5e94\u52a8\u6001\u53d8\u5316\u7684\u73af\u5883\u3002\n\n### Step 5: \u5206\u6790\u6700\u65b0\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027\n\n**\u7a33\u5b9a\u6027**\uff1a\n- **\u7b97\u6cd5\u4f18\u5316**\uff1a\u901a\u8fc7\u6539\u8fdbMCTS\u7684\u9009\u62e9\u7b56\u7565\u548cPRM\u7684\u53c2\u6570\u8c03\u6574\u673a\u5236\uff0c\u63d0\u5347\u4e86\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u3002\n- **\u9c81\u68d2\u6027\u589e\u5f3a**\uff1a\u5f15\u5165\u5bf9\u6297\u8bad\u7ec3\u548c\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\uff0c\u63d0\u9ad8\u6a21\u578b\u5728\u590d\u6742\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u6027\u3002\n\n**\u5bb9\u9519\u6027**\uff1a\n- **\u9519\u8bef\u5904\u7406**\uff1a\u5728\u6a21\u62df\u548c\u56de\u6eaf\u9636\u6bb5\u5f15\u5165\u5bb9\u9519\u673a\u5236\uff0c\u51cf\u5c11\u9519\u8bef\u51b3\u7b56\u7684\u5f71\u54cd\u3002\n- **\u5927\u89c4\u6a21\u6570\u636e\u9002\u5e94\u6027**\uff1a\u4f18\u5316\u7b97\u6cd5\u7ed3\u6784\uff0c\u63d0\u5347\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\u7684\u5904\u7406\u80fd\u529b\u3002\n\n### Step 6: \u8bc4\u4f30\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218\n\n**\u672a\u6765\u7814\u7a76\u65b9\u5411**\uff1a\n- **\u9ad8\u6548\u8ba1\u7b97**\uff1a\u7814\u7a76\u66f4\u9ad8\u6548\u7684MCTS\u7b97\u6cd5\uff0c\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002\n- **\u6570\u636e\u589e\u5f3a**\uff1a\u63a2\u7d22\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u51cf\u5c11PRM\u5bf9\u9ad8\u8d28\u91cf\u6570\u636e\u7684\u4f9d\u8d56\u3002\n\n**\u6311\u6218**\uff1a\n- **\u8de8\u6a21\u6001\u878d\u5408**\uff1a\u5982\u4f55\u6709\u6548\u878d\u5408\u591a\u79cd\u6a21\u6001\u6570\u636e\uff0c\u63d0\u5347\u6a21\u578b\u7684\u7efc\u5408\u6027\u80fd\u3002\n- **\u52a8\u6001\u73af\u5883\u9002\u5e94**\uff1a\u5728\u52a8\u6001\u73af\u5883\u4e0b\uff0c\u5982\u4f55\u4fdd\u6301\u6a21\u578b\u7684\u7a33\u5b9a\u6027\u548c\u9ad8\u6548\u6027\u3002\n\n**\u63a8\u52a8\u540e\u7eed\u7814\u7a76**\uff1a\n- **\u65b0\u95ee\u9898\u63d0\u51fa**\uff1a\u5982\u591a\u6a21\u6001\u6570\u636e\u4e0b\u7684\u504f\u597d\u5efa\u6a21\u3001\u52a8\u6001\u73af\u5883\u4e0b\u7684\u51b3\u7b56\u4f18\u5316\u7b49\u3002\n- **\u7814\u7a76\u5207\u5165\u70b9**\uff1a\u4ece\u7b97\u6cd5\u4f18\u5316\u3001\u6570\u636e\u589e\u5f3a\u3001\u8de8\u6a21\u6001\u878d\u5408\u7b49\u591a\u4e2a\u89d2\u5ea6\u5207\u5165\uff0c\u63a8\u52a8\u6280\u672f\u8fdb\u6b65\u3002\n\n### \u603b\u7ed3\n\n\u8fd1\u5e74\u6765\uff0cMCTS\u5728PRM\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u7684\u5e94\u7528\u63a2\u7d22\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4e3b\u8981\u4f53\u73b0\u5728\u6280\u672f\u6846\u67b6\u7684\u5b8c\u5584\u3001\u5e94\u7528\u573a\u666f\u7684\u62d3\u5c55\u3001\u6027\u80fd\u7684\u63d0\u5347\u4ee5\u53ca\u7a33\u5b9a\u6027\u548c\u5bb9\u9519\u6027\u7684\u589e\u5f3a\u3002\u7136\u800c\uff0c\u4ecd\u5b58\u5728\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u3001\u6570\u636e\u4f9d\u8d56\u6027\u5f3a\u7b49\u5c40\u9650\u6027\u3002\u672a\u6765\u7684\u7814\u7a76\u5e94\u5173\u6ce8\u9ad8\u6548\u8ba1\u7b97\u3001\u6570\u636e\u589e\u5f3a\u548c\u8de8\u6a21\u6001\u878d\u5408\u7b49\u65b9\u5411\uff0c\u4ee5\u5e94\u5bf9\u52a8\u6001\u73af\u5883\u4e0b\u7684\u51b3\u7b56\u4f18\u5316\u7b49\u6311\u6218\uff0c\u63a8\u52a8\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002", "task_step_name": "\u6e38\u620fAI", "task_step_description": "MCTS\u4e0ePRM\u7ed3\u5408\u7528\u4e8e\u6e38\u620f\u4e2d\u7684\u667a\u80fd\u51b3\u7b56\uff0c\u5982AlphaGo\u3002", "task_step_level": "1>1", "task_step_question": "### \u95ee\u9898\n\n\u5728\u6e38\u620fAI\u9886\u57df\uff0cMCTS\u4e0ePRM\u7ed3\u5408\u7528\u4e8e\u667a\u80fd\u51b3\u7b56\uff08\u5982AlphaGo\uff09\u65f6\uff0c\u5982\u4f55\u6709\u6548\u5e73\u8861MCTS\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u548cPRM\u7684\u504f\u597d\u5efa\u6a21\u7cbe\u5ea6\uff0c\u4ee5\u5728\u4fdd\u8bc1\u51b3\u7b56\u8d28\u91cf\u7684\u540c\u65f6\u63d0\u5347\u7b97\u6cd5\u7684\u5b9e\u65f6\u6027\uff1f", "task_step_question_context": [{"ref_id": "455026805323333778", "chunk_id": "1", "score": 0.404296875, "text": "# Spending Thinking Time Wisely: Accelerating MCTS with Virtual Expansions\nWeirui $\\mathbf{Ye}^{*}$ Pieter Abbeel \u2020Yang Gao $\\ast\\ddag\\S$ \u2217Tsinghua University, \u2020UC Berkeley, \u00a7Shanghai Qi Zhi Institute\n\n# Abstract\nOne of the most important AI research questions is to trade off computation versus performance since \u201cperfect rationality\" exists in theory but is impossible to achieve in practice. Recently, Monte-Carlo tree search (MCTS) has attracted considerable attention due to the significant performance improvement in various challenging domains. However, the expensive time cost during search severely restricts its scope for applications. This paper proposes the Virtual MCTS (V-MCTS), a variant of MCTS that spends more search time on harder states and less search time on simpler states adaptively. We give theoretical bounds of the proposed method and evaluate the performance and computations on $9\\times9$ Go board games and Atari games. Experiments show that our method can achieve comparable performances to the original search algorithm while requiring less than $50\\%$ search time on average. We believe that this approach is a viable alternative for tasks under limited time and resources. The code is available at https://github.com/YeWR/V-MCTS.git .\n\n# 1 Introduction\nWhen artificial intelligence was first studied in the 1950s, researchers have sought to find the solution to the question \u201cHow to build an agent with perfect rationality\". The term \u201cperfect rationality\" [7 ,24 ,26 ] here refers to the decision made with infinite amounts of computations. However, one can only solve small-scale problems without considering the practical computation time since classical search algorithms usually exhibit exponential running time. Therefore, recent AI research would no longer seek to achieve \u201cperfect rationality\", but instead carefully trade-off computation versus the level of rationality. People have developed computational models like \u201cbounded optimality\" to model these settings [ 26 ]. The increasing level of rationality under the same computational budget has given us a lot of AI successes. Algorithms include the Monte-Carlo sampling algorithms, the variational inference algorithms, and using DNNs as universal function approximators [9, 8, 13, 30, 17].  \n\nRecently, MCTS-based RL algorithms have achieved much success, mainly on board games. The most notable achievement is that AlphaGo beats Hui Fan in 2015 [ 30 ]. It is the first time a computer program beat a human professional Go player. Afterward, AlphaGo beats two top-ranking human players, Lee Sedol in 2016 and Jie Ke in 2017, the latter of which ranked first worldwide at the time. Later, MCTS-based RL algorithms were further extended to other board games and Atari games [ 27 ]. EfficientZero [ 34 ] significantly improves the sample efficiency of MCTS-based RL algorithms, shedding light on its future applications in the real world like robotics and self-driving.  \n\nDespite the impressive performance of MCTS-based RL algorithms, they require massive amounts of computation to train and evaluate. For example, MuZero [ 27 ] used 1000 TPUs trained for 12 hours to learn the game of Go, and for a single Atari game, it needs 40 TPUs to train 12 hours. Compared to previous algorithms on the Atari games benchmark, it needs around two orders of magnitude more compute. This prohibitively large computational requirement has slowed down both the further development of MCTS-based RL algorithms as well as its practical use.  \n\nUnder the hood, MCTS-based RL algorithms imagine the futures when taking different future action sequences. However, this imaging process for the current method is not computationally efficient. For example, AlphaGo needs to look ahead 1600 game states to place a single stone. On the contrary, top human professional players can only think through around 100-200 game states per minute [ 30 ]. Apart from the inefficiency, the current MCTS algorithm deals with easy and challenging cases with the same computational budget. However, human knows to use their time when it is most needed.  \n\nIn this paper, we aim to design new algorithms that save the computational time of the MCTSbased RL methods. We make three key contributions : (1) We present Virtual MCTS, a variant of MCTS, to approximate the vanilla MCTS search policies with less computation. Moreover, unlike previous pruning-based methods that focus on the selection or evaluation stage in MCTS, our method improves the search loop. It terminates the search iterations earlier adaptively when current states are simpler; (2) Theoretically, we provide some error bounds of the proposed method. Furthermore, the visualization results indicate that Virtual MCTS has a better computation and performance trade-off than vanilla MCTS; (3) Empirically, our method can save more than $50\\%$ of search times on the challenging game Go $9\\times9$ and more than $60\\%$ on the visually complex Atari games while keeping comparable performances to those of vanilla MCTS.\n\n# 2 Related Work\n\n# 2.1 Reinforcement Learning with MCTS\nFor a long time, Computer Go has been regarded as a remarkably challenging game [ 3 ,6 ]. Researchers attempt to use Monte-Carlo techniques that evaluate the value of the node state through random playouts [ 4 ,11 ,12 ,30 ]. Afterward, UCT algorithms have generally been applied in Monte-Carlo tree search (MCTS) algorithms, which use UCB1 to select action at each node of the tree [ 20 ]. Recently, MCTS-based RL methods [ 30 ,32 ,31 ,27 ] have become increasingly popular and achieved super-human performances on board games because of their strong ability to search.  \n\nModern MCTS-based RL algorithms include four stages in the search loop : selection, expansion, evaluation, and backpropagation. The computation bottlenecks in vanilla MCTS come from the search loop, especially for the evaluation stage and the selection stage of each iteration. The selection stage is time-consuming when the search tree becomes wider and deeper. The evaluation stage is quite expensive because people attempt to evaluate the node value by random playouts to the end in previous researches. Due to the search loop, MCTS-based algorithms have multiple model inferences compared to other model-free RL methods like PPO [28] and SAC [16]."}, {"ref_id": "455026805307867280", "chunk_id": "0", "score": 0.341796875, "text": "# 6 PLAYING CHESS WITH MONTE -C ARLO TREE SEARCH\nFinally, we turn from cooperative to adversarial decision-making tasks. We focus on chess, a popular two-player sequential game widely used as a benchmark for AI systems. Here, we are interested in  \n\n<html><body><table><tr><td>Model</td><td>Type</td><td>Accuracy</td></tr><tr><td>\u03b2runtime = 0</td><td></td><td>83.3</td></tr><tr><td>pruntime =1</td><td>\u4e00</td><td>83.0</td></tr><tr><td>Inferred \u03b2temp</td><td>player skill</td><td>83.9</td></tr><tr><td>Inferred 1pruntime (L-IBM)</td><td>playerskill</td><td>84.0</td></tr><tr><td>Inferred \u03b2temp</td><td>difficulty</td><td>83.5</td></tr><tr><td>Inferred Pruntime (L-IBM)</td><td>difficulty</td><td>82.7</td></tr></table></body></html>  \n\nTable 3: Performance of different RSA models in predicting the speaker target. All models (including literal models and fixed-depth RSA models) achieve similar predictive performance\u2014because even literal models have access to all three referents, all model variants can achieve good task performance. Note that $\\beta_{\\mathrm{runtime}}=0$ represents the base literal listener.  \n\n<html><body><table><tr><td>Model</td><td>Type</td><td>Accuracy</td></tr><tr><td>IL</td><td></td><td>42.06</td></tr><tr><td>\u03b2runtime =100</td><td></td><td>43.64</td></tr><tr><td>Inferred \u03b2puct</td><td>ActiveElo</td><td>43.77</td></tr><tr><td>Inferred Pruntime (L-IBM)</td><td>ActiveElo</td><td>44.17</td></tr><tr><td>Inferred Ppuct</td><td>OpponentElo</td><td>43.84</td></tr><tr><td>Inferred Pruntime (L-IBM)</td><td>OpponentElo</td><td>44.17</td></tr><tr><td>Inferred \u03b2puct</td><td>TimeControl</td><td>43.61</td></tr><tr><td>Inferred \u03b2runtime (L-IBM)</td><td>Time Control</td><td>44.15</td></tr></table></body></html>  \n\nTable 4: Accuracy of predicting an agent\u2019s next action in chess. Models with MCTS outperform the depth-0 (imitation learning) baseline. Learning subpopulation-specific $\\beta$ enhances performance, with L-IBM-based learning of $\\beta_{\\mathrm{runtime}}$ consistently outperforming $\\beta_{\\mathrm{puct}}$ by a slight margin.  \n\nmodeling human chess play\u2014specifically, observing data from a population of sub-optimal agents with a common reward function (winning the game) and attempting to infer those agents\u2019 computational constraints. In human human play, there can be numerous sources of such constraints: a player paired against a strong opponent will likely to plan for longer than against a weaker opponent; some variants (like blitz chess) deliberately limit players\u2019 time-per-move (and, we might expect, the quality of their plans). Given a dataset of human games played under different time constraints and player strengths, can we use L-IBM to model variability in players\u2019 decisions across game states?\n\n# 6.1 A GENT MODEL\nIn this work, we model chess players as selecting actions using Monte Carlo tree search (MCTS). Recent work (Jacob et al., 2022) has shown that MCTS is a good model of strong human players. Here, following (Silver et al., 2018; 2016; Jacob et al., 2022; Grill et al., 2020), we implement one of the most common modern forms of MCTS, which uses a value function $V$ predicting the expected total future reward and a policy prior $\\pi^{0}$ to guide exploration. At a high level, MCTS operates by incrementally growing a game tree starting at the root node, repeatedly picking some path to explore down the tree, performing a value function evaluation and then walking back up the tree updating all the value estimates based on that result. At each node, MCTS treats action selection as a multiarmed bandit problem. We use a standard exploration policy (Kocsis & Szepesv\u00b4ari, 2006): during inference at each node of the search tree, we choose actions according to:  \n\n$$\n\\arg\\operatorname*{max}_{a}\\,Q_{t}(a\\mid s)+\\beta_{\\mathrm{puct}}\\pi^{0}(a\\mid s)\\frac{\\sqrt{\\sum_{b}N(s,b)}}{N(s,a)+1}\n$$  \n\nwhere $Q_{t}(s,a)$ is the estimated expected future reward for $i$ from playing action $a$ in state $s$ at iteration $t$ , the visit c $N(s,a)$ is the number of times $a$ has been explored from $s$ ,$\\pi^{0}(a\\mid s)$ is an \u201canchor\u201d policy, and $\\beta_{\\mathrm{puct}}$ is a tunable parameter trading off exploration versus exploitation. After expanding $\\beta_{\\mathrm{runtime}}$ nodes of this tree, an agent\u2019s final action is sampled from a distribution:  \n\n$$\n\\pi(a\\mid s;\\beta_{\\mathrm{runtime}})=\\beta_{\\mathrm{puct}}{\\frac{\\sqrt{\\beta_{\\mathrm{runtime}}}}{N(s,a)+1}}{\\frac{\\pi^{0}(a|s)}{\\gamma-Q_{\\beta_{\\mathrm{runtime}}}(a\\mid s)}}\n$$  \n\nwhere $\\gamma$ is chosen such that $\\pi$ forms a proper probability distribution.  \n\nProposition 3. Monte-Carlo tree search (MCTS) is an anytime inference algorithm. (Let each inference state $f_{\\beta}$ be the tree of nodes and visitation counts after $\\beta$ evaluations. This tree is refined by evaluating Eq. (15) once.)  \n\nWith $\\pi(a\\mid s;\\beta_{\\mathrm{runtime}})$ as defined above, we may instantiate an L-IBM for MCTS:  \n\n$$\n\\pi^{\\mathrm{runtime}}(t|u;\\eta,\\theta)=\\sum_{\\beta_{\\mathrm{runtime}}}p_{\\mathrm{budget}}(\\beta_{\\mathrm{runtime}}\\mid\\eta_{i})\\cdot\\pi(a;s,\\beta_{\\mathrm{runtime}})\n$$\n\n# 6.2 DATA\nWe use similar data to previous models of human chess play by McIlroy-Young et al. (2020); Jacob et al. (2022); McIlroy-Young et al. (2022). Our experiments use two different datasets. First,  \n\n  \nFigure 6: Inferred distributions over $\\beta$ in Chess using MCTS. X-axis indicates the player Elo rating, opponent elo rating buckets and time control: Ultra Bullet (U), Bullet (B), Blitz (BZ), Rapid (R) and Classical (C). The top row depicts the distributions for $\\beta_{\\mathrm{puct}}$ and the bottom row depicts the distributions for $\\beta_{\\mathrm{runtime}}$ . When the player\u2019s or opponent\u2019s strength increases, $\\beta_{\\mathrm{runtime}}$ infers greater runtime. This pattern also holds true as the time control extends. $\\beta_{\\mathrm{puct}}$ displays a similar pattern, as the agents or opponents get stronger, or as the time control extends, $\\beta_{\\mathrm{puct}}$ suggests lower values, placing greater reliance on the search Q-values.  \n\na dataset $D_{\\mathrm{large}}$ containing roughly 6 million moves; second, a dataset $D_{\\mathrm{small}}$ containing roughly 75,000 moves. $D_{\\mathrm{small}}$ includes metadata describing players\u2019 Elo ratings (a measure of strength) and game formats (the amount of time players had to select moves). See Appendix B for details.\n\n# 6.3 MODELING DETAILS\nWe train the base initial policy $\\pi_{0}$ and a value model $\\tilde{v}_{0}$ as two different output heads of a deep neural network using imitation learning on the large dataset split $D_{\\mathrm{large}}$ . Our architecture is a 4-block residual network similar to those used in prior work (McIlroy-Young et al., 2020; Jacob et al., 2022; McIlroy-Young et al., 2022). Unlike previous sections, we do not learn the value functions jointly $p_{\\mathrm{budget}}$ , we first learn a single value function from $D_{\\mathrm{large}}$ , then fit $p_{\\mathrm{budget}}(\\beta_{\\mathrm{puct}}\\mid\\dot{\\eta}_{i})$ and $p_{\\mathrm{budget}}(\\beta_{\\mathrm{runtime}}^{\\mathsf{\\bar{\\alpha}}}\\mid\\eta_{i})$ Elo (a proxy for player skill), and opponent Elo and time control (both proxies for task difficulty). |. We investigate three ways of stratifying players into sub-populations: player As in Section 5, we estimate a separate $\\eta_{i}$ for each group within each stratified dataset."}, {"ref_id": "454967397448679476", "chunk_id": "1", "score": 0.3125, "text": "# 2.2 Acceleration of MCTS\nMCTS-based methods have proved their strong capability of solving complex games or tasks. However, the high computational cost of MCTS hinders its application to some real-time and more general scenarios. Therefore, numerous works are devoted to accelerating MCTS. For example, to make the selection stage more effective, some heuristic pruning methods [ 14 ,33 ,29 ,1 ,2 ] aim to reduce the width and depth of the search tree with some heuristic functions. Furthermore, for more efficient evaluations, Lorentz [ 22 ] proposed early playout termination of MCTS (MCTS-EPT) to stop the random playouts early and use an evaluation function to assess win or loss. Moreover, Hsueh et al. [18 ] applied MCTS-EPT to the Chinese dark chess and proved its effectiveness. Afterward, similar ideas have been applied in the evaluation stage of AlphaGoZero [ 32 ] and later MCTS-based methods [31 ,27 ,34 ]. They evaluate the $Q$ -values through a learnable evaluation network instead of running playouts to the end. Grill et al. [15 ] propose a novel regularized policy optimization method based on AlphaZero to decrease the search budget of MCTS, which is from the optimization perspective. Danihelka et al. [10 ] propose a policy improvement algorithm based on sampling actions without replacement, named Gumbel trick to achieve better performance when planning with few simulations. However, these methods mentioned above focus on the specific stage of the search iteration or reduce the total budget through pruning and optimization methods, which are orthogonal to us. And few works targets at the search loop. Lan et al. [21 ] propose DS-MCTS, which defines the uncertainty of MCTS and approximates it by extra DNNs with specific features for board games in training. During the evaluation, DS-MCTS will check periodically and stop the search if the state is certain.\n\n# 3 Background\nThe AlphaGo series of work [ 30 ,32 ,31 ,27 ] are all MCTS-based reinforcement learning algorithms. Those algorithms assume the environment transition dynamics are known or learn the environment dynamics. Based on the dynamics, they use the Monte-Carlo tree search (MCTS) as the policy improvement operator. I.e., taking in the current policy, MCTS returns a better policy with the search algorithm. The systematic search allows the MCTS-based RL algorithm to quickly improve the policy and perform much better in the setting where heavy reasoning is required.\n\n# 3.1 MCTS\nThis part briefly introduces the MCTS method implemented in reinforcement learning applications. As mentioned in the related works, modern MCTS-based RL algorithms include four stages in the search loop, namely selection, expansion, evaluation, and backpropagation.  \n\nMCTS takes in the current states and generates a policy after the search loop of $N$ iterations. Here $N$ is a constant number of iterations set by the designer, regarded as the total budget. In the selection stage of each iteration, an action will be selected by maximizing over UCB. Specifically, AlphaZero [31 ] and MuZero [ 27 ] are developed based on a variant of UCB, P-UCT [ 25 ] and have achieved great success on board games and Atari games. The formula of P-UCT is the Eq (1):  \n\n$$\na^{k}=\\arg\\operatorname*{max}_{a\\in\\mathcal{A}}Q(s,a)+P(s,a)\\frac{\\sqrt{\\sum_{b\\in\\mathcal{A}}N(s,b)}}{1+N(s,a)}(c_{1}+\\log((\\sum_{b\\in\\mathcal{A}}N(s,b)+c_{2}+1)/c_{2})),\n$$  \n\nwhere $k$ is the index of iteration, $\\boldsymbol{\\mathcal{A}}$ is the acti $Q(s,a)$ is the estimated Q-value, $P(s,a)$ is the policy prior obtained from neural networks, $N(s,a)$ is the visitations to select the action a from the state $s$ and $c_{1},c_{2}$ are hyper-parameters. The output of MCTS is the visitation of each action of the root node. After $N$ search iterations, the final policy $\\pi(s)$ is defined as the normalized root visitation distribution simplification, we use $\\pi_{N}(s)$ $\\pi_{k}$ in place of , where $\\begin{array}{r}{\\pi_{k}(s,a)=N(s,a)/\\sum_{b\\in\\mathcal{A}}N(s,b)=N(s,a)/k,a\\in\\mathcal{A}}\\end{array}$ $\\pi_{k}(s)$ sometimes. And the detailed procedure of MCTS is \u2208A . For introduced in Appendix. In our method, we propose to approximate the final policy $\\pi_{N}(s)$ with $\\hat{\\pi}_{k}(s)$ ,which we name as a virtual expanded policy, through a new expansion method and a termination rule. In this way, the number of iterations in MCTS can be reduced from $N$ to $k$ .\n\n# 3.2 Computation Requirement\nMost of the computations in MCTS-based RL are in the MCTS procedure. Each action taken by MCTS requires $N$ times neural network evaluations, where $N$ is a constant number of iterations in the search loop. Traditional RL algorithms, such as PPO [ 28 ] or DQN [ 23 ], only need a single neural network evaluation per action. Thus, MCTS-based RL is roughly $N$ times computationally more expensive than traditional RL algorithms. In practice, training a single Atari game needs 12 hours of computation time on 40 TPUs [ 27 ]. The computation need is roughly two orders of magnitude more than traditional RL algorithms [28], although the final performance of MuZero is much better.\n\n# 4 Method\nWe aim to spend more search time on harder states and less on easier states. Intuitively, human knows when to make a quick decision or a slow decision under different circumstances. Unfortunately, this situation-aware behavior is absent in current MCTS algorithms. Therefore, we propose an MCTS variant that terminates the search iteration adaptively. It consists of two components: a novel expansion method named virtual expansion to estimate the final visitation based on the current partial tree; a termination rule that decides when to terminate based on the hardness of the current scenario. And we will display the adaptive mechanism through visualizations in Section 5.5."}], "task_step_question_answer": "\u5728\u300a\u661f\u9645\u4e89\u9738\u300b\u4e2d\uff0cAI\u53ef\u4ee5\u901a\u8fc7\u81ea\u9002\u5e94\u641c\u7d22\u65f6\u95f4\u5206\u914d\u7b56\u7565\uff0c\u4f18\u5148\u5904\u7406\u5bf9\u6e38\u620f\u7ed3\u679c\u5f71\u54cd\u6700\u5927\u7684\u51b3\u7b56\u3002\u4f8b\u5982\uff0c\u5728\u8d44\u6e90\u7ba1\u7406\u65b9\u9762\uff0cAI\u53ef\u4ee5\u6839\u636e\u5f53\u524d\u7684\u8d44\u6e90\u6570\u91cf\u548c\u654c\u65b9\u5175\u529b\uff0c\u52a8\u6001\u8c03\u6574\u641c\u7d22\u65f6\u95f4\uff0c\u4f18\u5148\u8bc4\u4f30\u8d44\u6e90\u91c7\u96c6\u548c\u5175\u529b\u751f\u4ea7\u7684\u51b3\u7b56\u3002\u5728\u6218\u672f\u9009\u62e9\u65b9\u9762\uff0cAI\u53ef\u4ee5\u540c\u65f6\u8bc4\u4f30\u591a\u4e2a\u6218\u672f\u9009\u62e9\uff0c\u4f8b\u5982\u8fdb\u653b\u3001\u9632\u5b88\u6216\u6269\u5f20\uff0c\u901a\u8fc7\u5e76\u884c\u8ba1\u7b97\u7b56\u7565\uff0c\u5feb\u901f\u751f\u6210\u6700\u4f18\u6218\u672f\u65b9\u6848\u3002\u5728\u5361\u724c\u6e38\u620f\u4e2d\uff0cAI\u53ef\u4ee5\u901a\u8fc7\u4f18\u5316\u504f\u597d\u5efa\u6a21\uff0c\u57fa\u4e8e\u5bf9\u624b\u7684\u5386\u53f2\u51fa\u724c\u6a21\u5f0f\uff0c\u9884\u6d4b\u5176\u4e0b\u4e00\u6b65\u53ef\u80fd\u7684\u51fa\u724c\u7b56\u7565\u3002\u4f8b\u5982\uff0c\u5728\u300a\u7089\u77f3\u4f20\u8bf4\u300b\u4e2d\uff0cAI\u53ef\u4ee5\u5206\u6790\u5bf9\u624b\u7684\u51fa\u724c\u4e60\u60ef\uff0c\u9884\u6d4b\u5176\u53ef\u80fd\u7684\u5361\u724c\u7ec4\u5408\uff0c\u4ece\u800c\u63d0\u524d\u5236\u5b9a\u5e94\u5bf9\u7b56\u7565\u3002\u6df7\u5408\u7b56\u7565\u5219\u80fd\u591f\u5728\u521d\u59cb\u9636\u6bb5\u5feb\u901f\u7b5b\u9009\u51fa\u6709\u6f5c\u529b\u7684\u884c\u52a8\uff0c\u51cf\u5c11\u540e\u7eed\u641c\u7d22\u7684\u8ba1\u7b97\u8d1f\u62c5\u3002\u4f8b\u5982\uff0cAI\u53ef\u4ee5\u5728\u521d\u59cb\u9636\u6bb5\u4f7f\u7528PRM\u5feb\u901f\u8bc4\u4f30\u6bcf\u4e2a\u51fa\u724c\u9009\u9879\u7684\u6f5c\u5728\u4ef7\u503c\uff0c\u7136\u540e\u5728\u540e\u7eed\u9636\u6bb5\u4f7f\u7528MCTS\u8fdb\u884c\u6df1\u5165\u641c\u7d22\uff0c\u786e\u4fdd\u51b3\u7b56\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "ref_task_step_id": ""}, "__type__": "task_step_node"}, "0f8fcb2e-9570-4314-99ab-0d3606636375": {"__data__": {"id_": "0f8fcb2e-9570-4314-99ab-0d3606636375", "metadata": {}, "relationships": {}, "hash": "", "start_task_context": "MCTS\u5728PRM\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u7684\u5e94\u7528\u63a2\u7d22\u7efc\u8ff0", "aemo_representation_context": "### Step 1: \u5206\u6790\u8fd1\u51e0\u5e74\u7814\u7a76\u9886\u57df\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba\n\n**\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09**\uff1a\n- **\u57fa\u672c\u539f\u7406**\uff1aMCTS\u662f\u4e00\u79cd\u7528\u4e8e\u51b3\u7b56\u8fc7\u7a0b\u7684\u542f\u53d1\u5f0f\u641c\u7d22\u7b97\u6cd5\uff0c\u5e7f\u6cdb\u5e94\u7528\u4e8e\u6e38\u620fAI\u3001\u8def\u5f84\u89c4\u5212\u7b49\u9886\u57df\u3002\u5176\u6838\u5fc3\u601d\u60f3\u662f\u901a\u8fc7\u6a21\u62df\u591a\u6b21\u968f\u673a\u6e38\u620f\u6765\u8bc4\u4f30\u4e0d\u540c\u51b3\u7b56\u7684\u4f18\u52a3\u3002\n- **\u6280\u672f\u6846\u67b6**\uff1a\u4e3b\u8981\u5305\u62ec\u9009\u62e9\uff08Selection\uff09\u3001\u6269\u5c55\uff08Expansion\uff09\u3001\u6a21\u62df\uff08Simulation\uff09\u548c\u56de\u6eaf\uff08Backpropagation\uff09\u56db\u4e2a\u6b65\u9aa4\u3002\n\n**\u504f\u597d\u7b56\u7565\u6a21\u578b\uff08PRM\uff09**\uff1a\n- **\u57fa\u672c\u539f\u7406**\uff1aPRM\u65e8\u5728\u901a\u8fc7\u5b66\u4e60\u7528\u6237\u7684\u504f\u597d\u6765\u4f18\u5316\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5e38\u7528\u4e8e\u63a8\u8350\u7cfb\u7edf\u3001\u4e2a\u6027\u5316\u670d\u52a1\u7b49\u3002\n- **\u6280\u672f\u6846\u67b6**\uff1a\u901a\u5e38\u5305\u62ec\u504f\u597d\u5efa\u6a21\u3001\u7b56\u7565\u4f18\u5316\u548c\u6a21\u578b\u5fae\u8c03\u7b49\u73af\u8282\u3002\n\n**\u65b9\u6cd5\u8bba**\uff1a\n- **\u96c6\u6210\u5b66\u4e60**\uff1a\u5c06MCTS\u4e0ePRM\u7ed3\u5408\uff0c\u5229\u7528MCTS\u7684\u641c\u7d22\u80fd\u529b\u4f18\u5316PRM\u7684\u51b3\u7b56\u8fc7\u7a0b\u3002\n- **\u5f3a\u5316\u5b66\u4e60**\uff1a\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u673a\u5236\u4e0d\u65ad\u8c03\u6574PRM\u7684\u53c2\u6570\uff0c\u4ee5\u9002\u5e94\u52a8\u6001\u73af\u5883\u3002\n\n### Step 2: \u7814\u7a76\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u4e3b\u8981\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0e\u53d8\u4f53\n\n**\u5e94\u7528\u573a\u666f**\uff1a\n- **\u6e38\u620fAI**\uff1aMCTS\u4e0ePRM\u7ed3\u5408\u7528\u4e8e\u6e38\u620f\u4e2d\u7684\u667a\u80fd\u51b3\u7b56\uff0c\u5982AlphaGo\u3002\n- **\u8def\u5f84\u89c4\u5212**\uff1a\u5728\u673a\u5668\u4eba\u5bfc\u822a\u548c\u81ea\u52a8\u9a7e\u9a76\u4e2d\uff0c\u5229\u7528MCTS\u4f18\u5316\u8def\u5f84\u9009\u62e9\u3002\n- **\u63a8\u8350\u7cfb\u7edf**\uff1a\u901a\u8fc7PRM\u5b66\u4e60\u7528\u6237\u504f\u597d\uff0c\u7ed3\u5408MCTS\u8fdb\u884c\u66f4\u7cbe\u51c6\u7684\u63a8\u8350\u3002\n\n**\u53d8\u4f53**\uff1a\n- **\u6df1\u5ea6MCTS**\uff1a\u7ed3\u5408\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff0c\u63d0\u5347MCTS\u7684\u51b3\u7b56\u80fd\u529b\u3002\n- **\u591a\u6a21\u6001PRM**\uff1a\u878d\u5408\u591a\u79cd\u6570\u636e\u6e90\uff08\u5982\u6587\u672c\u3001\u56fe\u50cf\uff09\uff0c\u589e\u5f3aPRM\u7684\u6cdb\u5316\u80fd\u529b\u3002\n\n### Step 3: \u8bc4\u4f30\u5b66\u672f\u754c\u7684\u6280\u672f\u8fdb\u6b65\u4e0e\u5c40\u9650\u6027\n\n**\u6280\u672f\u8fdb\u6b65**\uff1a\n- **\u6027\u80fd\u63d0\u5347**\uff1aMCTS\u4e0ePRM\u7ed3\u5408\u663e\u8457\u63d0\u5347\u4e86\u51b3\u7b56\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002\n- **\u6cdb\u5316\u80fd\u529b**\uff1a\u591a\u6a21\u6001PRM\u5728\u5904\u7406\u591a\u6837\u5316\u6570\u636e\u65f6\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002\n\n**\u5c40\u9650\u6027**\uff1a\n- **\u8ba1\u7b97\u590d\u6742\u5ea6**\uff1aMCTS\u5728\u5927\u89c4\u6a21\u95ee\u9898\u4e2d\u8ba1\u7b97\u91cf\u5927\uff0c\u6548\u7387\u6709\u5f85\u63d0\u5347\u3002\n- **\u6570\u636e\u4f9d\u8d56**\uff1aPRM\u7684\u6027\u80fd\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u9ad8\u8d28\u91cf\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u6570\u636e\u504f\u5dee\u95ee\u9898\u4f9d\u7136\u5b58\u5728\u3002\n\n### Step 4: \u63a2\u8ba8\u8ba1\u7b97\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\n\n**\u9002\u7528\u6027**\uff1a\n- **\u591a\u9886\u57df\u5e94\u7528**\uff1aMCTS+PRM\u6846\u67b6\u5728\u6e38\u620f\u3001\u8def\u5f84\u89c4\u5212\u3001\u63a8\u8350\u7cfb\u7edf\u7b49\u591a\u4e2a\u9886\u57df\u8868\u73b0\u51fa\u826f\u597d\u7684\u9002\u7528\u6027\u3002\n- **\u591a\u6a21\u6001\u6570\u636e**\uff1a\u591a\u6a21\u6001PRM\u80fd\u591f\u5904\u7406\u591a\u79cd\u7c7b\u578b\u7684\u6570\u636e\uff0c\u9002\u7528\u4e8e\u590d\u6742\u5e94\u7528\u573a\u666f\u3002\n\n**\u6cdb\u5316\u80fd\u529b**\uff1a\n- **\u8de8\u9886\u57df\u6cdb\u5316**\uff1a\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\uff0cMCTS+PRM\u6846\u67b6\u5728\u4e0d\u540c\u9886\u57df\u95f4\u5c55\u73b0\u51fa\u8f83\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002\n- **\u52a8\u6001\u73af\u5883\u9002\u5e94**\uff1a\u5f3a\u5316\u5b66\u4e60\u673a\u5236\u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u9002\u5e94\u52a8\u6001\u53d8\u5316\u7684\u73af\u5883\u3002\n\n### Step 5: \u5206\u6790\u6700\u65b0\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027\n\n**\u7a33\u5b9a\u6027**\uff1a\n- **\u7b97\u6cd5\u4f18\u5316**\uff1a\u901a\u8fc7\u6539\u8fdbMCTS\u7684\u9009\u62e9\u7b56\u7565\u548cPRM\u7684\u53c2\u6570\u8c03\u6574\u673a\u5236\uff0c\u63d0\u5347\u4e86\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u3002\n- **\u9c81\u68d2\u6027\u589e\u5f3a**\uff1a\u5f15\u5165\u5bf9\u6297\u8bad\u7ec3\u548c\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\uff0c\u63d0\u9ad8\u6a21\u578b\u5728\u590d\u6742\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u6027\u3002\n\n**\u5bb9\u9519\u6027**\uff1a\n- **\u9519\u8bef\u5904\u7406**\uff1a\u5728\u6a21\u62df\u548c\u56de\u6eaf\u9636\u6bb5\u5f15\u5165\u5bb9\u9519\u673a\u5236\uff0c\u51cf\u5c11\u9519\u8bef\u51b3\u7b56\u7684\u5f71\u54cd\u3002\n- **\u5927\u89c4\u6a21\u6570\u636e\u9002\u5e94\u6027**\uff1a\u4f18\u5316\u7b97\u6cd5\u7ed3\u6784\uff0c\u63d0\u5347\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\u7684\u5904\u7406\u80fd\u529b\u3002\n\n### Step 6: \u8bc4\u4f30\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218\n\n**\u672a\u6765\u7814\u7a76\u65b9\u5411**\uff1a\n- **\u9ad8\u6548\u8ba1\u7b97**\uff1a\u7814\u7a76\u66f4\u9ad8\u6548\u7684MCTS\u7b97\u6cd5\uff0c\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002\n- **\u6570\u636e\u589e\u5f3a**\uff1a\u63a2\u7d22\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u51cf\u5c11PRM\u5bf9\u9ad8\u8d28\u91cf\u6570\u636e\u7684\u4f9d\u8d56\u3002\n\n**\u6311\u6218**\uff1a\n- **\u8de8\u6a21\u6001\u878d\u5408**\uff1a\u5982\u4f55\u6709\u6548\u878d\u5408\u591a\u79cd\u6a21\u6001\u6570\u636e\uff0c\u63d0\u5347\u6a21\u578b\u7684\u7efc\u5408\u6027\u80fd\u3002\n- **\u52a8\u6001\u73af\u5883\u9002\u5e94**\uff1a\u5728\u52a8\u6001\u73af\u5883\u4e0b\uff0c\u5982\u4f55\u4fdd\u6301\u6a21\u578b\u7684\u7a33\u5b9a\u6027\u548c\u9ad8\u6548\u6027\u3002\n\n**\u63a8\u52a8\u540e\u7eed\u7814\u7a76**\uff1a\n- **\u65b0\u95ee\u9898\u63d0\u51fa**\uff1a\u5982\u591a\u6a21\u6001\u6570\u636e\u4e0b\u7684\u504f\u597d\u5efa\u6a21\u3001\u52a8\u6001\u73af\u5883\u4e0b\u7684\u51b3\u7b56\u4f18\u5316\u7b49\u3002\n- **\u7814\u7a76\u5207\u5165\u70b9**\uff1a\u4ece\u7b97\u6cd5\u4f18\u5316\u3001\u6570\u636e\u589e\u5f3a\u3001\u8de8\u6a21\u6001\u878d\u5408\u7b49\u591a\u4e2a\u89d2\u5ea6\u5207\u5165\uff0c\u63a8\u52a8\u6280\u672f\u8fdb\u6b65\u3002\n\n### \u603b\u7ed3\n\n\u8fd1\u5e74\u6765\uff0cMCTS\u5728PRM\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u7684\u5e94\u7528\u63a2\u7d22\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4e3b\u8981\u4f53\u73b0\u5728\u6280\u672f\u6846\u67b6\u7684\u5b8c\u5584\u3001\u5e94\u7528\u573a\u666f\u7684\u62d3\u5c55\u3001\u6027\u80fd\u7684\u63d0\u5347\u4ee5\u53ca\u7a33\u5b9a\u6027\u548c\u5bb9\u9519\u6027\u7684\u589e\u5f3a\u3002\u7136\u800c\uff0c\u4ecd\u5b58\u5728\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u3001\u6570\u636e\u4f9d\u8d56\u6027\u5f3a\u7b49\u5c40\u9650\u6027\u3002\u672a\u6765\u7684\u7814\u7a76\u5e94\u5173\u6ce8\u9ad8\u6548\u8ba1\u7b97\u3001\u6570\u636e\u589e\u5f3a\u548c\u8de8\u6a21\u6001\u878d\u5408\u7b49\u65b9\u5411\uff0c\u4ee5\u5e94\u5bf9\u52a8\u6001\u73af\u5883\u4e0b\u7684\u51b3\u7b56\u4f18\u5316\u7b49\u6311\u6218\uff0c\u63a8\u52a8\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002", "task_step_name": "\u8def\u5f84\u89c4\u5212", "task_step_description": "\u5728\u673a\u5668\u4eba\u5bfc\u822a\u548c\u81ea\u52a8\u9a7e\u9a76\u4e2d\uff0c\u5229\u7528MCTS\u4f18\u5316\u8def\u5f84\u9009\u62e9\u3002", "task_step_level": "1>2", "task_step_question": "### \u95ee\u9898\u63d0\u51fa\n\n\u5728\u673a\u5668\u4eba\u5bfc\u822a\u548c\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\uff0c\u8def\u5f84\u89c4\u5212\u662f\u4e00\u4e2a\u5173\u952e\u4efb\u52a1\u3002\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09\u4f5c\u4e3a\u4e00\u79cd\u542f\u53d1\u5f0f\u641c\u7d22\u7b97\u6cd5\uff0c\u5df2\u88ab\u8bc1\u660e\u5728\u4f18\u5316\u8def\u5f84\u9009\u62e9\u65b9\u9762\u5177\u6709\u6f5c\u529b\u3002\u7136\u800c\uff0cMCTS\u5728\u5927\u89c4\u6a21\u95ee\u9898\u4e2d\u8ba1\u7b97\u590d\u6742\u5ea6\u8f83\u9ad8\uff0c\u8fd9\u53ef\u80fd\u9650\u5236\u4e86\u5176\u5728\u5b9e\u65f6\u5e94\u7528\u4e2d\u7684\u6548\u7387\u3002\n\n**\u95ee\u9898**\uff1a\u5728\u673a\u5668\u4eba\u5bfc\u822a\u548c\u81ea\u52a8\u9a7e\u9a76\u7684\u8def\u5f84\u89c4\u5212\u4efb\u52a1\u4e2d\uff0c\u5982\u4f55\u6709\u6548\u964d\u4f4eMCTS\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u5176\u8def\u5f84\u9009\u62e9\u7684\u4f18\u5316\u6027\u80fd\uff1f\u5177\u4f53\u6765\u8bf4\uff0c\u6709\u54ea\u4e9b\u7b56\u7565\u6216\u6280\u672f\u53ef\u4ee5\u5728MCTS\u7684\u9009\u62e9\u3001\u6269\u5c55\u3001\u6a21\u62df\u548c\u56de\u6eaf\u9636\u6bb5\u8fdb\u884c\u4f18\u5316\uff0c\u4ee5\u63d0\u9ad8\u5176\u5728\u5b9e\u65f6\u5e94\u7528\u4e2d\u7684\u6548\u7387\u548c\u5b9e\u7528\u6027\uff1f", "task_step_question_context": [{"ref_id": "454845766417543398", "chunk_id": "4", "score": 0.5390625, "text": "# Monte Carlo Tree Search in the Presence of Transition Uncertainty\nFarnaz Kohankhaki , Kiarash Aghakasiri , Hongming Zhang 1 , Ting-Han Wei 1 , Chao Gao 2 ,Martin M\u00a8uller 1  \n\n1 University of Alberta, 2 Edmonton Research Center, Huawei Canada {kohankha, aghakasi, hongmin2, tinghan, mmueller }@ualberta.ca, cgao3 $@$ outlook.com\n\n# Abstract\nMonte Carlo Tree Search (MCTS) is an immensely popular search-based framework used for decision making. It is traditionally applied to domains where a perfect simulation model of the environment is available. We study and improve MCTS in the context where the environment model is given but imperfect. We show that the discrepancy between the model and the actual environment can lead to significant performance degradation with standard MCTS. We therefore develop Uncertainty Adapted MCTS (UA-MCTS), a more robust algorithm within the MCTS framework. We estimate the transition uncertainty in the given model, and direct the search towards more certain transitions in the state space. We modify all four MCTS phases to improve the search behavior by considering these estimates. We prove, in the corrupted bandit case, that adding uncertainty information to adapt UCB leads to tighter regret bound than standard UCB. Empirically, we evaluate UA-MCTS and its individual components on the deterministic domains from the MinAtar test suite. Our results demonstrate that UA-MCTS strongly improves MCTS in the presence of model transition errors.\n\n# 1 Introduction\nThe Monte Carlo Tree Search (MCTS) framework (Browne et al. 2012) approaches sequential decision-making problems by selective lookahead search. It manages the balance of exploration and exploitation with techniques such as UCT (Kocsis, Szepesv\u00b4ari, and Willemson 2006). Often combined with machine learning, it has been enormously successful in both games (Silver et al. 2016; Banerjee 2020; Arneson, Hayward, and Henderson 2010; Saffidine 2008; Nijssen and Winands 2010) and non-game applications (Lu et al. 2016; Mansley, Weinstein, and Littman 2011; Sabharwal, Samulowitz, and Reddy 2012; Cazenave 2010). In these applications, a perfect simulation model allows for efficient lookahead search. However, in many practical applications, only an imperfect model is available to the agent. Yet lookahead using such a model can still be useful. We improve MCTS for this setting.  \n\nOne research area that studies imperfect models of the environment is model-based reinforcement learning (MBRL).  \n\nHere, an agent builds its own model through limited real world interactions. The resulting learned model, when used for lookahead search, can either be for planning or for producing more accurate training targets (Silver, Sutton, and M\u00a8uller 2008). It can also be used to generate simulated training samples for better sample efficiency (Sutton and Barto 2018). The learned model may be inaccurate for many reasons, including stochasticity of the environment, insufficient training, insufficient capacity, non stationary environments, etc. Consequently, there is a rich body of research on uncertainty in MBRL (Abbas et al. 2020; Xiao et al. 2019; Buckman et al. 2018).  \n\nWhile previous approaches to using search with imperfect models exist (Vemula et al. 2020; Vemula, Bagnell, and Likhachev 2021), to the best of our knowledge, there is no prior work that directly adapts MCTS to deal with model uncertainty. In our work, we define transition uncertainty as a measure of difference between the state transitions in the perfect model and in the model that is available to the agent. We use a neural network to estimate this uncertainty.  \n\nOur Uncertainty Adapted MCTS (UA-MCTS) approach implements the main components of the MCTS framework in a way that guides the search away from states with high uncertainty. We compare the performance of our proposed methods with MCTS baselines in three deterministic MinAtar environments (Young and Tian 2019). In each case the search agent \u201cbelieves\u201d it is playing the real game. However, the rules of the game itself have changed, and the agent only learns about this change slowly when it acts in the real environment. The results show that UA-MCTS is able to outperform the baseline MCTS with an imperfect model.  \n\nOur approach is inspired by the work of (Vemula et al. 2020) where a robotic arm has to solve tasks despite being handicapped, e.g. by a broken motor or by an unmodeled weight restriction. To show how an agent should adapt UCB-based exploration strategy in the presence of environment uncertainties, we first consider a case of stochastic bandits (Lattimore and Szepesv\u00b4ari 2020) along with corrupted feedback. We prove that incorporating uncertainty information can enhance the performance of UCB, yielding a regret bound that is more constrained compared to the standard UCB. We also prove that in the general case of tree search, with similar modification of UCT, our UA-MCTS approach maintains its completeness property, ensuring that as the number of iterations goes to infinity, all nodes will be consistently explored. To further motivate our approach, we compare the scenarios of learning to improve the transition function, using MCTS, directly against the easier task of just learning a transition uncertainty function with UA-MCTS. In both cases, learning occurs online; the former is used with MCTS while the latter is used with UA-MCTS. Our results show that learning the transition function is much harder than learning transition uncertainty, which justifies the use of UA-MCTS in such settings."}, {"ref_id": "454845659346651798", "chunk_id": "3", "score": 0.46484375, "text": "# Large-Scale Multi-Robot Coverage Path Planning via Local Search \\*\nJingtao Tang, Hang Ma  \n\nSimon Fraser University {jingtao tang, hangma }@sfu.ca\n\n# Abstract\nWe study graph-based Multi-Robot Coverage Path Planning (MCPP) that aims to compute coverage paths for multiple robots to cover all vertices of a given 2D grid terrain graph $G$ . Existing graph-based MCPP algorithms first compute a tree cover on $G$ \u2014a forest of multiple trees that cover all vertices\u2014and then employ the Spanning Tree Coverage (STC) paradigm to generate coverage paths on the decomposed graph $D$ of the terrain graph $G$ by circumnavigating the edges of the computed trees, aiming to optimize the makespan (i.e., the maximum coverage path cost among all robots). In this paper, we take a different approach by exploring how to systematically search for good coverage paths directly on $D$ . We introduce a new algorithmic framework, called LS-MCPP, which leverages a local search to operate directly on $D$ . We propose a novel standalone paradigm, Extended-STC (ESTC), that extends STC to achieve complete coverage for MCPP on any decomposed graphs, even those resulting from incomplete terrain graphs. Furthermore, we demonstrate how to integrate ESTC with three novel types of neighborhood operators into our framework to effectively guide its search process. Our extensive experiments demonstrate the effectiveness of LS-MCPP, consistently improving the initial solution returned by two state-of-the-art baseline algorithms that compute suboptimal tree covers on $G$ , with a notable reduction in makespan by up to $35.7\\%$ and $30.3\\%$ , respectively. Moreover, LS-MCPP consistently matches or surpasses the results of optimal tree cover computation, achieving these outcomes with orders of magnitude faster runtime, thereby showcasing its significant benefits for large-scale real-world coverage tasks."}, {"ref_id": "454846884311042388", "chunk_id": "1", "score": 0.4453125, "text": "# 1 Introduction\nCoverage path planning (CPP) is a fundamental problem (Galceran and Carreras 2013) in robotics, which aims to find a path for a robot to completely cover a terrain of interest, such as indoor floors (Bormann et al. 2018) and outdoor fields (Torres et al. 2016). Multi-Robot Coverage Path Planning (MCPP) is an extension of CPP tailored for multi-robot systems, aiming to coordinate the paths of multiple robots to completely cover the given terrain. With improved task efficiency and system robustness, MCPP has facilitated diverse real-world applications, including environmental monitoring (Collins et al. 2021) and search and rescue (Song et al. 2022). A fundamental challenge of MCPP lies in generating cost-balancing coverage paths to optimize task efficiency, commonly quantified by the makespan , which is the maximum path cost of all robots. This challenge is further compounded when dealing with large-scale applications where the number of robots and the size of the terrain increase.  \n\n  \nFigure 1: Graph-based CPP and MCPP: Gray squares, black circles, and black stars represent terrain graph vertices, decomposed graph vertices, and initial vertices of robots, respectively; Solid lines and dashed lines represent coverage paths and spanning edges, respectively. (a) The terrain to be covered where all terrain graph edges have uniform weights of 1 . (b) The single-robot coverage path generated by STC. (c)(d) Suboptimal and optimal 2 -robot coverage paths with makespans of 2 and 1 .5 , respectively.  \n\nIn this paper, we follow existing graph-based MCPP algorithms (Zheng et al. 2010; Li et al. 2023) that represent the terrain to be covered as a 4-connected 2D grid graph $G$ , where each edge connects horizontally or vertically adjacent vertices. The robots are required to start at and return to their respective initial vertices, as in the cover and return setting (Zheng and Koenig 2007). The foundation of these graph-based MCPP algorithms lies in the Spanning Tree Coverage (STC) paradigm (Gabriely and Rimon 2001, 2002), initially developed for (single-robot) CPP. STC operates on the terrain graph $G$ but finds a coverage path with minimal makespan on the decomposed graph $D$ derived from $G$ . The decomposed graph $D$ is also a 4-connected 2D grid graph, resulting from decomposing each vertex of $G$ into four decomposed vertices. Fig. 1 shows the terrain graph $G$ and its corresponding decomposed graph $D$ of an example terrain to be covered, where STC generates a single-robot coverage path on $D$ by circumnavigating (i.e., always moving along the right side of the spanning edges) the minimum spanning tree of $G$ .  \n\nLike STC, existing graph-based MCPP algorithms operate on the given terrain graph $G$ exclusively to build a tree cover\u2014a forest of multiple trees, each rooted at the initial vertex of a robot, that jointly cover all vertices of $G$ . The coverage path for each robot is then obtained by circumnavigating its corresponding tree. In essence, these algorithms reduce MCPP to the NP-hard Min-Max Rooted Tree Cover problem (Even et al. 2004; Nagamochi and Okada 2007) on $G$ that aims to optimize the weight of the largest-weighted tree in the tree cover since it determines the makespan of the resulting coverage paths on $D$ . However, operating on the terrain graph $G$ exclusively has two disadvantages. Firstly, it does not work for an incomplete terrain graph $G$ where some of the four decomposed vertices of a vertex might be blocked and thus absent in the decomposed graph $D$ . Secondly, an optimal tree cover on $G$ does not necessarily result in an optimal MCPP solution (as illustrated in Fig. 1-(c) and (d)), which yields an asymptotic suboptimality ratio of four for makespan in the worst case (Zheng et al. 2010), since circumnavigating the trees in a tree cover explores only a portion of the solution space that encompasses all possible sets of coverage paths on the decomposed graph $D$ .  \n\nTherefore, we take a different route to explore how to systematically search for good coverage paths directly on the decomposed graph. Our algorithmic contribution is through the following key aspects: (1) We propose a novel standalone algorithmic paradigm called Extended-STC (ESTC), an extension of STC, to address coverage planning problems on both complete and incomplete terrain graphs by directly operating on decomposed graphs. Importantly, we demonstrate that ESTC guarantees complete coverage for both singleand multi-robot settings, rendering it an efficient and versatile solution to coverage path planning. (2) We propose three types of specialized neighborhood operators to facilitate an effective local search process by identifying costefficient subgraphs of the decomposed graph that are used to generate coverage paths for the robots. The strategic integration of these operators enhances the efficiency of the local search in exploring the solution space. (3) We demonstrate how to combine these neighborhood operators with iterative calls to the ESTC paradigm to establish our proposed LS-MCPP framework for solving MCPP. To validate the effectiveness of LS-MCPP, we conduct extensive experiments, comparing it with three state-of-the-art baseline graph-based MCPP algorithms that operate on complete terrain graphs only. The results show that LS-MCPP achieves makespans that are smaller by up to $35.7\\%$ and $30.3\\%$ than two of the baseline algorithms, respectively, which compute suboptimal tree covers on the terrain graph. Additionally, LS-MCPP consistently achieves makespans comparable to or better than those achieved by the remaining baseline algorithm, which employs mixed integer programming (MIP) to compute optimal tree covers on the terrain graph. While the baseline algorithm takes more than tens of hours to complete, LS-MCPP accomplishes the same task in just a matter of minutes, showcasing its efficiency and practicality for large-scale real-world coverage problems."}], "task_step_question_answer": "\u5728\u8def\u5f84\u89c4\u5212\u7684\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u8fd9\u4e9b\u4f18\u5316\u7b56\u7565\u5df2\u7ecf\u663e\u793a\u51fa\u663e\u8457\u7684\u6548\u679c\u3002\u4f8b\u5982\uff0c\u5728\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\uff0cUA-MCTS\u548c\u542f\u53d1\u5f0f\u5f15\u5bfc\u7684\u7ed3\u5408\u4f7f\u5f97\u8f66\u8f86\u80fd\u591f\u5728\u590d\u6742\u7684\u57ce\u5e02\u73af\u5883\u4e2d\u5feb\u901f\u627e\u5230\u5b89\u5168\u7684\u884c\u9a76\u8def\u5f84\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u8ba1\u7b97\u8d44\u6e90\u7684\u6d88\u8017\u3002\u5728\u673a\u5668\u4eba\u5bfc\u822a\u4e2d\uff0c\u5c40\u90e8\u641c\u7d22\u7b56\u7565\u548c\u52a8\u6001\u526a\u679d\u7684\u5e94\u7528\u4f7f\u5f97\u673a\u5668\u4eba\u80fd\u591f\u5728\u52a8\u6001\u53d8\u5316\u7684\u73af\u5883\u4e2d\u5b9e\u65f6\u8c03\u6574\u8def\u5f84\uff0c\u907f\u514d\u4e86\u4e0d\u5fc5\u8981\u7684\u8ba1\u7b97\u5f00\u9500\u3002\u6b64\u5916\uff0c\u7b80\u5316\u6a21\u62df\u6a21\u578b\u548c\u5e76\u884c\u6a21\u62df\u7684\u7ed3\u5408\u5728\u65e0\u4eba\u673a\u8def\u5f84\u89c4\u5212\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8def\u5f84\u89c4\u5212\u7684\u5b9e\u65f6\u6027\u548c\u51c6\u786e\u6027\u3002\u5c3d\u7ba1\u8fd9\u4e9b\u4f18\u5316\u7b56\u7565\u5728\u4e0d\u540c\u573a\u666f\u4e2d\u53d6\u5f97\u4e86\u6210\u529f\uff0c\u4f46\u5728\u67d0\u4e9b\u6781\u7aef\u73af\u5883\u4e0b\uff0c\u5982\u9ad8\u52a8\u6001\u6216\u9ad8\u4e0d\u786e\u5b9a\u6027\u7684\u73af\u5883\u4e2d\uff0c\u8fd9\u4e9b\u7b56\u7565\u7684\u6548\u679c\u53ef\u80fd\u4f1a\u53d7\u5230\u9650\u5236\u3002\u672a\u6765\u7684\u7814\u7a76\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63a2\u7d22\u5982\u4f55\u5728\u8fd9\u4e9b\u6781\u7aef\u73af\u5883\u4e0b\u63d0\u5347\u7b97\u6cd5\u7684\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\uff0c\u6216\u8005\u901a\u8fc7\u5f15\u5165\u66f4\u5148\u8fdb\u7684\u6a21\u578b\u548c\u7b97\u6cd5\u6765\u8fdb\u4e00\u6b65\u63d0\u5347\u8def\u5f84\u89c4\u5212\u7684\u6027\u80fd\u3002", "ref_task_step_id": ""}, "__type__": "task_step_node"}, "5e563bfc-4301-43c2-9c34-d92ea2c5783a": {"__data__": {"id_": "5e563bfc-4301-43c2-9c34-d92ea2c5783a", "metadata": {}, "relationships": {}, "hash": "", "start_task_context": "MCTS\u5728PRM\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u7684\u5e94\u7528\u63a2\u7d22\u7efc\u8ff0", "aemo_representation_context": "### Step 1: \u5206\u6790\u8fd1\u51e0\u5e74\u7814\u7a76\u9886\u57df\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba\n\n**\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09**\uff1a\n- **\u57fa\u672c\u539f\u7406**\uff1aMCTS\u662f\u4e00\u79cd\u7528\u4e8e\u51b3\u7b56\u8fc7\u7a0b\u7684\u542f\u53d1\u5f0f\u641c\u7d22\u7b97\u6cd5\uff0c\u5e7f\u6cdb\u5e94\u7528\u4e8e\u6e38\u620fAI\u3001\u8def\u5f84\u89c4\u5212\u7b49\u9886\u57df\u3002\u5176\u6838\u5fc3\u601d\u60f3\u662f\u901a\u8fc7\u6a21\u62df\u591a\u6b21\u968f\u673a\u6e38\u620f\u6765\u8bc4\u4f30\u4e0d\u540c\u51b3\u7b56\u7684\u4f18\u52a3\u3002\n- **\u6280\u672f\u6846\u67b6**\uff1a\u4e3b\u8981\u5305\u62ec\u9009\u62e9\uff08Selection\uff09\u3001\u6269\u5c55\uff08Expansion\uff09\u3001\u6a21\u62df\uff08Simulation\uff09\u548c\u56de\u6eaf\uff08Backpropagation\uff09\u56db\u4e2a\u6b65\u9aa4\u3002\n\n**\u504f\u597d\u7b56\u7565\u6a21\u578b\uff08PRM\uff09**\uff1a\n- **\u57fa\u672c\u539f\u7406**\uff1aPRM\u65e8\u5728\u901a\u8fc7\u5b66\u4e60\u7528\u6237\u7684\u504f\u597d\u6765\u4f18\u5316\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5e38\u7528\u4e8e\u63a8\u8350\u7cfb\u7edf\u3001\u4e2a\u6027\u5316\u670d\u52a1\u7b49\u3002\n- **\u6280\u672f\u6846\u67b6**\uff1a\u901a\u5e38\u5305\u62ec\u504f\u597d\u5efa\u6a21\u3001\u7b56\u7565\u4f18\u5316\u548c\u6a21\u578b\u5fae\u8c03\u7b49\u73af\u8282\u3002\n\n**\u65b9\u6cd5\u8bba**\uff1a\n- **\u96c6\u6210\u5b66\u4e60**\uff1a\u5c06MCTS\u4e0ePRM\u7ed3\u5408\uff0c\u5229\u7528MCTS\u7684\u641c\u7d22\u80fd\u529b\u4f18\u5316PRM\u7684\u51b3\u7b56\u8fc7\u7a0b\u3002\n- **\u5f3a\u5316\u5b66\u4e60**\uff1a\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u673a\u5236\u4e0d\u65ad\u8c03\u6574PRM\u7684\u53c2\u6570\uff0c\u4ee5\u9002\u5e94\u52a8\u6001\u73af\u5883\u3002\n\n### Step 2: \u7814\u7a76\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u4e3b\u8981\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0e\u53d8\u4f53\n\n**\u5e94\u7528\u573a\u666f**\uff1a\n- **\u6e38\u620fAI**\uff1aMCTS\u4e0ePRM\u7ed3\u5408\u7528\u4e8e\u6e38\u620f\u4e2d\u7684\u667a\u80fd\u51b3\u7b56\uff0c\u5982AlphaGo\u3002\n- **\u8def\u5f84\u89c4\u5212**\uff1a\u5728\u673a\u5668\u4eba\u5bfc\u822a\u548c\u81ea\u52a8\u9a7e\u9a76\u4e2d\uff0c\u5229\u7528MCTS\u4f18\u5316\u8def\u5f84\u9009\u62e9\u3002\n- **\u63a8\u8350\u7cfb\u7edf**\uff1a\u901a\u8fc7PRM\u5b66\u4e60\u7528\u6237\u504f\u597d\uff0c\u7ed3\u5408MCTS\u8fdb\u884c\u66f4\u7cbe\u51c6\u7684\u63a8\u8350\u3002\n\n**\u53d8\u4f53**\uff1a\n- **\u6df1\u5ea6MCTS**\uff1a\u7ed3\u5408\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff0c\u63d0\u5347MCTS\u7684\u51b3\u7b56\u80fd\u529b\u3002\n- **\u591a\u6a21\u6001PRM**\uff1a\u878d\u5408\u591a\u79cd\u6570\u636e\u6e90\uff08\u5982\u6587\u672c\u3001\u56fe\u50cf\uff09\uff0c\u589e\u5f3aPRM\u7684\u6cdb\u5316\u80fd\u529b\u3002\n\n### Step 3: \u8bc4\u4f30\u5b66\u672f\u754c\u7684\u6280\u672f\u8fdb\u6b65\u4e0e\u5c40\u9650\u6027\n\n**\u6280\u672f\u8fdb\u6b65**\uff1a\n- **\u6027\u80fd\u63d0\u5347**\uff1aMCTS\u4e0ePRM\u7ed3\u5408\u663e\u8457\u63d0\u5347\u4e86\u51b3\u7b56\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002\n- **\u6cdb\u5316\u80fd\u529b**\uff1a\u591a\u6a21\u6001PRM\u5728\u5904\u7406\u591a\u6837\u5316\u6570\u636e\u65f6\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002\n\n**\u5c40\u9650\u6027**\uff1a\n- **\u8ba1\u7b97\u590d\u6742\u5ea6**\uff1aMCTS\u5728\u5927\u89c4\u6a21\u95ee\u9898\u4e2d\u8ba1\u7b97\u91cf\u5927\uff0c\u6548\u7387\u6709\u5f85\u63d0\u5347\u3002\n- **\u6570\u636e\u4f9d\u8d56**\uff1aPRM\u7684\u6027\u80fd\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u9ad8\u8d28\u91cf\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u6570\u636e\u504f\u5dee\u95ee\u9898\u4f9d\u7136\u5b58\u5728\u3002\n\n### Step 4: \u63a2\u8ba8\u8ba1\u7b97\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\n\n**\u9002\u7528\u6027**\uff1a\n- **\u591a\u9886\u57df\u5e94\u7528**\uff1aMCTS+PRM\u6846\u67b6\u5728\u6e38\u620f\u3001\u8def\u5f84\u89c4\u5212\u3001\u63a8\u8350\u7cfb\u7edf\u7b49\u591a\u4e2a\u9886\u57df\u8868\u73b0\u51fa\u826f\u597d\u7684\u9002\u7528\u6027\u3002\n- **\u591a\u6a21\u6001\u6570\u636e**\uff1a\u591a\u6a21\u6001PRM\u80fd\u591f\u5904\u7406\u591a\u79cd\u7c7b\u578b\u7684\u6570\u636e\uff0c\u9002\u7528\u4e8e\u590d\u6742\u5e94\u7528\u573a\u666f\u3002\n\n**\u6cdb\u5316\u80fd\u529b**\uff1a\n- **\u8de8\u9886\u57df\u6cdb\u5316**\uff1a\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\uff0cMCTS+PRM\u6846\u67b6\u5728\u4e0d\u540c\u9886\u57df\u95f4\u5c55\u73b0\u51fa\u8f83\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002\n- **\u52a8\u6001\u73af\u5883\u9002\u5e94**\uff1a\u5f3a\u5316\u5b66\u4e60\u673a\u5236\u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u9002\u5e94\u52a8\u6001\u53d8\u5316\u7684\u73af\u5883\u3002\n\n### Step 5: \u5206\u6790\u6700\u65b0\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027\n\n**\u7a33\u5b9a\u6027**\uff1a\n- **\u7b97\u6cd5\u4f18\u5316**\uff1a\u901a\u8fc7\u6539\u8fdbMCTS\u7684\u9009\u62e9\u7b56\u7565\u548cPRM\u7684\u53c2\u6570\u8c03\u6574\u673a\u5236\uff0c\u63d0\u5347\u4e86\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u3002\n- **\u9c81\u68d2\u6027\u589e\u5f3a**\uff1a\u5f15\u5165\u5bf9\u6297\u8bad\u7ec3\u548c\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\uff0c\u63d0\u9ad8\u6a21\u578b\u5728\u590d\u6742\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u6027\u3002\n\n**\u5bb9\u9519\u6027**\uff1a\n- **\u9519\u8bef\u5904\u7406**\uff1a\u5728\u6a21\u62df\u548c\u56de\u6eaf\u9636\u6bb5\u5f15\u5165\u5bb9\u9519\u673a\u5236\uff0c\u51cf\u5c11\u9519\u8bef\u51b3\u7b56\u7684\u5f71\u54cd\u3002\n- **\u5927\u89c4\u6a21\u6570\u636e\u9002\u5e94\u6027**\uff1a\u4f18\u5316\u7b97\u6cd5\u7ed3\u6784\uff0c\u63d0\u5347\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\u7684\u5904\u7406\u80fd\u529b\u3002\n\n### Step 6: \u8bc4\u4f30\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218\n\n**\u672a\u6765\u7814\u7a76\u65b9\u5411**\uff1a\n- **\u9ad8\u6548\u8ba1\u7b97**\uff1a\u7814\u7a76\u66f4\u9ad8\u6548\u7684MCTS\u7b97\u6cd5\uff0c\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002\n- **\u6570\u636e\u589e\u5f3a**\uff1a\u63a2\u7d22\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u51cf\u5c11PRM\u5bf9\u9ad8\u8d28\u91cf\u6570\u636e\u7684\u4f9d\u8d56\u3002\n\n**\u6311\u6218**\uff1a\n- **\u8de8\u6a21\u6001\u878d\u5408**\uff1a\u5982\u4f55\u6709\u6548\u878d\u5408\u591a\u79cd\u6a21\u6001\u6570\u636e\uff0c\u63d0\u5347\u6a21\u578b\u7684\u7efc\u5408\u6027\u80fd\u3002\n- **\u52a8\u6001\u73af\u5883\u9002\u5e94**\uff1a\u5728\u52a8\u6001\u73af\u5883\u4e0b\uff0c\u5982\u4f55\u4fdd\u6301\u6a21\u578b\u7684\u7a33\u5b9a\u6027\u548c\u9ad8\u6548\u6027\u3002\n\n**\u63a8\u52a8\u540e\u7eed\u7814\u7a76**\uff1a\n- **\u65b0\u95ee\u9898\u63d0\u51fa**\uff1a\u5982\u591a\u6a21\u6001\u6570\u636e\u4e0b\u7684\u504f\u597d\u5efa\u6a21\u3001\u52a8\u6001\u73af\u5883\u4e0b\u7684\u51b3\u7b56\u4f18\u5316\u7b49\u3002\n- **\u7814\u7a76\u5207\u5165\u70b9**\uff1a\u4ece\u7b97\u6cd5\u4f18\u5316\u3001\u6570\u636e\u589e\u5f3a\u3001\u8de8\u6a21\u6001\u878d\u5408\u7b49\u591a\u4e2a\u89d2\u5ea6\u5207\u5165\uff0c\u63a8\u52a8\u6280\u672f\u8fdb\u6b65\u3002\n\n### \u603b\u7ed3\n\n\u8fd1\u5e74\u6765\uff0cMCTS\u5728PRM\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u7684\u5e94\u7528\u63a2\u7d22\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4e3b\u8981\u4f53\u73b0\u5728\u6280\u672f\u6846\u67b6\u7684\u5b8c\u5584\u3001\u5e94\u7528\u573a\u666f\u7684\u62d3\u5c55\u3001\u6027\u80fd\u7684\u63d0\u5347\u4ee5\u53ca\u7a33\u5b9a\u6027\u548c\u5bb9\u9519\u6027\u7684\u589e\u5f3a\u3002\u7136\u800c\uff0c\u4ecd\u5b58\u5728\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u3001\u6570\u636e\u4f9d\u8d56\u6027\u5f3a\u7b49\u5c40\u9650\u6027\u3002\u672a\u6765\u7684\u7814\u7a76\u5e94\u5173\u6ce8\u9ad8\u6548\u8ba1\u7b97\u3001\u6570\u636e\u589e\u5f3a\u548c\u8de8\u6a21\u6001\u878d\u5408\u7b49\u65b9\u5411\uff0c\u4ee5\u5e94\u5bf9\u52a8\u6001\u73af\u5883\u4e0b\u7684\u51b3\u7b56\u4f18\u5316\u7b49\u6311\u6218\uff0c\u63a8\u52a8\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002", "task_step_name": "\u8bc4\u4f30\u5b66\u672f\u754c\u7684\u6280\u672f\u8fdb\u6b65\u4e0e\u5c40\u9650\u6027", "task_step_description": "MCTS\u4e0ePRM\u7ed3\u5408\u663e\u8457\u63d0\u5347\u4e86\u51b3\u7b56\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "task_step_level": "2", "task_step_question": "### \u95ee\u9898\n\n\u5728\u8bc4\u4f30\u5b66\u672f\u754c\u7684\u6280\u672f\u8fdb\u6b65\u4e0e\u5c40\u9650\u6027\u65f6\uff0cMCTS\u4e0ePRM\u7684\u7ed3\u5408\u663e\u8457\u63d0\u5347\u4e86\u51b3\u7b56\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u7ed3\u5408\u4e5f\u5e26\u6765\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u548c\u6570\u636e\u4f9d\u8d56\u6027\u5f3a\u7b49\u5c40\u9650\u6027\u3002\u57fa\u4e8e\u8fd9\u4e9b\u4fe1\u606f\uff0c\u6211\u4eec\u53ef\u4ee5\u63d0\u51fa\u4ee5\u4e0b\u95ee\u9898\uff1a\n\n**\u95ee\u9898**\uff1a\u5728MCTS\u4e0ePRM\u7ed3\u5408\u7684\u5e94\u7528\u4e2d\uff0c\u5982\u4f55\u6709\u6548\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u5e76\u51cf\u5c11\u5bf9\u9ad8\u8d28\u91cf\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u4ee5\u8fdb\u4e00\u6b65\u63d0\u5347\u51b3\u7b56\u7cfb\u7edf\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u548c\u9002\u7528\u6027\uff1f\n\n\u8fd9\u4e2a\u95ee\u9898\u65e8\u5728\u63a2\u8ba8\u5728\u4fdd\u6301MCTS\u4e0ePRM\u7ed3\u5408\u5e26\u6765\u7684\u6027\u80fd\u63d0\u5347\u7684\u540c\u65f6\uff0c\u5982\u4f55\u901a\u8fc7\u6280\u672f\u624b\u6bb5\u6216\u65b9\u6cd5\u8bba\u521b\u65b0\u6765\u514b\u670d\u5176\u5c40\u9650\u6027\uff0c\u4ece\u800c\u63a8\u52a8\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002", "task_step_question_context": [{"ref_id": "454847029383636420", "chunk_id": "9", "score": 0.181640625, "text": "# 4 Theoretical Analysis\nAlthough it is difficult to analyze the regret of MCTS-VS directly, we can theoretically analyze the influence of general variable selection by adopting the acquisition function GP-UCB. The considered general variable selection framework is as follows: after selecting a subset of variables at each iteration, the corresponding observation data (i.e., the data points sampled-so-far where only the selected variables are used) is used to build a GP model, and the next data point is sampled by maximizing GP-UCB. We use $\\mathbb{M}_{t}$ to denote the sampled variable index subset at iteration $t$ , and let $\\left|\\mathbb{M}_{t}\\right|=d_{t}$ .  \n\nRegret Analysis. Let $x^{*}$ denote an optimal solution. We analyze the cumulative regret $R_{T}\\,=$ $\\begin{array}{r}{\\sum_{t=1}^{\\bar{T}}(f(\\pmb{x}^{*})-f(\\pmb{x}^{t}))}\\end{array}$ the selected points by iteration \u2212, i.e., the Tum of the gap between the opti . To derive an upper bound on $R_{T}$ m and the function values of , we pessimistically assume that the worst function value, i.e., $\\begin{array}{r}{\\operatorname*{min}_{\\pmb{x}_{[D]\\setminus\\mathbb{M}_{t}}}f([\\pmb{x}_{\\mathbb{M}_{t}},\\pmb{x}_{[D]\\setminus\\mathbb{M}_{t}}])}\\end{array}$ , given ${\\pmb x}_{\\mathbb{M}_{t}}$ is returned in evaluation. As in [ Lipschitz assumption. 21 ,38 ], we assume that $\\mathcal{X}\\subset[0,r]^{D}$ is convex and compact, and $f$ satisfies the following Assumption 4.1. The function $f$ is a GP sample path. For some $a,b>0$ , given $L>0$ , the partial derivatives of $f$ satisfy that $\\forall i\\in[D]$ ,$\\exists\\alpha_{i}\\geq0$ ,  \n\n$$\nP\\left(\\operatorname*{sup}_{x\\in\\mathcal{X}}\\left|\\partial f/\\partial x_{i}\\right|<\\alpha_{i}L\\right)\\geq1-a e^{-\\left(L/b\\right)^{2}}.\n$$  \n\nBased on Assumption 4.1, we define $\\alpha_{i}^{*}$ to be the minimum value of $\\alpha_{i}$ such that Eq. (3) holds, which characterizes the importance of the $i$ -th variable $x_{i}$ . The larger $\\alpha_{i}^{*}$ , the greater influence of $x_{i}$ on the function $f$ . Let $\\alpha_{\\mathrm{max}}=\\operatorname*{max}_{i\\in[D]}\\alpha_{i}^{*}$ .  \n\nTheorem 4.2 gives an upper bound on the cumulative regret $R_{T}$ with high probability for general variable selection methods. The proof is inspired by that of GP-UCB without variable selection [ 38 ]$\\forall i:\\alpha_{i}^{*}\\leq1$ and provided in Appendix B.1. If we select all variables each time (i.e., \u2264(4) becomes $R_{T}\\leq\\sqrt{C_{1}T\\beta_{T}^{*}\\gamma_{T}}+2$ p, which is consistent with [ $\\forall t:\\mathbb{M}_{t}=[D])$ and assume 38 ]. Note that \u2200$\\forall t:|\\mathbb{M}_{t}|\\,=\\,d_{t}\\,=\\,D$ ||in this case, which implies that $\\beta_{t}$ increases with $t$ , leading to $\\beta_{T}^{*}=\\beta_{T}$ . We can see that usi variable selection will $R_{T}$ by $\\begin{array}{r}{2\\sum_{t=1}^{T}\\sum_{i\\in[D]\\backslash\\mathbb{M}_{t}}\\alpha_{i}^{*}L r}\\end{array}$ P,variables unselected, the larger related to the importance (i.e., $R_{T}$ $\\alpha_{i}^{*}$ . Meanwhile, the term ) of unselected variables at each iteration. The more important $\\sqrt{C_{1}T\\beta_{T}^{*}\\gamma_{T}}$ pwill decrease as \u2208$\\beta_{T}^{*}$ \\relies on the number $d_{t}$ of selected variables positively. Ideally, if the unselected variables at each iteration are always unrelated (i.e., $\\alpha_{i}^{*}\\!=\\!0$ ), the regret bound will be better than that of using all variables [38].  \n\n$b\\sqrt{\\log(4D a/\\delta)}$ Theorem 4.2. p$\\forall\\delta\\ \\in\\ (0,1)$ , where $r$ is the upper bound on each variable, and , let $\\beta_{t}\\ =\\ 2\\log(4\\pi_{t}/\\delta)\\,+\\,2d_{t}\\log(d_{t}t^{2}b r\\sqrt{\\log(4D a/\\delta)})$ {$\\{\\pi_{t}\\}_{t\\ge1}$ }\u2265satisfies $\\textstyle\\sum_{t\\geq1}\\pi_{t}^{-1}=1$ and $L\\;=$ and $\\pi_{t}>0$ . Let $\\beta_{T}^{*}=\\operatorname*{max}_{1\\leq i\\leq T}\\beta_{t}$ . At iteration $T$ , the cumulative regret  \n\n$$\nR_{T}\\leq\\sqrt{C_{1}T\\beta_{T}^{*}\\gamma_{T}}+2\\alpha_{\\operatorname*{max}}+2\\sum_{t=1}^{T}\\sum_{\\substack{i\\in[D]\\backslash\\mathbb{M}_{t}}}\\alpha_{i}^{*}L r\n$$  \n\nholds with probability least $1\\!-\\!\\delta$ , where $C_{1}$ is a constant, $\\begin{array}{r}{\\gamma_{T}\\!=\\!\\operatorname*{max}_{|\\mathcal{D}|=\\!T}I(\\pmb{y}_{\\mathcal{D}},\\pmb{f}_{\\mathcal{D}}),}\\end{array}$ $I(\\cdot,\\cdot)$ is the information gain, and $\\scriptstyle y_{\\mathcal{D}}$ Dand $f_{\\mathcal{D}}$ Dare the noisy and true observations of a set Dof points, respectively.  \n\nBy selecting been proved [21] that the cumulative regret of Dropout satisfies $d$ variables randomly at each iteration and assuming that $r=1$ and $\\forall i:\\alpha_{i}^{*}\\leq1$ \u2264, it has  \n\n$$\nR_{T}\\leq\\sqrt{C_{1}T\\beta_{T}\\gamma_{T}}+2+2T L(D-d).\n$$  \n\nIn this case, we have $d_{t}=|\\mathbb{M}_{t}|=d$ ,$r=1$ and $\\forall i:\\alpha_{i}^{*}\\leq1$ \u2264. Thus, Eq. (4) becomes  \n\n$$\nR_{T}\\leq\\sqrt{C_{1}T\\beta_{T}^{*}\\gamma_{T}}+2+2T L(D-d).\n$$  \n\nNote that $\\beta_{T}^{*}=\\beta_{T}$ here, as $\\beta_{t}$ increases with $t$ given $d_{t}=d$ . This implies that our bound Eq. (4) for general variable selection is a generalization of Eq. (5) for Dropout [ 21 ]. In [ 33 ], a regret bound analysis has also been performed for variable selection, by optimizing over $d$ fixed important variables and using a common parameter $\\alpha$ to characterize the importance of all the other $D-d$ variables.  \n\nComputational Complexity Analysis. The computational complexity of one iteration of BO depends on three critical components: fitting a GP surrogate model, maximizing an acquisition function and evaluating a sampled point. If using the squared exponential kernel, the computational complexity of fitting a GP model at iteration $t$ is $\\bar{O(t^{3}\\!+\\!t^{2}d_{t})}$ . Maximizing an acquisition function is related to the optimization algorithm. If we use the Quasi-Newton method to optimize GP-UCB, the computational complexity is $\\bar{\\mathcal{O}}(m(t^{2}+t d_{t}+d_{t}^{2}))$ [28 ], where $m$ denotes the Quasi-Newton\u2019s running rounds. The cost of evaluating a sampled point is fixed. Thus, by selecting only a subset of variables, instead of all variables, to optimize, the computational complexity can be decreased significantly. The detailed analysis is provided in Appendix B.2.  \n\nInsight. The above regret and computational complexity analyses have shown that variable selection can reduce the computational complexity while increasing the regret. Given the number $d_{t}$ of variables to be selected, a good variable selection method should select as important variables as possible, i.e., variables with as large $\\alpha_{i}^{*}$ as possible, which may help to design and evaluate variable selection methods. The experiments in Section 5.1 will show that MCTS-VS can select a good subset of variables while maintaining a small computational complexity."}, {"ref_id": "454984236281633338", "chunk_id": "4", "score": 0.1787109375, "text": "# DSensitivity Analysis of Hyper-parameters of MCTS-VS\nWe provide further studies to examine the influence of the hyper-parameters of MCTS-VS, including the employed optimization algorithm for optimizing the selected variables in each iteration, the \u201cfillin\u201d strategy, the hyper-parameter $k$ used in the best$k$ strategy, the hyper-parameter $C_{p}$ for calculating UCB in Eq. (1), the number $2\\,\\times\\,N_{v}\\,\\times\\,N_{s}$ sampled data in each iteration, the threshold $N_{b a d}$ for re-initializing a tree, and the threshold $N_{s p l i t}$ for splitting a tree node.  \n\nThe optimization algorithm is employed by MCTS-VS to optimize the selected variables in each iteration. We compare three different optimization algorithms, i.e., random search (RS), BO and TuRBO. First, we conduct experiments similar to \u201cEffectiveness of Variable Selection\u201d in Section 5.1, to show the effectiveness of MCTS-VS even when equipped with RS. Figure 6 shows that MCTSVS-RS is better than Dropout-RS and RS, revealing the advantage of MCTS-VS.  \n\n  \nFigure 6: Effectiveness of MCTS-VS when equipped with RS.  \n\nNext we compare the performance of MCTS-VS equipped with RS, BO and TuRBO, by experiments on the Hartmann functions with increasing ratio of valid variables. Hartmann 6 _500 has 6 valid variables. Hartmann 6 _5 _500 is generated by mixing 5 Hartmann 6 functions as Hartmann 6 $(\\pmb{x}_{1:6})+$ Hartmann 6 $\\backslash(\\pmb{x}_{7:12})+\\cdot\\cdot\\cdot+\\mathrm{Hartmann6}(\\pmb{x}_{25:30})$ , and appending 470 unrelated dimensions, where $\\pmb{x}_{i:j}$ denotes the $i$ -th to j-th variables. Hartmann 6 _10 _500 is generated alike. Thus, Hartmann 6 _5 _500 and Hartmann 6 _10 _500 have 30 and 60 valid variables, respectively. The results in Figure 7 show that as the ratio of valid variables increases, MCTS-VS-TuRBO gradually surpasses MCTS-VS-RS and MCTS-VS-BO, while MCTS-VS-RS becomes worse and worse. This is expected. If the ratio of valid variables is high, MCTS-VS is more likely to select the valid variables, so it is worth to use the expensive optimization algorithm, e.g., TuRBO, to optimize the selected variables. If the ratio is low, unrelated variables are more likely to be selected most of the time, so using a cheap optimization algorithm would be better. These observations also give us some guidance on selecting optimization algorithms in practice.  \n\n\u201cFill-in\u201d strategy is a basic component of variable selection methods, which influences the quality of the value of unselected variables. We compare the employed best$k$ strategy $(k=20)$ ) with the average best$k$ strategy and the random strategy. The average best$k$ strategy uses the average of the best $k$ data points for the unselected variables, and the random strategy samples the value of an unselected variable from its domain randomly. As shown in Figure 8(a), the random strategy leads to the poor performance of MCTS-VS-BO, which may be because it does not utilize the historical information and leads to over-exploration. The best${\\cdot k}$ strategy utilizes the historical points that have high objective values to fill in the unselected variables, thus behaving much better. The performance of the average strategy is between the best$k$ and random strategies. We recommend using the best$k$ strategy in practice.  \n\nThe hyper-parameter $k$ used in the best$k$ strategy controls the degree of exploitation for the unselected variables. As shown in Figure 8(b), a smaller $k$ encourages exploitation, which results in better performance in the early stage, but easily leads to premature convergence. A larger $k$ encourages exploration and behaves worse in the early stage, but may converge to a better value. We recommend using a larger $k$ if allowing enough evaluations.  \n\n  \nFigure 7: Sensitivity analysis of the optimization algorithm.  \n\n  \nFigure 8: Sensitivity analysis of the \u201cfill-in\u201d strategy and the hyper-parameter $k$ of the best$k$ strategy, using MCTS-VS-BO on Hartmann 6 _300 .  \n\nThe hyper-parameter $C_{p}$ for calculating UCB in Eq. (1) balances the exploration and exploitation of MCTS. As shown in Figure 9, a too small $C_{p}$ leads to relatively worse performance, highlighting the importance of exploration. A too large $C_{p}$ may also lead to over-exploration. But overall MCTSVS is not very sensitive to $C_{p}$ . We recommend setting $C_{p}$ between $1\\%$ and $10\\%$ of the optimum (i.e., max $f({\\boldsymbol{x}}))$ ), which is consistent with that for LA-MCTS [40].  \n\n  \nFigure 9: Sensitivity analysis of the hyper-parameter $C_{p}$ for calculating UCB in Eq. (1), using MCTS-VS-BO on Levy and Hartmann.  \n\nThe number $2\\,\\times\\,N_{v}\\,\\times\\,N_{s}$ of sampled data in ch iteration depends on the batch size $N_{v}$ of variable index subset and the sample batch size $N_{s}$ , and will influence the accuracy of estimating the variable score vector in Eq. (2). If we increase $N_{v}$ and $N_{s}$ , we can calculate the variable score more accurately, but also need more evaluations. Figure 10(a) shows that given the same number of evaluations, MCTS-VS-BO achieves the best performance when $N_{v}=2$ and $N_{s}=3$ . Thus, this setting may be a good choice to balance the accuracy of variable score and the number of evaluations, which is also used throughout the experiments.  \n\nThe threshold $N_{b a d}$ for re-initializing a tree controls the tolerance of selecting bad tree nodes (i.e., nodes containing unimportant variables). A smaller $N_{b a d}$ leads to frequent re-initialization, which can adjust quickly but may cause under-exploitation of the tree. A larger $N_{b a d}$ can make full use of the tree, but may optimize too much on unimportant variables. Figure 10(b) shows that MCTS-VS achieves the best performance when $N_{b a d}=5$ . Thus, we recommend to use this setting, to balance the re-initialization and exploitation of the tree.  \n\nThe threshold $N_{s p l i t}$ for splitting a node. If the number of variables in a node is larger than $N_{s p l i t}$ ,the node can be further partitioned. That is, the parameter $N_{s p l i t}$ controls the least number of variables in a leaf node and thus affects the number of selected variables, which has a direct influence on the wall clock time. Note that MCTS-VS selects a leaf node and optimizes the variables contained by this node in each iteration. The smaller $N_{s p l i t}$ , the shorter the time. Figure 10(c) shows that $N_{s p l i t}$ has little influence on the performance of MCTS-VS-BO, and thus we recommend to set $N_{s p l i t}=3$ to reduce the wall clock time.  \n\n  \nFigure 10: S vity analysis of the number $2\\,\\times\\,N_{v}\\,\\times\\,N_{s}$ pled data in each iteration, the threshold $N_{b a d}$ for re-initializing a tree and the threshold $N_{s p l i t}$ for splitting a node, using MCTS-VS-BO on Hartmann 6 _300 .  \n\nInfluence of the hyper-parameters on the runtime of MCTS-VS. We also provide some intuitive explanation about the influence of the hyper-parameters on the runtime. The threshold $N_{s p l i t}$ for splitting a node has a direct impact on the runtime, because it controls the least number of variables to be optimized in a leaf node. That is, the runtime will increase with $N_{s p l i t}$ . Other parameters may affect the depth of the tree and thus the runtime. For the threshold $N_{b a d}$ for re-initializing a tree, if it is set to a small value, MCTS-VS will re-build the tree frequently and the depth of the tree is small. The shallow nodes have more variables, leading to more runtime to optimize. For the hyper-parameter $C_{p}$ for calculating UCB, if it is set to a large value, the exploration is preferred and MCTS-VS will tend to select the right node (regarded as containing unimportant variables). The tree thus will be re-built freq tly, ding to more runtime. For the number $2\\,\\times\\,N_{v}\\,\\times\\,N_{s}$ of sampled data at each iteration, if $N_{v}$ and $N_{s}$ are set to large values, the depth of the tree will be small given the total number of evaluations, and thus lead to more runtime."}, {"ref_id": "454845993914250942", "chunk_id": "7", "score": 0.1708984375, "text": "# 1 Introduction\nPerformance complementarity, a phenomenon where no single algorithm consistently outperforms all others across diverse problem instances, is a well-established reality in the realm of optimization and learning problems [Kerschke et al. , 2019]. Over the past few years, the growing interest in automated algorithm selection techniques has become evident. These techniques aim to tackle the challenge of selecting the most appropriate algorithm from a predefined set for a given problem instance automatically [Ruhkopf et al. , 2022; Heins et al. , 2023]. As depicted in Figure 1(a), existing techniques predominantly rely on two sources of information: (1) the features of each problem instance and (2) the historical performance of various algorithms across problem instances [Pio et al. , 2023]. Machine learning methods are then employed to establish a mapping from problem features to a subset of algorithms that yield optimal performance. Consequently, extensive research has focused on two critical aspects within this field: (1) designing problem feature extraction methods tailored to specific problem categories or tasks [Alissa et al. , 2023], and (2) constructing advanced machine learning models to map problem features to algorithms [Tornede et al. , 2022].  \n\nHowever, it is noteworthy that there has been a conspicuous absence of research focusing on the features of the algorithms themselves. Most current research has centered on problem features, treating algorithm-related information merely as a supervisor. For instance, some studies treat the selected algorithms as labels, modeling the task as either single-label [Brazdil and Giraud-Carrier, 2018] or multi-label [Dantas and Pozo, 2020] classification. Recognizing the inherent complexity and diversity of algorithms, quantifying and describing their features can indeed be a formidable challenge, and a universal representation method applicable across different algorithms remains elusive. Nevertheless, neglecting this critical aspect of algorithm features undeniably affects the overall model performance, posing at least three issues. Firstly, disregarding algorithm features as an essential information source inevitably results in a loss of model accuracy. Furthermore, relying solely on problem features often implies a unidirectional relationship, characterized by a one-way mapping from problems to algorithms. This unidirectional mapping does not align with the underlying bidirectional nature of the relationship between algorithms and problems, potentially missing crucial information that could enhance the model\u2019s performance. Additionally, neglecting algorithm features could potentially slow down the convergence, necessitating larger training data. This contradicts the essence of algorithm selection, which seeks to reduce experimentation costs as a preprocessing step. Requiring substantial and hardto-acquire training data, such as performance data across diverse problems, undermines the original intent of algorithm selection.  \n\n  \nFigure 1: Comparison of the existing framework and the proposed framework.  \n\nOn the other hand, the algorithm set is typically much smaller than the problem set, and the candidate algorithm set usually remains fixed during the training process [Cunha et al. , 2018]. Therefore, once algorithm features can be effectively extracted, they offer a convenient and potent resource. With the advent of the era of the pre-trained large language model (LLM) [Ouyang et al. , 2022], extracting algorithm features has become more achievable. Code text data accurately represents the functionality and characteristics of an algorithm, and, with the assistance of pre-trained LLMs or dedicated pre-trained models for code text [Chen et al. , 2021], we can represent code features with minimal training overhead. The universality of this extraction process may even surpass that of problem feature extraction.  \n\nSpecifically, this paper introduces a novel algorithm selection framework, as depicted in Figure 1(b), which not only leverages information about problem features and algorithm performance but also captures algorithm representations. Accordingly, the learning objective of our model diverges from existing techniques. Instead of the one-way mapping from problems to algorithms, we directly model the matching degree between algorithm representations and problem representations. To achieve this goal, we propose an Algorithm Selection Model based on Large Language Model (AS-LLM) and deploy it in heuristic algorithm selection tasks for continuous optimization problems to demonstrate the merits of considering algorithm features. The AS-LLM model comprises two distinct tracks for extracting features from problems and algorithms. Problem features are traversed from the tree representation of the objective function, while algorithm features are extracted from corresponding code text. After passing through multiple layers of networks, the similarity computation between algorithm and problem representations determines the selected algorithm. The key contributions are summarized as follows:  \n\n\u2022 To the best of our knowledge, this paper pioneers the integration of algorithm features into the algorithm selection process, utilizing pre-trained LLMs for extracting features from algorithm code text. This method holds  \n\ngenerality across various algorithms. \u2022 The proposed AS-LLM offers at least three practical benefits: (i) A more accurate modeling of the bidirectional nature of algorithm selection tasks; (ii) Superior performance with fewer training samples; (iii) Versatility to adapt to various problem types simply by modifying the track used for problem feature extraction. \u2022 Within the AS-LLM framework, the algorithm selection task serves as a valuable benchmark for evaluating the algorithm comprehension capabilities of LLMs. We integrate various LLMs into AS-LLM and empirically demonstrate their performance differences."}], "task_step_question_answer": "\u5728\u533b\u7597\u8bca\u65ad\u9886\u57df\uff0cMCTS\u4e0ePRM\u7684\u7ed3\u5408\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u8bca\u65ad\u7684\u51c6\u786e\u6027\uff0c\u8fd8\u901a\u8fc7\u51cf\u5c11\u641c\u7d22\u7a7a\u95f4\u548c\u4f18\u5316\u6a21\u62df\u6548\u7387\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u7684\u8ba1\u7b97\u6548\u7387\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u7ed3\u5408\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u4ecd\u9762\u4e34\u4e00\u4e9b\u6311\u6218\u3002\u9996\u5148\uff0c\u8ba1\u7b97\u590d\u6742\u5ea6\u8f83\u9ad8\uff0c\u5c24\u5176\u662f\u5728\u590d\u6742\u7684\u8bca\u65ad\u573a\u666f\u4e2d\uff0c\u5c3d\u7ba1PRM\u901a\u8fc7\u5206\u6790\u60a3\u8005\u7684\u75c5\u53f2\u548c\u75c7\u72b6\u4e3aMCTS\u63d0\u4f9b\u5148\u9a8c\u4fe1\u606f\uff0c\u4f46\u5728\u9ad8\u7ef4\u6570\u636e\u548c\u591a\u6a21\u6001\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u7b97\u6cd5\u7684\u8ba1\u7b97\u6548\u7387\u4ecd\u9700\u8fdb\u4e00\u6b65\u63d0\u5347\u3002\u5176\u6b21\uff0cMCTS\u4e0ePRM\u7684\u7ed3\u5408\u5bf9\u9ad8\u8d28\u91cf\u6570\u636e\u7684\u4f9d\u8d56\u6027\u8f83\u5f3a\uff0c\u8fd9\u5728\u4e2a\u6027\u5316\u6cbb\u7597\u65b9\u6848\u63a8\u8350\u548c\u75be\u75c5\u9884\u6d4b\u4e2d\u5c24\u4e3a\u660e\u663e\u3002\u4e3a\u4e86\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u5e76\u51cf\u5c11\u5bf9\u9ad8\u8d28\u91cf\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u53ef\u4ee5\u91c7\u7528\u53d8\u91cf\u9009\u62e9\u4e0e\u4f18\u5316\u3001\u6570\u636e\u586b\u5145\u7b56\u7565\u3001\u8d85\u53c2\u6570\u8c03\u4f18\u7b49\u6280\u672f\u624b\u6bb5\u3002\u4f8b\u5982\uff0cMCTS-VS\u901a\u8fc7\u9009\u62e9\u91cd\u8981\u53d8\u91cf\u5e76\u4f18\u5316\u5b83\u4eec\uff0c\u80fd\u591f\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u51cf\u5c11\u8ba1\u7b97\u8d1f\u62c5\u3002\u6b64\u5916\uff0c\u4f7f\u7528\u201cbest-k\u201d\u7b56\u7565\u53ef\u4ee5\u5229\u7528\u5386\u53f2\u6570\u636e\u4e2d\u8868\u73b0\u6700\u4f73\u7684\u70b9\u6765\u586b\u5145\u672a\u9009\u62e9\u7684\u53d8\u91cf\uff0c\u4ece\u800c\u51cf\u5c11\u5bf9\u9ad8\u8d28\u91cf\u6570\u636e\u7684\u4f9d\u8d56\u3002\u5728\u91d1\u878d\u9884\u6d4b\u9886\u57df\uff0cMCTS\u4e0ePRM\u7684\u7ed3\u5408\u5728\u590d\u6742\u5e02\u573a\u73af\u5883\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u4f46\u5728\u6781\u7aef\u5e02\u573a\u6761\u4ef6\u4e0b\uff0c\u7b97\u6cd5\u7684\u8868\u73b0\u53ef\u80fd\u4f1a\u53d7\u5230\u9650\u5236\u3002\u672a\u6765\u7684\u7814\u7a76\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63a2\u7d22\u5982\u4f55\u5728\u8fd9\u4e9b\u6781\u7aef\u73af\u5883\u4e0b\u63d0\u5347\u7b97\u6cd5\u7684\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\uff0c\u6216\u8005\u901a\u8fc7\u5f15\u5165\u66f4\u5148\u8fdb\u7684\u6a21\u578b\u548c\u7b97\u6cd5\u6765\u8fdb\u4e00\u6b65\u63d0\u5347\u9884\u6d4b\u6027\u80fd\u3002\u6b64\u5916\uff0c\u8de8\u6a21\u6001\u878d\u5408\u548c\u9ad8\u6548\u8ba1\u7b97\u4e5f\u662f\u672a\u6765\u7814\u7a76\u7684\u91cd\u8981\u65b9\u5411\uff0c\u4f8b\u5982\uff0c\u5c06MCTS\u4e0e\u91cf\u5b50\u8ba1\u7b97\u6216\u8fb9\u7f18\u8ba1\u7b97\u7ed3\u5408\uff0c\u4ee5\u8fdb\u4e00\u6b65\u63d0\u5347\u5176\u8ba1\u7b97\u6548\u7387\u548c\u6269\u5c55\u6027\u3002", "ref_task_step_id": ""}, "__type__": "task_step_node"}, "589d43ac-52eb-46d5-a6e1-797b6c59a64e": {"__data__": {"id_": "589d43ac-52eb-46d5-a6e1-797b6c59a64e", "metadata": {}, "relationships": {}, "hash": "", "start_task_context": "MCTS\u5728PRM\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u7684\u5e94\u7528\u63a2\u7d22\u7efc\u8ff0", "aemo_representation_context": "### Step 1: \u5206\u6790\u8fd1\u51e0\u5e74\u7814\u7a76\u9886\u57df\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba\n\n**\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09**\uff1a\n- **\u57fa\u672c\u539f\u7406**\uff1aMCTS\u662f\u4e00\u79cd\u7528\u4e8e\u51b3\u7b56\u8fc7\u7a0b\u7684\u542f\u53d1\u5f0f\u641c\u7d22\u7b97\u6cd5\uff0c\u5e7f\u6cdb\u5e94\u7528\u4e8e\u6e38\u620fAI\u3001\u8def\u5f84\u89c4\u5212\u7b49\u9886\u57df\u3002\u5176\u6838\u5fc3\u601d\u60f3\u662f\u901a\u8fc7\u6a21\u62df\u591a\u6b21\u968f\u673a\u6e38\u620f\u6765\u8bc4\u4f30\u4e0d\u540c\u51b3\u7b56\u7684\u4f18\u52a3\u3002\n- **\u6280\u672f\u6846\u67b6**\uff1a\u4e3b\u8981\u5305\u62ec\u9009\u62e9\uff08Selection\uff09\u3001\u6269\u5c55\uff08Expansion\uff09\u3001\u6a21\u62df\uff08Simulation\uff09\u548c\u56de\u6eaf\uff08Backpropagation\uff09\u56db\u4e2a\u6b65\u9aa4\u3002\n\n**\u504f\u597d\u7b56\u7565\u6a21\u578b\uff08PRM\uff09**\uff1a\n- **\u57fa\u672c\u539f\u7406**\uff1aPRM\u65e8\u5728\u901a\u8fc7\u5b66\u4e60\u7528\u6237\u7684\u504f\u597d\u6765\u4f18\u5316\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5e38\u7528\u4e8e\u63a8\u8350\u7cfb\u7edf\u3001\u4e2a\u6027\u5316\u670d\u52a1\u7b49\u3002\n- **\u6280\u672f\u6846\u67b6**\uff1a\u901a\u5e38\u5305\u62ec\u504f\u597d\u5efa\u6a21\u3001\u7b56\u7565\u4f18\u5316\u548c\u6a21\u578b\u5fae\u8c03\u7b49\u73af\u8282\u3002\n\n**\u65b9\u6cd5\u8bba**\uff1a\n- **\u96c6\u6210\u5b66\u4e60**\uff1a\u5c06MCTS\u4e0ePRM\u7ed3\u5408\uff0c\u5229\u7528MCTS\u7684\u641c\u7d22\u80fd\u529b\u4f18\u5316PRM\u7684\u51b3\u7b56\u8fc7\u7a0b\u3002\n- **\u5f3a\u5316\u5b66\u4e60**\uff1a\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u673a\u5236\u4e0d\u65ad\u8c03\u6574PRM\u7684\u53c2\u6570\uff0c\u4ee5\u9002\u5e94\u52a8\u6001\u73af\u5883\u3002\n\n### Step 2: \u7814\u7a76\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u4e3b\u8981\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0e\u53d8\u4f53\n\n**\u5e94\u7528\u573a\u666f**\uff1a\n- **\u6e38\u620fAI**\uff1aMCTS\u4e0ePRM\u7ed3\u5408\u7528\u4e8e\u6e38\u620f\u4e2d\u7684\u667a\u80fd\u51b3\u7b56\uff0c\u5982AlphaGo\u3002\n- **\u8def\u5f84\u89c4\u5212**\uff1a\u5728\u673a\u5668\u4eba\u5bfc\u822a\u548c\u81ea\u52a8\u9a7e\u9a76\u4e2d\uff0c\u5229\u7528MCTS\u4f18\u5316\u8def\u5f84\u9009\u62e9\u3002\n- **\u63a8\u8350\u7cfb\u7edf**\uff1a\u901a\u8fc7PRM\u5b66\u4e60\u7528\u6237\u504f\u597d\uff0c\u7ed3\u5408MCTS\u8fdb\u884c\u66f4\u7cbe\u51c6\u7684\u63a8\u8350\u3002\n\n**\u53d8\u4f53**\uff1a\n- **\u6df1\u5ea6MCTS**\uff1a\u7ed3\u5408\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff0c\u63d0\u5347MCTS\u7684\u51b3\u7b56\u80fd\u529b\u3002\n- **\u591a\u6a21\u6001PRM**\uff1a\u878d\u5408\u591a\u79cd\u6570\u636e\u6e90\uff08\u5982\u6587\u672c\u3001\u56fe\u50cf\uff09\uff0c\u589e\u5f3aPRM\u7684\u6cdb\u5316\u80fd\u529b\u3002\n\n### Step 3: \u8bc4\u4f30\u5b66\u672f\u754c\u7684\u6280\u672f\u8fdb\u6b65\u4e0e\u5c40\u9650\u6027\n\n**\u6280\u672f\u8fdb\u6b65**\uff1a\n- **\u6027\u80fd\u63d0\u5347**\uff1aMCTS\u4e0ePRM\u7ed3\u5408\u663e\u8457\u63d0\u5347\u4e86\u51b3\u7b56\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002\n- **\u6cdb\u5316\u80fd\u529b**\uff1a\u591a\u6a21\u6001PRM\u5728\u5904\u7406\u591a\u6837\u5316\u6570\u636e\u65f6\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002\n\n**\u5c40\u9650\u6027**\uff1a\n- **\u8ba1\u7b97\u590d\u6742\u5ea6**\uff1aMCTS\u5728\u5927\u89c4\u6a21\u95ee\u9898\u4e2d\u8ba1\u7b97\u91cf\u5927\uff0c\u6548\u7387\u6709\u5f85\u63d0\u5347\u3002\n- **\u6570\u636e\u4f9d\u8d56**\uff1aPRM\u7684\u6027\u80fd\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u9ad8\u8d28\u91cf\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u6570\u636e\u504f\u5dee\u95ee\u9898\u4f9d\u7136\u5b58\u5728\u3002\n\n### Step 4: \u63a2\u8ba8\u8ba1\u7b97\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\n\n**\u9002\u7528\u6027**\uff1a\n- **\u591a\u9886\u57df\u5e94\u7528**\uff1aMCTS+PRM\u6846\u67b6\u5728\u6e38\u620f\u3001\u8def\u5f84\u89c4\u5212\u3001\u63a8\u8350\u7cfb\u7edf\u7b49\u591a\u4e2a\u9886\u57df\u8868\u73b0\u51fa\u826f\u597d\u7684\u9002\u7528\u6027\u3002\n- **\u591a\u6a21\u6001\u6570\u636e**\uff1a\u591a\u6a21\u6001PRM\u80fd\u591f\u5904\u7406\u591a\u79cd\u7c7b\u578b\u7684\u6570\u636e\uff0c\u9002\u7528\u4e8e\u590d\u6742\u5e94\u7528\u573a\u666f\u3002\n\n**\u6cdb\u5316\u80fd\u529b**\uff1a\n- **\u8de8\u9886\u57df\u6cdb\u5316**\uff1a\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\uff0cMCTS+PRM\u6846\u67b6\u5728\u4e0d\u540c\u9886\u57df\u95f4\u5c55\u73b0\u51fa\u8f83\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002\n- **\u52a8\u6001\u73af\u5883\u9002\u5e94**\uff1a\u5f3a\u5316\u5b66\u4e60\u673a\u5236\u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u9002\u5e94\u52a8\u6001\u53d8\u5316\u7684\u73af\u5883\u3002\n\n### Step 5: \u5206\u6790\u6700\u65b0\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027\n\n**\u7a33\u5b9a\u6027**\uff1a\n- **\u7b97\u6cd5\u4f18\u5316**\uff1a\u901a\u8fc7\u6539\u8fdbMCTS\u7684\u9009\u62e9\u7b56\u7565\u548cPRM\u7684\u53c2\u6570\u8c03\u6574\u673a\u5236\uff0c\u63d0\u5347\u4e86\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u3002\n- **\u9c81\u68d2\u6027\u589e\u5f3a**\uff1a\u5f15\u5165\u5bf9\u6297\u8bad\u7ec3\u548c\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\uff0c\u63d0\u9ad8\u6a21\u578b\u5728\u590d\u6742\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u6027\u3002\n\n**\u5bb9\u9519\u6027**\uff1a\n- **\u9519\u8bef\u5904\u7406**\uff1a\u5728\u6a21\u62df\u548c\u56de\u6eaf\u9636\u6bb5\u5f15\u5165\u5bb9\u9519\u673a\u5236\uff0c\u51cf\u5c11\u9519\u8bef\u51b3\u7b56\u7684\u5f71\u54cd\u3002\n- **\u5927\u89c4\u6a21\u6570\u636e\u9002\u5e94\u6027**\uff1a\u4f18\u5316\u7b97\u6cd5\u7ed3\u6784\uff0c\u63d0\u5347\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\u7684\u5904\u7406\u80fd\u529b\u3002\n\n### Step 6: \u8bc4\u4f30\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218\n\n**\u672a\u6765\u7814\u7a76\u65b9\u5411**\uff1a\n- **\u9ad8\u6548\u8ba1\u7b97**\uff1a\u7814\u7a76\u66f4\u9ad8\u6548\u7684MCTS\u7b97\u6cd5\uff0c\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002\n- **\u6570\u636e\u589e\u5f3a**\uff1a\u63a2\u7d22\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u51cf\u5c11PRM\u5bf9\u9ad8\u8d28\u91cf\u6570\u636e\u7684\u4f9d\u8d56\u3002\n\n**\u6311\u6218**\uff1a\n- **\u8de8\u6a21\u6001\u878d\u5408**\uff1a\u5982\u4f55\u6709\u6548\u878d\u5408\u591a\u79cd\u6a21\u6001\u6570\u636e\uff0c\u63d0\u5347\u6a21\u578b\u7684\u7efc\u5408\u6027\u80fd\u3002\n- **\u52a8\u6001\u73af\u5883\u9002\u5e94**\uff1a\u5728\u52a8\u6001\u73af\u5883\u4e0b\uff0c\u5982\u4f55\u4fdd\u6301\u6a21\u578b\u7684\u7a33\u5b9a\u6027\u548c\u9ad8\u6548\u6027\u3002\n\n**\u63a8\u52a8\u540e\u7eed\u7814\u7a76**\uff1a\n- **\u65b0\u95ee\u9898\u63d0\u51fa**\uff1a\u5982\u591a\u6a21\u6001\u6570\u636e\u4e0b\u7684\u504f\u597d\u5efa\u6a21\u3001\u52a8\u6001\u73af\u5883\u4e0b\u7684\u51b3\u7b56\u4f18\u5316\u7b49\u3002\n- **\u7814\u7a76\u5207\u5165\u70b9**\uff1a\u4ece\u7b97\u6cd5\u4f18\u5316\u3001\u6570\u636e\u589e\u5f3a\u3001\u8de8\u6a21\u6001\u878d\u5408\u7b49\u591a\u4e2a\u89d2\u5ea6\u5207\u5165\uff0c\u63a8\u52a8\u6280\u672f\u8fdb\u6b65\u3002\n\n### \u603b\u7ed3\n\n\u8fd1\u5e74\u6765\uff0cMCTS\u5728PRM\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u7684\u5e94\u7528\u63a2\u7d22\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4e3b\u8981\u4f53\u73b0\u5728\u6280\u672f\u6846\u67b6\u7684\u5b8c\u5584\u3001\u5e94\u7528\u573a\u666f\u7684\u62d3\u5c55\u3001\u6027\u80fd\u7684\u63d0\u5347\u4ee5\u53ca\u7a33\u5b9a\u6027\u548c\u5bb9\u9519\u6027\u7684\u589e\u5f3a\u3002\u7136\u800c\uff0c\u4ecd\u5b58\u5728\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u3001\u6570\u636e\u4f9d\u8d56\u6027\u5f3a\u7b49\u5c40\u9650\u6027\u3002\u672a\u6765\u7684\u7814\u7a76\u5e94\u5173\u6ce8\u9ad8\u6548\u8ba1\u7b97\u3001\u6570\u636e\u589e\u5f3a\u548c\u8de8\u6a21\u6001\u878d\u5408\u7b49\u65b9\u5411\uff0c\u4ee5\u5e94\u5bf9\u52a8\u6001\u73af\u5883\u4e0b\u7684\u51b3\u7b56\u4f18\u5316\u7b49\u6311\u6218\uff0c\u63a8\u52a8\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002", "task_step_name": "\u6280\u672f\u8fdb\u6b65", "task_step_description": "MCTS\u4e0ePRM\u7ed3\u5408\u663e\u8457\u63d0\u5347\u4e86\u51b3\u7b56\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "task_step_level": "2>1", "task_step_question": "### \u95ee\u9898\u63d0\u51fa\n\n\u5728MCTS\u4e0ePRM\u7ed3\u5408\u663e\u8457\u63d0\u5347\u51b3\u7b56\u7cfb\u7edf\u6027\u80fd\u7684\u80cc\u666f\u4e0b\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u7684\u60c5\u51b5\u4e0b\uff0c\u5982\u4f55\u8fdb\u4e00\u6b65\u4f18\u5316MCTS\u7684\u8ba1\u7b97\u6548\u7387\uff0c\u4ee5\u964d\u4f4e\u5176\u5728\u5904\u7406\u5927\u89c4\u6a21\u95ee\u9898\u65f6\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u4ece\u800c\u66f4\u5e7f\u6cdb\u5730\u5e94\u7528\u4e8e\u5b9e\u9645\u573a\u666f\u4e2d\uff1f\n\n### \u95ee\u9898\u80cc\u666f\n\nMCTS\u4e0ePRM\u7684\u7ed3\u5408\u5728\u63d0\u5347\u51b3\u7b56\u7cfb\u7edf\u6027\u80fd\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u5c24\u5176\u662f\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002\u7136\u800c\uff0cMCTS\u5728\u5927\u89c4\u6a21\u95ee\u9898\u4e2d\u8ba1\u7b97\u91cf\u5927\uff0c\u6548\u7387\u6709\u5f85\u63d0\u5347\u3002\u56e0\u6b64\uff0c\u5982\u4f55\u8fdb\u4e00\u6b65\u4f18\u5316MCTS\u7684\u8ba1\u7b97\u6548\u7387\uff0c\u4ee5\u964d\u4f4e\u5176\u5728\u5904\u7406\u5927\u89c4\u6a21\u95ee\u9898\u65f6\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u6210\u4e3a\u4e00\u4e2a\u4e9f\u5f85\u89e3\u51b3\u7684\u95ee\u9898\u3002\n\n### \u95ee\u9898\u610f\u4e49\n\n\u4f18\u5316MCTS\u7684\u8ba1\u7b97\u6548\u7387\u4e0d\u4ec5\u80fd\u591f\u63d0\u5347\u5176\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u8fd8\u80fd\u4f7f\u5176\u66f4\u5e7f\u6cdb\u5730\u5e94\u7528\u4e8e\u5b9e\u9645\u573a\u666f\u4e2d\uff0c\u5982\u6e38\u620fAI\u3001\u8def\u5f84\u89c4\u5212\u548c\u63a8\u8350\u7cfb\u7edf\u7b49\u3002\u8fd9\u5c06\u6709\u52a9\u4e8e\u63a8\u52a8MCTS\u4e0ePRM\u7ed3\u5408\u6280\u672f\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\uff0c\u63d0\u5347\u51b3\u7b56\u7cfb\u7edf\u7684\u6574\u4f53\u6027\u80fd\u3002\n\n### \u95ee\u9898\u63a2\u8ba8\n\n1. **\u7b97\u6cd5\u4f18\u5316**\uff1a\u5982\u4f55\u901a\u8fc7\u6539\u8fdbMCTS\u7684\u9009\u62e9\u7b56\u7565\u548cPRM\u7684\u53c2\u6570\u8c03\u6574\u673a\u5236\uff0c\u63d0\u5347\u7b97\u6cd5\u7684\u8ba1\u7b97\u6548\u7387\uff1f\n2. **\u5e76\u884c\u8ba1\u7b97**\uff1a\u5982\u4f55\u5229\u7528\u5e76\u884c\u8ba1\u7b97\u6280\u672f\uff0c\u52a0\u901fMCTS\u7684\u6a21\u62df\u548c\u56de\u6eaf\u8fc7\u7a0b\uff0c\u4ece\u800c\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff1f\n3. **\u6570\u636e\u7ed3\u6784\u4f18\u5316**\uff1a\u5982\u4f55\u4f18\u5316MCTS\u7684\u6570\u636e\u7ed3\u6784\uff0c\u51cf\u5c11\u5185\u5b58\u5360\u7528\u548c\u8ba1\u7b97\u65f6\u95f4\uff0c\u63d0\u5347\u6574\u4f53\u6548\u7387\uff1f\n4. **\u542f\u53d1\u5f0f\u65b9\u6cd5**\uff1a\u5982\u4f55\u5f15\u5165\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u51cf\u5c11MCTS\u7684\u641c\u7d22\u7a7a\u95f4\uff0c\u4ece\u800c\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff1f\n\n\u901a\u8fc7\u63a2\u8ba8\u8fd9\u4e9b\u95ee\u9898\uff0c\u53ef\u4ee5\u4e3a\u8fdb\u4e00\u6b65\u4f18\u5316MCTS\u7684\u8ba1\u7b97\u6548\u7387\u63d0\u4f9b\u65b0\u7684\u601d\u8def\u548c\u65b9\u6cd5\uff0c\u63a8\u52a8MCTS\u4e0ePRM\u7ed3\u5408\u6280\u672f\u7684\u5e7f\u6cdb\u5e94\u7528\u3002", "task_step_question_context": [{"ref_id": "454846649385965806", "chunk_id": "1", "score": 0.55859375, "text": "# 2.2 Acceleration of MCTS\nMCTS-based methods have proved their strong capability of solving complex games or tasks. However, the high computational cost of MCTS hinders its application to some real-time and more general scenarios. Therefore, numerous works are devoted to accelerating MCTS. For example, to make the selection stage more effective, some heuristic pruning methods [ 14 ,33 ,29 ,1 ,2 ] aim to reduce the width and depth of the search tree with some heuristic functions. Furthermore, for more efficient evaluations, Lorentz [ 22 ] proposed early playout termination of MCTS (MCTS-EPT) to stop the random playouts early and use an evaluation function to assess win or loss. Moreover, Hsueh et al. [18 ] applied MCTS-EPT to the Chinese dark chess and proved its effectiveness. Afterward, similar ideas have been applied in the evaluation stage of AlphaGoZero [ 32 ] and later MCTS-based methods [31 ,27 ,34 ]. They evaluate the $Q$ -values through a learnable evaluation network instead of running playouts to the end. Grill et al. [15 ] propose a novel regularized policy optimization method based on AlphaZero to decrease the search budget of MCTS, which is from the optimization perspective. Danihelka et al. [10 ] propose a policy improvement algorithm based on sampling actions without replacement, named Gumbel trick to achieve better performance when planning with few simulations. However, these methods mentioned above focus on the specific stage of the search iteration or reduce the total budget through pruning and optimization methods, which are orthogonal to us. And few works targets at the search loop. Lan et al. [21 ] propose DS-MCTS, which defines the uncertainty of MCTS and approximates it by extra DNNs with specific features for board games in training. During the evaluation, DS-MCTS will check periodically and stop the search if the state is certain.\n\n# 3 Background\nThe AlphaGo series of work [ 30 ,32 ,31 ,27 ] are all MCTS-based reinforcement learning algorithms. Those algorithms assume the environment transition dynamics are known or learn the environment dynamics. Based on the dynamics, they use the Monte-Carlo tree search (MCTS) as the policy improvement operator. I.e., taking in the current policy, MCTS returns a better policy with the search algorithm. The systematic search allows the MCTS-based RL algorithm to quickly improve the policy and perform much better in the setting where heavy reasoning is required.\n\n# 3.1 MCTS\nThis part briefly introduces the MCTS method implemented in reinforcement learning applications. As mentioned in the related works, modern MCTS-based RL algorithms include four stages in the search loop, namely selection, expansion, evaluation, and backpropagation.  \n\nMCTS takes in the current states and generates a policy after the search loop of $N$ iterations. Here $N$ is a constant number of iterations set by the designer, regarded as the total budget. In the selection stage of each iteration, an action will be selected by maximizing over UCB. Specifically, AlphaZero [31 ] and MuZero [ 27 ] are developed based on a variant of UCB, P-UCT [ 25 ] and have achieved great success on board games and Atari games. The formula of P-UCT is the Eq (1):  \n\n$$\na^{k}=\\arg\\operatorname*{max}_{a\\in\\mathcal{A}}Q(s,a)+P(s,a)\\frac{\\sqrt{\\sum_{b\\in\\mathcal{A}}N(s,b)}}{1+N(s,a)}(c_{1}+\\log((\\sum_{b\\in\\mathcal{A}}N(s,b)+c_{2}+1)/c_{2})),\n$$  \n\nwhere $k$ is the index of iteration, $\\boldsymbol{\\mathcal{A}}$ is the acti $Q(s,a)$ is the estimated Q-value, $P(s,a)$ is the policy prior obtained from neural networks, $N(s,a)$ is the visitations to select the action a from the state $s$ and $c_{1},c_{2}$ are hyper-parameters. The output of MCTS is the visitation of each action of the root node. After $N$ search iterations, the final policy $\\pi(s)$ is defined as the normalized root visitation distribution simplification, we use $\\pi_{N}(s)$ $\\pi_{k}$ in place of , where $\\begin{array}{r}{\\pi_{k}(s,a)=N(s,a)/\\sum_{b\\in\\mathcal{A}}N(s,b)=N(s,a)/k,a\\in\\mathcal{A}}\\end{array}$ $\\pi_{k}(s)$ sometimes. And the detailed procedure of MCTS is \u2208A . For introduced in Appendix. In our method, we propose to approximate the final policy $\\pi_{N}(s)$ with $\\hat{\\pi}_{k}(s)$ ,which we name as a virtual expanded policy, through a new expansion method and a termination rule. In this way, the number of iterations in MCTS can be reduced from $N$ to $k$ .\n\n# 3.2 Computation Requirement\nMost of the computations in MCTS-based RL are in the MCTS procedure. Each action taken by MCTS requires $N$ times neural network evaluations, where $N$ is a constant number of iterations in the search loop. Traditional RL algorithms, such as PPO [ 28 ] or DQN [ 23 ], only need a single neural network evaluation per action. Thus, MCTS-based RL is roughly $N$ times computationally more expensive than traditional RL algorithms. In practice, training a single Atari game needs 12 hours of computation time on 40 TPUs [ 27 ]. The computation need is roughly two orders of magnitude more than traditional RL algorithms [28], although the final performance of MuZero is much better.\n\n# 4 Method\nWe aim to spend more search time on harder states and less on easier states. Intuitively, human knows when to make a quick decision or a slow decision under different circumstances. Unfortunately, this situation-aware behavior is absent in current MCTS algorithms. Therefore, we propose an MCTS variant that terminates the search iteration adaptively. It consists of two components: a novel expansion method named virtual expansion to estimate the final visitation based on the current partial tree; a termination rule that decides when to terminate based on the hardness of the current scenario. And we will display the adaptive mechanism through visualizations in Section 5.5."}, {"ref_id": "454845581169535994", "chunk_id": "7", "score": 0.484375, "text": "# Spending Thinking Time Wisely: Accelerating MCTS with Virtual Expansions\nWeirui $\\mathbf{Ye}^{*}$ Pieter Abbeel \u2020Yang Gao $\\ast\\ddag\\S$ \u2217Tsinghua University, \u2020UC Berkeley, \u00a7Shanghai Qi Zhi Institute\n\n# Abstract\nOne of the most important AI research questions is to trade off computation versus performance since \u201cperfect rationality\" exists in theory but is impossible to achieve in practice. Recently, Monte-Carlo tree search (MCTS) has attracted considerable attention due to the significant performance improvement in various challenging domains. However, the expensive time cost during search severely restricts its scope for applications. This paper proposes the Virtual MCTS (V-MCTS), a variant of MCTS that spends more search time on harder states and less search time on simpler states adaptively. We give theoretical bounds of the proposed method and evaluate the performance and computations on $9\\times9$ Go board games and Atari games. Experiments show that our method can achieve comparable performances to the original search algorithm while requiring less than $50\\%$ search time on average. We believe that this approach is a viable alternative for tasks under limited time and resources. The code is available at https://github.com/YeWR/V-MCTS.git .\n\n# 1 Introduction\nWhen artificial intelligence was first studied in the 1950s, researchers have sought to find the solution to the question \u201cHow to build an agent with perfect rationality\". The term \u201cperfect rationality\" [7 ,24 ,26 ] here refers to the decision made with infinite amounts of computations. However, one can only solve small-scale problems without considering the practical computation time since classical search algorithms usually exhibit exponential running time. Therefore, recent AI research would no longer seek to achieve \u201cperfect rationality\", but instead carefully trade-off computation versus the level of rationality. People have developed computational models like \u201cbounded optimality\" to model these settings [ 26 ]. The increasing level of rationality under the same computational budget has given us a lot of AI successes. Algorithms include the Monte-Carlo sampling algorithms, the variational inference algorithms, and using DNNs as universal function approximators [9, 8, 13, 30, 17].  \n\nRecently, MCTS-based RL algorithms have achieved much success, mainly on board games. The most notable achievement is that AlphaGo beats Hui Fan in 2015 [ 30 ]. It is the first time a computer program beat a human professional Go player. Afterward, AlphaGo beats two top-ranking human players, Lee Sedol in 2016 and Jie Ke in 2017, the latter of which ranked first worldwide at the time. Later, MCTS-based RL algorithms were further extended to other board games and Atari games [ 27 ]. EfficientZero [ 34 ] significantly improves the sample efficiency of MCTS-based RL algorithms, shedding light on its future applications in the real world like robotics and self-driving.  \n\nDespite the impressive performance of MCTS-based RL algorithms, they require massive amounts of computation to train and evaluate. For example, MuZero [ 27 ] used 1000 TPUs trained for 12 hours to learn the game of Go, and for a single Atari game, it needs 40 TPUs to train 12 hours. Compared to previous algorithms on the Atari games benchmark, it needs around two orders of magnitude more compute. This prohibitively large computational requirement has slowed down both the further development of MCTS-based RL algorithms as well as its practical use.  \n\nUnder the hood, MCTS-based RL algorithms imagine the futures when taking different future action sequences. However, this imaging process for the current method is not computationally efficient. For example, AlphaGo needs to look ahead 1600 game states to place a single stone. On the contrary, top human professional players can only think through around 100-200 game states per minute [ 30 ]. Apart from the inefficiency, the current MCTS algorithm deals with easy and challenging cases with the same computational budget. However, human knows to use their time when it is most needed.  \n\nIn this paper, we aim to design new algorithms that save the computational time of the MCTSbased RL methods. We make three key contributions : (1) We present Virtual MCTS, a variant of MCTS, to approximate the vanilla MCTS search policies with less computation. Moreover, unlike previous pruning-based methods that focus on the selection or evaluation stage in MCTS, our method improves the search loop. It terminates the search iterations earlier adaptively when current states are simpler; (2) Theoretically, we provide some error bounds of the proposed method. Furthermore, the visualization results indicate that Virtual MCTS has a better computation and performance trade-off than vanilla MCTS; (3) Empirically, our method can save more than $50\\%$ of search times on the challenging game Go $9\\times9$ and more than $60\\%$ on the visually complex Atari games while keeping comparable performances to those of vanilla MCTS.\n\n# 2 Related Work\n\n# 2.1 Reinforcement Learning with MCTS\nFor a long time, Computer Go has been regarded as a remarkably challenging game [ 3 ,6 ]. Researchers attempt to use Monte-Carlo techniques that evaluate the value of the node state through random playouts [ 4 ,11 ,12 ,30 ]. Afterward, UCT algorithms have generally been applied in Monte-Carlo tree search (MCTS) algorithms, which use UCB1 to select action at each node of the tree [ 20 ]. Recently, MCTS-based RL methods [ 30 ,32 ,31 ,27 ] have become increasingly popular and achieved super-human performances on board games because of their strong ability to search.  \n\nModern MCTS-based RL algorithms include four stages in the search loop : selection, expansion, evaluation, and backpropagation. The computation bottlenecks in vanilla MCTS come from the search loop, especially for the evaluation stage and the selection stage of each iteration. The selection stage is time-consuming when the search tree becomes wider and deeper. The evaluation stage is quite expensive because people attempt to evaluate the node value by random playouts to the end in previous researches. Due to the search loop, MCTS-based algorithms have multiple model inferences compared to other model-free RL methods like PPO [28] and SAC [16]."}, {"ref_id": "454845771530662550", "chunk_id": "1", "score": 0.458984375, "text": "# 5 Experiment\nTo examine the performance of MCTS-VS, we conduct experiments on different tasks, including synthetic functions, NAS-bench problems and MuJoCo locomotion tasks, to compare MCTS-VS with other black-box optimization methods. For MCTS-VS, we use the same hyper-parameters except $C_{p}$ , which is used for calculating UCB in Eq. (1). For Dropout and embedding-based methods, we set the parameter $d$ to the number of valid dimensions for synthetic functions, and a reasonable value for real-world problems. The hyper-parameters of the same components of different methods are set to the same. We use five identical random seeds (2021\u20132025) for all problems and methods. More details about the settings can be found in Appendix C. Our code is available at https://github.com/lamda-bbo/MCTS-VS ."}], "task_step_question_answer": "\u5728\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\uff0c\u91cf\u5b50\u8ba1\u7b97\u7684\u5e94\u7528\u53ef\u4ee5\u901a\u8fc7\u52a0\u901fMCTS\u7684\u6a21\u62df\u8fc7\u7a0b\uff0c\u4f7f\u5f97\u8f66\u8f86\u80fd\u591f\u5728\u590d\u6742\u7684\u4ea4\u901a\u573a\u666f\u4e2d\u66f4\u5feb\u5730\u505a\u51fa\u51b3\u7b56\u3002\u4f8b\u5982\uff0c\u5728\u907f\u8ba9\u884c\u4eba\u6216\u5e94\u5bf9\u7a81\u53d1\u4ea4\u901a\u4e8b\u4ef6\u65f6\uff0c\u91cf\u5b50\u8ba1\u7b97\u53ef\u4ee5\u663e\u8457\u51cf\u5c11\u51b3\u7b56\u65f6\u95f4\uff0c\u4ece\u800c\u63d0\u9ad8\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u3002\u8fb9\u7f18\u8ba1\u7b97\u5219\u53ef\u4ee5\u5c06MCTS\u7684\u8ba1\u7b97\u4efb\u52a1\u5206\u5e03\u5230\u8f66\u8f7d\u8ba1\u7b97\u8bbe\u5907\u4e0a\uff0c\u4f7f\u5f97\u8f66\u8f86\u80fd\u591f\u5728\u5b9e\u65f6\u73af\u5883\u4e2d\u5feb\u901f\u8c03\u6574\u8def\u5f84\u89c4\u5212\uff0c\u51cf\u5c11\u6570\u636e\u4f20\u8f93\u5ef6\u8fdf\u3002\u5728\u533b\u7597\u8bca\u65ad\u4e2d\uff0c\u91cf\u5b50\u8ba1\u7b97\u53ef\u4ee5\u52a0\u901fMCTS\u7684\u8bca\u65ad\u8fc7\u7a0b\uff0c\u4f7f\u5f97\u533b\u751f\u80fd\u591f\u5728\u66f4\u77ed\u7684\u65f6\u95f4\u5185\u83b7\u5f97\u66f4\u7cbe\u51c6\u7684\u8bca\u65ad\u5efa\u8bae\u3002\u8fb9\u7f18\u8ba1\u7b97\u5219\u53ef\u4ee5\u5c06MCTS\u7684\u8bca\u65ad\u4efb\u52a1\u5206\u5e03\u5230\u672c\u5730\u8bbe\u5907\u4e0a\uff0c\u4f7f\u5f97\u533b\u751f\u80fd\u591f\u5728\u5b9e\u65f6\u73af\u5883\u4e2d\u5feb\u901f\u83b7\u5f97\u8bca\u65ad\u5efa\u8bae\uff0c\u51cf\u5c11\u6570\u636e\u4f20\u8f93\u5ef6\u8fdf\u3002\u5728\u91d1\u878d\u9884\u6d4b\u9886\u57df\uff0c\u91cf\u5b50\u8ba1\u7b97\u53ef\u4ee5\u52a0\u901fMCTS\u7684\u5e02\u573a\u8d8b\u52bf\u9884\u6d4b\u8fc7\u7a0b\uff0c\u4f7f\u5f97\u6295\u8d44\u8005\u80fd\u591f\u5728\u66f4\u77ed\u7684\u65f6\u95f4\u5185\u505a\u51fa\u66f4\u667a\u80fd\u7684\u6295\u8d44\u51b3\u7b56\u3002\u8fb9\u7f18\u8ba1\u7b97\u5219\u53ef\u4ee5\u5c06MCTS\u7684\u9884\u6d4b\u4efb\u52a1\u5206\u5e03\u5230\u672c\u5730\u8bbe\u5907\u4e0a\uff0c\u4f7f\u5f97\u6295\u8d44\u8005\u80fd\u591f\u5728\u5b9e\u65f6\u73af\u5883\u4e2d\u5feb\u901f\u8c03\u6574\u6295\u8d44\u7b56\u7565\uff0c\u51cf\u5c11\u6570\u636e\u4f20\u8f93\u5ef6\u8fdf\u3002\u901a\u8fc7\u4e0d\u65ad\u63a2\u7d22\u548c\u521b\u65b0\uff0cMCTS\u5728\u672a\u6765\u6709\u671b\u5728\u66f4\u591a\u9886\u57df\u53d1\u6325\u91cd\u8981\u4f5c\u7528\uff0c\u4e3a\u590d\u6742\u51b3\u7b56\u95ee\u9898\u63d0\u4f9b\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "ref_task_step_id": ""}, "__type__": "task_step_node"}, "64b86178-229d-4430-83ef-e052788e3896": {"__data__": {"id_": "64b86178-229d-4430-83ef-e052788e3896", "metadata": {}, "relationships": {}, "hash": "", "start_task_context": "MCTS\u5728PRM\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u7684\u5e94\u7528\u63a2\u7d22\u7efc\u8ff0", "aemo_representation_context": "### Step 1: \u5206\u6790\u8fd1\u51e0\u5e74\u7814\u7a76\u9886\u57df\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba\n\n**\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09**\uff1a\n- **\u57fa\u672c\u539f\u7406**\uff1aMCTS\u662f\u4e00\u79cd\u7528\u4e8e\u51b3\u7b56\u8fc7\u7a0b\u7684\u542f\u53d1\u5f0f\u641c\u7d22\u7b97\u6cd5\uff0c\u5e7f\u6cdb\u5e94\u7528\u4e8e\u6e38\u620fAI\u3001\u8def\u5f84\u89c4\u5212\u7b49\u9886\u57df\u3002\u5176\u6838\u5fc3\u601d\u60f3\u662f\u901a\u8fc7\u6a21\u62df\u591a\u6b21\u968f\u673a\u6e38\u620f\u6765\u8bc4\u4f30\u4e0d\u540c\u51b3\u7b56\u7684\u4f18\u52a3\u3002\n- **\u6280\u672f\u6846\u67b6**\uff1a\u4e3b\u8981\u5305\u62ec\u9009\u62e9\uff08Selection\uff09\u3001\u6269\u5c55\uff08Expansion\uff09\u3001\u6a21\u62df\uff08Simulation\uff09\u548c\u56de\u6eaf\uff08Backpropagation\uff09\u56db\u4e2a\u6b65\u9aa4\u3002\n\n**\u504f\u597d\u7b56\u7565\u6a21\u578b\uff08PRM\uff09**\uff1a\n- **\u57fa\u672c\u539f\u7406**\uff1aPRM\u65e8\u5728\u901a\u8fc7\u5b66\u4e60\u7528\u6237\u7684\u504f\u597d\u6765\u4f18\u5316\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5e38\u7528\u4e8e\u63a8\u8350\u7cfb\u7edf\u3001\u4e2a\u6027\u5316\u670d\u52a1\u7b49\u3002\n- **\u6280\u672f\u6846\u67b6**\uff1a\u901a\u5e38\u5305\u62ec\u504f\u597d\u5efa\u6a21\u3001\u7b56\u7565\u4f18\u5316\u548c\u6a21\u578b\u5fae\u8c03\u7b49\u73af\u8282\u3002\n\n**\u65b9\u6cd5\u8bba**\uff1a\n- **\u96c6\u6210\u5b66\u4e60**\uff1a\u5c06MCTS\u4e0ePRM\u7ed3\u5408\uff0c\u5229\u7528MCTS\u7684\u641c\u7d22\u80fd\u529b\u4f18\u5316PRM\u7684\u51b3\u7b56\u8fc7\u7a0b\u3002\n- **\u5f3a\u5316\u5b66\u4e60**\uff1a\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u673a\u5236\u4e0d\u65ad\u8c03\u6574PRM\u7684\u53c2\u6570\uff0c\u4ee5\u9002\u5e94\u52a8\u6001\u73af\u5883\u3002\n\n### Step 2: \u7814\u7a76\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u4e3b\u8981\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0e\u53d8\u4f53\n\n**\u5e94\u7528\u573a\u666f**\uff1a\n- **\u6e38\u620fAI**\uff1aMCTS\u4e0ePRM\u7ed3\u5408\u7528\u4e8e\u6e38\u620f\u4e2d\u7684\u667a\u80fd\u51b3\u7b56\uff0c\u5982AlphaGo\u3002\n- **\u8def\u5f84\u89c4\u5212**\uff1a\u5728\u673a\u5668\u4eba\u5bfc\u822a\u548c\u81ea\u52a8\u9a7e\u9a76\u4e2d\uff0c\u5229\u7528MCTS\u4f18\u5316\u8def\u5f84\u9009\u62e9\u3002\n- **\u63a8\u8350\u7cfb\u7edf**\uff1a\u901a\u8fc7PRM\u5b66\u4e60\u7528\u6237\u504f\u597d\uff0c\u7ed3\u5408MCTS\u8fdb\u884c\u66f4\u7cbe\u51c6\u7684\u63a8\u8350\u3002\n\n**\u53d8\u4f53**\uff1a\n- **\u6df1\u5ea6MCTS**\uff1a\u7ed3\u5408\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff0c\u63d0\u5347MCTS\u7684\u51b3\u7b56\u80fd\u529b\u3002\n- **\u591a\u6a21\u6001PRM**\uff1a\u878d\u5408\u591a\u79cd\u6570\u636e\u6e90\uff08\u5982\u6587\u672c\u3001\u56fe\u50cf\uff09\uff0c\u589e\u5f3aPRM\u7684\u6cdb\u5316\u80fd\u529b\u3002\n\n### Step 3: \u8bc4\u4f30\u5b66\u672f\u754c\u7684\u6280\u672f\u8fdb\u6b65\u4e0e\u5c40\u9650\u6027\n\n**\u6280\u672f\u8fdb\u6b65**\uff1a\n- **\u6027\u80fd\u63d0\u5347**\uff1aMCTS\u4e0ePRM\u7ed3\u5408\u663e\u8457\u63d0\u5347\u4e86\u51b3\u7b56\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002\n- **\u6cdb\u5316\u80fd\u529b**\uff1a\u591a\u6a21\u6001PRM\u5728\u5904\u7406\u591a\u6837\u5316\u6570\u636e\u65f6\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002\n\n**\u5c40\u9650\u6027**\uff1a\n- **\u8ba1\u7b97\u590d\u6742\u5ea6**\uff1aMCTS\u5728\u5927\u89c4\u6a21\u95ee\u9898\u4e2d\u8ba1\u7b97\u91cf\u5927\uff0c\u6548\u7387\u6709\u5f85\u63d0\u5347\u3002\n- **\u6570\u636e\u4f9d\u8d56**\uff1aPRM\u7684\u6027\u80fd\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u9ad8\u8d28\u91cf\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u6570\u636e\u504f\u5dee\u95ee\u9898\u4f9d\u7136\u5b58\u5728\u3002\n\n### Step 4: \u63a2\u8ba8\u8ba1\u7b97\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\n\n**\u9002\u7528\u6027**\uff1a\n- **\u591a\u9886\u57df\u5e94\u7528**\uff1aMCTS+PRM\u6846\u67b6\u5728\u6e38\u620f\u3001\u8def\u5f84\u89c4\u5212\u3001\u63a8\u8350\u7cfb\u7edf\u7b49\u591a\u4e2a\u9886\u57df\u8868\u73b0\u51fa\u826f\u597d\u7684\u9002\u7528\u6027\u3002\n- **\u591a\u6a21\u6001\u6570\u636e**\uff1a\u591a\u6a21\u6001PRM\u80fd\u591f\u5904\u7406\u591a\u79cd\u7c7b\u578b\u7684\u6570\u636e\uff0c\u9002\u7528\u4e8e\u590d\u6742\u5e94\u7528\u573a\u666f\u3002\n\n**\u6cdb\u5316\u80fd\u529b**\uff1a\n- **\u8de8\u9886\u57df\u6cdb\u5316**\uff1a\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\uff0cMCTS+PRM\u6846\u67b6\u5728\u4e0d\u540c\u9886\u57df\u95f4\u5c55\u73b0\u51fa\u8f83\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002\n- **\u52a8\u6001\u73af\u5883\u9002\u5e94**\uff1a\u5f3a\u5316\u5b66\u4e60\u673a\u5236\u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u9002\u5e94\u52a8\u6001\u53d8\u5316\u7684\u73af\u5883\u3002\n\n### Step 5: \u5206\u6790\u6700\u65b0\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027\n\n**\u7a33\u5b9a\u6027**\uff1a\n- **\u7b97\u6cd5\u4f18\u5316**\uff1a\u901a\u8fc7\u6539\u8fdbMCTS\u7684\u9009\u62e9\u7b56\u7565\u548cPRM\u7684\u53c2\u6570\u8c03\u6574\u673a\u5236\uff0c\u63d0\u5347\u4e86\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u3002\n- **\u9c81\u68d2\u6027\u589e\u5f3a**\uff1a\u5f15\u5165\u5bf9\u6297\u8bad\u7ec3\u548c\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\uff0c\u63d0\u9ad8\u6a21\u578b\u5728\u590d\u6742\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u6027\u3002\n\n**\u5bb9\u9519\u6027**\uff1a\n- **\u9519\u8bef\u5904\u7406**\uff1a\u5728\u6a21\u62df\u548c\u56de\u6eaf\u9636\u6bb5\u5f15\u5165\u5bb9\u9519\u673a\u5236\uff0c\u51cf\u5c11\u9519\u8bef\u51b3\u7b56\u7684\u5f71\u54cd\u3002\n- **\u5927\u89c4\u6a21\u6570\u636e\u9002\u5e94\u6027**\uff1a\u4f18\u5316\u7b97\u6cd5\u7ed3\u6784\uff0c\u63d0\u5347\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\u7684\u5904\u7406\u80fd\u529b\u3002\n\n### Step 6: \u8bc4\u4f30\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218\n\n**\u672a\u6765\u7814\u7a76\u65b9\u5411**\uff1a\n- **\u9ad8\u6548\u8ba1\u7b97**\uff1a\u7814\u7a76\u66f4\u9ad8\u6548\u7684MCTS\u7b97\u6cd5\uff0c\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002\n- **\u6570\u636e\u589e\u5f3a**\uff1a\u63a2\u7d22\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u51cf\u5c11PRM\u5bf9\u9ad8\u8d28\u91cf\u6570\u636e\u7684\u4f9d\u8d56\u3002\n\n**\u6311\u6218**\uff1a\n- **\u8de8\u6a21\u6001\u878d\u5408**\uff1a\u5982\u4f55\u6709\u6548\u878d\u5408\u591a\u79cd\u6a21\u6001\u6570\u636e\uff0c\u63d0\u5347\u6a21\u578b\u7684\u7efc\u5408\u6027\u80fd\u3002\n- **\u52a8\u6001\u73af\u5883\u9002\u5e94**\uff1a\u5728\u52a8\u6001\u73af\u5883\u4e0b\uff0c\u5982\u4f55\u4fdd\u6301\u6a21\u578b\u7684\u7a33\u5b9a\u6027\u548c\u9ad8\u6548\u6027\u3002\n\n**\u63a8\u52a8\u540e\u7eed\u7814\u7a76**\uff1a\n- **\u65b0\u95ee\u9898\u63d0\u51fa**\uff1a\u5982\u591a\u6a21\u6001\u6570\u636e\u4e0b\u7684\u504f\u597d\u5efa\u6a21\u3001\u52a8\u6001\u73af\u5883\u4e0b\u7684\u51b3\u7b56\u4f18\u5316\u7b49\u3002\n- **\u7814\u7a76\u5207\u5165\u70b9**\uff1a\u4ece\u7b97\u6cd5\u4f18\u5316\u3001\u6570\u636e\u589e\u5f3a\u3001\u8de8\u6a21\u6001\u878d\u5408\u7b49\u591a\u4e2a\u89d2\u5ea6\u5207\u5165\uff0c\u63a8\u52a8\u6280\u672f\u8fdb\u6b65\u3002\n\n### \u603b\u7ed3\n\n\u8fd1\u5e74\u6765\uff0cMCTS\u5728PRM\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u7684\u5e94\u7528\u63a2\u7d22\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4e3b\u8981\u4f53\u73b0\u5728\u6280\u672f\u6846\u67b6\u7684\u5b8c\u5584\u3001\u5e94\u7528\u573a\u666f\u7684\u62d3\u5c55\u3001\u6027\u80fd\u7684\u63d0\u5347\u4ee5\u53ca\u7a33\u5b9a\u6027\u548c\u5bb9\u9519\u6027\u7684\u589e\u5f3a\u3002\u7136\u800c\uff0c\u4ecd\u5b58\u5728\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u3001\u6570\u636e\u4f9d\u8d56\u6027\u5f3a\u7b49\u5c40\u9650\u6027\u3002\u672a\u6765\u7684\u7814\u7a76\u5e94\u5173\u6ce8\u9ad8\u6548\u8ba1\u7b97\u3001\u6570\u636e\u589e\u5f3a\u548c\u8de8\u6a21\u6001\u878d\u5408\u7b49\u65b9\u5411\uff0c\u4ee5\u5e94\u5bf9\u52a8\u6001\u73af\u5883\u4e0b\u7684\u51b3\u7b56\u4f18\u5316\u7b49\u6311\u6218\uff0c\u63a8\u52a8\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002", "task_step_name": "\u5c40\u9650\u6027", "task_step_description": "MCTS\u5728\u5927\u89c4\u6a21\u95ee\u9898\u4e2d\u8ba1\u7b97\u91cf\u5927\uff0c\u6548\u7387\u6709\u5f85\u63d0\u5347\u3002", "task_step_level": "2>2", "task_step_question": "### \u95ee\u9898\n\n\u5728MCTS\u4e0ePRM\u7ed3\u5408\u7684\u5e94\u7528\u4e2d\uff0cMCTS\u5728\u5927\u89c4\u6a21\u95ee\u9898\u4e2d\u8ba1\u7b97\u91cf\u5927\u3001\u6548\u7387\u4f4e\u7684\u95ee\u9898\u5c24\u4e3a\u7a81\u51fa\u3002\u9488\u5bf9\u8fd9\u4e00\u5c40\u9650\u6027\uff0c\u6709\u54ea\u4e9b\u5177\u4f53\u7684\u6280\u672f\u6216\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u964d\u4f4eMCTS\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u5176\u5728PRM\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u4e2d\u7684\u51b3\u7b56\u6027\u80fd\uff1f", "task_step_question_context": [{"ref_id": "454984236281633338", "chunk_id": "4", "score": 0.306640625, "text": "# 2.2 Acceleration of MCTS\nMCTS-based methods have proved their strong capability of solving complex games or tasks. However, the high computational cost of MCTS hinders its application to some real-time and more general scenarios. Therefore, numerous works are devoted to accelerating MCTS. For example, to make the selection stage more effective, some heuristic pruning methods [ 14 ,33 ,29 ,1 ,2 ] aim to reduce the width and depth of the search tree with some heuristic functions. Furthermore, for more efficient evaluations, Lorentz [ 22 ] proposed early playout termination of MCTS (MCTS-EPT) to stop the random playouts early and use an evaluation function to assess win or loss. Moreover, Hsueh et al. [18 ] applied MCTS-EPT to the Chinese dark chess and proved its effectiveness. Afterward, similar ideas have been applied in the evaluation stage of AlphaGoZero [ 32 ] and later MCTS-based methods [31 ,27 ,34 ]. They evaluate the $Q$ -values through a learnable evaluation network instead of running playouts to the end. Grill et al. [15 ] propose a novel regularized policy optimization method based on AlphaZero to decrease the search budget of MCTS, which is from the optimization perspective. Danihelka et al. [10 ] propose a policy improvement algorithm based on sampling actions without replacement, named Gumbel trick to achieve better performance when planning with few simulations. However, these methods mentioned above focus on the specific stage of the search iteration or reduce the total budget through pruning and optimization methods, which are orthogonal to us. And few works targets at the search loop. Lan et al. [21 ] propose DS-MCTS, which defines the uncertainty of MCTS and approximates it by extra DNNs with specific features for board games in training. During the evaluation, DS-MCTS will check periodically and stop the search if the state is certain.\n\n# 3 Background\nThe AlphaGo series of work [ 30 ,32 ,31 ,27 ] are all MCTS-based reinforcement learning algorithms. Those algorithms assume the environment transition dynamics are known or learn the environment dynamics. Based on the dynamics, they use the Monte-Carlo tree search (MCTS) as the policy improvement operator. I.e., taking in the current policy, MCTS returns a better policy with the search algorithm. The systematic search allows the MCTS-based RL algorithm to quickly improve the policy and perform much better in the setting where heavy reasoning is required.\n\n# 3.1 MCTS\nThis part briefly introduces the MCTS method implemented in reinforcement learning applications. As mentioned in the related works, modern MCTS-based RL algorithms include four stages in the search loop, namely selection, expansion, evaluation, and backpropagation.  \n\nMCTS takes in the current states and generates a policy after the search loop of $N$ iterations. Here $N$ is a constant number of iterations set by the designer, regarded as the total budget. In the selection stage of each iteration, an action will be selected by maximizing over UCB. Specifically, AlphaZero [31 ] and MuZero [ 27 ] are developed based on a variant of UCB, P-UCT [ 25 ] and have achieved great success on board games and Atari games. The formula of P-UCT is the Eq (1):  \n\n$$\na^{k}=\\arg\\operatorname*{max}_{a\\in\\mathcal{A}}Q(s,a)+P(s,a)\\frac{\\sqrt{\\sum_{b\\in\\mathcal{A}}N(s,b)}}{1+N(s,a)}(c_{1}+\\log((\\sum_{b\\in\\mathcal{A}}N(s,b)+c_{2}+1)/c_{2})),\n$$  \n\nwhere $k$ is the index of iteration, $\\boldsymbol{\\mathcal{A}}$ is the acti $Q(s,a)$ is the estimated Q-value, $P(s,a)$ is the policy prior obtained from neural networks, $N(s,a)$ is the visitations to select the action a from the state $s$ and $c_{1},c_{2}$ are hyper-parameters. The output of MCTS is the visitation of each action of the root node. After $N$ search iterations, the final policy $\\pi(s)$ is defined as the normalized root visitation distribution simplification, we use $\\pi_{N}(s)$ $\\pi_{k}$ in place of , where $\\begin{array}{r}{\\pi_{k}(s,a)=N(s,a)/\\sum_{b\\in\\mathcal{A}}N(s,b)=N(s,a)/k,a\\in\\mathcal{A}}\\end{array}$ $\\pi_{k}(s)$ sometimes. And the detailed procedure of MCTS is \u2208A . For introduced in Appendix. In our method, we propose to approximate the final policy $\\pi_{N}(s)$ with $\\hat{\\pi}_{k}(s)$ ,which we name as a virtual expanded policy, through a new expansion method and a termination rule. In this way, the number of iterations in MCTS can be reduced from $N$ to $k$ .\n\n# 3.2 Computation Requirement\nMost of the computations in MCTS-based RL are in the MCTS procedure. Each action taken by MCTS requires $N$ times neural network evaluations, where $N$ is a constant number of iterations in the search loop. Traditional RL algorithms, such as PPO [ 28 ] or DQN [ 23 ], only need a single neural network evaluation per action. Thus, MCTS-based RL is roughly $N$ times computationally more expensive than traditional RL algorithms. In practice, training a single Atari game needs 12 hours of computation time on 40 TPUs [ 27 ]. The computation need is roughly two orders of magnitude more than traditional RL algorithms [28], although the final performance of MuZero is much better.\n\n# 4 Method\nWe aim to spend more search time on harder states and less on easier states. Intuitively, human knows when to make a quick decision or a slow decision under different circumstances. Unfortunately, this situation-aware behavior is absent in current MCTS algorithms. Therefore, we propose an MCTS variant that terminates the search iteration adaptively. It consists of two components: a novel expansion method named virtual expansion to estimate the final visitation based on the current partial tree; a termination rule that decides when to terminate based on the hardness of the current scenario. And we will display the adaptive mechanism through visualizations in Section 5.5."}, {"ref_id": "454984236379937352", "chunk_id": "11", "score": 0.306640625, "text": "# 5.4 Ablation Study\nThe results in the previous section suggest that our method reduces the response time of MCTS while keeping comparable performance on challenging tasks. This section tries to figure out which component contributes to the performance and how the hyperparameters affect it. And we also ablate the effects of different normalization criterions in VET-Rule and the larger budget $(N)$ in MCTS.  \n\nVirtual Expansion In Section 4.2, we introduce the virtual expansion. To prove the effectiveness of virtual expansion, we compare it with another two baseline expansion methods. One is the vanilla expansion, mentioned in Algorithm 1, which returns at iteration $k$ and outputs $\\pi_{k}$ . Another is greedy expansion, wh $N-k$ current best action greedily, where indicating th proces $k=r N$ $k=30$ and $\\hat{\\pi}_{k}(s,a)=\\bigl(N_{k}(s,a)+(N-k)\\mathbf{1}_{b=\\arg\\operatorname*{max}N_{k}(s,b)}\\bigr)/N$ $r=0.2$ $N=150$ $N-k$ .\u2212\u2212times virtual expansion or greedy expansion or nothing, . Briefly, we stop the search We compare the winning rate against the same engine, and the results are listed as Table 3 shows. The winning rate of virtual expansion can achieve $32\\%$ , which is much better than the others. Besides, MCTS with greedy expansion does not work because it over-exploits and results in severe exploration issues. Consequently, virtual expansion can generate a better policy distribution because it can balance exploration and exploitation with UCT.  \n\nTermination Rule It is significant to explore a better termination rule to keep the sound performance while decreasing the tree size as much as possible. As mentioned in Section 4.1, VETRule has two hyperparameters $r,\\epsilon$ . Here $r$ is the factor of the minimum budget $r N$ , and $\\epsilon$ is the minimum distance $\\hat{\\Delta}_{s}(k,k/2)$ . To explore the VET-Rule with better computation and performance trade-off, we do ablations for the different values of $r$ and $\\epsilon$ , respectively. The default values of $r,\\epsilon$ are set to 0 .2 ,0 .1 .  \n\nTable 3: Ablation results of different expansion methods on Go $9\\times9$ for 3 separate training runs.   \n\n\n<html><body><table><tr><td>Algorithm</td><td>Size Avg.</td><td>Winning Rate</td></tr><tr><td>Vanilla expansion</td><td>30</td><td>17%\u00b13.2%</td></tr><tr><td>Greedye expansion</td><td>30</td><td>3%\u00b1 2.0%</td></tr><tr><td>Virtual expansion</td><td>30</td><td>32% \u00b1 3.5%</td></tr></table></body></html>  \n\nFigure 2 compares the winning rate as well as the average tree size across the training stage. Firstly, Figure 3(a) gives the results of different minimum search times factor $r$ . The winning probability is not sensitive to $r$ when $r\\geq0.2$ .ertheless, the average tree size is sensitive to $r$ because V is supposed to search for at least $r N$ times. In addition, there is a performance drop between $r=0.1$ and $r=0.2$ . Therefore, it is reasonable to choose $r=0.2$ to balance the speed and the performance.  \n\nBesides, the comparisons of the different minimum distance $\\epsilon$ are shown in Figure 3(b). A larger $\\epsilon$ makes the tree size smaller because $\\hat{\\Delta}_{s}(k,k/2)<\\epsilon$ is easier to satisfy. In practice, the performance is highly correlated with $\\epsilon$ . In terms of the winning rate, a smaller $\\epsilon$ outperforms a larger one. However, better performances are at the cost of more computations. We suggest selecting an appropriate minimum distance to balance the computation and performance $(r=0.2,\\epsilon=0.1)$ ).  \n\nNormalization criterion in VET-Rule The proposed VET-Rule, $||\\hat{\\pi}_{k}(s)-\\hat{\\pi}_{k/2}(s)||\\;<\\;\\epsilon$ is a termination condition for V-MCTS. And L2 norm is another reasonable choice to amplify the bigger deviations. Therefore, we make ablations of the normalization criterion for the policy distributions. Specifically, we take a pretrained model, and compare the different strategies of L1 norm and L2 norm, namely, $\\left|\\left|\\hat{\\pi}_{k}(s)\\right|^{\\bf2}-\\hat{\\pi}_{k/2}(s)\\right|\\right|_{1}<\\epsilon$ and $\\left|\\left|\\hat{\\pi}_{k}\\dot{(s)}-\\hat{\\pi}_{k/2}(s)\\right|\\right|_{2}<\\epsilon.$ . The results are as Tab. 4 shows. We can find that (1) L2 norm can also work for V-MCTS; (2) L1 norm is better than L2 norm. And we attribute this to the formulation of ucb scores. Because the ucb scores have already taken into account the difference in the visitations (see the $\\mathbf{N}(\\mathbf{s},\\mathbf{a})$ in Eq (1)). Therefore, amplifying the deviations may result in some bias.  \n\nTable 4: Comparison of the winning rate and the average budget with different norm strategies in VETRule. L1 Norm means $\\left|\\left|\\hat{\\pi}_{k}(s)-\\check{\\hat{\\pi}}_{k/2}(s)\\right|\\right|_{1}<\\epsilon$ and L2 Norm means $\\left|\\left|\\hat{\\pi}_{k}(s)-\\hat{\\pi}_{k/2}(s)\\right|\\right|_{2}<\\epsilon$ .  \n\n\n<html><body><table><tr><td></td><td>Average budget</td><td>Winningrate</td></tr><tr><td>MCTS (N = 150)</td><td>150</td><td>82.0%</td></tr><tr><td>V-MCTS L1 Norm, N = 150,r = 0.2,E= 0.1</td><td>96.2</td><td>81.5%</td></tr><tr><td>V-MCTS L2 Norm, N = 150,r= 0.2,E= 0.1</td><td>97.1</td><td>79.8%</td></tr><tr><td>V-MCTS L2 Norm, N = 150, r = 0.2, = 0.05</td><td>119.3</td><td>81.0%</td></tr></table></body></html>  \n\nLarger budget $(N)$ in MCTS To investigate whether our method still holds with larger amounts of MCTS expansions, we take a pretrained model and compare two strategies: (1) vanilla expansion with $\\mathrm{N{=}150/400/600/800}$ nodes in MCTS (2) virtual expanded policy with $N=800,r=0.2,\\epsilon=0.1$ .The results are listed in Tab. 5. The result shows that (1) V-MCTS $\\mathit{\\Omega}^{N}=800,r=0.2,\\epsilon=0.1)$ is better than MCTS ( $N=600)$ ) in both the average budget and the winning rate, (2) V-MCTS can achieve comparable performance to the oracle MCTS( $N=800)$ ) while keeping much less average budget. Therefore, V-MCTS works with a larger amount of MCTS expansions.  \n\nTable 5: Comparison of the winning rate and the average budget with larger amounts of MCTS expansions. Here the hyper-parameters of our method are $N=800,r=0.2,\\epsilon=0.1$ .  \n\n\n<html><body><table><tr><td>MCTS</td><td>N = 150</td><td>N = 400</td><td>N=600</td><td>N=800</td><td>Ours</td></tr><tr><td>Average budget Winningrate</td><td>150 82.0%</td><td>400 84.5%</td><td>600 84.9%</td><td>800 85.9%</td><td>431.1 85.0%</td></tr></table></body></html>  \n\n  \nFigure 3: Heatmap of policy distributions from the MCTS ( $N=150)$ ) and the V-MCTS. The agent play as Black in (a) and White in (b) against the GnuGo (level 10). Our agent wins in both of the games. A darker red color represents larger visitations of the corresponding action. The V-MCTS will terminate with different search times $k$ according to the situations and generate a near-oracle policy distribution."}, {"ref_id": "454984236248078902", "chunk_id": "2", "score": 0.236328125, "text": "# DSensitivity Analysis of Hyper-parameters of MCTS-VS\nWe provide further studies to examine the influence of the hyper-parameters of MCTS-VS, including the employed optimization algorithm for optimizing the selected variables in each iteration, the \u201cfillin\u201d strategy, the hyper-parameter $k$ used in the best$k$ strategy, the hyper-parameter $C_{p}$ for calculating UCB in Eq. (1), the number $2\\,\\times\\,N_{v}\\,\\times\\,N_{s}$ sampled data in each iteration, the threshold $N_{b a d}$ for re-initializing a tree, and the threshold $N_{s p l i t}$ for splitting a tree node.  \n\nThe optimization algorithm is employed by MCTS-VS to optimize the selected variables in each iteration. We compare three different optimization algorithms, i.e., random search (RS), BO and TuRBO. First, we conduct experiments similar to \u201cEffectiveness of Variable Selection\u201d in Section 5.1, to show the effectiveness of MCTS-VS even when equipped with RS. Figure 6 shows that MCTSVS-RS is better than Dropout-RS and RS, revealing the advantage of MCTS-VS.  \n\n  \nFigure 6: Effectiveness of MCTS-VS when equipped with RS.  \n\nNext we compare the performance of MCTS-VS equipped with RS, BO and TuRBO, by experiments on the Hartmann functions with increasing ratio of valid variables. Hartmann 6 _500 has 6 valid variables. Hartmann 6 _5 _500 is generated by mixing 5 Hartmann 6 functions as Hartmann 6 $(\\pmb{x}_{1:6})+$ Hartmann 6 $\\backslash(\\pmb{x}_{7:12})+\\cdot\\cdot\\cdot+\\mathrm{Hartmann6}(\\pmb{x}_{25:30})$ , and appending 470 unrelated dimensions, where $\\pmb{x}_{i:j}$ denotes the $i$ -th to j-th variables. Hartmann 6 _10 _500 is generated alike. Thus, Hartmann 6 _5 _500 and Hartmann 6 _10 _500 have 30 and 60 valid variables, respectively. The results in Figure 7 show that as the ratio of valid variables increases, MCTS-VS-TuRBO gradually surpasses MCTS-VS-RS and MCTS-VS-BO, while MCTS-VS-RS becomes worse and worse. This is expected. If the ratio of valid variables is high, MCTS-VS is more likely to select the valid variables, so it is worth to use the expensive optimization algorithm, e.g., TuRBO, to optimize the selected variables. If the ratio is low, unrelated variables are more likely to be selected most of the time, so using a cheap optimization algorithm would be better. These observations also give us some guidance on selecting optimization algorithms in practice.  \n\n\u201cFill-in\u201d strategy is a basic component of variable selection methods, which influences the quality of the value of unselected variables. We compare the employed best$k$ strategy $(k=20)$ ) with the average best$k$ strategy and the random strategy. The average best$k$ strategy uses the average of the best $k$ data points for the unselected variables, and the random strategy samples the value of an unselected variable from its domain randomly. As shown in Figure 8(a), the random strategy leads to the poor performance of MCTS-VS-BO, which may be because it does not utilize the historical information and leads to over-exploration. The best${\\cdot k}$ strategy utilizes the historical points that have high objective values to fill in the unselected variables, thus behaving much better. The performance of the average strategy is between the best$k$ and random strategies. We recommend using the best$k$ strategy in practice.  \n\nThe hyper-parameter $k$ used in the best$k$ strategy controls the degree of exploitation for the unselected variables. As shown in Figure 8(b), a smaller $k$ encourages exploitation, which results in better performance in the early stage, but easily leads to premature convergence. A larger $k$ encourages exploration and behaves worse in the early stage, but may converge to a better value. We recommend using a larger $k$ if allowing enough evaluations.  \n\n  \nFigure 7: Sensitivity analysis of the optimization algorithm.  \n\n  \nFigure 8: Sensitivity analysis of the \u201cfill-in\u201d strategy and the hyper-parameter $k$ of the best$k$ strategy, using MCTS-VS-BO on Hartmann 6 _300 .  \n\nThe hyper-parameter $C_{p}$ for calculating UCB in Eq. (1) balances the exploration and exploitation of MCTS. As shown in Figure 9, a too small $C_{p}$ leads to relatively worse performance, highlighting the importance of exploration. A too large $C_{p}$ may also lead to over-exploration. But overall MCTSVS is not very sensitive to $C_{p}$ . We recommend setting $C_{p}$ between $1\\%$ and $10\\%$ of the optimum (i.e., max $f({\\boldsymbol{x}}))$ ), which is consistent with that for LA-MCTS [40].  \n\n  \nFigure 9: Sensitivity analysis of the hyper-parameter $C_{p}$ for calculating UCB in Eq. (1), using MCTS-VS-BO on Levy and Hartmann.  \n\nThe number $2\\,\\times\\,N_{v}\\,\\times\\,N_{s}$ of sampled data in ch iteration depends on the batch size $N_{v}$ of variable index subset and the sample batch size $N_{s}$ , and will influence the accuracy of estimating the variable score vector in Eq. (2). If we increase $N_{v}$ and $N_{s}$ , we can calculate the variable score more accurately, but also need more evaluations. Figure 10(a) shows that given the same number of evaluations, MCTS-VS-BO achieves the best performance when $N_{v}=2$ and $N_{s}=3$ . Thus, this setting may be a good choice to balance the accuracy of variable score and the number of evaluations, which is also used throughout the experiments.  \n\nThe threshold $N_{b a d}$ for re-initializing a tree controls the tolerance of selecting bad tree nodes (i.e., nodes containing unimportant variables). A smaller $N_{b a d}$ leads to frequent re-initialization, which can adjust quickly but may cause under-exploitation of the tree. A larger $N_{b a d}$ can make full use of the tree, but may optimize too much on unimportant variables. Figure 10(b) shows that MCTS-VS achieves the best performance when $N_{b a d}=5$ . Thus, we recommend to use this setting, to balance the re-initialization and exploitation of the tree.  \n\nThe threshold $N_{s p l i t}$ for splitting a node. If the number of variables in a node is larger than $N_{s p l i t}$ ,the node can be further partitioned. That is, the parameter $N_{s p l i t}$ controls the least number of variables in a leaf node and thus affects the number of selected variables, which has a direct influence on the wall clock time. Note that MCTS-VS selects a leaf node and optimizes the variables contained by this node in each iteration. The smaller $N_{s p l i t}$ , the shorter the time. Figure 10(c) shows that $N_{s p l i t}$ has little influence on the performance of MCTS-VS-BO, and thus we recommend to set $N_{s p l i t}=3$ to reduce the wall clock time.  \n\n  \nFigure 10: S vity analysis of the number $2\\,\\times\\,N_{v}\\,\\times\\,N_{s}$ pled data in each iteration, the threshold $N_{b a d}$ for re-initializing a tree and the threshold $N_{s p l i t}$ for splitting a node, using MCTS-VS-BO on Hartmann 6 _300 .  \n\nInfluence of the hyper-parameters on the runtime of MCTS-VS. We also provide some intuitive explanation about the influence of the hyper-parameters on the runtime. The threshold $N_{s p l i t}$ for splitting a node has a direct impact on the runtime, because it controls the least number of variables to be optimized in a leaf node. That is, the runtime will increase with $N_{s p l i t}$ . Other parameters may affect the depth of the tree and thus the runtime. For the threshold $N_{b a d}$ for re-initializing a tree, if it is set to a small value, MCTS-VS will re-build the tree frequently and the depth of the tree is small. The shallow nodes have more variables, leading to more runtime to optimize. For the hyper-parameter $C_{p}$ for calculating UCB, if it is set to a large value, the exploration is preferred and MCTS-VS will tend to select the right node (regarded as containing unimportant variables). The tree thus will be re-built freq tly, ding to more runtime. For the number $2\\,\\times\\,N_{v}\\,\\times\\,N_{s}$ of sampled data at each iteration, if $N_{v}$ and $N_{s}$ are set to large values, the depth of the tree will be small given the total number of evaluations, and thus lead to more runtime."}], "task_step_question_answer": "\u5728\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\uff0c\u542f\u53d1\u5f0f\u526a\u679d\u65b9\u6cd5\u867d\u7136\u80fd\u591f\u6709\u6548\u51cf\u5c11\u8ba1\u7b97\u91cf\uff0c\u4f46\u5176\u6548\u679c\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u5b9e\u65f6\u4ea4\u901a\u6570\u636e\u7684\u51c6\u786e\u6027\u548c\u53ca\u65f6\u6027\u3002\u4f8b\u5982\uff0c\u5728\u9ad8\u5cf0\u65f6\u6bb5\uff0c\u82e5\u4ea4\u901a\u6570\u636e\u66f4\u65b0\u5ef6\u8fdf\uff0c\u7cfb\u7edf\u53ef\u80fd\u65e0\u6cd5\u51c6\u786e\u8bc6\u522b\u4f4e\u6d41\u91cf\u8def\u5f84\uff0c\u5bfc\u81f4\u526a\u679d\u6548\u679c\u4e0d\u4f73\u3002\u6b64\u5916\uff0c\u542f\u53d1\u5f0f\u526a\u679d\u65b9\u6cd5\u5728\u9762\u5bf9\u590d\u6742\u7684\u4ea4\u901a\u573a\u666f\u65f6\uff0c\u53ef\u80fd\u65e0\u6cd5\u5168\u9762\u8003\u8651\u6240\u6709\u53ef\u80fd\u7684\u8def\u5f84\u9009\u62e9\uff0c\u5bfc\u81f4\u51b3\u7b56\u5931\u8bef\u3002\u65e9\u671f\u7ec8\u6b62\u968f\u673a\u6a21\u62df\uff08MCTS-EPT\uff09\u5728\u7a81\u53d1\u4ea4\u901a\u4e8b\u4ef6\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u6781\u7aef\u590d\u6742\u7684\u4ea4\u901a\u573a\u666f\u4e2d\uff0c\u63d0\u524d\u7ec8\u6b62\u6a21\u62df\u53ef\u80fd\u5bfc\u81f4\u7cfb\u7edf\u65e0\u6cd5\u5168\u9762\u8bc4\u4f30\u6240\u6709\u53ef\u80fd\u7684\u907f\u8ba9\u7b56\u7565\uff0c\u4ece\u800c\u5f71\u54cd\u51b3\u7b56\u7684\u51c6\u786e\u6027\u3002\u865a\u62df\u6269\u5c55\u4e0e\u7ec8\u6b62\u89c4\u5219\u5728\u667a\u80fd\u533b\u7597\u4e2d\u7684\u5e94\u7528\u4f9d\u8d56\u4e8e\u60a3\u8005\u6570\u636e\u7684\u5b9e\u65f6\u66f4\u65b0\uff0c\u82e5\u6570\u636e\u66f4\u65b0\u4e0d\u53ca\u65f6\uff0c\u53ef\u80fd\u5bfc\u81f4\u6cbb\u7597\u65b9\u6848\u7684\u4e0d\u51c6\u786e\u3002\u4f8b\u5982\uff0c\u5728\u75ab\u60c5\u671f\u95f4\uff0c\u82e5\u60a3\u8005\u6570\u636e\u66f4\u65b0\u5ef6\u8fdf\uff0c\u7cfb\u7edf\u53ef\u80fd\u65e0\u6cd5\u53ca\u65f6\u8c03\u6574\u6cbb\u7597\u65b9\u6848\uff0c\u5f71\u54cd\u6cbb\u7597\u6548\u679c\u3002\u6b63\u5219\u5316\u7b56\u7565\u4f18\u5316\u5728\u91d1\u878d\u9884\u6d4b\u4e2d\u867d\u7136\u80fd\u591f\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u4f46\u5728\u5e02\u573a\u6ce2\u52a8\u5267\u70c8\u7684\u60c5\u51b5\u4e0b\uff0c\u51cf\u5c11\u641c\u7d22\u9884\u7b97\u53ef\u80fd\u5bfc\u81f4\u9884\u6d4b\u7ed3\u679c\u7684\u4e0d\u7a33\u5b9a\u3002\u4f8b\u5982\uff0c\u5728\u91d1\u878d\u5371\u673a\u671f\u95f4\uff0c\u5e02\u573a\u6ce2\u52a8\u5267\u70c8\uff0c\u51cf\u5c11\u641c\u7d22\u9884\u7b97\u53ef\u80fd\u5bfc\u81f4\u9884\u6d4b\u7ed3\u679c\u7684\u4e0d\u51c6\u786e\uff0c\u5f71\u54cd\u6295\u8d44\u51b3\u7b56\u3002Gumbel Trick\u65b9\u6cd5\u5728\u5361\u724c\u6e38\u620f\u4e2d\u867d\u7136\u80fd\u591f\u901a\u8fc7\u65e0\u653e\u56de\u91c7\u6837\u52a8\u4f5c\u5b9e\u73b0\u8f83\u597d\u7684\u6027\u80fd\uff0c\u4f46\u5728\u9762\u5bf9\u590d\u6742\u591a\u53d8\u7684\u5bf9\u624b\u7b56\u7565\u65f6\uff0c\u53ef\u80fd\u65e0\u6cd5\u51c6\u786e\u9884\u6d4b\u5bf9\u624b\u7684\u4e0b\u4e00\u6b65\u884c\u52a8\u3002\u4f8b\u5982\uff0c\u5728\u300a\u7089\u77f3\u4f20\u8bf4\u300b\u4e2d\uff0c\u82e5\u5bf9\u624b\u9891\u7e41\u6539\u53d8\u51fa\u724c\u7b56\u7565\uff0cGumbel Trick\u65b9\u6cd5\u53ef\u80fd\u65e0\u6cd5\u53ca\u65f6\u8c03\u6574\u9884\u6d4b\u7b56\u7565\uff0c\u5bfc\u81f4\u51b3\u7b56\u5931\u8bef\u3002\u81ea\u9002\u5e94\u7ec8\u6b62\u673a\u5236\u5728\u673a\u5668\u4eba\u5bfc\u822a\u4e2d\u867d\u7136\u80fd\u591f\u6839\u636e\u73af\u5883\u52a8\u6001\u8c03\u6574\u641c\u7d22\u65f6\u95f4\uff0c\u4f46\u5728\u9ad8\u52a8\u6001\u6216\u9ad8\u4e0d\u786e\u5b9a\u6027\u7684\u73af\u5883\u4e2d\uff0c\u81ea\u9002\u5e94\u7ec8\u6b62\u673a\u5236\u53ef\u80fd\u65e0\u6cd5\u53ca\u65f6\u8c03\u6574\u641c\u7d22\u7b56\u7565\uff0c\u5bfc\u81f4\u8def\u5f84\u89c4\u5212\u7684\u5931\u8d25\u3002\u4f8b\u5982\uff0c\u5728\u706b\u707e\u6551\u63f4\u573a\u666f\u4e2d\uff0c\u73af\u5883\u53d8\u5316\u8fc5\u901f\uff0c\u81ea\u9002\u5e94\u7ec8\u6b62\u673a\u5236\u53ef\u80fd\u65e0\u6cd5\u53ca\u65f6\u8c03\u6574\u641c\u7d22\u7b56\u7565\uff0c\u5f71\u54cd\u6551\u63f4\u6548\u7387\u3002\u4f18\u5316UCB\u516c\u5f0f\u4e2d\u7684\u8d85\u53c2\u6570\u5728\u65e0\u4eba\u673a\u8def\u5f84\u89c4\u5212\u4e2d\u867d\u7136\u80fd\u591f\u4f18\u5316\u8ba1\u7b97\u6548\u7387\uff0c\u4f46\u8d85\u53c2\u6570\u7684\u8c03\u6574\u9700\u8981\u5927\u91cf\u7684\u5b9e\u9a8c\u548c\u8c03\u4f18\uff0c\u589e\u52a0\u4e86\u7cfb\u7edf\u7684\u590d\u6742\u6027\u3002\u4f8b\u5982\uff0c\u5728\u65e0\u4eba\u673a\u96c6\u7fa4\u4efb\u52a1\u4e2d\uff0c\u82e5\u8d85\u53c2\u6570\u8c03\u6574\u4e0d\u5f53\uff0c\u53ef\u80fd\u5bfc\u81f4\u8def\u5f84\u89c4\u5212\u7684\u4e0d\u51c6\u786e\uff0c\u5f71\u54cd\u4efb\u52a1\u6267\u884c\u3002\u51cf\u5c11\u6bcf\u6b21\u8fed\u4ee3\u7684\u91c7\u6837\u6570\u636e\u91cf\u5728\u89c6\u9891\u63a8\u8350\u7cfb\u7edf\u4e2d\u867d\u7136\u80fd\u591f\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\uff0c\u4f46\u5728\u9762\u5bf9\u7528\u6237\u5174\u8da3\u5feb\u901f\u53d8\u5316\u65f6\uff0c\u53ef\u80fd\u65e0\u6cd5\u53ca\u65f6\u6355\u6349\u7528\u6237\u7684\u5174\u8da3\u53d8\u5316\uff0c\u5bfc\u81f4\u63a8\u8350\u6548\u679c\u4e0b\u964d\u3002\u4f8b\u5982\uff0c\u5728\u77ed\u89c6\u9891\u5e73\u53f0\u4e2d\uff0c\u82e5\u7528\u6237\u5174\u8da3\u5feb\u901f\u53d8\u5316\uff0c\u51cf\u5c11\u91c7\u6837\u6570\u636e\u91cf\u53ef\u80fd\u5bfc\u81f4\u63a8\u8350\u5185\u5bb9\u4e0e\u7528\u6237\u5f53\u524d\u5174\u8da3\u4e0d\u5339\u914d\uff0c\u5f71\u54cd\u7528\u6237\u4f53\u9a8c\u3002", "ref_task_step_id": ""}, "__type__": "task_step_node"}, "6dba87e8-762d-4764-9182-3555b4b50a86": {"__data__": {"id_": "6dba87e8-762d-4764-9182-3555b4b50a86", "metadata": {}, "relationships": {}, "hash": "", "start_task_context": "MCTS\u5728PRM\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u7684\u5e94\u7528\u63a2\u7d22\u7efc\u8ff0", "aemo_representation_context": "### Step 1: \u5206\u6790\u8fd1\u51e0\u5e74\u7814\u7a76\u9886\u57df\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba\n\n**\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09**\uff1a\n- **\u57fa\u672c\u539f\u7406**\uff1aMCTS\u662f\u4e00\u79cd\u7528\u4e8e\u51b3\u7b56\u8fc7\u7a0b\u7684\u542f\u53d1\u5f0f\u641c\u7d22\u7b97\u6cd5\uff0c\u5e7f\u6cdb\u5e94\u7528\u4e8e\u6e38\u620fAI\u3001\u8def\u5f84\u89c4\u5212\u7b49\u9886\u57df\u3002\u5176\u6838\u5fc3\u601d\u60f3\u662f\u901a\u8fc7\u6a21\u62df\u591a\u6b21\u968f\u673a\u6e38\u620f\u6765\u8bc4\u4f30\u4e0d\u540c\u51b3\u7b56\u7684\u4f18\u52a3\u3002\n- **\u6280\u672f\u6846\u67b6**\uff1a\u4e3b\u8981\u5305\u62ec\u9009\u62e9\uff08Selection\uff09\u3001\u6269\u5c55\uff08Expansion\uff09\u3001\u6a21\u62df\uff08Simulation\uff09\u548c\u56de\u6eaf\uff08Backpropagation\uff09\u56db\u4e2a\u6b65\u9aa4\u3002\n\n**\u504f\u597d\u7b56\u7565\u6a21\u578b\uff08PRM\uff09**\uff1a\n- **\u57fa\u672c\u539f\u7406**\uff1aPRM\u65e8\u5728\u901a\u8fc7\u5b66\u4e60\u7528\u6237\u7684\u504f\u597d\u6765\u4f18\u5316\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5e38\u7528\u4e8e\u63a8\u8350\u7cfb\u7edf\u3001\u4e2a\u6027\u5316\u670d\u52a1\u7b49\u3002\n- **\u6280\u672f\u6846\u67b6**\uff1a\u901a\u5e38\u5305\u62ec\u504f\u597d\u5efa\u6a21\u3001\u7b56\u7565\u4f18\u5316\u548c\u6a21\u578b\u5fae\u8c03\u7b49\u73af\u8282\u3002\n\n**\u65b9\u6cd5\u8bba**\uff1a\n- **\u96c6\u6210\u5b66\u4e60**\uff1a\u5c06MCTS\u4e0ePRM\u7ed3\u5408\uff0c\u5229\u7528MCTS\u7684\u641c\u7d22\u80fd\u529b\u4f18\u5316PRM\u7684\u51b3\u7b56\u8fc7\u7a0b\u3002\n- **\u5f3a\u5316\u5b66\u4e60**\uff1a\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u673a\u5236\u4e0d\u65ad\u8c03\u6574PRM\u7684\u53c2\u6570\uff0c\u4ee5\u9002\u5e94\u52a8\u6001\u73af\u5883\u3002\n\n### Step 2: \u7814\u7a76\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u4e3b\u8981\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0e\u53d8\u4f53\n\n**\u5e94\u7528\u573a\u666f**\uff1a\n- **\u6e38\u620fAI**\uff1aMCTS\u4e0ePRM\u7ed3\u5408\u7528\u4e8e\u6e38\u620f\u4e2d\u7684\u667a\u80fd\u51b3\u7b56\uff0c\u5982AlphaGo\u3002\n- **\u8def\u5f84\u89c4\u5212**\uff1a\u5728\u673a\u5668\u4eba\u5bfc\u822a\u548c\u81ea\u52a8\u9a7e\u9a76\u4e2d\uff0c\u5229\u7528MCTS\u4f18\u5316\u8def\u5f84\u9009\u62e9\u3002\n- **\u63a8\u8350\u7cfb\u7edf**\uff1a\u901a\u8fc7PRM\u5b66\u4e60\u7528\u6237\u504f\u597d\uff0c\u7ed3\u5408MCTS\u8fdb\u884c\u66f4\u7cbe\u51c6\u7684\u63a8\u8350\u3002\n\n**\u53d8\u4f53**\uff1a\n- **\u6df1\u5ea6MCTS**\uff1a\u7ed3\u5408\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff0c\u63d0\u5347MCTS\u7684\u51b3\u7b56\u80fd\u529b\u3002\n- **\u591a\u6a21\u6001PRM**\uff1a\u878d\u5408\u591a\u79cd\u6570\u636e\u6e90\uff08\u5982\u6587\u672c\u3001\u56fe\u50cf\uff09\uff0c\u589e\u5f3aPRM\u7684\u6cdb\u5316\u80fd\u529b\u3002\n\n### Step 3: \u8bc4\u4f30\u5b66\u672f\u754c\u7684\u6280\u672f\u8fdb\u6b65\u4e0e\u5c40\u9650\u6027\n\n**\u6280\u672f\u8fdb\u6b65**\uff1a\n- **\u6027\u80fd\u63d0\u5347**\uff1aMCTS\u4e0ePRM\u7ed3\u5408\u663e\u8457\u63d0\u5347\u4e86\u51b3\u7b56\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002\n- **\u6cdb\u5316\u80fd\u529b**\uff1a\u591a\u6a21\u6001PRM\u5728\u5904\u7406\u591a\u6837\u5316\u6570\u636e\u65f6\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002\n\n**\u5c40\u9650\u6027**\uff1a\n- **\u8ba1\u7b97\u590d\u6742\u5ea6**\uff1aMCTS\u5728\u5927\u89c4\u6a21\u95ee\u9898\u4e2d\u8ba1\u7b97\u91cf\u5927\uff0c\u6548\u7387\u6709\u5f85\u63d0\u5347\u3002\n- **\u6570\u636e\u4f9d\u8d56**\uff1aPRM\u7684\u6027\u80fd\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u9ad8\u8d28\u91cf\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u6570\u636e\u504f\u5dee\u95ee\u9898\u4f9d\u7136\u5b58\u5728\u3002\n\n### Step 4: \u63a2\u8ba8\u8ba1\u7b97\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\n\n**\u9002\u7528\u6027**\uff1a\n- **\u591a\u9886\u57df\u5e94\u7528**\uff1aMCTS+PRM\u6846\u67b6\u5728\u6e38\u620f\u3001\u8def\u5f84\u89c4\u5212\u3001\u63a8\u8350\u7cfb\u7edf\u7b49\u591a\u4e2a\u9886\u57df\u8868\u73b0\u51fa\u826f\u597d\u7684\u9002\u7528\u6027\u3002\n- **\u591a\u6a21\u6001\u6570\u636e**\uff1a\u591a\u6a21\u6001PRM\u80fd\u591f\u5904\u7406\u591a\u79cd\u7c7b\u578b\u7684\u6570\u636e\uff0c\u9002\u7528\u4e8e\u590d\u6742\u5e94\u7528\u573a\u666f\u3002\n\n**\u6cdb\u5316\u80fd\u529b**\uff1a\n- **\u8de8\u9886\u57df\u6cdb\u5316**\uff1a\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\uff0cMCTS+PRM\u6846\u67b6\u5728\u4e0d\u540c\u9886\u57df\u95f4\u5c55\u73b0\u51fa\u8f83\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002\n- **\u52a8\u6001\u73af\u5883\u9002\u5e94**\uff1a\u5f3a\u5316\u5b66\u4e60\u673a\u5236\u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u9002\u5e94\u52a8\u6001\u53d8\u5316\u7684\u73af\u5883\u3002\n\n### Step 5: \u5206\u6790\u6700\u65b0\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027\n\n**\u7a33\u5b9a\u6027**\uff1a\n- **\u7b97\u6cd5\u4f18\u5316**\uff1a\u901a\u8fc7\u6539\u8fdbMCTS\u7684\u9009\u62e9\u7b56\u7565\u548cPRM\u7684\u53c2\u6570\u8c03\u6574\u673a\u5236\uff0c\u63d0\u5347\u4e86\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u3002\n- **\u9c81\u68d2\u6027\u589e\u5f3a**\uff1a\u5f15\u5165\u5bf9\u6297\u8bad\u7ec3\u548c\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\uff0c\u63d0\u9ad8\u6a21\u578b\u5728\u590d\u6742\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u6027\u3002\n\n**\u5bb9\u9519\u6027**\uff1a\n- **\u9519\u8bef\u5904\u7406**\uff1a\u5728\u6a21\u62df\u548c\u56de\u6eaf\u9636\u6bb5\u5f15\u5165\u5bb9\u9519\u673a\u5236\uff0c\u51cf\u5c11\u9519\u8bef\u51b3\u7b56\u7684\u5f71\u54cd\u3002\n- **\u5927\u89c4\u6a21\u6570\u636e\u9002\u5e94\u6027**\uff1a\u4f18\u5316\u7b97\u6cd5\u7ed3\u6784\uff0c\u63d0\u5347\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\u7684\u5904\u7406\u80fd\u529b\u3002\n\n### Step 6: \u8bc4\u4f30\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218\n\n**\u672a\u6765\u7814\u7a76\u65b9\u5411**\uff1a\n- **\u9ad8\u6548\u8ba1\u7b97**\uff1a\u7814\u7a76\u66f4\u9ad8\u6548\u7684MCTS\u7b97\u6cd5\uff0c\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002\n- **\u6570\u636e\u589e\u5f3a**\uff1a\u63a2\u7d22\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u51cf\u5c11PRM\u5bf9\u9ad8\u8d28\u91cf\u6570\u636e\u7684\u4f9d\u8d56\u3002\n\n**\u6311\u6218**\uff1a\n- **\u8de8\u6a21\u6001\u878d\u5408**\uff1a\u5982\u4f55\u6709\u6548\u878d\u5408\u591a\u79cd\u6a21\u6001\u6570\u636e\uff0c\u63d0\u5347\u6a21\u578b\u7684\u7efc\u5408\u6027\u80fd\u3002\n- **\u52a8\u6001\u73af\u5883\u9002\u5e94**\uff1a\u5728\u52a8\u6001\u73af\u5883\u4e0b\uff0c\u5982\u4f55\u4fdd\u6301\u6a21\u578b\u7684\u7a33\u5b9a\u6027\u548c\u9ad8\u6548\u6027\u3002\n\n**\u63a8\u52a8\u540e\u7eed\u7814\u7a76**\uff1a\n- **\u65b0\u95ee\u9898\u63d0\u51fa**\uff1a\u5982\u591a\u6a21\u6001\u6570\u636e\u4e0b\u7684\u504f\u597d\u5efa\u6a21\u3001\u52a8\u6001\u73af\u5883\u4e0b\u7684\u51b3\u7b56\u4f18\u5316\u7b49\u3002\n- **\u7814\u7a76\u5207\u5165\u70b9**\uff1a\u4ece\u7b97\u6cd5\u4f18\u5316\u3001\u6570\u636e\u589e\u5f3a\u3001\u8de8\u6a21\u6001\u878d\u5408\u7b49\u591a\u4e2a\u89d2\u5ea6\u5207\u5165\uff0c\u63a8\u52a8\u6280\u672f\u8fdb\u6b65\u3002\n\n### \u603b\u7ed3\n\n\u8fd1\u5e74\u6765\uff0cMCTS\u5728PRM\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u7684\u5e94\u7528\u63a2\u7d22\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4e3b\u8981\u4f53\u73b0\u5728\u6280\u672f\u6846\u67b6\u7684\u5b8c\u5584\u3001\u5e94\u7528\u573a\u666f\u7684\u62d3\u5c55\u3001\u6027\u80fd\u7684\u63d0\u5347\u4ee5\u53ca\u7a33\u5b9a\u6027\u548c\u5bb9\u9519\u6027\u7684\u589e\u5f3a\u3002\u7136\u800c\uff0c\u4ecd\u5b58\u5728\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u3001\u6570\u636e\u4f9d\u8d56\u6027\u5f3a\u7b49\u5c40\u9650\u6027\u3002\u672a\u6765\u7684\u7814\u7a76\u5e94\u5173\u6ce8\u9ad8\u6548\u8ba1\u7b97\u3001\u6570\u636e\u589e\u5f3a\u548c\u8de8\u6a21\u6001\u878d\u5408\u7b49\u65b9\u5411\uff0c\u4ee5\u5e94\u5bf9\u52a8\u6001\u73af\u5883\u4e0b\u7684\u51b3\u7b56\u4f18\u5316\u7b49\u6311\u6218\uff0c\u63a8\u52a8\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002", "task_step_name": "\u63a2\u8ba8\u8ba1\u7b97\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u4e0e\u6cdb\u5316\u80fd\u529b", "task_step_description": "MCTS+PRM\u6846\u67b6\u5728\u6e38\u620f\u3001\u8def\u5f84\u89c4\u5212\u3001\u63a8\u8350\u7cfb\u7edf\u7b49\u591a\u4e2a\u9886\u57df\u8868\u73b0\u51fa\u826f\u597d\u7684\u9002\u7528\u6027\u3002", "task_step_level": "3", "task_step_question": "### \u95ee\u9898\u63d0\u51fa\n\n\u5728\u63a2\u8ba8MCTS+PRM\u6846\u67b6\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\u65f6\uff0c\u5982\u4f55\u6709\u6548\u8bc4\u4f30\u8be5\u6846\u67b6\u5728\u8de8\u9886\u57df\u5e94\u7528\u4e2d\u7684\u6027\u80fd\u8868\u73b0\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u591a\u6a21\u6001\u6570\u636e\u65f6\uff0c\u5982\u4f55\u786e\u4fdd\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u6e90\u4e4b\u95f4\u7684\u6cdb\u5316\u80fd\u529b\uff1f\u6b64\u5916\uff0c\u5728\u52a8\u6001\u73af\u5883\u4e0b\uff0c\u5982\u4f55\u8c03\u6574MCTS+PRM\u6846\u67b6\u4ee5\u9002\u5e94\u4e0d\u65ad\u53d8\u5316\u7684\u5e94\u7528\u573a\u666f\uff0c\u5e76\u4fdd\u6301\u5176\u51b3\u7b56\u7684\u7a33\u5b9a\u6027\u548c\u9ad8\u6548\u6027\uff1f", "task_step_question_context": [{"ref_id": "454847538467043982", "chunk_id": "7", "score": 0.2255859375, "text": "# 5. Conclusion\nIn this work, we proposed a meta learning-based framework (MCRES) to improve the generalization performance of RES models, especially when handling novel compositions of learned concepts. By constructing a virtual training set and multiple virtual testing sets w.r.t. various levels of novel compositions and then optimizing the model via meta optimization, our framework can effectively improve model generalization performance. Extensive experiments show that our framework achieves superior performance on widely used benchmarks. Moreover, our framework is flexible, and can be seamlessly applied on various models with different architectures to enhance their performance.  \n\nAcknowledgement. This work is supported by MOE AcRF Tier 2 (Proposal ID: T2EP20222-0035), National Research Foundation Singapore under its AI Singapore Programme (AISG-100E-2020-065), and SUTD SKI Project (SKI 2021 02 06)."}, {"ref_id": "454984236281633338", "chunk_id": "4", "score": 0.2177734375, "text": "# 5 Experiment\nTo examine the performance of MCTS-VS, we conduct experiments on different tasks, including synthetic functions, NAS-bench problems and MuJoCo locomotion tasks, to compare MCTS-VS with other black-box optimization methods. For MCTS-VS, we use the same hyper-parameters except $C_{p}$ , which is used for calculating UCB in Eq. (1). For Dropout and embedding-based methods, we set the parameter $d$ to the number of valid dimensions for synthetic functions, and a reasonable value for real-world problems. The hyper-parameters of the same components of different methods are set to the same. We use five identical random seeds (2021\u20132025) for all problems and methods. More details about the settings can be found in Appendix C. Our code is available at https://github.com/lamda-bbo/MCTS-VS ."}, {"ref_id": "454984236293691964", "chunk_id": "5", "score": 0.1767578125, "text": "# DSensitivity Analysis of Hyper-parameters of MCTS-VS\nWe provide further studies to examine the influence of the hyper-parameters of MCTS-VS, including the employed optimization algorithm for optimizing the selected variables in each iteration, the \u201cfillin\u201d strategy, the hyper-parameter $k$ used in the best$k$ strategy, the hyper-parameter $C_{p}$ for calculating UCB in Eq. (1), the number $2\\,\\times\\,N_{v}\\,\\times\\,N_{s}$ sampled data in each iteration, the threshold $N_{b a d}$ for re-initializing a tree, and the threshold $N_{s p l i t}$ for splitting a tree node.  \n\nThe optimization algorithm is employed by MCTS-VS to optimize the selected variables in each iteration. We compare three different optimization algorithms, i.e., random search (RS), BO and TuRBO. First, we conduct experiments similar to \u201cEffectiveness of Variable Selection\u201d in Section 5.1, to show the effectiveness of MCTS-VS even when equipped with RS. Figure 6 shows that MCTSVS-RS is better than Dropout-RS and RS, revealing the advantage of MCTS-VS.  \n\n  \nFigure 6: Effectiveness of MCTS-VS when equipped with RS.  \n\nNext we compare the performance of MCTS-VS equipped with RS, BO and TuRBO, by experiments on the Hartmann functions with increasing ratio of valid variables. Hartmann 6 _500 has 6 valid variables. Hartmann 6 _5 _500 is generated by mixing 5 Hartmann 6 functions as Hartmann 6 $(\\pmb{x}_{1:6})+$ Hartmann 6 $\\backslash(\\pmb{x}_{7:12})+\\cdot\\cdot\\cdot+\\mathrm{Hartmann6}(\\pmb{x}_{25:30})$ , and appending 470 unrelated dimensions, where $\\pmb{x}_{i:j}$ denotes the $i$ -th to j-th variables. Hartmann 6 _10 _500 is generated alike. Thus, Hartmann 6 _5 _500 and Hartmann 6 _10 _500 have 30 and 60 valid variables, respectively. The results in Figure 7 show that as the ratio of valid variables increases, MCTS-VS-TuRBO gradually surpasses MCTS-VS-RS and MCTS-VS-BO, while MCTS-VS-RS becomes worse and worse. This is expected. If the ratio of valid variables is high, MCTS-VS is more likely to select the valid variables, so it is worth to use the expensive optimization algorithm, e.g., TuRBO, to optimize the selected variables. If the ratio is low, unrelated variables are more likely to be selected most of the time, so using a cheap optimization algorithm would be better. These observations also give us some guidance on selecting optimization algorithms in practice.  \n\n\u201cFill-in\u201d strategy is a basic component of variable selection methods, which influences the quality of the value of unselected variables. We compare the employed best$k$ strategy $(k=20)$ ) with the average best$k$ strategy and the random strategy. The average best$k$ strategy uses the average of the best $k$ data points for the unselected variables, and the random strategy samples the value of an unselected variable from its domain randomly. As shown in Figure 8(a), the random strategy leads to the poor performance of MCTS-VS-BO, which may be because it does not utilize the historical information and leads to over-exploration. The best${\\cdot k}$ strategy utilizes the historical points that have high objective values to fill in the unselected variables, thus behaving much better. The performance of the average strategy is between the best$k$ and random strategies. We recommend using the best$k$ strategy in practice.  \n\nThe hyper-parameter $k$ used in the best$k$ strategy controls the degree of exploitation for the unselected variables. As shown in Figure 8(b), a smaller $k$ encourages exploitation, which results in better performance in the early stage, but easily leads to premature convergence. A larger $k$ encourages exploration and behaves worse in the early stage, but may converge to a better value. We recommend using a larger $k$ if allowing enough evaluations.  \n\n  \nFigure 7: Sensitivity analysis of the optimization algorithm.  \n\n  \nFigure 8: Sensitivity analysis of the \u201cfill-in\u201d strategy and the hyper-parameter $k$ of the best$k$ strategy, using MCTS-VS-BO on Hartmann 6 _300 .  \n\nThe hyper-parameter $C_{p}$ for calculating UCB in Eq. (1) balances the exploration and exploitation of MCTS. As shown in Figure 9, a too small $C_{p}$ leads to relatively worse performance, highlighting the importance of exploration. A too large $C_{p}$ may also lead to over-exploration. But overall MCTSVS is not very sensitive to $C_{p}$ . We recommend setting $C_{p}$ between $1\\%$ and $10\\%$ of the optimum (i.e., max $f({\\boldsymbol{x}}))$ ), which is consistent with that for LA-MCTS [40].  \n\n  \nFigure 9: Sensitivity analysis of the hyper-parameter $C_{p}$ for calculating UCB in Eq. (1), using MCTS-VS-BO on Levy and Hartmann.  \n\nThe number $2\\,\\times\\,N_{v}\\,\\times\\,N_{s}$ of sampled data in ch iteration depends on the batch size $N_{v}$ of variable index subset and the sample batch size $N_{s}$ , and will influence the accuracy of estimating the variable score vector in Eq. (2). If we increase $N_{v}$ and $N_{s}$ , we can calculate the variable score more accurately, but also need more evaluations. Figure 10(a) shows that given the same number of evaluations, MCTS-VS-BO achieves the best performance when $N_{v}=2$ and $N_{s}=3$ . Thus, this setting may be a good choice to balance the accuracy of variable score and the number of evaluations, which is also used throughout the experiments.  \n\nThe threshold $N_{b a d}$ for re-initializing a tree controls the tolerance of selecting bad tree nodes (i.e., nodes containing unimportant variables). A smaller $N_{b a d}$ leads to frequent re-initialization, which can adjust quickly but may cause under-exploitation of the tree. A larger $N_{b a d}$ can make full use of the tree, but may optimize too much on unimportant variables. Figure 10(b) shows that MCTS-VS achieves the best performance when $N_{b a d}=5$ . Thus, we recommend to use this setting, to balance the re-initialization and exploitation of the tree.  \n\nThe threshold $N_{s p l i t}$ for splitting a node. If the number of variables in a node is larger than $N_{s p l i t}$ ,the node can be further partitioned. That is, the parameter $N_{s p l i t}$ controls the least number of variables in a leaf node and thus affects the number of selected variables, which has a direct influence on the wall clock time. Note that MCTS-VS selects a leaf node and optimizes the variables contained by this node in each iteration. The smaller $N_{s p l i t}$ , the shorter the time. Figure 10(c) shows that $N_{s p l i t}$ has little influence on the performance of MCTS-VS-BO, and thus we recommend to set $N_{s p l i t}=3$ to reduce the wall clock time.  \n\n  \nFigure 10: S vity analysis of the number $2\\,\\times\\,N_{v}\\,\\times\\,N_{s}$ pled data in each iteration, the threshold $N_{b a d}$ for re-initializing a tree and the threshold $N_{s p l i t}$ for splitting a node, using MCTS-VS-BO on Hartmann 6 _300 .  \n\nInfluence of the hyper-parameters on the runtime of MCTS-VS. We also provide some intuitive explanation about the influence of the hyper-parameters on the runtime. The threshold $N_{s p l i t}$ for splitting a node has a direct impact on the runtime, because it controls the least number of variables to be optimized in a leaf node. That is, the runtime will increase with $N_{s p l i t}$ . Other parameters may affect the depth of the tree and thus the runtime. For the threshold $N_{b a d}$ for re-initializing a tree, if it is set to a small value, MCTS-VS will re-build the tree frequently and the depth of the tree is small. The shallow nodes have more variables, leading to more runtime to optimize. For the hyper-parameter $C_{p}$ for calculating UCB, if it is set to a large value, the exploration is preferred and MCTS-VS will tend to select the right node (regarded as containing unimportant variables). The tree thus will be re-built freq tly, ding to more runtime. For the number $2\\,\\times\\,N_{v}\\,\\times\\,N_{s}$ of sampled data at each iteration, if $N_{v}$ and $N_{s}$ are set to large values, the depth of the tree will be small given the total number of evaluations, and thus lead to more runtime."}], "task_step_question_answer": "\u5728\u533b\u7597\u8bca\u65ad\u4e2d\uff0cMCTS+PRM\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u57fa\u56e0\u7ec4\u6570\u636e\u548c\u4e34\u5e8a\u8bb0\u5f55\uff0c\u80fd\u591f\u4e3a\u60a3\u8005\u63d0\u4f9b\u66f4\u7cbe\u51c6\u7684\u6cbb\u7597\u5efa\u8bae\u3002\u4f8b\u5982\uff0c\u5728\u764c\u75c7\u6cbb\u7597\u4e2d\uff0cMCTS\u53ef\u4ee5\u6a21\u62df\u4e0d\u540c\u7684\u6cbb\u7597\u65b9\u6848\uff0c\u7ed3\u5408PRM\u7684\u5148\u9a8c\u4fe1\u606f\uff0c\u4f18\u5148\u8003\u8651\u53ef\u80fd\u7684\u6cbb\u7597\u65b9\u6848\uff0c\u4ece\u800c\u63d0\u5347\u8bca\u65ad\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002\u5728\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\uff0cMCTS+PRM\u6846\u67b6\u901a\u8fc7\u878d\u5408\u6444\u50cf\u5934\u548c\u96f7\u8fbe\u6570\u636e\uff0c\u80fd\u591f\u5728\u590d\u6742\u7684\u4ea4\u901a\u573a\u666f\u4e2d\u505a\u51fa\u66f4\u667a\u80fd\u7684\u51b3\u7b56\uff0c\u5982\u907f\u8ba9\u884c\u4eba\u3001\u5e94\u5bf9\u7a81\u53d1\u4ea4\u901a\u4e8b\u4ef6\u7b49\u3002\u5728\u63a8\u8350\u7cfb\u7edf\u4e2d\uff0cMCTS+PRM\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u7528\u6237\u7684\u5386\u53f2\u89c2\u770b\u8bb0\u5f55\u548c\u5b9e\u65f6\u53cd\u9988\uff0c\u80fd\u591f\u52a8\u6001\u8c03\u6574\u63a8\u8350\u7b56\u7565\uff0c\u786e\u4fdd\u63a8\u8350\u5185\u5bb9\u4e0e\u7528\u6237\u5f53\u524d\u5174\u8da3\u9ad8\u5ea6\u5339\u914d\u3002\u5728\u91d1\u878d\u9884\u6d4b\u9886\u57df\uff0cMCTS+PRM\u6846\u67b6\u901a\u8fc7\u5206\u6790\u5386\u53f2\u5e02\u573a\u6570\u636e\u548c\u6295\u8d44\u8005\u884c\u4e3a\uff0c\u80fd\u591f\u9884\u6d4b\u5e02\u573a\u8d8b\u52bf\u548c\u6295\u8d44\u51b3\u7b56\uff0c\u63d0\u5347\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002\u5728\u6e38\u620fAI\u4e2d\uff0cMCTS+PRM\u6846\u67b6\u901a\u8fc7\u81ea\u9002\u5e94\u641c\u7d22\u65f6\u95f4\u5206\u914d\u7b56\u7565\uff0c\u80fd\u591f\u4f18\u5148\u5904\u7406\u5bf9\u6e38\u620f\u7ed3\u679c\u5f71\u54cd\u6700\u5927\u7684\u51b3\u7b56\uff0c\u63d0\u5347\u6e38\u620f\u7684\u667a\u80fd\u6027\u548c\u8da3\u5473\u6027\u3002\u5728\u8def\u5f84\u89c4\u5212\u4e2d\uff0cMCTS+PRM\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u542f\u53d1\u5f0f\u5f15\u5bfc\u548c\u5c40\u90e8\u641c\u7d22\u7b56\u7565\uff0c\u80fd\u591f\u5728\u52a8\u6001\u53d8\u5316\u7684\u73af\u5883\u4e2d\u5b9e\u65f6\u8c03\u6574\u8def\u5f84\uff0c\u63d0\u5347\u8def\u5f84\u89c4\u5212\u7684\u5b9e\u65f6\u6027\u548c\u51c6\u786e\u6027\u3002", "ref_task_step_id": ""}, "__type__": "task_step_node"}, "d338c352-b6ad-45ff-b9c3-0418aa6cccc4": {"__data__": {"id_": "d338c352-b6ad-45ff-b9c3-0418aa6cccc4", "metadata": {}, "relationships": {}, "hash": "", "start_task_context": "MCTS\u5728PRM\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u7684\u5e94\u7528\u63a2\u7d22\u7efc\u8ff0", "aemo_representation_context": "### Step 1: \u5206\u6790\u8fd1\u51e0\u5e74\u7814\u7a76\u9886\u57df\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba\n\n**\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09**\uff1a\n- **\u57fa\u672c\u539f\u7406**\uff1aMCTS\u662f\u4e00\u79cd\u7528\u4e8e\u51b3\u7b56\u8fc7\u7a0b\u7684\u542f\u53d1\u5f0f\u641c\u7d22\u7b97\u6cd5\uff0c\u5e7f\u6cdb\u5e94\u7528\u4e8e\u6e38\u620fAI\u3001\u8def\u5f84\u89c4\u5212\u7b49\u9886\u57df\u3002\u5176\u6838\u5fc3\u601d\u60f3\u662f\u901a\u8fc7\u6a21\u62df\u591a\u6b21\u968f\u673a\u6e38\u620f\u6765\u8bc4\u4f30\u4e0d\u540c\u51b3\u7b56\u7684\u4f18\u52a3\u3002\n- **\u6280\u672f\u6846\u67b6**\uff1a\u4e3b\u8981\u5305\u62ec\u9009\u62e9\uff08Selection\uff09\u3001\u6269\u5c55\uff08Expansion\uff09\u3001\u6a21\u62df\uff08Simulation\uff09\u548c\u56de\u6eaf\uff08Backpropagation\uff09\u56db\u4e2a\u6b65\u9aa4\u3002\n\n**\u504f\u597d\u7b56\u7565\u6a21\u578b\uff08PRM\uff09**\uff1a\n- **\u57fa\u672c\u539f\u7406**\uff1aPRM\u65e8\u5728\u901a\u8fc7\u5b66\u4e60\u7528\u6237\u7684\u504f\u597d\u6765\u4f18\u5316\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5e38\u7528\u4e8e\u63a8\u8350\u7cfb\u7edf\u3001\u4e2a\u6027\u5316\u670d\u52a1\u7b49\u3002\n- **\u6280\u672f\u6846\u67b6**\uff1a\u901a\u5e38\u5305\u62ec\u504f\u597d\u5efa\u6a21\u3001\u7b56\u7565\u4f18\u5316\u548c\u6a21\u578b\u5fae\u8c03\u7b49\u73af\u8282\u3002\n\n**\u65b9\u6cd5\u8bba**\uff1a\n- **\u96c6\u6210\u5b66\u4e60**\uff1a\u5c06MCTS\u4e0ePRM\u7ed3\u5408\uff0c\u5229\u7528MCTS\u7684\u641c\u7d22\u80fd\u529b\u4f18\u5316PRM\u7684\u51b3\u7b56\u8fc7\u7a0b\u3002\n- **\u5f3a\u5316\u5b66\u4e60**\uff1a\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u673a\u5236\u4e0d\u65ad\u8c03\u6574PRM\u7684\u53c2\u6570\uff0c\u4ee5\u9002\u5e94\u52a8\u6001\u73af\u5883\u3002\n\n### Step 2: \u7814\u7a76\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u4e3b\u8981\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0e\u53d8\u4f53\n\n**\u5e94\u7528\u573a\u666f**\uff1a\n- **\u6e38\u620fAI**\uff1aMCTS\u4e0ePRM\u7ed3\u5408\u7528\u4e8e\u6e38\u620f\u4e2d\u7684\u667a\u80fd\u51b3\u7b56\uff0c\u5982AlphaGo\u3002\n- **\u8def\u5f84\u89c4\u5212**\uff1a\u5728\u673a\u5668\u4eba\u5bfc\u822a\u548c\u81ea\u52a8\u9a7e\u9a76\u4e2d\uff0c\u5229\u7528MCTS\u4f18\u5316\u8def\u5f84\u9009\u62e9\u3002\n- **\u63a8\u8350\u7cfb\u7edf**\uff1a\u901a\u8fc7PRM\u5b66\u4e60\u7528\u6237\u504f\u597d\uff0c\u7ed3\u5408MCTS\u8fdb\u884c\u66f4\u7cbe\u51c6\u7684\u63a8\u8350\u3002\n\n**\u53d8\u4f53**\uff1a\n- **\u6df1\u5ea6MCTS**\uff1a\u7ed3\u5408\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff0c\u63d0\u5347MCTS\u7684\u51b3\u7b56\u80fd\u529b\u3002\n- **\u591a\u6a21\u6001PRM**\uff1a\u878d\u5408\u591a\u79cd\u6570\u636e\u6e90\uff08\u5982\u6587\u672c\u3001\u56fe\u50cf\uff09\uff0c\u589e\u5f3aPRM\u7684\u6cdb\u5316\u80fd\u529b\u3002\n\n### Step 3: \u8bc4\u4f30\u5b66\u672f\u754c\u7684\u6280\u672f\u8fdb\u6b65\u4e0e\u5c40\u9650\u6027\n\n**\u6280\u672f\u8fdb\u6b65**\uff1a\n- **\u6027\u80fd\u63d0\u5347**\uff1aMCTS\u4e0ePRM\u7ed3\u5408\u663e\u8457\u63d0\u5347\u4e86\u51b3\u7b56\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002\n- **\u6cdb\u5316\u80fd\u529b**\uff1a\u591a\u6a21\u6001PRM\u5728\u5904\u7406\u591a\u6837\u5316\u6570\u636e\u65f6\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002\n\n**\u5c40\u9650\u6027**\uff1a\n- **\u8ba1\u7b97\u590d\u6742\u5ea6**\uff1aMCTS\u5728\u5927\u89c4\u6a21\u95ee\u9898\u4e2d\u8ba1\u7b97\u91cf\u5927\uff0c\u6548\u7387\u6709\u5f85\u63d0\u5347\u3002\n- **\u6570\u636e\u4f9d\u8d56**\uff1aPRM\u7684\u6027\u80fd\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u9ad8\u8d28\u91cf\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u6570\u636e\u504f\u5dee\u95ee\u9898\u4f9d\u7136\u5b58\u5728\u3002\n\n### Step 4: \u63a2\u8ba8\u8ba1\u7b97\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\n\n**\u9002\u7528\u6027**\uff1a\n- **\u591a\u9886\u57df\u5e94\u7528**\uff1aMCTS+PRM\u6846\u67b6\u5728\u6e38\u620f\u3001\u8def\u5f84\u89c4\u5212\u3001\u63a8\u8350\u7cfb\u7edf\u7b49\u591a\u4e2a\u9886\u57df\u8868\u73b0\u51fa\u826f\u597d\u7684\u9002\u7528\u6027\u3002\n- **\u591a\u6a21\u6001\u6570\u636e**\uff1a\u591a\u6a21\u6001PRM\u80fd\u591f\u5904\u7406\u591a\u79cd\u7c7b\u578b\u7684\u6570\u636e\uff0c\u9002\u7528\u4e8e\u590d\u6742\u5e94\u7528\u573a\u666f\u3002\n\n**\u6cdb\u5316\u80fd\u529b**\uff1a\n- **\u8de8\u9886\u57df\u6cdb\u5316**\uff1a\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\uff0cMCTS+PRM\u6846\u67b6\u5728\u4e0d\u540c\u9886\u57df\u95f4\u5c55\u73b0\u51fa\u8f83\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002\n- **\u52a8\u6001\u73af\u5883\u9002\u5e94**\uff1a\u5f3a\u5316\u5b66\u4e60\u673a\u5236\u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u9002\u5e94\u52a8\u6001\u53d8\u5316\u7684\u73af\u5883\u3002\n\n### Step 5: \u5206\u6790\u6700\u65b0\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027\n\n**\u7a33\u5b9a\u6027**\uff1a\n- **\u7b97\u6cd5\u4f18\u5316**\uff1a\u901a\u8fc7\u6539\u8fdbMCTS\u7684\u9009\u62e9\u7b56\u7565\u548cPRM\u7684\u53c2\u6570\u8c03\u6574\u673a\u5236\uff0c\u63d0\u5347\u4e86\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u3002\n- **\u9c81\u68d2\u6027\u589e\u5f3a**\uff1a\u5f15\u5165\u5bf9\u6297\u8bad\u7ec3\u548c\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\uff0c\u63d0\u9ad8\u6a21\u578b\u5728\u590d\u6742\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u6027\u3002\n\n**\u5bb9\u9519\u6027**\uff1a\n- **\u9519\u8bef\u5904\u7406**\uff1a\u5728\u6a21\u62df\u548c\u56de\u6eaf\u9636\u6bb5\u5f15\u5165\u5bb9\u9519\u673a\u5236\uff0c\u51cf\u5c11\u9519\u8bef\u51b3\u7b56\u7684\u5f71\u54cd\u3002\n- **\u5927\u89c4\u6a21\u6570\u636e\u9002\u5e94\u6027**\uff1a\u4f18\u5316\u7b97\u6cd5\u7ed3\u6784\uff0c\u63d0\u5347\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\u7684\u5904\u7406\u80fd\u529b\u3002\n\n### Step 6: \u8bc4\u4f30\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218\n\n**\u672a\u6765\u7814\u7a76\u65b9\u5411**\uff1a\n- **\u9ad8\u6548\u8ba1\u7b97**\uff1a\u7814\u7a76\u66f4\u9ad8\u6548\u7684MCTS\u7b97\u6cd5\uff0c\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002\n- **\u6570\u636e\u589e\u5f3a**\uff1a\u63a2\u7d22\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u51cf\u5c11PRM\u5bf9\u9ad8\u8d28\u91cf\u6570\u636e\u7684\u4f9d\u8d56\u3002\n\n**\u6311\u6218**\uff1a\n- **\u8de8\u6a21\u6001\u878d\u5408**\uff1a\u5982\u4f55\u6709\u6548\u878d\u5408\u591a\u79cd\u6a21\u6001\u6570\u636e\uff0c\u63d0\u5347\u6a21\u578b\u7684\u7efc\u5408\u6027\u80fd\u3002\n- **\u52a8\u6001\u73af\u5883\u9002\u5e94**\uff1a\u5728\u52a8\u6001\u73af\u5883\u4e0b\uff0c\u5982\u4f55\u4fdd\u6301\u6a21\u578b\u7684\u7a33\u5b9a\u6027\u548c\u9ad8\u6548\u6027\u3002\n\n**\u63a8\u52a8\u540e\u7eed\u7814\u7a76**\uff1a\n- **\u65b0\u95ee\u9898\u63d0\u51fa**\uff1a\u5982\u591a\u6a21\u6001\u6570\u636e\u4e0b\u7684\u504f\u597d\u5efa\u6a21\u3001\u52a8\u6001\u73af\u5883\u4e0b\u7684\u51b3\u7b56\u4f18\u5316\u7b49\u3002\n- **\u7814\u7a76\u5207\u5165\u70b9**\uff1a\u4ece\u7b97\u6cd5\u4f18\u5316\u3001\u6570\u636e\u589e\u5f3a\u3001\u8de8\u6a21\u6001\u878d\u5408\u7b49\u591a\u4e2a\u89d2\u5ea6\u5207\u5165\uff0c\u63a8\u52a8\u6280\u672f\u8fdb\u6b65\u3002\n\n### \u603b\u7ed3\n\n\u8fd1\u5e74\u6765\uff0cMCTS\u5728PRM\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u7684\u5e94\u7528\u63a2\u7d22\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4e3b\u8981\u4f53\u73b0\u5728\u6280\u672f\u6846\u67b6\u7684\u5b8c\u5584\u3001\u5e94\u7528\u573a\u666f\u7684\u62d3\u5c55\u3001\u6027\u80fd\u7684\u63d0\u5347\u4ee5\u53ca\u7a33\u5b9a\u6027\u548c\u5bb9\u9519\u6027\u7684\u589e\u5f3a\u3002\u7136\u800c\uff0c\u4ecd\u5b58\u5728\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u3001\u6570\u636e\u4f9d\u8d56\u6027\u5f3a\u7b49\u5c40\u9650\u6027\u3002\u672a\u6765\u7684\u7814\u7a76\u5e94\u5173\u6ce8\u9ad8\u6548\u8ba1\u7b97\u3001\u6570\u636e\u589e\u5f3a\u548c\u8de8\u6a21\u6001\u878d\u5408\u7b49\u65b9\u5411\uff0c\u4ee5\u5e94\u5bf9\u52a8\u6001\u73af\u5883\u4e0b\u7684\u51b3\u7b56\u4f18\u5316\u7b49\u6311\u6218\uff0c\u63a8\u52a8\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002", "task_step_name": "\u9002\u7528\u6027", "task_step_description": "MCTS+PRM\u6846\u67b6\u5728\u6e38\u620f\u3001\u8def\u5f84\u89c4\u5212\u3001\u63a8\u8350\u7cfb\u7edf\u7b49\u591a\u4e2a\u9886\u57df\u8868\u73b0\u51fa\u826f\u597d\u7684\u9002\u7528\u6027\u3002", "task_step_level": "3>1", "task_step_question": "### \u95ee\u9898\u63d0\u51fa\n\n\u5728MCTS+PRM\u6846\u67b6\u7684\u9002\u7528\u6027\u5206\u6790\u4e2d\uff0c\u8be5\u6846\u67b6\u5728\u6e38\u620f\u3001\u8def\u5f84\u89c4\u5212\u3001\u63a8\u8350\u7cfb\u7edf\u7b49\u591a\u4e2a\u9886\u57df\u8868\u73b0\u51fa\u826f\u597d\u7684\u9002\u7528\u6027\u3002\u7136\u800c\uff0c\u4e0d\u540c\u9886\u57df\u7684\u5e94\u7528\u573a\u666f\u548c\u9700\u6c42\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002\u4f8b\u5982\uff0c\u6e38\u620fAI\u9700\u8981\u5feb\u901f\u54cd\u5e94\u548c\u5b9e\u65f6\u51b3\u7b56\uff0c\u800c\u63a8\u8350\u7cfb\u7edf\u5219\u66f4\u6ce8\u91cd\u957f\u671f\u7528\u6237\u504f\u597d\u7684\u5b66\u4e60\u548c\u9884\u6d4b\u3002\u56e0\u6b64\uff0c\u5982\u4f55\u9488\u5bf9\u4e0d\u540c\u9886\u57df\u7684\u7279\u70b9\uff0c\u4f18\u5316MCTS+PRM\u6846\u67b6\u7684\u5177\u4f53\u5b9e\u73b0\uff0c\u4ee5\u6700\u5927\u5316\u5176\u9002\u7528\u6027\u548c\u6027\u80fd\uff0c\u662f\u4e00\u4e2a\u503c\u5f97\u6df1\u5165\u7814\u7a76\u7684\u95ee\u9898\u3002\n\n**\u5177\u4f53\u95ee\u9898**\uff1a\u5728MCTS+PRM\u6846\u67b6\u4e2d\uff0c\u5982\u4f55\u6839\u636e\u4e0d\u540c\u5e94\u7528\u9886\u57df\uff08\u5982\u6e38\u620fAI\u3001\u8def\u5f84\u89c4\u5212\u3001\u63a8\u8350\u7cfb\u7edf\uff09\u7684\u7279\u70b9\uff0c\u8c03\u6574\u548c\u4f18\u5316MCTS\u7684\u9009\u62e9\u7b56\u7565\u548cPRM\u7684\u504f\u597d\u5efa\u6a21\u673a\u5236\uff0c\u4ee5\u63d0\u5347\u6846\u67b6\u5728\u8be5\u9886\u57df\u7684\u9002\u7528\u6027\u548c\u6027\u80fd\uff1f", "task_step_question_context": [{"ref_id": "454845941740251190", "chunk_id": "9", "score": 0.361328125, "text": "# Related Works\nTwo streams of research are particularly relevant to our work: learnable (lifelong) MAPF methods and utilizing MCTS for multi-agent systems and MAPF in particular. Next, we review both of these domains.  \n\nLearnable (L)MAPF Solvers Among the recent works dedicated to MAPF, one of the first ones that were specifically dedicated to creating a learning-based MAPF solver was (Sartoretti et al. 2019). A combination of reinforcement learning and learning from expert demonstrations was used to create a learnable policy called Primal, tailored to solve conventional MAPF problems. Later in (Damani et al. 2021), an enhanced version of this solver, Primal2, was introduced. The latter was equipped with special corridor reasoning techniques, aiming at avoiding the deadlocks in narrow corridors, and it supported lifelong MAPF setting (therefore, we choose Primal2 as one of the baselines we compare our method to). Among the other learnable MAPF solvers that use reinforcement learning to obtain a decision-making policy, one can name (Riviere et al. 2020; Wang et al. 2020). The learnable methods introduced in (Li et al. 2020; Ma, Luo, and Ma 2021; Li et al. 2022) add communication capabilities to the agents, i.e., allow the agents to communicate to resolve deadlocks and avoid congestion. In this work, we compare with one of the most recent communication-based methods, i.e., SCRIMP (Wang et al. 2023). However, it is worth noting that our method does not rely on agent communication.  \n\nMCTS for MAPF Initially, Monte Carlo Tree Search (MCTS) algorithms demonstrated their effectiveness in competitive games with complete information, such as chess or Go (Silver et al. 2017). More recent versions of MCTS utilize deep neural networks to approximate the values of game states instead of relying solely on simulations. These approaches have also shown promising results in singleagent scenarios, where agents can learn a model of the environment and play Atari games (Schrittwieser et al. 2020; Ye et al. 2021). Besides gaming, MCTS methods have found applications in other domains, such as matrix multiplication optimization (Fawzi et al. 2022) and theorem proving using the Hyper Tree approach (Lample et al. 2022). Additionally, MCTS techniques have demonstrated applicability in robotics (Best et al. 2019; Dam et al. 2022).  \n\nDespite the growing interest in utilizing MCTS for multiagent tasks, there have been limited applications of MCTS for MAPF. In their work (Zerbel and Yliniemi 2019), the authors propose a multi-agent MCTS for Anonymous MAPF in a grid-world environment. Their environment has a dense reward signal (the agent who reached any goal on the map received a reward and ended the episode), and there are no obstacles, making collision avoidance easier. The authors build a separate tree for each agent using a classical algorithm. They then jointly apply the best actions (forming a plan) from the trees in the simulator to receive true scores of the solution and update the trees on that difference. This approach performs well even with a large number of agents.  \n\nA recent paper (Skrynnik et al. 2021) proposed a more sophisticated approach for multi-agent planning that combines RL and MCTS. The authors suggested a two-part scheme that includes a goal achievement module and a conflict resolution module. The latter was trained using MCTS. The construction of the search tree for each of the agents was also performed independently, and actions for other agents were selected using the currently trained policy. This work used MCTS only during training to train the conflict resolution policy."}, {"ref_id": "454848213962351312", "chunk_id": "1", "score": 0.3125, "text": "# 2.2 Acceleration of MCTS\nMCTS-based methods have proved their strong capability of solving complex games or tasks. However, the high computational cost of MCTS hinders its application to some real-time and more general scenarios. Therefore, numerous works are devoted to accelerating MCTS. For example, to make the selection stage more effective, some heuristic pruning methods [ 14 ,33 ,29 ,1 ,2 ] aim to reduce the width and depth of the search tree with some heuristic functions. Furthermore, for more efficient evaluations, Lorentz [ 22 ] proposed early playout termination of MCTS (MCTS-EPT) to stop the random playouts early and use an evaluation function to assess win or loss. Moreover, Hsueh et al. [18 ] applied MCTS-EPT to the Chinese dark chess and proved its effectiveness. Afterward, similar ideas have been applied in the evaluation stage of AlphaGoZero [ 32 ] and later MCTS-based methods [31 ,27 ,34 ]. They evaluate the $Q$ -values through a learnable evaluation network instead of running playouts to the end. Grill et al. [15 ] propose a novel regularized policy optimization method based on AlphaZero to decrease the search budget of MCTS, which is from the optimization perspective. Danihelka et al. [10 ] propose a policy improvement algorithm based on sampling actions without replacement, named Gumbel trick to achieve better performance when planning with few simulations. However, these methods mentioned above focus on the specific stage of the search iteration or reduce the total budget through pruning and optimization methods, which are orthogonal to us. And few works targets at the search loop. Lan et al. [21 ] propose DS-MCTS, which defines the uncertainty of MCTS and approximates it by extra DNNs with specific features for board games in training. During the evaluation, DS-MCTS will check periodically and stop the search if the state is certain.\n\n# 3 Background\nThe AlphaGo series of work [ 30 ,32 ,31 ,27 ] are all MCTS-based reinforcement learning algorithms. Those algorithms assume the environment transition dynamics are known or learn the environment dynamics. Based on the dynamics, they use the Monte-Carlo tree search (MCTS) as the policy improvement operator. I.e., taking in the current policy, MCTS returns a better policy with the search algorithm. The systematic search allows the MCTS-based RL algorithm to quickly improve the policy and perform much better in the setting where heavy reasoning is required.\n\n# 3.1 MCTS\nThis part briefly introduces the MCTS method implemented in reinforcement learning applications. As mentioned in the related works, modern MCTS-based RL algorithms include four stages in the search loop, namely selection, expansion, evaluation, and backpropagation.  \n\nMCTS takes in the current states and generates a policy after the search loop of $N$ iterations. Here $N$ is a constant number of iterations set by the designer, regarded as the total budget. In the selection stage of each iteration, an action will be selected by maximizing over UCB. Specifically, AlphaZero [31 ] and MuZero [ 27 ] are developed based on a variant of UCB, P-UCT [ 25 ] and have achieved great success on board games and Atari games. The formula of P-UCT is the Eq (1):  \n\n$$\na^{k}=\\arg\\operatorname*{max}_{a\\in\\mathcal{A}}Q(s,a)+P(s,a)\\frac{\\sqrt{\\sum_{b\\in\\mathcal{A}}N(s,b)}}{1+N(s,a)}(c_{1}+\\log((\\sum_{b\\in\\mathcal{A}}N(s,b)+c_{2}+1)/c_{2})),\n$$  \n\nwhere $k$ is the index of iteration, $\\boldsymbol{\\mathcal{A}}$ is the acti $Q(s,a)$ is the estimated Q-value, $P(s,a)$ is the policy prior obtained from neural networks, $N(s,a)$ is the visitations to select the action a from the state $s$ and $c_{1},c_{2}$ are hyper-parameters. The output of MCTS is the visitation of each action of the root node. After $N$ search iterations, the final policy $\\pi(s)$ is defined as the normalized root visitation distribution simplification, we use $\\pi_{N}(s)$ $\\pi_{k}$ in place of , where $\\begin{array}{r}{\\pi_{k}(s,a)=N(s,a)/\\sum_{b\\in\\mathcal{A}}N(s,b)=N(s,a)/k,a\\in\\mathcal{A}}\\end{array}$ $\\pi_{k}(s)$ sometimes. And the detailed procedure of MCTS is \u2208A . For introduced in Appendix. In our method, we propose to approximate the final policy $\\pi_{N}(s)$ with $\\hat{\\pi}_{k}(s)$ ,which we name as a virtual expanded policy, through a new expansion method and a termination rule. In this way, the number of iterations in MCTS can be reduced from $N$ to $k$ .\n\n# 3.2 Computation Requirement\nMost of the computations in MCTS-based RL are in the MCTS procedure. Each action taken by MCTS requires $N$ times neural network evaluations, where $N$ is a constant number of iterations in the search loop. Traditional RL algorithms, such as PPO [ 28 ] or DQN [ 23 ], only need a single neural network evaluation per action. Thus, MCTS-based RL is roughly $N$ times computationally more expensive than traditional RL algorithms. In practice, training a single Atari game needs 12 hours of computation time on 40 TPUs [ 27 ]. The computation need is roughly two orders of magnitude more than traditional RL algorithms [28], although the final performance of MuZero is much better.\n\n# 4 Method\nWe aim to spend more search time on harder states and less on easier states. Intuitively, human knows when to make a quick decision or a slow decision under different circumstances. Unfortunately, this situation-aware behavior is absent in current MCTS algorithms. Therefore, we propose an MCTS variant that terminates the search iteration adaptively. It consists of two components: a novel expansion method named virtual expansion to estimate the final visitation based on the current partial tree; a termination rule that decides when to terminate based on the hardness of the current scenario. And we will display the adaptive mechanism through visualizations in Section 5.5."}, {"ref_id": "454984236281633338", "chunk_id": "4", "score": 0.248046875, "text": "# 2 RELATED WORK\nThe full version of the related work is in Appendix A, we briefly introduce several highly related works here. In general, model-based RL for solving decision-making problems can be divided into three perspectives: model learning, policy learning, and decision-making. Moreover, optimal control theory also concerns the decision-making problem and is deeply related to model-based RL.  \n\nModel learning: How to learn a good model to support decision-making is crucial in model-based RL. There are two main aspects of the work: the model structure designing (Chua et al., 2018; Zhang  \n\net al., 2021; 2020; Hafner et al., 2021; Chen et al., 2022) and the loss designing (D\u2019Oro et al., 2020;   \nFarahmand et al., 2017; Li et al., 2021).  \n\nPolicy learning: Two methods are always used to learn the policy by using the learned model. One is to serve the learned model as a black-box simulator to generate the data (Janner et al., 2019b; Yu et al., 2020; Lee et al., 2020). Another way is to use the learned model to calculate the policy gradient (Heess et al., 2015b; Clavera et al., 2019; Amos et al., 2021).  \n\nDecision-making: When making the decision, we need to generate the actions that can achieve our goal. Many of the model-based RL methods make the decision by using the learned policy solely (Hafner et al., 2021). Similar to our paper, some works also try to make decisions by using the learned model, but the majority only focus on the discrete action space. The well-known MCTS method achieves a lot of success. For example, the well-known Alpha Zero (Silver et al., 2017), MuZero (Schrittwieser et al., 2020). There are only a few works that study the continuous action space, such as the Continuous UCT (Cou\u00a8etoux et al., 2011), the sampled MuZero (Hubert et al., 2021), the TreePI (Springenberg et al., 2020), and the TD-MPC (Hansen et al., 2022a).  \n\nOptimal control theory: Beyond deep RL, optimal control also considers the decision-making problem but rather relies on the known and continuous transition model. In modern optimal control, Model Predictive Control (MPC) (Camacho & Alba, 2013) framework is always adopted when the environment is highly non-linear. In MPC, the action is planned during the execution by using the model, and such a procedure is called trajectory optimization. Plenty of previous works (Byravan et al., 2021; Chua et al., 2018; Pinneri et al., 2021; Nagabandi et al., 2020) use MPC framework to solve the continuous control tasks, but most of them are based on zero-order or sample-based method to do the planning. The most relevant works are DDP (Murray & Yakowitz, 1984), iLQR (Li & Todorov, 2004), and iLQG (Todorov & Li, 2005; Tassa et al., 2012). We discuss the detailed differences between our method and these methods in Appendix A.  \n\nSince our planning algorithm relies on the learned model and learned policy, we build our algorithm based on these works on model learning and policy learning . Our POMP algorithm tries to solve a more challenging task compared to the related work on decision-making : efficiently optimize the trajectory in continuous action space when the environment model is unknown. Different from our works, the MPC with DDP as trajectory optimizer from optimal control theory requires the known environment model, and also requires the hessian matrix for online optimization from scratch.\n\n# 3 PRELIMINARIES\nReinforcement Le onsider discrete-time Marko Decision Process (M $\\mathcal{M}$ the tuple ($(\\mathcal{X},\\mathcal{A},f,r,\\gamma)$ XA $\\mathcal{X}$ state space, A is the action space, $f\\,:\\,x_{t+1}\\,=$   \n$f(x_{t},a_{t})$ is the transition model, $r:\\mathcal{X}\\times\\mathcal{A}\\to\\mathbb{R}$ X \u00d7 A \u2192 is the reward function, $\\gamma$ is the discount factor. $t$ $\\begin{array}{r}{R_{t}=\\sum_{t^{\\prime}=t}^{\\infty}\\gamma^{t^{\\prime}-t}r_{t^{\\prime}}}\\end{array}$ , and Reinforcement Learn  \n$\\begin{array}{r}{\\operatorname*{max}_{\\theta}J(\\theta)=\\operatorname*{max}_{\\theta}\\mathbb{E}_{\\pi_{\\theta}}R_{t}=\\operatorname*{max}_{\\theta}\\mathbb{E}_{\\pi_{\\theta}}\\Big[\\sum_{t^{\\prime}=t}^{\\infty}\\gamma^{t^{\\prime}-t}r(x_{t^{\\prime}},a_{t^{\\prime}})\\Big].}\\end{array}$ ing (RL) aims to find a policy $\\pi_{\\theta}:\\mathcal{X}\\times\\mathcal{A}\\rightarrow\\mathbb{R}^{+}$ X \u00d7 A \u2192 h P that can maximize the expected return .$J$ . where  \n\n$\\begin{array}{r}{\\operatorname*{max}_{a_{t}}\\mathbb{E}\\Big[r(x_{t},a_{t}|x_{t}\\,=\\,x)+\\gamma V^{*}(x_{t+1})\\Big]}\\end{array}$ value function obeys an important identity known as the Bellman optimality equation Bellman Equation. hWe define the optimal value function i . The idea behind this equation is that if we know the $V^{*}(x)=\\operatorname*{max}\\mathbb{E}[R_{t}|x_{t}=x]$ |. The optimal $V^{*}(x)\\;=\\;$ $r(x_{t},a_{t})$ for any $a_{t}$ and next step value function $V^{*}(x_{t+1})$ for any $s_{t+1}$ , we can recursively select the action $a_{t}$ which m $r(x_{t},a_{t}|x_{t}=x)+\\gamma V^{*}(x_{t+1})$ milarly, we can denote the optimal action-value function $Q^{*}(x,a)\\;=\\;\\operatorname*{max}\\mathbb{E}[R_{t}|x_{t}\\;=\\;x,a_{t}\\;=\\;a]$ |], and it obeys a similar Bellman optimility equation $\\begin{array}{r}{Q^{*}(x,a)=\\operatorname*{max}_{a_{t+1}}\\mathbb{E}\\Big[r(x_{t},a_{t}|x_{t}=x,a_{t}=a)+\\gamma Q^{*}(x_{t+1},a_{t+1})\\Big].}\\end{array}$ .  \n\nModel-based RL. Model-based RL method distinguishes itself from model-free counterparts by using the data to learn a transition model. Following Janner et al. (2019a) and Clavera et al. (2019), we use parametric neural networks to approximate the transition function, reward function, policy function and $\\mathrm{^Q}$ -value function with the following objective function to be optimized  $J_{f}(\\psi)\\,=\\,\\mathbb{E}\\big[\\log f(x_{t+1}|x_{t},a_{t})\\big]$ '', $J_{r}(\\omega)\\,=\\,\\mathbb{E}\\big[\\log r(r_{t}|x_{t},a_{t})\\big]$ '', $\\begin{array}{r}{\\bar{J_{\\pi}}(\\theta)\\,=\\,\\mathbb{E}\\bigl[\\sum_{t=0}^{H-1}\\gamma^{t}r(\\bar{x}_{t},a_{t})\\,+\\,}\\end{array}$ ' P$\\gamma^{H}Q(x_{H},a_{H})]$ 'and $J_{Q}\\,=\\,\\mathbb{E}\\bigl[\\|Q(x_{t},a_{t})-(r+\\tilde{Q}(x_{t+1},a_{t+1}))\\|_{2}\\bigr]$ '\u2225\u2212\u2225', respectively. In ${\\cal J}_{\\pi}(\\theta)$ , we truncate the trajectory in horizon Hto avoid long time model rollout.  \n\nNotations. For one-dimensional state and action case, we denote the partial differentiation of function by using its output with subscripts, e.g. ,$\\begin{array}{r}{r_{x}\\ \\triangleq\\ \\frac{\\partial r(x,a)}{\\partial x},\\ r_{a}\\ \\triangleq\\ \\frac{\\bigtriangleup r(x,a)}{\\partial a},\\ f_{x}\\ \\triangleq\\ \\frac{\\partial f(x,a)}{\\partial x}}\\end{array}$ ,$f_{a}\\triangleq{\\frac{\\partial f(x,a)}{\\partial a}}$ ,$\\begin{array}{r}{Q_{x}\\triangleq\\frac{\\partial Q(x,a)}{\\partial x}}\\end{array}$ and $\\begin{array}{r}{Q_{a}\\triangleq\\frac{\\partial Q(x,a)}{\\partial a}}\\end{array}$ . See Appendix E for the multi-dimension case."}], "task_step_question_answer": "\u5728\u8de8\u9886\u57df\u9002\u7528\u6027\u5206\u6790\u4e2d\uff0cMCTS+PRM\u6846\u67b6\u5728\u6e38\u620fAI\u548c\u8def\u5f84\u89c4\u5212\u4e2d\u90fd\u9700\u8981\u5b9e\u65f6\u51b3\u7b56\uff0c\u4f46\u6e38\u620fAI\u66f4\u6ce8\u91cd\u52a8\u6001\u73af\u5883\u9002\u5e94\uff0c\u800c\u8def\u5f84\u89c4\u5212\u66f4\u6ce8\u91cd\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u3002\u63a8\u8350\u7cfb\u7edf\u548c\u6e38\u620fAI\u90fd\u9700\u8981\u504f\u597d\u5efa\u6a21\uff0c\u4f46\u63a8\u8350\u7cfb\u7edf\u66f4\u6ce8\u91cd\u957f\u671f\u7528\u6237\u504f\u597d\uff0c\u800c\u6e38\u620fAI\u66f4\u6ce8\u91cd\u77ed\u671f\u7b56\u7565\u3002\u901a\u8fc7\u5bf9\u6bd4\u5206\u6790\uff0c\u53ef\u4ee5\u66f4\u5168\u9762\u5730\u5c55\u793aMCTS+PRM\u6846\u67b6\u7684\u8de8\u9886\u57df\u9002\u7528\u6027\u3002\u4f8b\u5982\uff0c\u5728\u6e38\u620fAI\u4e2d\uff0cMCTS\u901a\u8fc7\u5feb\u901f\u54cd\u5e94\u548c\u5b9e\u65f6\u51b3\u7b56\uff0c\u786e\u4fdd\u4e86AI\u5728\u590d\u6742\u4ea4\u4e92\u4e2d\u7684\u9ad8\u6548\u8868\u73b0\uff1b\u5728\u8def\u5f84\u89c4\u5212\u4e2d\uff0cMCTS\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u548c\u901a\u4fe1\u673a\u5236\uff0c\u786e\u4fdd\u4e86\u8def\u5f84\u89c4\u5212\u7684\u5b9e\u65f6\u6027\u548c\u5b89\u5168\u6027\uff1b\u5728\u63a8\u8350\u7cfb\u7edf\u4e2d\uff0cMCTS\u901a\u8fc7\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u8bc4\u4f30\u548c\u6b63\u5219\u5316\u7b56\u7565\u4f18\u5316\uff0c\u786e\u4fdd\u4e86\u63a8\u8350\u5185\u5bb9\u7684\u4e2a\u6027\u5316\u7a0b\u5ea6\u3002\u901a\u8fc7\u4e0d\u65ad\u63a2\u7d22\u548c\u521b\u65b0\uff0cMCTS+PRM\u6846\u67b6\u5728\u672a\u6765\u6709\u671b\u5728\u66f4\u591a\u9886\u57df\u53d1\u6325\u91cd\u8981\u4f5c\u7528\uff0c\u4e3a\u590d\u6742\u51b3\u7b56\u95ee\u9898\u63d0\u4f9b\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "ref_task_step_id": ""}, "__type__": "task_step_node"}, "edaf9c7c-601d-4068-b117-92e5eaa45af5": {"__data__": {"id_": "edaf9c7c-601d-4068-b117-92e5eaa45af5", "metadata": {}, "relationships": {}, "hash": "", "start_task_context": "MCTS\u5728PRM\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u7684\u5e94\u7528\u63a2\u7d22\u7efc\u8ff0", "aemo_representation_context": "### Step 1: \u5206\u6790\u8fd1\u51e0\u5e74\u7814\u7a76\u9886\u57df\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba\n\n**\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09**\uff1a\n- **\u57fa\u672c\u539f\u7406**\uff1aMCTS\u662f\u4e00\u79cd\u7528\u4e8e\u51b3\u7b56\u8fc7\u7a0b\u7684\u542f\u53d1\u5f0f\u641c\u7d22\u7b97\u6cd5\uff0c\u5e7f\u6cdb\u5e94\u7528\u4e8e\u6e38\u620fAI\u3001\u8def\u5f84\u89c4\u5212\u7b49\u9886\u57df\u3002\u5176\u6838\u5fc3\u601d\u60f3\u662f\u901a\u8fc7\u6a21\u62df\u591a\u6b21\u968f\u673a\u6e38\u620f\u6765\u8bc4\u4f30\u4e0d\u540c\u51b3\u7b56\u7684\u4f18\u52a3\u3002\n- **\u6280\u672f\u6846\u67b6**\uff1a\u4e3b\u8981\u5305\u62ec\u9009\u62e9\uff08Selection\uff09\u3001\u6269\u5c55\uff08Expansion\uff09\u3001\u6a21\u62df\uff08Simulation\uff09\u548c\u56de\u6eaf\uff08Backpropagation\uff09\u56db\u4e2a\u6b65\u9aa4\u3002\n\n**\u504f\u597d\u7b56\u7565\u6a21\u578b\uff08PRM\uff09**\uff1a\n- **\u57fa\u672c\u539f\u7406**\uff1aPRM\u65e8\u5728\u901a\u8fc7\u5b66\u4e60\u7528\u6237\u7684\u504f\u597d\u6765\u4f18\u5316\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5e38\u7528\u4e8e\u63a8\u8350\u7cfb\u7edf\u3001\u4e2a\u6027\u5316\u670d\u52a1\u7b49\u3002\n- **\u6280\u672f\u6846\u67b6**\uff1a\u901a\u5e38\u5305\u62ec\u504f\u597d\u5efa\u6a21\u3001\u7b56\u7565\u4f18\u5316\u548c\u6a21\u578b\u5fae\u8c03\u7b49\u73af\u8282\u3002\n\n**\u65b9\u6cd5\u8bba**\uff1a\n- **\u96c6\u6210\u5b66\u4e60**\uff1a\u5c06MCTS\u4e0ePRM\u7ed3\u5408\uff0c\u5229\u7528MCTS\u7684\u641c\u7d22\u80fd\u529b\u4f18\u5316PRM\u7684\u51b3\u7b56\u8fc7\u7a0b\u3002\n- **\u5f3a\u5316\u5b66\u4e60**\uff1a\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u673a\u5236\u4e0d\u65ad\u8c03\u6574PRM\u7684\u53c2\u6570\uff0c\u4ee5\u9002\u5e94\u52a8\u6001\u73af\u5883\u3002\n\n### Step 2: \u7814\u7a76\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u4e3b\u8981\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0e\u53d8\u4f53\n\n**\u5e94\u7528\u573a\u666f**\uff1a\n- **\u6e38\u620fAI**\uff1aMCTS\u4e0ePRM\u7ed3\u5408\u7528\u4e8e\u6e38\u620f\u4e2d\u7684\u667a\u80fd\u51b3\u7b56\uff0c\u5982AlphaGo\u3002\n- **\u8def\u5f84\u89c4\u5212**\uff1a\u5728\u673a\u5668\u4eba\u5bfc\u822a\u548c\u81ea\u52a8\u9a7e\u9a76\u4e2d\uff0c\u5229\u7528MCTS\u4f18\u5316\u8def\u5f84\u9009\u62e9\u3002\n- **\u63a8\u8350\u7cfb\u7edf**\uff1a\u901a\u8fc7PRM\u5b66\u4e60\u7528\u6237\u504f\u597d\uff0c\u7ed3\u5408MCTS\u8fdb\u884c\u66f4\u7cbe\u51c6\u7684\u63a8\u8350\u3002\n\n**\u53d8\u4f53**\uff1a\n- **\u6df1\u5ea6MCTS**\uff1a\u7ed3\u5408\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff0c\u63d0\u5347MCTS\u7684\u51b3\u7b56\u80fd\u529b\u3002\n- **\u591a\u6a21\u6001PRM**\uff1a\u878d\u5408\u591a\u79cd\u6570\u636e\u6e90\uff08\u5982\u6587\u672c\u3001\u56fe\u50cf\uff09\uff0c\u589e\u5f3aPRM\u7684\u6cdb\u5316\u80fd\u529b\u3002\n\n### Step 3: \u8bc4\u4f30\u5b66\u672f\u754c\u7684\u6280\u672f\u8fdb\u6b65\u4e0e\u5c40\u9650\u6027\n\n**\u6280\u672f\u8fdb\u6b65**\uff1a\n- **\u6027\u80fd\u63d0\u5347**\uff1aMCTS\u4e0ePRM\u7ed3\u5408\u663e\u8457\u63d0\u5347\u4e86\u51b3\u7b56\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002\n- **\u6cdb\u5316\u80fd\u529b**\uff1a\u591a\u6a21\u6001PRM\u5728\u5904\u7406\u591a\u6837\u5316\u6570\u636e\u65f6\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002\n\n**\u5c40\u9650\u6027**\uff1a\n- **\u8ba1\u7b97\u590d\u6742\u5ea6**\uff1aMCTS\u5728\u5927\u89c4\u6a21\u95ee\u9898\u4e2d\u8ba1\u7b97\u91cf\u5927\uff0c\u6548\u7387\u6709\u5f85\u63d0\u5347\u3002\n- **\u6570\u636e\u4f9d\u8d56**\uff1aPRM\u7684\u6027\u80fd\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u9ad8\u8d28\u91cf\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u6570\u636e\u504f\u5dee\u95ee\u9898\u4f9d\u7136\u5b58\u5728\u3002\n\n### Step 4: \u63a2\u8ba8\u8ba1\u7b97\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\n\n**\u9002\u7528\u6027**\uff1a\n- **\u591a\u9886\u57df\u5e94\u7528**\uff1aMCTS+PRM\u6846\u67b6\u5728\u6e38\u620f\u3001\u8def\u5f84\u89c4\u5212\u3001\u63a8\u8350\u7cfb\u7edf\u7b49\u591a\u4e2a\u9886\u57df\u8868\u73b0\u51fa\u826f\u597d\u7684\u9002\u7528\u6027\u3002\n- **\u591a\u6a21\u6001\u6570\u636e**\uff1a\u591a\u6a21\u6001PRM\u80fd\u591f\u5904\u7406\u591a\u79cd\u7c7b\u578b\u7684\u6570\u636e\uff0c\u9002\u7528\u4e8e\u590d\u6742\u5e94\u7528\u573a\u666f\u3002\n\n**\u6cdb\u5316\u80fd\u529b**\uff1a\n- **\u8de8\u9886\u57df\u6cdb\u5316**\uff1a\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\uff0cMCTS+PRM\u6846\u67b6\u5728\u4e0d\u540c\u9886\u57df\u95f4\u5c55\u73b0\u51fa\u8f83\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002\n- **\u52a8\u6001\u73af\u5883\u9002\u5e94**\uff1a\u5f3a\u5316\u5b66\u4e60\u673a\u5236\u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u9002\u5e94\u52a8\u6001\u53d8\u5316\u7684\u73af\u5883\u3002\n\n### Step 5: \u5206\u6790\u6700\u65b0\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027\n\n**\u7a33\u5b9a\u6027**\uff1a\n- **\u7b97\u6cd5\u4f18\u5316**\uff1a\u901a\u8fc7\u6539\u8fdbMCTS\u7684\u9009\u62e9\u7b56\u7565\u548cPRM\u7684\u53c2\u6570\u8c03\u6574\u673a\u5236\uff0c\u63d0\u5347\u4e86\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u3002\n- **\u9c81\u68d2\u6027\u589e\u5f3a**\uff1a\u5f15\u5165\u5bf9\u6297\u8bad\u7ec3\u548c\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\uff0c\u63d0\u9ad8\u6a21\u578b\u5728\u590d\u6742\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u6027\u3002\n\n**\u5bb9\u9519\u6027**\uff1a\n- **\u9519\u8bef\u5904\u7406**\uff1a\u5728\u6a21\u62df\u548c\u56de\u6eaf\u9636\u6bb5\u5f15\u5165\u5bb9\u9519\u673a\u5236\uff0c\u51cf\u5c11\u9519\u8bef\u51b3\u7b56\u7684\u5f71\u54cd\u3002\n- **\u5927\u89c4\u6a21\u6570\u636e\u9002\u5e94\u6027**\uff1a\u4f18\u5316\u7b97\u6cd5\u7ed3\u6784\uff0c\u63d0\u5347\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\u7684\u5904\u7406\u80fd\u529b\u3002\n\n### Step 6: \u8bc4\u4f30\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218\n\n**\u672a\u6765\u7814\u7a76\u65b9\u5411**\uff1a\n- **\u9ad8\u6548\u8ba1\u7b97**\uff1a\u7814\u7a76\u66f4\u9ad8\u6548\u7684MCTS\u7b97\u6cd5\uff0c\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002\n- **\u6570\u636e\u589e\u5f3a**\uff1a\u63a2\u7d22\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u51cf\u5c11PRM\u5bf9\u9ad8\u8d28\u91cf\u6570\u636e\u7684\u4f9d\u8d56\u3002\n\n**\u6311\u6218**\uff1a\n- **\u8de8\u6a21\u6001\u878d\u5408**\uff1a\u5982\u4f55\u6709\u6548\u878d\u5408\u591a\u79cd\u6a21\u6001\u6570\u636e\uff0c\u63d0\u5347\u6a21\u578b\u7684\u7efc\u5408\u6027\u80fd\u3002\n- **\u52a8\u6001\u73af\u5883\u9002\u5e94**\uff1a\u5728\u52a8\u6001\u73af\u5883\u4e0b\uff0c\u5982\u4f55\u4fdd\u6301\u6a21\u578b\u7684\u7a33\u5b9a\u6027\u548c\u9ad8\u6548\u6027\u3002\n\n**\u63a8\u52a8\u540e\u7eed\u7814\u7a76**\uff1a\n- **\u65b0\u95ee\u9898\u63d0\u51fa**\uff1a\u5982\u591a\u6a21\u6001\u6570\u636e\u4e0b\u7684\u504f\u597d\u5efa\u6a21\u3001\u52a8\u6001\u73af\u5883\u4e0b\u7684\u51b3\u7b56\u4f18\u5316\u7b49\u3002\n- **\u7814\u7a76\u5207\u5165\u70b9**\uff1a\u4ece\u7b97\u6cd5\u4f18\u5316\u3001\u6570\u636e\u589e\u5f3a\u3001\u8de8\u6a21\u6001\u878d\u5408\u7b49\u591a\u4e2a\u89d2\u5ea6\u5207\u5165\uff0c\u63a8\u52a8\u6280\u672f\u8fdb\u6b65\u3002\n\n### \u603b\u7ed3\n\n\u8fd1\u5e74\u6765\uff0cMCTS\u5728PRM\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u7684\u5e94\u7528\u63a2\u7d22\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4e3b\u8981\u4f53\u73b0\u5728\u6280\u672f\u6846\u67b6\u7684\u5b8c\u5584\u3001\u5e94\u7528\u573a\u666f\u7684\u62d3\u5c55\u3001\u6027\u80fd\u7684\u63d0\u5347\u4ee5\u53ca\u7a33\u5b9a\u6027\u548c\u5bb9\u9519\u6027\u7684\u589e\u5f3a\u3002\u7136\u800c\uff0c\u4ecd\u5b58\u5728\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u3001\u6570\u636e\u4f9d\u8d56\u6027\u5f3a\u7b49\u5c40\u9650\u6027\u3002\u672a\u6765\u7684\u7814\u7a76\u5e94\u5173\u6ce8\u9ad8\u6548\u8ba1\u7b97\u3001\u6570\u636e\u589e\u5f3a\u548c\u8de8\u6a21\u6001\u878d\u5408\u7b49\u65b9\u5411\uff0c\u4ee5\u5e94\u5bf9\u52a8\u6001\u73af\u5883\u4e0b\u7684\u51b3\u7b56\u4f18\u5316\u7b49\u6311\u6218\uff0c\u63a8\u52a8\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002", "task_step_name": "\u6cdb\u5316\u80fd\u529b", "task_step_description": "\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\uff0cMCTS+PRM\u6846\u67b6\u5728\u4e0d\u540c\u9886\u57df\u95f4\u5c55\u73b0\u51fa\u8f83\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "task_step_level": "3>2", "task_step_question": "### \u95ee\u9898\u63d0\u51fa\n\n\u5728MCTS\u4e0ePRM\u7ed3\u5408\u7684\u6846\u67b6\u4e2d\uff0c\u8fc1\u79fb\u5b66\u4e60\u88ab\u7528\u6765\u589e\u5f3a\u6a21\u578b\u5728\u4e0d\u540c\u9886\u57df\u95f4\u7684\u6cdb\u5316\u80fd\u529b\u3002\u7136\u800c\uff0c\u8fc1\u79fb\u5b66\u4e60\u7684\u6709\u6548\u6027\u5f80\u5f80\u4f9d\u8d56\u4e8e\u6e90\u9886\u57df\u548c\u76ee\u6807\u9886\u57df\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027\u3002\u57fa\u4e8e\u6b64\uff0c\u63d0\u51fa\u4ee5\u4e0b\u95ee\u9898\uff1a\n\n**\u95ee\u9898**\uff1a\u5728MCTS+PRM\u6846\u67b6\u4e2d\uff0c\u5982\u4f55\u91cf\u5316\u6e90\u9886\u57df\u548c\u76ee\u6807\u9886\u57df\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027\uff0c\u4ee5\u786e\u4fdd\u8fc1\u79fb\u5b66\u4e60\u7684\u6709\u6548\u6027\uff1f\u5177\u4f53\u6765\u8bf4\uff0c\u6709\u54ea\u4e9b\u6307\u6807\u6216\u65b9\u6cd5\u53ef\u4ee5\u7528\u6765\u8bc4\u4f30\u4e24\u4e2a\u9886\u57df\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027\uff0c\u4ece\u800c\u4f18\u5316\u8fc1\u79fb\u5b66\u4e60\u7b56\u7565\uff0c\u63d0\u5347\u6a21\u578b\u5728\u76ee\u6807\u9886\u57df\u7684\u6cdb\u5316\u80fd\u529b\uff1f\n\n### \u95ee\u9898\u80cc\u666f\n\n\u8fc1\u79fb\u5b66\u4e60\u662f\u4e00\u79cd\u901a\u8fc7\u5c06\u5728\u4e00\u4e2a\u9886\u57df\uff08\u6e90\u9886\u57df\uff09\u5b66\u5230\u7684\u77e5\u8bc6\u5e94\u7528\u5230\u53e6\u4e00\u4e2a\u9886\u57df\uff08\u76ee\u6807\u9886\u57df\uff09\u7684\u6280\u672f\u3002\u5728MCTS+PRM\u6846\u67b6\u4e2d\uff0c\u8fc1\u79fb\u5b66\u4e60\u88ab\u7528\u6765\u589e\u5f3a\u6a21\u578b\u5728\u4e0d\u540c\u9886\u57df\u95f4\u7684\u6cdb\u5316\u80fd\u529b\u3002\u7136\u800c\uff0c\u8fc1\u79fb\u5b66\u4e60\u7684\u6709\u6548\u6027\u5f80\u5f80\u4f9d\u8d56\u4e8e\u6e90\u9886\u57df\u548c\u76ee\u6807\u9886\u57df\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027\u3002\u5982\u679c\u4e24\u4e2a\u9886\u57df\u4e4b\u95f4\u7684\u5dee\u5f02\u8fc7\u5927\uff0c\u8fc1\u79fb\u5b66\u4e60\u53ef\u80fd\u4f1a\u5bfc\u81f4\u8d1f\u8fc1\u79fb\uff0c\u5373\u6a21\u578b\u5728\u76ee\u6807\u9886\u57df\u7684\u6027\u80fd\u4e0b\u964d\u3002\n\n### \u95ee\u9898\u5206\u6790\n\n\u4e3a\u4e86\u786e\u4fdd\u8fc1\u79fb\u5b66\u4e60\u7684\u6709\u6548\u6027\uff0c\u9700\u8981\u91cf\u5316\u6e90\u9886\u57df\u548c\u76ee\u6807\u9886\u57df\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u53ef\u4ee5\u8003\u8651\u4ee5\u4e0b\u51e0\u4e2a\u65b9\u9762\uff1a\n\n1. **\u7279\u5f81\u76f8\u4f3c\u6027**\uff1a\u6e90\u9886\u57df\u548c\u76ee\u6807\u9886\u57df\u7684\u7279\u5f81\u7a7a\u95f4\u662f\u5426\u76f8\u4f3c\uff1f\u4f8b\u5982\uff0c\u5728\u63a8\u8350\u7cfb\u7edf\u4e2d\uff0c\u7528\u6237\u7684\u884c\u4e3a\u7279\u5f81\u662f\u5426\u5728\u4e0d\u540c\u9886\u57df\u95f4\u4fdd\u6301\u4e00\u81f4\uff1f\n2. **\u4efb\u52a1\u76f8\u4f3c\u6027**\uff1a\u6e90\u9886\u57df\u548c\u76ee\u6807\u9886\u57df\u7684\u4efb\u52a1\u662f\u5426\u76f8\u4f3c\uff1f\u4f8b\u5982\uff0c\u5728\u6e38\u620fAI\u548c\u8def\u5f84\u89c4\u5212\u4e2d\uff0c\u51b3\u7b56\u8fc7\u7a0b\u662f\u5426\u5177\u6709\u76f8\u4f3c\u7684\u7ed3\u6784\uff1f\n3. **\u6570\u636e\u5206\u5e03\u76f8\u4f3c\u6027**\uff1a\u6e90\u9886\u57df\u548c\u76ee\u6807\u9886\u57df\u7684\u6570\u636e\u5206\u5e03\u662f\u5426\u76f8\u4f3c\uff1f\u4f8b\u5982\uff0c\u5728\u63a8\u8350\u7cfb\u7edf\u4e2d\uff0c\u7528\u6237\u504f\u597d\u5206\u5e03\u662f\u5426\u5728\u4e0d\u540c\u9886\u57df\u95f4\u4fdd\u6301\u4e00\u81f4\uff1f\n\n### \u53ef\u80fd\u7684\u89e3\u51b3\u65b9\u6848\n\n1. **\u7279\u5f81\u76f8\u4f3c\u6027\u5ea6\u91cf**\uff1a\u53ef\u4ee5\u4f7f\u7528\u4f59\u5f26\u76f8\u4f3c\u5ea6\u3001\u6b27\u6c0f\u8ddd\u79bb\u7b49\u5ea6\u91cf\u65b9\u6cd5\u6765\u8bc4\u4f30\u6e90\u9886\u57df\u548c\u76ee\u6807\u9886\u57df\u7279\u5f81\u7a7a\u95f4\u7684\u76f8\u4f3c\u6027\u3002\n2. **\u4efb\u52a1\u76f8\u4f3c\u6027\u5ea6\u91cf**\uff1a\u53ef\u4ee5\u901a\u8fc7\u4efb\u52a1\u7684\u7ed3\u6784\u76f8\u4f3c\u6027\u3001\u51b3\u7b56\u8fc7\u7a0b\u7684\u590d\u6742\u6027\u7b49\u6307\u6807\u6765\u8bc4\u4f30\u6e90\u9886\u57df\u548c\u76ee\u6807\u9886\u57df\u4efb\u52a1\u7684\u76f8\u4f3c\u6027\u3002\n3. **\u6570\u636e\u5206\u5e03\u76f8\u4f3c\u6027\u5ea6\u91cf**\uff1a\u53ef\u4ee5\u4f7f\u7528KL\u6563\u5ea6\u3001JS\u6563\u5ea6\u7b49\u5ea6\u91cf\u65b9\u6cd5\u6765\u8bc4\u4f30\u6e90\u9886\u57df\u548c\u76ee\u6807\u9886\u57df\u6570\u636e\u5206\u5e03\u7684\u76f8\u4f3c\u6027\u3002\n\n### \u7ed3\u8bba\n\n\u901a\u8fc7\u91cf\u5316\u6e90\u9886\u57df\u548c\u76ee\u6807\u9886\u57df\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027\uff0c\u53ef\u4ee5\u4f18\u5316\u8fc1\u79fb\u5b66\u4e60\u7b56\u7565\uff0c\u63d0\u5347MCTS+PRM\u6846\u67b6\u5728\u76ee\u6807\u9886\u57df\u7684\u6cdb\u5316\u80fd\u529b\u3002\u8fd9\u9700\u8981\u7efc\u5408\u8003\u8651\u7279\u5f81\u76f8\u4f3c\u6027\u3001\u4efb\u52a1\u76f8\u4f3c\u6027\u548c\u6570\u636e\u5206\u5e03\u76f8\u4f3c\u6027\u7b49\u591a\u4e2a\u65b9\u9762\uff0c\u5e76\u9009\u62e9\u5408\u9002\u7684\u5ea6\u91cf\u65b9\u6cd5\u8fdb\u884c\u8bc4\u4f30\u3002", "task_step_question_context": [{"ref_id": "454848214017925842", "chunk_id": "2", "score": 0.275390625, "text": "# C.2 Compared Methods\nWe compare DRIP with various methods from related research fields. As discussed in the paper, MDRAU task has not been studied well in previous literature, and many existing methods cannot be directly applied. Therefore, we have modified the original methods to perform MDRAU, and the modified versions are annotated with the suffix $\\cdot_{+},$ . The following are our competing methods.  \n\n\u2022BPRMF (Rendle et al. 2009) is a representative method for implicit feedback. It learns pair-wise ranking in the latent space.   \n\u2022MMOE (Ma et al. 2018) utilizes multiple experts that share user embeddings over multiple domains, and it adopts a domain-specific gating module to effectively capture inter-domain relationships.   \n\u2022PLE (Tang et al. 2020) also utilizes multiple experts, but it separates domain-specific and domain-shared experts for more balanced training.  \n\nNote that the above methods learn global user embeddings that are shared across all domains, so that the global user embeddings can be directly used for recommendation in any target domain.  \n\nEMCDR and PTUPCDR learn a mapping function for each source-target domain pair. Due to a large number of seen-unseen domain combinations, it is infeasible to directly apply them to MDRAU. For this reason, we tailor their learning task for a many-to-one mapping. That is, for each target domain, we learn a mapping function that takes concatenated domain-specific user embeddings as input 5 and predicts user preference in the target domain.  \n\n\u2022$\\mathbf{EMCDR+}$ (Man et al. 2017) is a representative CDR method based on the embedding-and-mapping approach for unseen domain recommendation (or cold-start recommendation). It trains a mapping function to predict user embedding in the target domain from the source domain. The $\\cdot+\\rangle$ version (i.e., EMCDR+) is modified according to the above description.  \n\n\u2022PTUPCDR $^{+}$ (Zhu et al. 2022) improves $\\operatorname{EMCDR+}$ by employing a meta-network as its personalized mapping function. It trains the mapping function to predict ratings in the target domain.  \n\n\u2022CAT-ART $^+$ (Li et al. 2023) generates shared user embeddings that summarize user preferences from all interacted domains and exploits the shared embeddings to improve the recommendation quality in the target domain. As user embeddings do not exist for unseen domains, we use the reconstructed user embedding from the autoencoder module used to generate the shared embedding.  \n\n\u2022UniCDR (Cao et al. 2023) is the state-of-the-art CDR method that can handle various CDR scenarios. It models domain-specific and domain-shared user embeddings separately and transfers the knowledge from other domains based on the domain-shared embeddings.  \n\n  \nFigure 4: Sensitivity analysis of DRIP.  \n\nAll the compared methods except BPRMF were proposed to recommend items for a single target domain. To evaluate such single target methods in the MDRAU-MT, the recommendations for each unseen domain need to be integrated over multiple unseen domains. For effective integration, we generate a unified recommendation list by using the scores normalized within each domain.\n\n# C.3 Implementation Details\nWe use Xeon Gold 6226R CPU and A5000 24GB GPU on Ubuntu 20.04 LTS for experiments. We implement DRIP and other competing methods with PyTorch (Paszke et al. 2019) and use Adam optimizer (Kingma and Ba 2014) with $\\beta_{1}~~=~~0.9,\\beta_{2}~~=~~0.999$ . For domainspecific encoders, we use BPR (Rendle et al. 2009) with embedding size 64. We ran five times for each experiment. We tune all hyperparameters by grid search using the validation set. The learning rates are searched in the range of $\\{0.0001,0.0005,0.001,0.005,0.01\\}$ .The weight decay is searched in the range of $\\{0.0001,0.0005,0.001,0.005,0.01\\}$ .For MMOE and PLE, the number of experts is searched in the range of $\\{2,3,5,8,10\\}$ . For the mapping function in EMCDR, we use multi-layer perception (MLP) with one-hidden layer: $[64\\,\\times\\,4\\,\\rightarrow\\,2\\,\\times\\,64\\,\\times\\,4\\,\\rightarrow\\,64\\,\\times\\,4]$ and tanh activation (Man et al. 2017; Kang et al. 2019). For CAT-ART and UniCDR, we follow the recommended values from the public implementation and from the original papers (Li et al. 2023; Cao et al. 2023). For DRIP, the number of heads, the number of layers, and random masking probabilities $\\{0,1,0.2,0.3,0.4,0.5,0.6,0.7,\\dot{0}.9\\}$ results are reported in Sec. D. For the masking process, {$\\{2^{0},2^{1},2^{2},2^{3}\\}$ },respectively. ,$\\{1,2,3,4\\}$ The ,we introduce a selection probability $\\epsilon_{i}$ in epoch $i$ . When $\\epsilon_{i}=1$ , we use only random masking, while when $\\epsilon_{i}=0$ ,we only use adaptive masking. We linearly decrease the $\\epsilon_{i}=\\operatorname*{max}(0.5,1-0.002i)$ in training phase.\n\n# DSensitivity Analysis\nWe provide a sensitivity analysis to guide the hyperparameter selection of DRIP. In Fig 4, we report the recommendation accuracy $(\\mathbf{R}@20)$ with varying random masking probabilities, the number of heads, and the number of layers for MDRAU-MT on P1. Similar tendencies are observed in the other scenario. We first observe that the masking probability has a small impact within the range of [0.1, 0.7]. However, the performance drastically drops when the masking probability exceeds 0.7. If the masking probability is too high, the model cannot obtain sufficient information from other domains, leading to degraded performance. Also, we observe stable performances with varying numbers of heads and layers of the multi-domain encoder. DRIP achieves a slightly improved performance with more number of heads, and a slightly degraded performance when the number of layers exceeds 3."}, {"ref_id": "454845771530662550", "chunk_id": "1", "score": 0.2177734375, "text": "# 2 RELATED WORK\nLLMs for reasoning. For LLMs, reasoning typically involves decomposing complex inputs into sequential intermediate steps before producing a final answer ( Cobbe et al. ,2021 ), commonly demonstrated with Chain-of-Thought (CoT) prompting ( Wei et al. ,2022 ) and its variants ( Wei et al. ,2022 ;Kojima et al. ,2022 ;Wang et al. ,2022 ). However, these methods, which create chains autoregressively in a single step, often suffer from error propagation as the number of steps increases (Guo et al. ,2018 ;Chen et al. ,2022b ), in which errors tend to compound. Various advancements have aimed to mitigate this issue; some approaches, such as Self-Consistency ( Wang et al. ,2022 ), employ majority voting over sampled chains, while others focus on multi-step decomposition, such as least-to-most prompting ( Zhou et al. ,2022 ), or use of external tools such as a scratchpad ( Nye et al. ,2021 ) or compiler ( Gao et al. ,2022 ). More recently, CoT has been improved with search algorithms ( Yao et al. ,2023a ;Hao et al. ,2023 ;Besta et al. ,2023 ) that can sample trajectories more effectively. Tree-of-thought (ToT) prompting ( Yao et al. ,2023a ) uses DFS or BFS-based search guided by an LM-generated heuristic while Reasoning via Planning (RAP) ( Hao et al. ,2023 ) uses MCTS with rollouts simulated by the LM. Despite using a search algorithm, these frameworks rely solely on the internal knowledge of the LM and cannot adapt to external inputs that could enhance the reasoning process.  \n\n  \nFigure 2: An overview of the differences between LATS and recently proposed LM search algorithms ToT ( Yao et al. ,2023a ) and RAP ( Hao et al. ,2023 ). LATS leverages environmental feedback and self-reflection to further adapt search and improve performance.  \n\nLLMs for decision-making. The strong reasoning and common-sense abilities of LLMs have also been adapted for decision-making tasks as a policy model in interactive environments. In the realm of robotics LLMs have been employed as high-level controllers of control policies ( Ahn et al. ,2022 ;Huang et al. ,2022 ;Driess et al. ,2023 ). Similar work ( Baker et al. ,2022 ;Wang et al. ,2023 ;Zhu et al. ,2023 ) has also adapted LLM agents to complex multimodal games such as Minecraft ( Guss et al. ,2019 ;Fan et al. ,2022 ). LLMs are particularly useful in text-based environments ( Liu et al. ,2018 ;Shridhar et al. ,2020 ;Liu et al. ,2023 ), where acting-based prompting techniques such as ReAct ( Yao et al. ,2023b ) have seen success. Similar to CoT, ReAct is limited by its simplicity and cannot effectively adapt to environment conditions. Many extensions have been proposed to address this, including Self-refine ( Madaan et al. ,2023 ) and Reflexion ( Shinn et al. ,2023 ;Yao et al. ,2023c ), which uses self-reflection to enhance reasoning and decision-making, and AdaPlanner ( Sun et al. ,2023 ), which incorporates both positive and negative environmental feedback. However these methods focus on refining an individual plan or trajectory and do not consider alternative choices at each step. Alternatively to pure decision-making environments, the reasoning and practical abilities of LLMs have been enhanced by access to external tools, such as APIs, search engines, calculators, or other models ( Schick et al. ,2023 ;Shen et al. ,2023 ;Sur\u00b4\u0131s et al. ,2023 ). Contrary to reasoningbased approaches, these methods have not been improved with planning, limiting their effectiveness.  \n\nTree-based search. Tree-based search, where multiple branches of outcomes are explored during search, is widely used in many planning algorithms ( Swiechowski et al. ,2023 ;LaValle et al. ,2001 )and Reinforcement Learning (RL) ( Hafner et al. ,2019 ;Du et al. ,2023 ;Wu et al. ,2023 ) algorithms for its good exploration-exploitation trade-off. Though tree-based search requires an environment model that can expand from arbitrary state ( Vodopivec et al. ,2017 ), which often requires extra training in RL ( Hafner et al. ,2023 ), such problem does not exist for LM tasks as we can conveniently backup to any state by setting the input to be the context and corresponding previous output by the LM. Thus, we work on the tree-based framework and use MCTS ( Swiechowski et al. ,2023 ) to fully release the potential of LMs, while avoiding the cost of training a value function over language descriptions by leveraging the in-context learning ( Brown et al. ,2020 ) abilities of LLMs.\n\n# 3 PRELIMINARIES\nBefore describing LATS, we first define our problem and outline a few established methods that leverage large language models for reasoning or decision-making. In LM reasoning or decision making, we are given an input $x$ in natural language and a pretrained language model $p_{\\theta}(x)$ parameterized by $\\theta$ ; our goal is to generate a final outp $y\\sim p_{\\theta}(x)$ corresponding to the answer (reasoning) or completes the task (decision-making). Both xand $y$ are language sequences , which are comprised of a list of tokens (the basic elements of natural language, often words), denoted as $x=(x[1],\\dots,x[n])$ and $y=(y[1],\\dots,y[n])$ . The LM decodes text autoregressively, i.e., without other inputs, the probability for an LM to generate a sequence $x$ is given by $\\begin{array}{r}{\\bar{p}_{\\theta}(x)=\\dot{\\prod}_{i=1}^{n}p_{\\theta}(x[i]|x[1\\dots i-1])}\\end{array}$ |\u2212. Usually, to improve the LM, prompts are provided along with the input x, which are specific instructions or few-shot input-output examples. We denote the generic process where an input $x$ is transformed into an output $y$ by LM: $y\\sim p_{\\theta}(y|\\mathsf{p r o m p t}_{I O}(x))$ , where prompt $\\ _{I O}(x)$ denotes the input $x$ along with the prompts.  \n\nChain-of-thought $(\\mathbf{CoT})$ Prompting (Wei et al. ,2022 ) was introduced to cater to scenarios where direct mapping from $x$ to $y$ is intricate, such as when $x$ is from a mathematical query or challenging question. This method hinges on creating thoughts $z_{1},\\ldots,z_{n}$ that act as stepping stones between $x$ and $y$ ; each thought $z_{i}$ is a language sequence. To employ CoT prompting, thoughts are extracted sequentially as $z_{i}^{-}\\!\\sim p_{\\theta}^{C o T}(z_{i}|x,\\overline{{z_{1}}}\\overline{{{...}}}{-1})^{\\!2}$ |\u00b7\u00b7\u00b7 \u2212, with the final output being $y\\stackrel{\\cdot}{\\sim}p_{\\theta}^{\\overleftarrow{C}o T}(y|\\breve{x},z_{1\\cdots n})$ |\u00b7\u00b7\u00b7 .  \n\nTree-of-thought (ToT) Prompting (Yao et al. ,2023a ) extends CoT prompting by exploring multiple reasoning paths over thoughts. It frames problems as a search over a tree where each node $\\boldsymbol{s}^{\\bar{}}=\\left[\\boldsymbol{x},\\boldsymbol{z}_{1\\cdot i}\\right]$ represents a partial solution state comprising the original input $x$ and thought sequence $z_{1\\cdots i}$ liberate search algorithms like breadth-first or depth-first search are used to systematically explore . Thoughts $z_{i}$ are generated by proposal or sampling with CoT $z_{i}\\stackrel{\\triangledown}{\\sim}p_{\\theta}^{C o T}(z_{i}|x,\\breve{z_{1}}...i-1)$ |\u00b7\u00b7\u00b7 \u2212. Dethe tree, guided by heuristics based on language model evaluations $V(s)$ of each state.  \n\nReasoning via Planning (RAP) ( Hao et al. ,2023 ) is similar to ToT, except that MCTS is used over DFS or BFS. Heuristics are designed from an LM, such as the likelihood or confidence of an action, and the LM is used as a world model to predict subsequent states during the simulation step.  \n\nReAct (Yao et al. ,2023b ) extends language models to tasks where the mapping from $x$ to $y$ is enhanced by or requires interactions with an external environment, such as a game or API. This techique constructs an action ace $\\hat{A}=A\\cup Z$ that adds permissible actions $a$ to the reasoning traces $z$ from CoT. Observations ofrom the environment are used to improve both reasoning and acting. To solve problems with ReAct, after each observation, actions are generated from $p_{\\theta}$ sequentially as $a_{i}\\sim p_{\\theta}^{R e A c t}(a_{i}|\\boldsymbol{x},o_{1\\cdots i-1},a_{1\\cdots i-1})$ |\u00b7\u00b7\u00b7 \u2212\u00b7\u00b7\u00b7 \u2212, with the final output being $y\\stackrel{<}{\\sim}p_{\\theta}^{R e A c t}(y\\mid x,\\stackrel{<}{o_{1}...n},a_{1}...n)$ |\u00b7\u00b7\u00b7 \u00b7\u00b7\u00b7 .  \n\nWhile the previously described prompting techniques improve LM performance on reasoning tasks, they falter on difficult tasks that involve multifaceted decision-making due to several shortcomings: 1) Flexibility : Base prompting methods (CoT or ReAct) autoregressively sample from the LM, neglecting potential alternative continuations from specific states. 2) Sensibility : Reasoning-based methods (CoT, RAP, or ToT) rely solely on the internal representations of the LM and cannot consider external observations. This dependency risks fact hallucination and error propagation while setting a performance ceiling. 3) Adaptability : Current planning frameworks (RAP or ToT) use simple search algorithms such as BFS or cannot leverage environmental feedback to improve planning. Additionally, the agent is static and cannot reuse previous experience or learn from trial and error. While RAP also adopts MCTS, it is constrained to tasks where the LM can become a world model and accurately predict states. These shortcomings limit the ability of LMs to be deployed as general problem-solving agents and form the motivation for LATS.  \n\n4 UNIFYING PLANNING , R EASONING ,AND A CTING"}, {"ref_id": "454984221666055348", "chunk_id": "5", "score": 0.193359375, "text": "# 6 Conclusion\nLLM-based RL algorithms have shown generalization across multiple tasks and games. We argue that this ability comes from implicit memory that fits a large number of parameters to the training data, which is inefficient in terms of model size. In contrast, we propose a new approach inspired by the concept of \u201cworking memory\u201d called Decision Transformers with Memory (DT-Mem), which stores training experience explicitly in a content-addressable matrix module for later retrieval and use. Evaluation demonstrates that DT-Mem achieves better generalization on Atari games with only $10\\%$ of the model parameters compared to the state-of-the-art method. Furthermore, we demonstrate that fine-tuning DT-Memwith a small amount of data can produce state-of-the-art results on both Atari games and the Meta-World environment, when compared to MDT [22], PDT [37], and HDT [38].  \n\nLimitations The first limitation of our work is the sample efficiency of memory fine-tuning. The $10\\%$ fine-tuning dataset is still sizeable, and we plan to explore more sample-efficient methods in the future. We could for instance consider a setting with more tasks, each one with less data so that the inter-task generalization would be even more crucial to its performance. Additionally, this work does not propose a control strategy for collecting data on a new task. For future work, we plan to investigate online data collection methods, which includes the design and learning of exploration strategies for an efficient fine-tuning on new tasks. Finally, the approach has been intuitively motivated, but it would be valuable to have a theoretical grounding that would show the structural limits of large models and how equipping them with a memory component overcomes them.  \n\nSocietal Impact We do not foresee any significant societal impact resulting from our proposed method. The current algorithm is not designed to interact with humans, nor any realistic environment yet. If one chooses to extend our methods to such situations, caution should be exercised to ensure that any safety and ethical concerns are appropriately addressed. As our work is categorized in the offline-RL domain, it is feasible to supplement its training with a dataset that aligns with human intents and values. However, one must be wary that the way our architecture generalizes across tasks is still not well understood and as a consequence we cannot guarantee the generalization of its desirable features: performance, robustness, fairness, etc. By working towards methods that improve the computational efficiency of large models, we contribute to increase their access and reduce their ecological impact.\n\n\n\n# A Implementation Details\n\n# A.1 DT-Mem network architecture\nTable 3 summarizes the different model configurations used for evaluation. In this section, we describe these model configurations in detail. While Table 3 provides a summary, we will also provide additional information here. DT-Mem, PDT and HDT are all share the same transformer architectures. However, for task-adaptation, HDT utilizes a pre-trained $2.3\\mathbf{M}$ hyper-network, while DT-Mem introduces 147K LoRA parameters. To compare with MDT, we use the same parameter size as reported in [22].  \n\nTable 3: Detailed Model Sizes   \n\n\n<html><body><table><tr><td>Model</td><td>Layers</td><td>Hidden size (d)</td><td>Heads</td><td>Params</td><td>Memory Size</td><td>Memory Module Params</td></tr><tr><td>HDT</td><td>4</td><td>512</td><td>8</td><td>13M</td><td>N.A.</td><td>N.A.</td></tr><tr><td>MDT-200M</td><td>10</td><td>1280</td><td>20</td><td>200M</td><td>N.A.</td><td>N.A.</td></tr><tr><td>DT-Mem</td><td>4</td><td>512</td><td>8</td><td>13M</td><td>559K</td><td>7M</td></tr></table></body></html>\n\n# A.2 Hyper-parameters\nIn this section, we will delve into the specifics of the model parameters. Understanding these parameters is key to understanding the workings of the model. It is worth noting that the source code for this model is publicly available at https://github.com/luciferkonn/DT_Mem/tree/main .This allows for a deeper understanding of the model\u2019s inner workings and may facilitate the replication of its results.  \n\nTable 4: Hyperparameters for DT-Mem training   \n\n\n<html><body><table><tr><td>Hyperparameters</td><td>Value</td></tr><tr><td>K (length of context)</td><td>28</td></tr><tr><td>dropoutrate</td><td>0.1</td></tr><tr><td>maximum epochs</td><td>1000</td></tr><tr><td>steps for each epoch</td><td>1000</td></tr><tr><td>optimizer learning rate</td><td>1e-4</td></tr><tr><td>weight decay</td><td>1e-4</td></tr><tr><td>gradient norm clip</td><td>1.</td></tr><tr><td>data points for each dataset</td><td>500,000</td></tr><tr><td>batch size</td><td>64</td></tr><tr><td>memory slots</td><td>1290</td></tr><tr><td>activation</td><td>GELU</td></tr><tr><td>optimizer</td><td>Adamw</td></tr><tr><td>scheduler</td><td>LambdaLR</td></tr></table></body></html>\n\n# A.3 Training and fine-tuning algorithm\nIn this section, we present the pre-training DT-Memin Appendix A.3 and fine-tuning DT-Mem with LoRA in Appendix 5.5.  \n\nWe pre-train DT-Mem on multiple offline datasets. Each gradient update of the DT-Memmodel considers information from each training task.  \n\nWe fine-tune the memory module to adapt to each downstream task. To achieve this, we fix the pre-trained DT-Mem model parameters and add additional LoRA parameters for the memory module feed-forward neural networks. The fine-tune dataset is used to update these LoRA parameters only.\n\n# Algorithm 1 Pre-train DT-Mem\n1: for T episodes do   \n2: 3: 4: for Sample trajectories Split trajectories into different segments with length K and calculate return-to-go in the Task $\\mathcal{T}_{i}\\in T^{t r a i n}\\;.$ do $\\tau=(s_{0},a_{0},r_{0},\\cdot\\cdot\\cdot\\,,s_{H},a_{H},r_{H})$ m the dataset $\\mathcal{D}_{i}$ .  \ninput sequence.   \n5: Given $\\hat{\\tau}_{t+1:t+K}$ , compute the sequence embedding $e_{s e q}$ .  \n6: Update the working memory and retrieve the relative information as $E_{o u t}$   \n7: Given $E_{o u t}$ , predict actions $\\tilde{a}_{t}$ , reward $\\tilde{r}_{t}$ , and return-to-go ${\\tilde{R}}_{t}$ .  \n8: Compute the loss according to Eqn. 1.   \n9: Update all modules parameters.   \n10: end for   \n11: end for  \n\nAlgorithm 2 Fine-tuning DT-Mem  \n\n$\\hat{B}^{q},\\hat{B}^{k},\\hat{B}^{v},\\hat{A}^{q},\\hat{A}^{k},\\hat{A}^{v},B^{q},A^{q},B^{k},A^{k}$ Require: Fine-tuning dataset $\\mathcal{T}^{i}~\\in~T^{t e s t}$ .dataset $\\mathcal{D}^{i}$ for $\\mathcal{T}^{i}$ .Initialize LoRA parameters 1: for T steps do   \n2: Split trajectories into different segments with length $\\mathbf{K}$ and calculate return-to-go in the input sequence.   \n3: Given $\\hat{\\tau}_{t+1:t+K}$ , compute the sequence embedding $e_{s e q}$ .  \n4: Update working memory using $\\hat{Q}\\,=\\,M(\\hat{W}^{q}+\\hat{B}^{q}\\bar{A}^{q})$ ,$\\hat{K}\\,=\\,M(\\hat{W}^{k}\\,+\\,\\hat{B}^{k}\\hat{A}^{k}),\\hat{V}\\,=$ $M(\\hat{W}^{v}+\\hat{B}^{v}\\hat{A}^{v})$ ,$Q=M(W^{q}+B^{q}A^{q}),K=M(W^{k}+B^{k}A^{k})$   \n5: Retrieve the relative information as $E_{o u t}$   \n6: Given $E_{o u t}$ , predict actions $\\tilde{a}_{t}$ , reward $\\tilde{r}_{t}$ , and return-to-go ${\\tilde{R}}_{t}$ .  \n7: Compute the loss according to Eqn. 1.   \n8: Update LoRA parameters only.   \n9: end for"}], "task_step_question_answer": "\u5728\u63a8\u8350\u7cfb\u7edf\u4e2d\uff0c\u8fc1\u79fb\u5b66\u4e60\u7684\u6709\u6548\u6027\u53ef\u4ee5\u901a\u8fc7\u5c06BERT\u6a21\u578b\u4e0e\u56fe\u795e\u7ecf\u7f51\u7edc\u7ed3\u5408\u6765\u4f18\u5316MCTS+PRM\u6846\u67b6\u3002\u5177\u4f53\u6765\u8bf4\uff0cBERT\u6a21\u578b\u53ef\u4ee5\u7528\u4e8e\u63d0\u53d6\u7528\u6237\u884c\u4e3a\u7279\u5f81\uff0c\u8fd9\u4e9b\u7279\u5f81\u968f\u540e\u901a\u8fc7\u56fe\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u76f8\u4f3c\u6027\u5ea6\u91cf\uff0c\u4ece\u800c\u4e3aMCTS\u63d0\u4f9b\u66f4\u7cbe\u786e\u7684\u641c\u7d22\u7b56\u7565\u3002\u4f8b\u5982\uff0cBERT\u63d0\u53d6\u7684\u7279\u5f81\u53ef\u4ee5\u7528\u4e8e\u521d\u59cb\u5316MCTS\u7684\u641c\u7d22\u6811\uff0c\u800c\u56fe\u795e\u7ecf\u7f51\u7edc\u5219\u53ef\u4ee5\u5e2e\u52a9\u8bc6\u522b\u7528\u6237\u884c\u4e3a\u6a21\u5f0f\uff0c\u4ece\u800c\u4f18\u5316PRM\u7684\u504f\u597d\u5efa\u6a21\u3002\u5728\u6e38\u620fAI\u4e2d\uff0c\u4efb\u52a1\u76f8\u4f3c\u6027\u5206\u6790\u53ef\u4ee5\u901a\u8fc7\u4efb\u52a1\u5206\u89e3\u548c\u4efb\u52a1\u5339\u914d\u7b97\u6cd5\u6765\u7ec6\u5316\u3002\u4f8b\u5982\uff0c\u53ef\u4ee5\u4f7f\u7528\u5c42\u6b21\u5316\u4efb\u52a1\u5206\u89e3\u65b9\u6cd5\u5c06\u590d\u6742\u6e38\u620f\u4efb\u52a1\u5206\u89e3\u4e3a\u5b50\u4efb\u52a1\uff0c\u5e76\u901a\u8fc7\u4efb\u52a1\u5339\u914d\u7b97\u6cd5\u8bc4\u4f30\u4e0d\u540c\u6e38\u620f\u4efb\u52a1\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027\u3002\u8fd9\u4e9b\u7b97\u6cd5\u53ef\u4ee5\u5e2e\u52a9MCTS+PRM\u6846\u67b6\u5728\u4e0d\u540c\u6e38\u620f\u4efb\u52a1\u4e4b\u95f4\u8fdb\u884c\u8fc1\u79fb\u5b66\u4e60\uff0c\u4ece\u800c\u63d0\u5347\u5176\u6cdb\u5316\u80fd\u529b\u3002\u5728\u8def\u5f84\u89c4\u5212\u4e2d\uff0c\u6570\u636e\u5206\u5e03\u76f8\u4f3c\u6027\u5206\u6790\u53ef\u4ee5\u901a\u8fc7\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GAN\uff09\u751f\u6210\u76ee\u6807\u9886\u57df\u7684\u6570\u636e\u5206\u5e03\uff0c\u5e76\u7ed3\u5408Wasserstein\u8ddd\u79bb\u8fdb\u884c\u5ea6\u91cf\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u65b9\u6cd5\u5728\u5904\u7406\u9ad8\u7ef4\u6570\u636e\u6216\u52a8\u6001\u73af\u5883\u4e0b\u7684\u6570\u636e\u5206\u5e03\u53d8\u5316\u65f6\u53ef\u80fd\u9762\u4e34\u6311\u6218\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u53ef\u4ee5\u5f15\u5165\u52a8\u6001GAN\u6a21\u578b\uff0c\u4f7f\u5176\u80fd\u591f\u9002\u5e94\u73af\u5883\u53d8\u5316\uff0c\u5e76\u901a\u8fc7\u81ea\u9002\u5e94Wasserstein\u8ddd\u79bb\u5ea6\u91cf\u6570\u636e\u5206\u5e03\u7684\u76f8\u4f3c\u6027\u3002\u901a\u8fc7\u5f15\u5165\u57fa\u4e8e\u5143\u5b66\u4e60\u7684\u8fc1\u79fb\u5b66\u4e60\u6280\u672f\uff0c\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63d0\u5347\u6a21\u578b\u5728\u76ee\u6807\u9886\u57df\u7684\u6cdb\u5316\u80fd\u529b\u3002\u4f8b\u5982\uff0c\u5143\u5b66\u4e60\u53ef\u4ee5\u7528\u4e8e\u4f18\u5316MCTS\u7684\u641c\u7d22\u7b56\u7565\uff0c\u4f7f\u5176\u80fd\u591f\u5feb\u901f\u9002\u5e94\u65b0\u73af\u5883\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u5143\u5b66\u4e60\u53ef\u4ee5\u901a\u8fc7\u5b66\u4e60\u4e0d\u540c\u4efb\u52a1\u4e4b\u95f4\u7684\u5171\u6027\uff0c\u4e3aMCTS\u63d0\u4f9b\u901a\u7528\u7684\u641c\u7d22\u7b56\u7565\uff0c\u4ece\u800c\u51cf\u5c11\u5728\u65b0\u4efb\u52a1\u4e2d\u7684\u641c\u7d22\u65f6\u95f4\u3002\u6b64\u5916\uff0c\u5143\u5b66\u4e60\u8fd8\u53ef\u4ee5\u7528\u4e8e\u4f18\u5316PRM\u7684\u504f\u597d\u5efa\u6a21\uff0c\u4f7f\u5176\u80fd\u591f\u5feb\u901f\u9002\u5e94\u65b0\u7528\u6237\u7684\u884c\u4e3a\u6a21\u5f0f\u3002\u901a\u8fc7\u7ed3\u5408\u8fd9\u4e9b\u6280\u672f\uff0cMCTS+PRM\u6846\u67b6\u53ef\u4ee5\u5728\u66f4\u5e7f\u6cdb\u7684\u5e94\u7528\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "ref_task_step_id": ""}, "__type__": "task_step_node"}, "29d7d9b0-9dd0-42e2-a847-f9e6d0c0b37f": {"__data__": {"id_": "29d7d9b0-9dd0-42e2-a847-f9e6d0c0b37f", "metadata": {}, "relationships": {}, "hash": "", "start_task_context": "MCTS\u5728PRM\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u7684\u5e94\u7528\u63a2\u7d22\u7efc\u8ff0", "aemo_representation_context": "### Step 1: \u5206\u6790\u8fd1\u51e0\u5e74\u7814\u7a76\u9886\u57df\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba\n\n**\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09**\uff1a\n- **\u57fa\u672c\u539f\u7406**\uff1aMCTS\u662f\u4e00\u79cd\u7528\u4e8e\u51b3\u7b56\u8fc7\u7a0b\u7684\u542f\u53d1\u5f0f\u641c\u7d22\u7b97\u6cd5\uff0c\u5e7f\u6cdb\u5e94\u7528\u4e8e\u6e38\u620fAI\u3001\u8def\u5f84\u89c4\u5212\u7b49\u9886\u57df\u3002\u5176\u6838\u5fc3\u601d\u60f3\u662f\u901a\u8fc7\u6a21\u62df\u591a\u6b21\u968f\u673a\u6e38\u620f\u6765\u8bc4\u4f30\u4e0d\u540c\u51b3\u7b56\u7684\u4f18\u52a3\u3002\n- **\u6280\u672f\u6846\u67b6**\uff1a\u4e3b\u8981\u5305\u62ec\u9009\u62e9\uff08Selection\uff09\u3001\u6269\u5c55\uff08Expansion\uff09\u3001\u6a21\u62df\uff08Simulation\uff09\u548c\u56de\u6eaf\uff08Backpropagation\uff09\u56db\u4e2a\u6b65\u9aa4\u3002\n\n**\u504f\u597d\u7b56\u7565\u6a21\u578b\uff08PRM\uff09**\uff1a\n- **\u57fa\u672c\u539f\u7406**\uff1aPRM\u65e8\u5728\u901a\u8fc7\u5b66\u4e60\u7528\u6237\u7684\u504f\u597d\u6765\u4f18\u5316\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5e38\u7528\u4e8e\u63a8\u8350\u7cfb\u7edf\u3001\u4e2a\u6027\u5316\u670d\u52a1\u7b49\u3002\n- **\u6280\u672f\u6846\u67b6**\uff1a\u901a\u5e38\u5305\u62ec\u504f\u597d\u5efa\u6a21\u3001\u7b56\u7565\u4f18\u5316\u548c\u6a21\u578b\u5fae\u8c03\u7b49\u73af\u8282\u3002\n\n**\u65b9\u6cd5\u8bba**\uff1a\n- **\u96c6\u6210\u5b66\u4e60**\uff1a\u5c06MCTS\u4e0ePRM\u7ed3\u5408\uff0c\u5229\u7528MCTS\u7684\u641c\u7d22\u80fd\u529b\u4f18\u5316PRM\u7684\u51b3\u7b56\u8fc7\u7a0b\u3002\n- **\u5f3a\u5316\u5b66\u4e60**\uff1a\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u673a\u5236\u4e0d\u65ad\u8c03\u6574PRM\u7684\u53c2\u6570\uff0c\u4ee5\u9002\u5e94\u52a8\u6001\u73af\u5883\u3002\n\n### Step 2: \u7814\u7a76\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u4e3b\u8981\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0e\u53d8\u4f53\n\n**\u5e94\u7528\u573a\u666f**\uff1a\n- **\u6e38\u620fAI**\uff1aMCTS\u4e0ePRM\u7ed3\u5408\u7528\u4e8e\u6e38\u620f\u4e2d\u7684\u667a\u80fd\u51b3\u7b56\uff0c\u5982AlphaGo\u3002\n- **\u8def\u5f84\u89c4\u5212**\uff1a\u5728\u673a\u5668\u4eba\u5bfc\u822a\u548c\u81ea\u52a8\u9a7e\u9a76\u4e2d\uff0c\u5229\u7528MCTS\u4f18\u5316\u8def\u5f84\u9009\u62e9\u3002\n- **\u63a8\u8350\u7cfb\u7edf**\uff1a\u901a\u8fc7PRM\u5b66\u4e60\u7528\u6237\u504f\u597d\uff0c\u7ed3\u5408MCTS\u8fdb\u884c\u66f4\u7cbe\u51c6\u7684\u63a8\u8350\u3002\n\n**\u53d8\u4f53**\uff1a\n- **\u6df1\u5ea6MCTS**\uff1a\u7ed3\u5408\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff0c\u63d0\u5347MCTS\u7684\u51b3\u7b56\u80fd\u529b\u3002\n- **\u591a\u6a21\u6001PRM**\uff1a\u878d\u5408\u591a\u79cd\u6570\u636e\u6e90\uff08\u5982\u6587\u672c\u3001\u56fe\u50cf\uff09\uff0c\u589e\u5f3aPRM\u7684\u6cdb\u5316\u80fd\u529b\u3002\n\n### Step 3: \u8bc4\u4f30\u5b66\u672f\u754c\u7684\u6280\u672f\u8fdb\u6b65\u4e0e\u5c40\u9650\u6027\n\n**\u6280\u672f\u8fdb\u6b65**\uff1a\n- **\u6027\u80fd\u63d0\u5347**\uff1aMCTS\u4e0ePRM\u7ed3\u5408\u663e\u8457\u63d0\u5347\u4e86\u51b3\u7b56\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002\n- **\u6cdb\u5316\u80fd\u529b**\uff1a\u591a\u6a21\u6001PRM\u5728\u5904\u7406\u591a\u6837\u5316\u6570\u636e\u65f6\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002\n\n**\u5c40\u9650\u6027**\uff1a\n- **\u8ba1\u7b97\u590d\u6742\u5ea6**\uff1aMCTS\u5728\u5927\u89c4\u6a21\u95ee\u9898\u4e2d\u8ba1\u7b97\u91cf\u5927\uff0c\u6548\u7387\u6709\u5f85\u63d0\u5347\u3002\n- **\u6570\u636e\u4f9d\u8d56**\uff1aPRM\u7684\u6027\u80fd\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u9ad8\u8d28\u91cf\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u6570\u636e\u504f\u5dee\u95ee\u9898\u4f9d\u7136\u5b58\u5728\u3002\n\n### Step 4: \u63a2\u8ba8\u8ba1\u7b97\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\n\n**\u9002\u7528\u6027**\uff1a\n- **\u591a\u9886\u57df\u5e94\u7528**\uff1aMCTS+PRM\u6846\u67b6\u5728\u6e38\u620f\u3001\u8def\u5f84\u89c4\u5212\u3001\u63a8\u8350\u7cfb\u7edf\u7b49\u591a\u4e2a\u9886\u57df\u8868\u73b0\u51fa\u826f\u597d\u7684\u9002\u7528\u6027\u3002\n- **\u591a\u6a21\u6001\u6570\u636e**\uff1a\u591a\u6a21\u6001PRM\u80fd\u591f\u5904\u7406\u591a\u79cd\u7c7b\u578b\u7684\u6570\u636e\uff0c\u9002\u7528\u4e8e\u590d\u6742\u5e94\u7528\u573a\u666f\u3002\n\n**\u6cdb\u5316\u80fd\u529b**\uff1a\n- **\u8de8\u9886\u57df\u6cdb\u5316**\uff1a\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\uff0cMCTS+PRM\u6846\u67b6\u5728\u4e0d\u540c\u9886\u57df\u95f4\u5c55\u73b0\u51fa\u8f83\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002\n- **\u52a8\u6001\u73af\u5883\u9002\u5e94**\uff1a\u5f3a\u5316\u5b66\u4e60\u673a\u5236\u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u9002\u5e94\u52a8\u6001\u53d8\u5316\u7684\u73af\u5883\u3002\n\n### Step 5: \u5206\u6790\u6700\u65b0\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027\n\n**\u7a33\u5b9a\u6027**\uff1a\n- **\u7b97\u6cd5\u4f18\u5316**\uff1a\u901a\u8fc7\u6539\u8fdbMCTS\u7684\u9009\u62e9\u7b56\u7565\u548cPRM\u7684\u53c2\u6570\u8c03\u6574\u673a\u5236\uff0c\u63d0\u5347\u4e86\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u3002\n- **\u9c81\u68d2\u6027\u589e\u5f3a**\uff1a\u5f15\u5165\u5bf9\u6297\u8bad\u7ec3\u548c\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\uff0c\u63d0\u9ad8\u6a21\u578b\u5728\u590d\u6742\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u6027\u3002\n\n**\u5bb9\u9519\u6027**\uff1a\n- **\u9519\u8bef\u5904\u7406**\uff1a\u5728\u6a21\u62df\u548c\u56de\u6eaf\u9636\u6bb5\u5f15\u5165\u5bb9\u9519\u673a\u5236\uff0c\u51cf\u5c11\u9519\u8bef\u51b3\u7b56\u7684\u5f71\u54cd\u3002\n- **\u5927\u89c4\u6a21\u6570\u636e\u9002\u5e94\u6027**\uff1a\u4f18\u5316\u7b97\u6cd5\u7ed3\u6784\uff0c\u63d0\u5347\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\u7684\u5904\u7406\u80fd\u529b\u3002\n\n### Step 6: \u8bc4\u4f30\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218\n\n**\u672a\u6765\u7814\u7a76\u65b9\u5411**\uff1a\n- **\u9ad8\u6548\u8ba1\u7b97**\uff1a\u7814\u7a76\u66f4\u9ad8\u6548\u7684MCTS\u7b97\u6cd5\uff0c\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002\n- **\u6570\u636e\u589e\u5f3a**\uff1a\u63a2\u7d22\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u51cf\u5c11PRM\u5bf9\u9ad8\u8d28\u91cf\u6570\u636e\u7684\u4f9d\u8d56\u3002\n\n**\u6311\u6218**\uff1a\n- **\u8de8\u6a21\u6001\u878d\u5408**\uff1a\u5982\u4f55\u6709\u6548\u878d\u5408\u591a\u79cd\u6a21\u6001\u6570\u636e\uff0c\u63d0\u5347\u6a21\u578b\u7684\u7efc\u5408\u6027\u80fd\u3002\n- **\u52a8\u6001\u73af\u5883\u9002\u5e94**\uff1a\u5728\u52a8\u6001\u73af\u5883\u4e0b\uff0c\u5982\u4f55\u4fdd\u6301\u6a21\u578b\u7684\u7a33\u5b9a\u6027\u548c\u9ad8\u6548\u6027\u3002\n\n**\u63a8\u52a8\u540e\u7eed\u7814\u7a76**\uff1a\n- **\u65b0\u95ee\u9898\u63d0\u51fa**\uff1a\u5982\u591a\u6a21\u6001\u6570\u636e\u4e0b\u7684\u504f\u597d\u5efa\u6a21\u3001\u52a8\u6001\u73af\u5883\u4e0b\u7684\u51b3\u7b56\u4f18\u5316\u7b49\u3002\n- **\u7814\u7a76\u5207\u5165\u70b9**\uff1a\u4ece\u7b97\u6cd5\u4f18\u5316\u3001\u6570\u636e\u589e\u5f3a\u3001\u8de8\u6a21\u6001\u878d\u5408\u7b49\u591a\u4e2a\u89d2\u5ea6\u5207\u5165\uff0c\u63a8\u52a8\u6280\u672f\u8fdb\u6b65\u3002\n\n### \u603b\u7ed3\n\n\u8fd1\u5e74\u6765\uff0cMCTS\u5728PRM\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u7684\u5e94\u7528\u63a2\u7d22\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4e3b\u8981\u4f53\u73b0\u5728\u6280\u672f\u6846\u67b6\u7684\u5b8c\u5584\u3001\u5e94\u7528\u573a\u666f\u7684\u62d3\u5c55\u3001\u6027\u80fd\u7684\u63d0\u5347\u4ee5\u53ca\u7a33\u5b9a\u6027\u548c\u5bb9\u9519\u6027\u7684\u589e\u5f3a\u3002\u7136\u800c\uff0c\u4ecd\u5b58\u5728\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u3001\u6570\u636e\u4f9d\u8d56\u6027\u5f3a\u7b49\u5c40\u9650\u6027\u3002\u672a\u6765\u7684\u7814\u7a76\u5e94\u5173\u6ce8\u9ad8\u6548\u8ba1\u7b97\u3001\u6570\u636e\u589e\u5f3a\u548c\u8de8\u6a21\u6001\u878d\u5408\u7b49\u65b9\u5411\uff0c\u4ee5\u5e94\u5bf9\u52a8\u6001\u73af\u5883\u4e0b\u7684\u51b3\u7b56\u4f18\u5316\u7b49\u6311\u6218\uff0c\u63a8\u52a8\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002", "task_step_name": "\u5206\u6790\u6700\u65b0\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027", "task_step_description": "\u901a\u8fc7\u6539\u8fdbMCTS\u7684\u9009\u62e9\u7b56\u7565\u548cPRM\u7684\u53c2\u6570\u8c03\u6574\u673a\u5236\uff0c\u63d0\u5347\u4e86\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u3002", "task_step_level": "4", "task_step_question": "### \u95ee\u9898\u63d0\u51fa\n\n\u5728**\u5206\u6790\u6700\u65b0\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027**\u8fd9\u4e00\u4efb\u52a1\u6b65\u9aa4\u4e2d\uff0c\u63d0\u5230\u901a\u8fc7\u6539\u8fdbMCTS\u7684\u9009\u62e9\u7b56\u7565\u548cPRM\u7684\u53c2\u6570\u8c03\u6574\u673a\u5236\uff0c\u63d0\u5347\u4e86\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u3002\u57fa\u4e8e\u8fd9\u4e00\u63cf\u8ff0\uff0c\u53ef\u4ee5\u63d0\u51fa\u4ee5\u4e0b\u95ee\u9898\uff1a\n\n**\u95ee\u9898**\uff1a\u5728MCTS\u4e0ePRM\u7ed3\u5408\u7684\u6846\u67b6\u4e2d\uff0c\u5177\u4f53\u6709\u54ea\u4e9b\u6539\u8fdb\u7684\u9009\u62e9\u7b56\u7565\u548c\u53c2\u6570\u8c03\u6574\u673a\u5236\u88ab\u63d0\u51fa\uff0c\u8fd9\u4e9b\u6539\u8fdb\u5982\u4f55\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u63d0\u5347\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\uff1f\u8fd9\u4e9b\u6539\u8fdb\u662f\u5426\u5728\u4e0d\u540c\u5e94\u7528\u573a\u666f\uff08\u5982\u6e38\u620fAI\u3001\u8def\u5f84\u89c4\u5212\u3001\u63a8\u8350\u7cfb\u7edf\uff09\u4e2d\u8868\u73b0\u51fa\u76f8\u4f3c\u7684\u7a33\u5b9a\u6027\u63d0\u5347\u6548\u679c\uff1f", "task_step_question_context": [{"ref_id": "454845771530662550", "chunk_id": "1", "score": 0.232421875, "text": "# DSensitivity Analysis of Hyper-parameters of MCTS-VS\nWe provide further studies to examine the influence of the hyper-parameters of MCTS-VS, including the employed optimization algorithm for optimizing the selected variables in each iteration, the \u201cfillin\u201d strategy, the hyper-parameter $k$ used in the best$k$ strategy, the hyper-parameter $C_{p}$ for calculating UCB in Eq. (1), the number $2\\,\\times\\,N_{v}\\,\\times\\,N_{s}$ sampled data in each iteration, the threshold $N_{b a d}$ for re-initializing a tree, and the threshold $N_{s p l i t}$ for splitting a tree node.  \n\nThe optimization algorithm is employed by MCTS-VS to optimize the selected variables in each iteration. We compare three different optimization algorithms, i.e., random search (RS), BO and TuRBO. First, we conduct experiments similar to \u201cEffectiveness of Variable Selection\u201d in Section 5.1, to show the effectiveness of MCTS-VS even when equipped with RS. Figure 6 shows that MCTSVS-RS is better than Dropout-RS and RS, revealing the advantage of MCTS-VS.  \n\n  \nFigure 6: Effectiveness of MCTS-VS when equipped with RS.  \n\nNext we compare the performance of MCTS-VS equipped with RS, BO and TuRBO, by experiments on the Hartmann functions with increasing ratio of valid variables. Hartmann 6 _500 has 6 valid variables. Hartmann 6 _5 _500 is generated by mixing 5 Hartmann 6 functions as Hartmann 6 $(\\pmb{x}_{1:6})+$ Hartmann 6 $\\backslash(\\pmb{x}_{7:12})+\\cdot\\cdot\\cdot+\\mathrm{Hartmann6}(\\pmb{x}_{25:30})$ , and appending 470 unrelated dimensions, where $\\pmb{x}_{i:j}$ denotes the $i$ -th to j-th variables. Hartmann 6 _10 _500 is generated alike. Thus, Hartmann 6 _5 _500 and Hartmann 6 _10 _500 have 30 and 60 valid variables, respectively. The results in Figure 7 show that as the ratio of valid variables increases, MCTS-VS-TuRBO gradually surpasses MCTS-VS-RS and MCTS-VS-BO, while MCTS-VS-RS becomes worse and worse. This is expected. If the ratio of valid variables is high, MCTS-VS is more likely to select the valid variables, so it is worth to use the expensive optimization algorithm, e.g., TuRBO, to optimize the selected variables. If the ratio is low, unrelated variables are more likely to be selected most of the time, so using a cheap optimization algorithm would be better. These observations also give us some guidance on selecting optimization algorithms in practice.  \n\n\u201cFill-in\u201d strategy is a basic component of variable selection methods, which influences the quality of the value of unselected variables. We compare the employed best$k$ strategy $(k=20)$ ) with the average best$k$ strategy and the random strategy. The average best$k$ strategy uses the average of the best $k$ data points for the unselected variables, and the random strategy samples the value of an unselected variable from its domain randomly. As shown in Figure 8(a), the random strategy leads to the poor performance of MCTS-VS-BO, which may be because it does not utilize the historical information and leads to over-exploration. The best${\\cdot k}$ strategy utilizes the historical points that have high objective values to fill in the unselected variables, thus behaving much better. The performance of the average strategy is between the best$k$ and random strategies. We recommend using the best$k$ strategy in practice.  \n\nThe hyper-parameter $k$ used in the best$k$ strategy controls the degree of exploitation for the unselected variables. As shown in Figure 8(b), a smaller $k$ encourages exploitation, which results in better performance in the early stage, but easily leads to premature convergence. A larger $k$ encourages exploration and behaves worse in the early stage, but may converge to a better value. We recommend using a larger $k$ if allowing enough evaluations.  \n\n  \nFigure 7: Sensitivity analysis of the optimization algorithm.  \n\n  \nFigure 8: Sensitivity analysis of the \u201cfill-in\u201d strategy and the hyper-parameter $k$ of the best$k$ strategy, using MCTS-VS-BO on Hartmann 6 _300 .  \n\nThe hyper-parameter $C_{p}$ for calculating UCB in Eq. (1) balances the exploration and exploitation of MCTS. As shown in Figure 9, a too small $C_{p}$ leads to relatively worse performance, highlighting the importance of exploration. A too large $C_{p}$ may also lead to over-exploration. But overall MCTSVS is not very sensitive to $C_{p}$ . We recommend setting $C_{p}$ between $1\\%$ and $10\\%$ of the optimum (i.e., max $f({\\boldsymbol{x}}))$ ), which is consistent with that for LA-MCTS [40].  \n\n  \nFigure 9: Sensitivity analysis of the hyper-parameter $C_{p}$ for calculating UCB in Eq. (1), using MCTS-VS-BO on Levy and Hartmann.  \n\nThe number $2\\,\\times\\,N_{v}\\,\\times\\,N_{s}$ of sampled data in ch iteration depends on the batch size $N_{v}$ of variable index subset and the sample batch size $N_{s}$ , and will influence the accuracy of estimating the variable score vector in Eq. (2). If we increase $N_{v}$ and $N_{s}$ , we can calculate the variable score more accurately, but also need more evaluations. Figure 10(a) shows that given the same number of evaluations, MCTS-VS-BO achieves the best performance when $N_{v}=2$ and $N_{s}=3$ . Thus, this setting may be a good choice to balance the accuracy of variable score and the number of evaluations, which is also used throughout the experiments.  \n\nThe threshold $N_{b a d}$ for re-initializing a tree controls the tolerance of selecting bad tree nodes (i.e., nodes containing unimportant variables). A smaller $N_{b a d}$ leads to frequent re-initialization, which can adjust quickly but may cause under-exploitation of the tree. A larger $N_{b a d}$ can make full use of the tree, but may optimize too much on unimportant variables. Figure 10(b) shows that MCTS-VS achieves the best performance when $N_{b a d}=5$ . Thus, we recommend to use this setting, to balance the re-initialization and exploitation of the tree.  \n\nThe threshold $N_{s p l i t}$ for splitting a node. If the number of variables in a node is larger than $N_{s p l i t}$ ,the node can be further partitioned. That is, the parameter $N_{s p l i t}$ controls the least number of variables in a leaf node and thus affects the number of selected variables, which has a direct influence on the wall clock time. Note that MCTS-VS selects a leaf node and optimizes the variables contained by this node in each iteration. The smaller $N_{s p l i t}$ , the shorter the time. Figure 10(c) shows that $N_{s p l i t}$ has little influence on the performance of MCTS-VS-BO, and thus we recommend to set $N_{s p l i t}=3$ to reduce the wall clock time.  \n\n  \nFigure 10: S vity analysis of the number $2\\,\\times\\,N_{v}\\,\\times\\,N_{s}$ pled data in each iteration, the threshold $N_{b a d}$ for re-initializing a tree and the threshold $N_{s p l i t}$ for splitting a node, using MCTS-VS-BO on Hartmann 6 _300 .  \n\nInfluence of the hyper-parameters on the runtime of MCTS-VS. We also provide some intuitive explanation about the influence of the hyper-parameters on the runtime. The threshold $N_{s p l i t}$ for splitting a node has a direct impact on the runtime, because it controls the least number of variables to be optimized in a leaf node. That is, the runtime will increase with $N_{s p l i t}$ . Other parameters may affect the depth of the tree and thus the runtime. For the threshold $N_{b a d}$ for re-initializing a tree, if it is set to a small value, MCTS-VS will re-build the tree frequently and the depth of the tree is small. The shallow nodes have more variables, leading to more runtime to optimize. For the hyper-parameter $C_{p}$ for calculating UCB, if it is set to a large value, the exploration is preferred and MCTS-VS will tend to select the right node (regarded as containing unimportant variables). The tree thus will be re-built freq tly, ding to more runtime. For the number $2\\,\\times\\,N_{v}\\,\\times\\,N_{s}$ of sampled data at each iteration, if $N_{v}$ and $N_{s}$ are set to large values, the depth of the tree will be small given the total number of evaluations, and thus lead to more runtime."}, {"ref_id": "454984236281633338", "chunk_id": "4", "score": 0.1953125, "text": "# 4.2 RESULTS AND DISCUSSION\n\n# Question 1: How does TS-LLM perform in different generation tasks regarding Path@1? Does Path $@1$ provide a reasonable evaluation metric? (Sec. 3.2.2 and Sec. 3.4)\nIn Table 2, we first provide the comparison between TS-LLM variants with the CoT baseline regarding the Path $@1$ performance. Path $@1$ means each algorithm only generates one answer. TS-LLM variants generally outperform the baseline, underscoring the advantages of tree-search algorithms due to their capability to efficiently prune the search tree. However, notable exceptions are also seen in the RLHF task where MCTS, BFS, and DFS underperform. From our results, we want to highlight two things. Firstly, MCTS, BFS, and DFS generally perform better in shallow search problems (8 for GSM8K and 4 for Game24) while MCTS$\\alpha$ and MCTS-Rollout are dominant in deep search problems (15 for PrOntoQA and 64 for RLHF). Secondly, the the backward/backpropagation operation is quite important, since the first three methods\u2014MCTS, BFS, and DFS\u2014do not incorporate this operation and depend greedily on the value function. Note that the MCTS Path $@1$ also does not include the backward operation because it only happens after finding one complete path.  \n\nTable 5: Different value training for iterative update on GSM8k  \n\n  \nFigure 2: Different Value training on GSM8k   \nFigure 3: mean/max reward for RLHF alignment and the best results of 3 aggregations for the rest three tasks w.r.t. number of sequences on the 1st row and number of computations on the 2nd row.  \n\nDespite the superiority of TS-LLM, we argue that Path $@1$ is in fact not a reasonable evaluation metric. In table 2, we also include the number of computations used in Path $@1$ generation (number of tokens in sentence-level and number of forward computation in token-level). All TS-LLM variants consume much more computation than the CoT baseline. To enable a fair comparison, we provide additional baselines, CoT-SC and COT-SC-Tree with two aggregation methods: majorityvote (MAJ) and ORM-vote (denoted as ORM, and it utilizes the learned ORM in TS-LLM). We show results within a similar scale of computation consumption with TS-LLM variants. Under this situation, TS-LLM\u2019s advantages largely decrease when compared with $\\mathrm{CoT-SC_{\\mathrm{ORM}}}$ , especially on GSM8K (only BFS is better). We are surprised to see that such simple algorithms can also have outstanding performance when compared fairly. Despite this, most tree-search algorithms are still dominant in the rest three tasks given the same (CoT-SC-Tree) or larger search space (CoT-SC).\n\n# Question 2: How do different node constructions influence the performance? (Sec. 3.1)\nAs discussed in Sec. 3.1), the search space for sentence-level action nodes is limited. Thus, we investigate the possible influence introduced by different tree constructions on Game24 with different node sizes. Specifically, we set the number of maximal expanded node size as 6, 20, and 50. Table 3 lists the performance and the number of tokens generated comparing TS-LLM\u2019s variants, CoT-SC and CoT-SC-Tree. The almost doubled performance boost from 44.2 to 79.8 indicates the importance of appropriate expansion node size, improving TS-LLM\u2019s performance upper bound.\n\n# Question 3: Why do we need a learned value function and how to train that? (Sec. 3.2.1)\nIn Table 4, we provide a motivating example by prompting LLaMA2-7b-base as the value function and reward model for TS-LLM on Game24. The performance drop of BFS and MCTS$\\alpha$ indicates the incapability of small language models as reliable evaluators, which motivates us to substitute it with a learned value function and ORM as a general solution for any task and LLM with any size.  \n\nTable 6: Iterative update results. $\\theta_{0}$ is the old parameter while $\\theta_{1}$ is the new one after one iteration. We compare all combinations of policy and value on GSM8k (left) and RLHF alignment (right).   \n\n\n<html><body><table><tr><td>Method</td><td>Policy</td><td>Value</td><td>Performance(%) Method</td><td></td><td>Policy</td><td>Value</td><td>Performance(%)</td></tr><tr><td>Greedy</td><td>T00</td><td>1</td><td>41.4 \u00b1 0.0</td><td>Greedy</td><td>T00</td><td></td><td>0.39 \u00b1 0.0</td></tr><tr><td>Greedy</td><td>T01</td><td>1</td><td>47.9 \u00b1 0.0</td><td>Greedy</td><td>T01</td><td></td><td>1.87 \u00b1 0.0</td></tr><tr><td>Greedy</td><td>RFT k=50</td><td></td><td>47.0 \u00b1 0.0</td><td>Greedy</td><td>RFTN=5</td><td></td><td>1.16 \u00b1 0.0</td></tr><tr><td>Greedy</td><td>RFTk=100</td><td>-</td><td>47.5 \u00b1 0.0</td><td>Greedy</td><td>PPO</td><td></td><td>2.53\u00b10.0</td></tr><tr><td>MCTS-\u03b1</td><td>T00</td><td>{,r}00</td><td>51.9 \u00b1 0.6</td><td>MCTS-0</td><td>T00</td><td>{,r}00</td><td>2.221\u00b10.0</td></tr><tr><td>MCTS-\u03b1</td><td>T00</td><td>U,r}01</td><td>53.2\u00b10.3</td><td>MCTS-0</td><td>T00</td><td>U,r}01</td><td>2.482\u00b10.0</td></tr><tr><td>MCTS-\u03b1</td><td>T01</td><td>v,r}00</td><td>54.1 \u00b1 0.9</td><td>MCTS-0</td><td>T01</td><td>v,r}00</td><td>2.532\u00b10.0</td></tr><tr><td>MCTS-\u03b1</td><td>T01</td><td>\u03b8</td><td>56.5\u00b1 0.6</td><td>MCTS-\u03b1</td><td>T01</td><td>0</td><td>2.670 \u00b1 0.0</td></tr></table></body></html>  \n\nTherefore, we investigate data collection and training paradigms for value function and ORM in TSLLM. In Figure 2, we investigate the influence of data amount and diversity by training with mixed data uniformly sampled from checkpoints of all SFT epochs ( mixed ); data purely sampled from the last checkpoint ( pure ); 1/3 data of the pure setting $(p u r e,l e s s)$ . The results of CoT-SC ORM-vote $@10$ underscore the diversity of sampled data in learning a better ORM. The Path $@1$ results of 3 TSLLM variants show that the amount of sampled data is of great importance. We leave a detailed discussion of how value and reward function training is influenced in iterative training (Sec. 3.3) when answering Question 5. Our final conclusion is that collecting a set of diverse data as much as possible is always better for TS-LLM\u2019s value function and ORM training .\n\n# Question 4: How TS-LLM is improved by aggregating over multiple results? (Sec. 3.2.3)\nIn Fig. 3, we demonstrate the mean/max reward for the RLHF task and the best of 3 aggregation results for the rest three tasks. We measure the performance of aggregation w.r.t path number and token consumption. From the figure, we mainly summarize two conclusions: Firstly, Most TS-LLM variants benefit from aggregation and can show large strengths compared with other baselines. CoT-SC only beats TS-LLM in GSM8k with the same token size, mainly because of its larger search space. TS-LLM variants are still dominant when compared with CoT-SC-Tree. Secondly, treesearch algorithms\u2019 aggregation benefits less than CoT-SC in small-scale problems. In GSM8K and Game24, TS-LLM struggles to improve under large aggregation numbers. We believe this is because of: (1) The search space gap between CoT-SC and tree-search algorithms. Tree-search algorithms inherently explore fewer sentences, which is validated by comparing token consumption between CoT-SC-Tree $@50$ and $\\mathrm{CoT-SC}@50$ . (2) Different tree searches are not independent. The latter search might be influenced by the previous one, which decreases generation diversity."}, {"ref_id": "454984236379937352", "chunk_id": "11", "score": 0.162109375, "text": "# 3 MCTS-VS Method\nIn this section, we propose a Variable Selection method based on MCTS for high-dimensional BO, briefly called MCTS-VS. The main idea is to apply MCTS to iteratively partition all variables into important and unimportant ones, and perform BO only for those important variables. Let $[D]=\\bar{\\{}1,2,\\dots,D\\}$ denote the indexes of all variables $\\textbf{\\em x}$ , and ${\\pmb x}_{\\mathbb{M}}$ denote the subset of variables indexed by $\\mathbb{M}\\subseteq[D]$ \u2286].  \n\nWe first introduce a $D$ -dimensional vector named variable score , which is a key component of MCTS-VS. Its $i$ -th element represents the importance of the $i$ -th variable $x_{i}$ . During the running process of MCTS-VS, fter optimizing a subset ${\\pmb x}_{\\mathbb{M}}$ of variables where $\\mathbb{M}\\subseteq[D]$ notes the indexes of the var into a set D, called bles, a set information set Dof sampled points will be generated, and the pai . The variable score vector is based on D$(\\mathbb{M},\\mathscr{D})$ , and calculated as Dwill be recorded  \n\n$$\ns=\\left(\\sum_{(\\mathbb{M},\\mathcal{D})\\in\\mathbb{D}}\\sum_{({\\pmb x}^{i},{\\pmb y}^{i})\\in\\mathcal{D}}y^{i}\\cdot g(\\mathbb{M})\\right)/\\left(\\sum_{(\\mathbb{M},\\mathcal{D})\\in\\mathbb{D}}|\\mathcal{D}|\\cdot g(\\mathbb{M})\\right),\n$$  \n\nwhere ion $g:2^{[D]}\\rightarrow\\{0,1\\}^{D}$ ghe Boo ector representation of variable index wise division. Each dime subset $\\mathbb{M}\\subseteq[D]$ \u2286(i.e., the i -th element of sion of $\\scriptstyle\\sum_{(\\mathbb{M},{\\mathcal{D}})\\in\\mathbb{D}}\\sum_{({\\pmb x}^{i},{\\pmb y}^{i})\\in{\\mathcal{D}}}y^{i}\\cdot g(\\mathbb{M})$ $g(\\mathbb{M})$ Pis 1 if i $i\\in\\mathbb{M}$ \u2208, and 0 otherwise), and is the sum of query evaluations /is the elementusing ea each variable. Thus, the variable, and each dimension of i -th element of variable score D\u2208$\\sum_{(\\mathbb{M},\\mathbb{D})\\in\\mathbb{D}}|\\mathcal{D}|\\cdot g(\\mathbb{M})$ D\u2208\u2208D s, representing the importance of the is the number of queries u i ing -th variable $x_{i}$ , is actually measured by the average goodness of all the sampled points that are generated by optimizing a subset of variables containing $x_{i}$ . The variable score $\\pmb{s}$ will be used to define the value of each tree node of MCTS as well as for node expansion.  \n\nIn MCTS-VS, the root of the tree represents all variables. A tree node $X$ represents a subset of variables, whose index set is denoted by $\\mathbb{A}_{X}\\subseteq[D]$ , and it stores the value $v_{X}$ an e number $n_{X}$ of visits, which are used to calculate the value of UCB as in Eq. (1). The value $v_{X}$ is defined as the average score (i.e., importance) of the variables contained by $X$ , which can be calculated by $s\\cdot g(\\mathbb{A}_{X})/|\\mathbb{A}_{X}|$ , where $g(\\mathbb{A}_{X})$ is the Bo ean vector representation of $\\mathbb{A}_{X}$ and $|\\mathbb{A}_{X}|$ is the size of $\\mathbb{A}_{X}$ , i.e., the number of variables in node X.  \n\nAt each iteration, MCTS-VS first recursively selects a node with larger UCB until a leaf node (denoted as $X$ ), which is regarded as containing important variables. Note that if we optimize the subset $\\mathbf{\\Delta}x_{\\mathbb{A}_{X}}$ of variables represented by the leaf $X$ directly, the variables in $x_{\\mathbb{A}_{X}}$ will have the same score (because they are optimized together), and their relative importance cannot be further distinguished. Thus, MCTS-VS uniformly selects a variable index subset $\\mathbb{M}$ from $\\mathbb{A}_{X}$ at random, and employs BO to optimize $x_{\\mathbb{M}}$ as well as $\\pmb{x}_{\\mathbb{A}_{X}\\setminus\\mathbb{M}}$ ; this process is repeated for several times. After that, the information set $\\mathbb{D}$ will be augmented by the pairs of the selected variable index subset $\\mathbb{M}$ (or $\\mathbb{A}_{X}\\setminus\\mathbb{M})$ and the corresponding sampled points generated by BO. The variable score vector swill be updated using this new $\\mathbb{D}$ . Based on $\\pmb{s}$ , the variable index set $\\mathbb{A}_{X}$ represented by the leaf $X$ will be divided into two disjoint subsets, containing variables with larger and smaller scores (i.e., important and unimportant variables), respectively, and the leaf $X$ will be bifurcated into two child nodes accordingly. Finally, the $v$ values of these two children will be calculated using the variable score vector $\\pmb{s}$ , and backpropagation will be performed to update the $v$ value and the number of visits of the nodes along the current path of the tree.  \n\nMCTS-VS can be equipped with any specific BO optimizer, resulting in the concrete algorithm MCTS-VS-BO, where BO is used to optimize the selected subsets of variables during the running of MCTS-VS. Compared with LA-MCTS [ 40 ], MCTS-VS applies MCTS to partition the variables instead of the search space, and thus can be more scalable. Compared with the previous variable selection method Dropout [ 21 ], MCTS-VS can select important variables automatically instead of randomly selecting a fixed number of variables in each iteration. Next we introduce it in detail.\n\n# 3.1 Details of MCTS-VS\nThe procedure of MCTS-VS is described in Algorithm 1. In line 1, it first initializes the information set $\\mathbb{D}$ . In particular, a variable index subset $\\mathbb{M}_{i}$ is randomly sampled from $[D]$ , and the Latin hypercube sam ] is u nerate t d as $\\mathcal{D}_{i}$ and $\\mathcal{D}_{\\overline{{i}}}$ ) of $N_{s}$ points to f the two pairs of times, resulting i $(\\mathbb{M}_{i},\\mathcal{D}_{i})$ he initial Dand $\\mathbb{D}=\\{(\\mathbb{M}_{i},\\mathcal{D}_{i}),(\\bar{\\mathbb{M}}_{i},\\mathcal{D}_{\\bar{i}})\\}_{i=1}^{N_{v}}$ $(\\bar{\\mathbb{M}}_{i},\\bar{D_{i}})$ D, where $\\bar{\\mathbb{M}}_{i}=[\\dot{D}]\\setminus\\mathbb{M}_{i}$ \\. The variable score vector . This process will be repeated for $\\pmb{s}$ is calculated $N_{v}$ using this initial Din line 3, and the Monte Carlo tree is initialized in line 4 by adding only a root node, whose $v$ value is calculated according to $\\pmb{s}$ and number of visits is 0. MCTS-VS uses the variable $t$ to record he numbe ns it has performed, and thus $t$ is set to $2\\,\\times\\,N_{v}\\,\\times\\,N_{s}$ in line 5 as the initial Dcontains $2\\,\\times\\,N_{v}\\,\\times\\,N_{s}$ \u00d7\u00d7sampled points in total.  \n\nIn each iteration (i.e., lines 7\u201328) of MCTS-VS, it selects a leaf node $X$ by UCB in line 10, and optimizes the variables (i.e., $x_{\\mathbb{A}_{X}}$ ) represented by $X$ in lines 13\u201323. Note that to measure the relative importance of variables in $\\mathbf{\\Delta}x_{\\mathbb{A}_{X}}$ , MCTS-VS optimizes different subsets of variables of $\\mathbf{\\Delta}x_{\\mathbb{A}_{X}}$"}], "task_step_question_answer": "29d7d9b0-9dd0-42e2-a847-f9e6d0c0b37f:\u300c\u5728MCTS\u4e0ePRM\u7ed3\u5408\u7684\u6846\u67b6\u4e2d\uff0c\u6539\u8fdb\u7684\u9009\u62e9\u7b56\u7565\u548c\u53c2\u6570\u8c03\u6574\u673a\u5236\u4e3b\u8981\u5305\u62ec\u4ee5\u4e0b\u51e0\u4e2a\u65b9\u9762\uff1a\n\n1. **MCTS\u7684\u9009\u62e9\u7b56\u7565\u6539\u8fdb**\uff1a\n   - **UCB\u516c\u5f0f\u7684\u8c03\u6574**\uff1a\u901a\u8fc7\u8c03\u6574UCB\u516c\u5f0f\u4e2d\u7684\u63a2\u7d22\u53c2\u6570 \\( C_p \\)\uff0c\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\\( C_p \\) \u8bbe\u7f6e\u57281%\u523010%\u4e4b\u95f4\u65f6\uff0cMCTS\u7684\u7a33\u5b9a\u6027\u6700\u4f73\u3002\n   - **\u8282\u70b9\u5206\u88c2\u4e0e\u91cd\u65b0\u521d\u59cb\u5316**\uff1a\u5f15\u5165\u9608\u503c \\( N_{split} \\) \u548c \\( N_{bad} \\) \u6765\u63a7\u5236\u8282\u70b9\u7684\u5206\u88c2\u548c\u91cd\u65b0\u521d\u59cb\u5316\u3002\u8f83\u5c0f\u7684 \\( N_{split} \\) \u53ef\u4ee5\u51cf\u5c11\u8ba1\u7b97\u65f6\u95f4\uff0c\u800c\u9002\u4e2d\u7684 \\( N_{bad} \\) \u53ef\u4ee5\u907f\u514d\u8fc7\u5ea6\u4f18\u5316\u4e0d\u91cd\u8981\u7684\u53d8\u91cf\u3002\n   - **\u53d8\u91cf\u8bc4\u5206\u673a\u5236**\uff1a\u901a\u8fc7\u53d8\u91cf\u8bc4\u5206\u5411\u91cf \\( s \\) \u6765\u8861\u91cf\u6bcf\u4e2a\u53d8\u91cf\u7684\u91cd\u8981\u6027\uff0c\u5e76\u57fa\u4e8e\u6b64\u8fdb\u884c\u8282\u70b9\u6269\u5c55\u548c\u9009\u62e9\u3002\u8fd9\u79cd\u673a\u5236\u80fd\u591f\u66f4\u51c6\u786e\u5730\u8bc6\u522b\u91cd\u8981\u53d8\u91cf\uff0c\u63d0\u5347\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u3002\n\n2. **PRM\u7684\u53c2\u6570\u8c03\u6574\u673a\u5236**\uff1a\n   - **\u4f18\u5316\u7b97\u6cd5\u7684\u9009\u62e9**\uff1a\u5728MCTS-VS\u4e2d\uff0c\u5bf9\u6bd4\u4e86\u968f\u673a\u641c\u7d22\uff08RS\uff09\u3001\u8d1d\u53f6\u65af\u4f18\u5316\uff08BO\uff09\u548cTuRBO\u7b49\u4f18\u5316\u7b97\u6cd5\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u9ad8\u7ef4\u95ee\u9898\u4e2d\uff0cTuRBO\u8868\u73b0\u66f4\u4f18\uff0c\u800c\u5728\u4f4e\u7ef4\u95ee\u9898\u4e2d\uff0c\u968f\u673a\u641c\u7d22\u66f4\u4e3a\u9ad8\u6548\u3002\n   - **\u586b\u5145\u7b56\u7565**\uff1a\u91c7\u7528\u6700\u4f73 \\( k \\) \u7b56\u7565\uff08best\\(k\\)\uff09\u6765\u586b\u5145\u672a\u9009\u62e9\u7684\u53d8\u91cf\uff0c\u5229\u7528\u5386\u53f2\u4fe1\u606f\u8fdb\u884c\u586b\u5145\uff0c\u907f\u514d\u8fc7\u5ea6\u63a2\u7d22\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u6700\u4f73 \\( k \\) \u7b56\u7565\u5728\u63d0\u5347\u7b97\u6cd5\u7a33\u5b9a\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002\n\n3. **\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u7a33\u5b9a\u6027\u63d0\u5347**\uff1a\n   - **\u6e38\u620fAI**\uff1a\u5728\u6e38\u620fAI\u4e2d\uff0cMCTS\u7684\u6539\u8fdb\u7b56\u7565\u80fd\u591f\u66f4\u9ad8\u6548\u5730\u641c\u7d22\u6e38\u620f\u6811\uff0c\u63d0\u5347\u51b3\u7b56\u7684\u7a33\u5b9a\u6027\u548c\u51c6\u786e\u6027\u3002\n   - **\u8def\u5f84\u89c4\u5212**\uff1a\u5728\u8def\u5f84\u89c4\u5212\u4e2d\uff0cPRM\u7684\u53c2\u6570\u8c03\u6574\u673a\u5236\u80fd\u591f\u66f4\u6709\u6548\u5730\u5904\u7406\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u7684\u8def\u5f84\u641c\u7d22\u95ee\u9898\uff0c\u63d0\u5347\u7b97\u6cd5\u7684\u9c81\u68d2\u6027\u3002\n   - **\u63a8\u8350\u7cfb\u7edf**\uff1a\u5728\u63a8\u8350\u7cfb\u7edf\u4e2d\uff0cMCTS\u4e0ePRM\u7684\u7ed3\u5408\u80fd\u591f\u66f4\u51c6\u786e\u5730\u8bc6\u522b\u7528\u6237\u504f\u597d\uff0c\u63d0\u5347\u63a8\u8350\u7684\u7a33\u5b9a\u6027\u548c\u4e2a\u6027\u5316\u6548\u679c\u3002\n\n4. **\u4e0d\u540c\u5e94\u7528\u573a\u666f\u4e2d\u7684\u8868\u73b0**\uff1a\n   - **\u76f8\u4f3c\u6027**\uff1a\u5728\u4e0d\u540c\u5e94\u7528\u573a\u666f\u4e2d\uff0c\u6539\u8fdb\u7684MCTS\u9009\u62e9\u7b56\u7565\u548cPRM\u53c2\u6570\u8c03\u6574\u673a\u5236\u5747\u8868\u73b0\u51fa\u76f8\u4f3c\u7684\u7a33\u5b9a\u6027\u63d0\u5347\u6548\u679c\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u7ef4\u590d\u6742\u95ee\u9898\u4e2d\u3002\n   - **\u5dee\u5f02\u6027**\uff1a\u7531\u4e8e\u4e0d\u540c\u5e94\u7528\u573a\u666f\u7684\u95ee\u9898\u7279\u6027\u4e0d\u540c\uff0c\u5177\u4f53\u7684\u53c2\u6570\u8bbe\u7f6e\u548c\u4f18\u5316\u7b56\u7565\u53ef\u80fd\u9700\u8981\u6839\u636e\u5177\u4f53\u573a\u666f\u8fdb\u884c\u8c03\u6574\uff0c\u4ee5\u8fbe\u5230\u6700\u4f73\u6548\u679c\u3002\n\n\u7efc\u4e0a\u6240\u8ff0\uff0c\u901a\u8fc7\u6539\u8fdbMCTS\u7684\u9009\u62e9\u7b56\u7565\u548cPRM\u7684\u53c2\u6570\u8c03\u6574\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\uff0c\u5e76\u5728\u4e0d\u540c\u5e94\u7528\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u76f8\u4f3c\u7684\u7a33\u5b9a\u6027\u63d0\u5347\u6548\u679c\u3002\u300d", "ref_task_step_id": ""}, "__type__": "task_step_node"}, "c438cc1b-a630-4359-b8d9-abeaf797d240": {"__data__": {"id_": "c438cc1b-a630-4359-b8d9-abeaf797d240", "metadata": {}, "relationships": {}, "hash": "", "start_task_context": "MCTS\u5728PRM\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u7684\u5e94\u7528\u63a2\u7d22\u7efc\u8ff0", "aemo_representation_context": "### Step 1: \u5206\u6790\u8fd1\u51e0\u5e74\u7814\u7a76\u9886\u57df\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba\n\n**\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09**\uff1a\n- **\u57fa\u672c\u539f\u7406**\uff1aMCTS\u662f\u4e00\u79cd\u7528\u4e8e\u51b3\u7b56\u8fc7\u7a0b\u7684\u542f\u53d1\u5f0f\u641c\u7d22\u7b97\u6cd5\uff0c\u5e7f\u6cdb\u5e94\u7528\u4e8e\u6e38\u620fAI\u3001\u8def\u5f84\u89c4\u5212\u7b49\u9886\u57df\u3002\u5176\u6838\u5fc3\u601d\u60f3\u662f\u901a\u8fc7\u6a21\u62df\u591a\u6b21\u968f\u673a\u6e38\u620f\u6765\u8bc4\u4f30\u4e0d\u540c\u51b3\u7b56\u7684\u4f18\u52a3\u3002\n- **\u6280\u672f\u6846\u67b6**\uff1a\u4e3b\u8981\u5305\u62ec\u9009\u62e9\uff08Selection\uff09\u3001\u6269\u5c55\uff08Expansion\uff09\u3001\u6a21\u62df\uff08Simulation\uff09\u548c\u56de\u6eaf\uff08Backpropagation\uff09\u56db\u4e2a\u6b65\u9aa4\u3002\n\n**\u504f\u597d\u7b56\u7565\u6a21\u578b\uff08PRM\uff09**\uff1a\n- **\u57fa\u672c\u539f\u7406**\uff1aPRM\u65e8\u5728\u901a\u8fc7\u5b66\u4e60\u7528\u6237\u7684\u504f\u597d\u6765\u4f18\u5316\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5e38\u7528\u4e8e\u63a8\u8350\u7cfb\u7edf\u3001\u4e2a\u6027\u5316\u670d\u52a1\u7b49\u3002\n- **\u6280\u672f\u6846\u67b6**\uff1a\u901a\u5e38\u5305\u62ec\u504f\u597d\u5efa\u6a21\u3001\u7b56\u7565\u4f18\u5316\u548c\u6a21\u578b\u5fae\u8c03\u7b49\u73af\u8282\u3002\n\n**\u65b9\u6cd5\u8bba**\uff1a\n- **\u96c6\u6210\u5b66\u4e60**\uff1a\u5c06MCTS\u4e0ePRM\u7ed3\u5408\uff0c\u5229\u7528MCTS\u7684\u641c\u7d22\u80fd\u529b\u4f18\u5316PRM\u7684\u51b3\u7b56\u8fc7\u7a0b\u3002\n- **\u5f3a\u5316\u5b66\u4e60**\uff1a\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u673a\u5236\u4e0d\u65ad\u8c03\u6574PRM\u7684\u53c2\u6570\uff0c\u4ee5\u9002\u5e94\u52a8\u6001\u73af\u5883\u3002\n\n### Step 2: \u7814\u7a76\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u4e3b\u8981\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0e\u53d8\u4f53\n\n**\u5e94\u7528\u573a\u666f**\uff1a\n- **\u6e38\u620fAI**\uff1aMCTS\u4e0ePRM\u7ed3\u5408\u7528\u4e8e\u6e38\u620f\u4e2d\u7684\u667a\u80fd\u51b3\u7b56\uff0c\u5982AlphaGo\u3002\n- **\u8def\u5f84\u89c4\u5212**\uff1a\u5728\u673a\u5668\u4eba\u5bfc\u822a\u548c\u81ea\u52a8\u9a7e\u9a76\u4e2d\uff0c\u5229\u7528MCTS\u4f18\u5316\u8def\u5f84\u9009\u62e9\u3002\n- **\u63a8\u8350\u7cfb\u7edf**\uff1a\u901a\u8fc7PRM\u5b66\u4e60\u7528\u6237\u504f\u597d\uff0c\u7ed3\u5408MCTS\u8fdb\u884c\u66f4\u7cbe\u51c6\u7684\u63a8\u8350\u3002\n\n**\u53d8\u4f53**\uff1a\n- **\u6df1\u5ea6MCTS**\uff1a\u7ed3\u5408\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff0c\u63d0\u5347MCTS\u7684\u51b3\u7b56\u80fd\u529b\u3002\n- **\u591a\u6a21\u6001PRM**\uff1a\u878d\u5408\u591a\u79cd\u6570\u636e\u6e90\uff08\u5982\u6587\u672c\u3001\u56fe\u50cf\uff09\uff0c\u589e\u5f3aPRM\u7684\u6cdb\u5316\u80fd\u529b\u3002\n\n### Step 3: \u8bc4\u4f30\u5b66\u672f\u754c\u7684\u6280\u672f\u8fdb\u6b65\u4e0e\u5c40\u9650\u6027\n\n**\u6280\u672f\u8fdb\u6b65**\uff1a\n- **\u6027\u80fd\u63d0\u5347**\uff1aMCTS\u4e0ePRM\u7ed3\u5408\u663e\u8457\u63d0\u5347\u4e86\u51b3\u7b56\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002\n- **\u6cdb\u5316\u80fd\u529b**\uff1a\u591a\u6a21\u6001PRM\u5728\u5904\u7406\u591a\u6837\u5316\u6570\u636e\u65f6\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002\n\n**\u5c40\u9650\u6027**\uff1a\n- **\u8ba1\u7b97\u590d\u6742\u5ea6**\uff1aMCTS\u5728\u5927\u89c4\u6a21\u95ee\u9898\u4e2d\u8ba1\u7b97\u91cf\u5927\uff0c\u6548\u7387\u6709\u5f85\u63d0\u5347\u3002\n- **\u6570\u636e\u4f9d\u8d56**\uff1aPRM\u7684\u6027\u80fd\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u9ad8\u8d28\u91cf\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u6570\u636e\u504f\u5dee\u95ee\u9898\u4f9d\u7136\u5b58\u5728\u3002\n\n### Step 4: \u63a2\u8ba8\u8ba1\u7b97\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\n\n**\u9002\u7528\u6027**\uff1a\n- **\u591a\u9886\u57df\u5e94\u7528**\uff1aMCTS+PRM\u6846\u67b6\u5728\u6e38\u620f\u3001\u8def\u5f84\u89c4\u5212\u3001\u63a8\u8350\u7cfb\u7edf\u7b49\u591a\u4e2a\u9886\u57df\u8868\u73b0\u51fa\u826f\u597d\u7684\u9002\u7528\u6027\u3002\n- **\u591a\u6a21\u6001\u6570\u636e**\uff1a\u591a\u6a21\u6001PRM\u80fd\u591f\u5904\u7406\u591a\u79cd\u7c7b\u578b\u7684\u6570\u636e\uff0c\u9002\u7528\u4e8e\u590d\u6742\u5e94\u7528\u573a\u666f\u3002\n\n**\u6cdb\u5316\u80fd\u529b**\uff1a\n- **\u8de8\u9886\u57df\u6cdb\u5316**\uff1a\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\uff0cMCTS+PRM\u6846\u67b6\u5728\u4e0d\u540c\u9886\u57df\u95f4\u5c55\u73b0\u51fa\u8f83\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002\n- **\u52a8\u6001\u73af\u5883\u9002\u5e94**\uff1a\u5f3a\u5316\u5b66\u4e60\u673a\u5236\u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u9002\u5e94\u52a8\u6001\u53d8\u5316\u7684\u73af\u5883\u3002\n\n### Step 5: \u5206\u6790\u6700\u65b0\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027\n\n**\u7a33\u5b9a\u6027**\uff1a\n- **\u7b97\u6cd5\u4f18\u5316**\uff1a\u901a\u8fc7\u6539\u8fdbMCTS\u7684\u9009\u62e9\u7b56\u7565\u548cPRM\u7684\u53c2\u6570\u8c03\u6574\u673a\u5236\uff0c\u63d0\u5347\u4e86\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u3002\n- **\u9c81\u68d2\u6027\u589e\u5f3a**\uff1a\u5f15\u5165\u5bf9\u6297\u8bad\u7ec3\u548c\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\uff0c\u63d0\u9ad8\u6a21\u578b\u5728\u590d\u6742\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u6027\u3002\n\n**\u5bb9\u9519\u6027**\uff1a\n- **\u9519\u8bef\u5904\u7406**\uff1a\u5728\u6a21\u62df\u548c\u56de\u6eaf\u9636\u6bb5\u5f15\u5165\u5bb9\u9519\u673a\u5236\uff0c\u51cf\u5c11\u9519\u8bef\u51b3\u7b56\u7684\u5f71\u54cd\u3002\n- **\u5927\u89c4\u6a21\u6570\u636e\u9002\u5e94\u6027**\uff1a\u4f18\u5316\u7b97\u6cd5\u7ed3\u6784\uff0c\u63d0\u5347\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\u7684\u5904\u7406\u80fd\u529b\u3002\n\n### Step 6: \u8bc4\u4f30\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218\n\n**\u672a\u6765\u7814\u7a76\u65b9\u5411**\uff1a\n- **\u9ad8\u6548\u8ba1\u7b97**\uff1a\u7814\u7a76\u66f4\u9ad8\u6548\u7684MCTS\u7b97\u6cd5\uff0c\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002\n- **\u6570\u636e\u589e\u5f3a**\uff1a\u63a2\u7d22\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u51cf\u5c11PRM\u5bf9\u9ad8\u8d28\u91cf\u6570\u636e\u7684\u4f9d\u8d56\u3002\n\n**\u6311\u6218**\uff1a\n- **\u8de8\u6a21\u6001\u878d\u5408**\uff1a\u5982\u4f55\u6709\u6548\u878d\u5408\u591a\u79cd\u6a21\u6001\u6570\u636e\uff0c\u63d0\u5347\u6a21\u578b\u7684\u7efc\u5408\u6027\u80fd\u3002\n- **\u52a8\u6001\u73af\u5883\u9002\u5e94**\uff1a\u5728\u52a8\u6001\u73af\u5883\u4e0b\uff0c\u5982\u4f55\u4fdd\u6301\u6a21\u578b\u7684\u7a33\u5b9a\u6027\u548c\u9ad8\u6548\u6027\u3002\n\n**\u63a8\u52a8\u540e\u7eed\u7814\u7a76**\uff1a\n- **\u65b0\u95ee\u9898\u63d0\u51fa**\uff1a\u5982\u591a\u6a21\u6001\u6570\u636e\u4e0b\u7684\u504f\u597d\u5efa\u6a21\u3001\u52a8\u6001\u73af\u5883\u4e0b\u7684\u51b3\u7b56\u4f18\u5316\u7b49\u3002\n- **\u7814\u7a76\u5207\u5165\u70b9**\uff1a\u4ece\u7b97\u6cd5\u4f18\u5316\u3001\u6570\u636e\u589e\u5f3a\u3001\u8de8\u6a21\u6001\u878d\u5408\u7b49\u591a\u4e2a\u89d2\u5ea6\u5207\u5165\uff0c\u63a8\u52a8\u6280\u672f\u8fdb\u6b65\u3002\n\n### \u603b\u7ed3\n\n\u8fd1\u5e74\u6765\uff0cMCTS\u5728PRM\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u7684\u5e94\u7528\u63a2\u7d22\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4e3b\u8981\u4f53\u73b0\u5728\u6280\u672f\u6846\u67b6\u7684\u5b8c\u5584\u3001\u5e94\u7528\u573a\u666f\u7684\u62d3\u5c55\u3001\u6027\u80fd\u7684\u63d0\u5347\u4ee5\u53ca\u7a33\u5b9a\u6027\u548c\u5bb9\u9519\u6027\u7684\u589e\u5f3a\u3002\u7136\u800c\uff0c\u4ecd\u5b58\u5728\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u3001\u6570\u636e\u4f9d\u8d56\u6027\u5f3a\u7b49\u5c40\u9650\u6027\u3002\u672a\u6765\u7684\u7814\u7a76\u5e94\u5173\u6ce8\u9ad8\u6548\u8ba1\u7b97\u3001\u6570\u636e\u589e\u5f3a\u548c\u8de8\u6a21\u6001\u878d\u5408\u7b49\u65b9\u5411\uff0c\u4ee5\u5e94\u5bf9\u52a8\u6001\u73af\u5883\u4e0b\u7684\u51b3\u7b56\u4f18\u5316\u7b49\u6311\u6218\uff0c\u63a8\u52a8\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002", "task_step_name": "\u7a33\u5b9a\u6027", "task_step_description": "\u901a\u8fc7\u6539\u8fdbMCTS\u7684\u9009\u62e9\u7b56\u7565\u548cPRM\u7684\u53c2\u6570\u8c03\u6574\u673a\u5236\uff0c\u63d0\u5347\u4e86\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u3002", "task_step_level": "4>1", "task_step_question": "### \u95ee\u9898\u63d0\u51fa\n\n\u5728MCTS\u4e0ePRM\u7ed3\u5408\u7684\u5e94\u7528\u4e2d\uff0c\u5982\u4f55\u901a\u8fc7\u6539\u8fdbMCTS\u7684\u9009\u62e9\u7b56\u7565\u548cPRM\u7684\u53c2\u6570\u8c03\u6574\u673a\u5236\u6765\u8fdb\u4e00\u6b65\u63d0\u5347\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\uff1f\u5177\u4f53\u6765\u8bf4\uff0c\u6709\u54ea\u4e9b\u5177\u4f53\u7684\u6280\u672f\u624b\u6bb5\u6216\u65b9\u6cd5\u53ef\u4ee5\u7528\u4e8e\u4f18\u5316MCTS\u7684\u9009\u62e9\u7b56\u7565\u548cPRM\u7684\u53c2\u6570\u8c03\u6574\uff0c\u4ee5\u786e\u4fdd\u5728\u52a8\u6001\u548c\u590d\u6742\u73af\u5883\u4e0b\uff0c\u6a21\u578b\u80fd\u591f\u4fdd\u6301\u8f83\u9ad8\u7684\u7a33\u5b9a\u6027\u548c\u9c81\u68d2\u6027\uff1f\u6b64\u5916\uff0c\u8fd9\u4e9b\u6539\u8fdb\u63aa\u65bd\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff08\u5982\u6e38\u620fAI\u3001\u8def\u5f84\u89c4\u5212\u6216\u63a8\u8350\u7cfb\u7edf\uff09\u7684\u6548\u679c\u5982\u4f55\uff0c\u662f\u5426\u5b58\u5728\u6f5c\u5728\u7684\u5c40\u9650\u6027\u6216\u6311\u6218\uff1f", "task_step_question_context": [{"ref_id": "455026805323333778", "chunk_id": "1", "score": 0.345703125, "text": "# 2.2 Acceleration of MCTS\nMCTS-based methods have proved their strong capability of solving complex games or tasks. However, the high computational cost of MCTS hinders its application to some real-time and more general scenarios. Therefore, numerous works are devoted to accelerating MCTS. For example, to make the selection stage more effective, some heuristic pruning methods [ 14 ,33 ,29 ,1 ,2 ] aim to reduce the width and depth of the search tree with some heuristic functions. Furthermore, for more efficient evaluations, Lorentz [ 22 ] proposed early playout termination of MCTS (MCTS-EPT) to stop the random playouts early and use an evaluation function to assess win or loss. Moreover, Hsueh et al. [18 ] applied MCTS-EPT to the Chinese dark chess and proved its effectiveness. Afterward, similar ideas have been applied in the evaluation stage of AlphaGoZero [ 32 ] and later MCTS-based methods [31 ,27 ,34 ]. They evaluate the $Q$ -values through a learnable evaluation network instead of running playouts to the end. Grill et al. [15 ] propose a novel regularized policy optimization method based on AlphaZero to decrease the search budget of MCTS, which is from the optimization perspective. Danihelka et al. [10 ] propose a policy improvement algorithm based on sampling actions without replacement, named Gumbel trick to achieve better performance when planning with few simulations. However, these methods mentioned above focus on the specific stage of the search iteration or reduce the total budget through pruning and optimization methods, which are orthogonal to us. And few works targets at the search loop. Lan et al. [21 ] propose DS-MCTS, which defines the uncertainty of MCTS and approximates it by extra DNNs with specific features for board games in training. During the evaluation, DS-MCTS will check periodically and stop the search if the state is certain.\n\n# 3 Background\nThe AlphaGo series of work [ 30 ,32 ,31 ,27 ] are all MCTS-based reinforcement learning algorithms. Those algorithms assume the environment transition dynamics are known or learn the environment dynamics. Based on the dynamics, they use the Monte-Carlo tree search (MCTS) as the policy improvement operator. I.e., taking in the current policy, MCTS returns a better policy with the search algorithm. The systematic search allows the MCTS-based RL algorithm to quickly improve the policy and perform much better in the setting where heavy reasoning is required.\n\n# 3.1 MCTS\nThis part briefly introduces the MCTS method implemented in reinforcement learning applications. As mentioned in the related works, modern MCTS-based RL algorithms include four stages in the search loop, namely selection, expansion, evaluation, and backpropagation.  \n\nMCTS takes in the current states and generates a policy after the search loop of $N$ iterations. Here $N$ is a constant number of iterations set by the designer, regarded as the total budget. In the selection stage of each iteration, an action will be selected by maximizing over UCB. Specifically, AlphaZero [31 ] and MuZero [ 27 ] are developed based on a variant of UCB, P-UCT [ 25 ] and have achieved great success on board games and Atari games. The formula of P-UCT is the Eq (1):  \n\n$$\na^{k}=\\arg\\operatorname*{max}_{a\\in\\mathcal{A}}Q(s,a)+P(s,a)\\frac{\\sqrt{\\sum_{b\\in\\mathcal{A}}N(s,b)}}{1+N(s,a)}(c_{1}+\\log((\\sum_{b\\in\\mathcal{A}}N(s,b)+c_{2}+1)/c_{2})),\n$$  \n\nwhere $k$ is the index of iteration, $\\boldsymbol{\\mathcal{A}}$ is the acti $Q(s,a)$ is the estimated Q-value, $P(s,a)$ is the policy prior obtained from neural networks, $N(s,a)$ is the visitations to select the action a from the state $s$ and $c_{1},c_{2}$ are hyper-parameters. The output of MCTS is the visitation of each action of the root node. After $N$ search iterations, the final policy $\\pi(s)$ is defined as the normalized root visitation distribution simplification, we use $\\pi_{N}(s)$ $\\pi_{k}$ in place of , where $\\begin{array}{r}{\\pi_{k}(s,a)=N(s,a)/\\sum_{b\\in\\mathcal{A}}N(s,b)=N(s,a)/k,a\\in\\mathcal{A}}\\end{array}$ $\\pi_{k}(s)$ sometimes. And the detailed procedure of MCTS is \u2208A . For introduced in Appendix. In our method, we propose to approximate the final policy $\\pi_{N}(s)$ with $\\hat{\\pi}_{k}(s)$ ,which we name as a virtual expanded policy, through a new expansion method and a termination rule. In this way, the number of iterations in MCTS can be reduced from $N$ to $k$ .\n\n# 3.2 Computation Requirement\nMost of the computations in MCTS-based RL are in the MCTS procedure. Each action taken by MCTS requires $N$ times neural network evaluations, where $N$ is a constant number of iterations in the search loop. Traditional RL algorithms, such as PPO [ 28 ] or DQN [ 23 ], only need a single neural network evaluation per action. Thus, MCTS-based RL is roughly $N$ times computationally more expensive than traditional RL algorithms. In practice, training a single Atari game needs 12 hours of computation time on 40 TPUs [ 27 ]. The computation need is roughly two orders of magnitude more than traditional RL algorithms [28], although the final performance of MuZero is much better.\n\n# 4 Method\nWe aim to spend more search time on harder states and less on easier states. Intuitively, human knows when to make a quick decision or a slow decision under different circumstances. Unfortunately, this situation-aware behavior is absent in current MCTS algorithms. Therefore, we propose an MCTS variant that terminates the search iteration adaptively. It consists of two components: a novel expansion method named virtual expansion to estimate the final visitation based on the current partial tree; a termination rule that decides when to terminate based on the hardness of the current scenario. And we will display the adaptive mechanism through visualizations in Section 5.5."}, {"ref_id": "454984236379937352", "chunk_id": "11", "score": 0.298828125, "text": "# 5 Experiment\nTo examine the performance of MCTS-VS, we conduct experiments on different tasks, including synthetic functions, NAS-bench problems and MuJoCo locomotion tasks, to compare MCTS-VS with other black-box optimization methods. For MCTS-VS, we use the same hyper-parameters except $C_{p}$ , which is used for calculating UCB in Eq. (1). For Dropout and embedding-based methods, we set the parameter $d$ to the number of valid dimensions for synthetic functions, and a reasonable value for real-world problems. The hyper-parameters of the same components of different methods are set to the same. We use five identical random seeds (2021\u20132025) for all problems and methods. More details about the settings can be found in Appendix C. Our code is available at https://github.com/lamda-bbo/MCTS-VS ."}, {"ref_id": "454984278060050946", "chunk_id": "3", "score": 0.2734375, "text": "# DSensitivity Analysis of Hyper-parameters of MCTS-VS\nWe provide further studies to examine the influence of the hyper-parameters of MCTS-VS, including the employed optimization algorithm for optimizing the selected variables in each iteration, the \u201cfillin\u201d strategy, the hyper-parameter $k$ used in the best$k$ strategy, the hyper-parameter $C_{p}$ for calculating UCB in Eq. (1), the number $2\\,\\times\\,N_{v}\\,\\times\\,N_{s}$ sampled data in each iteration, the threshold $N_{b a d}$ for re-initializing a tree, and the threshold $N_{s p l i t}$ for splitting a tree node.  \n\nThe optimization algorithm is employed by MCTS-VS to optimize the selected variables in each iteration. We compare three different optimization algorithms, i.e., random search (RS), BO and TuRBO. First, we conduct experiments similar to \u201cEffectiveness of Variable Selection\u201d in Section 5.1, to show the effectiveness of MCTS-VS even when equipped with RS. Figure 6 shows that MCTSVS-RS is better than Dropout-RS and RS, revealing the advantage of MCTS-VS.  \n\n  \nFigure 6: Effectiveness of MCTS-VS when equipped with RS.  \n\nNext we compare the performance of MCTS-VS equipped with RS, BO and TuRBO, by experiments on the Hartmann functions with increasing ratio of valid variables. Hartmann 6 _500 has 6 valid variables. Hartmann 6 _5 _500 is generated by mixing 5 Hartmann 6 functions as Hartmann 6 $(\\pmb{x}_{1:6})+$ Hartmann 6 $\\backslash(\\pmb{x}_{7:12})+\\cdot\\cdot\\cdot+\\mathrm{Hartmann6}(\\pmb{x}_{25:30})$ , and appending 470 unrelated dimensions, where $\\pmb{x}_{i:j}$ denotes the $i$ -th to j-th variables. Hartmann 6 _10 _500 is generated alike. Thus, Hartmann 6 _5 _500 and Hartmann 6 _10 _500 have 30 and 60 valid variables, respectively. The results in Figure 7 show that as the ratio of valid variables increases, MCTS-VS-TuRBO gradually surpasses MCTS-VS-RS and MCTS-VS-BO, while MCTS-VS-RS becomes worse and worse. This is expected. If the ratio of valid variables is high, MCTS-VS is more likely to select the valid variables, so it is worth to use the expensive optimization algorithm, e.g., TuRBO, to optimize the selected variables. If the ratio is low, unrelated variables are more likely to be selected most of the time, so using a cheap optimization algorithm would be better. These observations also give us some guidance on selecting optimization algorithms in practice.  \n\n\u201cFill-in\u201d strategy is a basic component of variable selection methods, which influences the quality of the value of unselected variables. We compare the employed best$k$ strategy $(k=20)$ ) with the average best$k$ strategy and the random strategy. The average best$k$ strategy uses the average of the best $k$ data points for the unselected variables, and the random strategy samples the value of an unselected variable from its domain randomly. As shown in Figure 8(a), the random strategy leads to the poor performance of MCTS-VS-BO, which may be because it does not utilize the historical information and leads to over-exploration. The best${\\cdot k}$ strategy utilizes the historical points that have high objective values to fill in the unselected variables, thus behaving much better. The performance of the average strategy is between the best$k$ and random strategies. We recommend using the best$k$ strategy in practice.  \n\nThe hyper-parameter $k$ used in the best$k$ strategy controls the degree of exploitation for the unselected variables. As shown in Figure 8(b), a smaller $k$ encourages exploitation, which results in better performance in the early stage, but easily leads to premature convergence. A larger $k$ encourages exploration and behaves worse in the early stage, but may converge to a better value. We recommend using a larger $k$ if allowing enough evaluations.  \n\n  \nFigure 7: Sensitivity analysis of the optimization algorithm.  \n\n  \nFigure 8: Sensitivity analysis of the \u201cfill-in\u201d strategy and the hyper-parameter $k$ of the best$k$ strategy, using MCTS-VS-BO on Hartmann 6 _300 .  \n\nThe hyper-parameter $C_{p}$ for calculating UCB in Eq. (1) balances the exploration and exploitation of MCTS. As shown in Figure 9, a too small $C_{p}$ leads to relatively worse performance, highlighting the importance of exploration. A too large $C_{p}$ may also lead to over-exploration. But overall MCTSVS is not very sensitive to $C_{p}$ . We recommend setting $C_{p}$ between $1\\%$ and $10\\%$ of the optimum (i.e., max $f({\\boldsymbol{x}}))$ ), which is consistent with that for LA-MCTS [40].  \n\n  \nFigure 9: Sensitivity analysis of the hyper-parameter $C_{p}$ for calculating UCB in Eq. (1), using MCTS-VS-BO on Levy and Hartmann.  \n\nThe number $2\\,\\times\\,N_{v}\\,\\times\\,N_{s}$ of sampled data in ch iteration depends on the batch size $N_{v}$ of variable index subset and the sample batch size $N_{s}$ , and will influence the accuracy of estimating the variable score vector in Eq. (2). If we increase $N_{v}$ and $N_{s}$ , we can calculate the variable score more accurately, but also need more evaluations. Figure 10(a) shows that given the same number of evaluations, MCTS-VS-BO achieves the best performance when $N_{v}=2$ and $N_{s}=3$ . Thus, this setting may be a good choice to balance the accuracy of variable score and the number of evaluations, which is also used throughout the experiments.  \n\nThe threshold $N_{b a d}$ for re-initializing a tree controls the tolerance of selecting bad tree nodes (i.e., nodes containing unimportant variables). A smaller $N_{b a d}$ leads to frequent re-initialization, which can adjust quickly but may cause under-exploitation of the tree. A larger $N_{b a d}$ can make full use of the tree, but may optimize too much on unimportant variables. Figure 10(b) shows that MCTS-VS achieves the best performance when $N_{b a d}=5$ . Thus, we recommend to use this setting, to balance the re-initialization and exploitation of the tree.  \n\nThe threshold $N_{s p l i t}$ for splitting a node. If the number of variables in a node is larger than $N_{s p l i t}$ ,the node can be further partitioned. That is, the parameter $N_{s p l i t}$ controls the least number of variables in a leaf node and thus affects the number of selected variables, which has a direct influence on the wall clock time. Note that MCTS-VS selects a leaf node and optimizes the variables contained by this node in each iteration. The smaller $N_{s p l i t}$ , the shorter the time. Figure 10(c) shows that $N_{s p l i t}$ has little influence on the performance of MCTS-VS-BO, and thus we recommend to set $N_{s p l i t}=3$ to reduce the wall clock time.  \n\n  \nFigure 10: S vity analysis of the number $2\\,\\times\\,N_{v}\\,\\times\\,N_{s}$ pled data in each iteration, the threshold $N_{b a d}$ for re-initializing a tree and the threshold $N_{s p l i t}$ for splitting a node, using MCTS-VS-BO on Hartmann 6 _300 .  \n\nInfluence of the hyper-parameters on the runtime of MCTS-VS. We also provide some intuitive explanation about the influence of the hyper-parameters on the runtime. The threshold $N_{s p l i t}$ for splitting a node has a direct impact on the runtime, because it controls the least number of variables to be optimized in a leaf node. That is, the runtime will increase with $N_{s p l i t}$ . Other parameters may affect the depth of the tree and thus the runtime. For the threshold $N_{b a d}$ for re-initializing a tree, if it is set to a small value, MCTS-VS will re-build the tree frequently and the depth of the tree is small. The shallow nodes have more variables, leading to more runtime to optimize. For the hyper-parameter $C_{p}$ for calculating UCB, if it is set to a large value, the exploration is preferred and MCTS-VS will tend to select the right node (regarded as containing unimportant variables). The tree thus will be re-built freq tly, ding to more runtime. For the number $2\\,\\times\\,N_{v}\\,\\times\\,N_{s}$ of sampled data at each iteration, if $N_{v}$ and $N_{s}$ are set to large values, the depth of the tree will be small given the total number of evaluations, and thus lead to more runtime."}], "task_step_question_answer": "\u5728\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\uff0c\u542f\u53d1\u5f0f\u526a\u679d\u65b9\u6cd5\u53ef\u4ee5\u901a\u8fc7\u5f15\u5165\u66f4\u590d\u6742\u7684\u4ea4\u901a\u6a21\u578b\u6765\u63d0\u5347\u5176\u7a33\u5b9a\u6027\u3002\u4f8b\u5982\uff0c\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5bf9\u4ea4\u901a\u6d41\u91cf\u8fdb\u884c\u9884\u6d4b\uff0c\u53ef\u4ee5\u5728\u6570\u636e\u66f4\u65b0\u5ef6\u8fdf\u7684\u60c5\u51b5\u4e0b\uff0c\u4f9d\u7136\u63d0\u4f9b\u8f83\u4e3a\u51c6\u786e\u7684\u526a\u679d\u51b3\u7b56\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u5f15\u5165\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u673a\u5236\uff0c\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u53ef\u4ee5\u5728\u590d\u6742\u7684\u4ea4\u901a\u573a\u666f\u4e2d\u5171\u4eab\u4fe1\u606f\uff0c\u4ece\u800c\u63d0\u5347\u526a\u679d\u7684\u51c6\u786e\u6027\u548c\u51b3\u7b56\u7684\u7a33\u5b9a\u6027\u3002\u5728\u667a\u80fd\u533b\u7597\u9886\u57df\uff0c\u865a\u62df\u6269\u5c55\u4e0e\u7ec8\u6b62\u89c4\u5219\u53ef\u4ee5\u901a\u8fc7\u5f15\u5165\u6570\u636e\u589e\u5f3a\u6280\u672f\u6765\u51cf\u5c11\u5bf9\u5b9e\u65f6\u6570\u636e\u7684\u4f9d\u8d56\u3002\u4f8b\u5982\uff0c\u5229\u7528\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GAN\uff09\u751f\u6210\u6a21\u62df\u60a3\u8005\u6570\u636e\uff0c\u53ef\u4ee5\u5728\u6570\u636e\u66f4\u65b0\u5ef6\u8fdf\u7684\u60c5\u51b5\u4e0b\uff0c\u4f9d\u7136\u63d0\u4f9b\u8f83\u4e3a\u51c6\u786e\u7684\u6cbb\u7597\u65b9\u6848\u3002\u5728\u91d1\u878d\u9884\u6d4b\u9886\u57df\uff0c\u6b63\u5219\u5316\u7b56\u7565\u4f18\u5316\u53ef\u4ee5\u901a\u8fc7\u5f15\u5165\u52a8\u6001\u53c2\u6570\u8c03\u6574\u673a\u5236\u6765\u5e94\u5bf9\u5e02\u573a\u6ce2\u52a8\u3002\u4f8b\u5982\uff0c\u901a\u8fc7\u5b9e\u65f6\u76d1\u63a7\u5e02\u573a\u6ce2\u52a8\u60c5\u51b5\uff0c\u52a8\u6001\u8c03\u6574\u641c\u7d22\u9884\u7b97\uff0c\u53ef\u4ee5\u5728\u5e02\u573a\u6ce2\u52a8\u5267\u70c8\u7684\u60c5\u51b5\u4e0b\uff0c\u4f9d\u7136\u4fdd\u6301\u9884\u6d4b\u7ed3\u679c\u7684\u7a33\u5b9a\u6027\u3002\u5728\u6e38\u620fAI\u9886\u57df\uff0cGumbel Trick\u65b9\u6cd5\u53ef\u4ee5\u901a\u8fc7\u5f15\u5165\u81ea\u9002\u5e94\u7b56\u7565\u6765\u5e94\u5bf9\u590d\u6742\u591a\u53d8\u7684\u5bf9\u624b\u7b56\u7565\u3002\u4f8b\u5982\uff0c\u901a\u8fc7\u5b9e\u65f6\u5206\u6790\u5bf9\u624b\u7684\u51fa\u724c\u6a21\u5f0f\uff0c\u52a8\u6001\u8c03\u6574\u9884\u6d4b\u7b56\u7565\uff0c\u53ef\u4ee5\u5728\u5bf9\u624b\u9891\u7e41\u6539\u53d8\u7b56\u7565\u7684\u60c5\u51b5\u4e0b\uff0c\u4f9d\u7136\u4fdd\u6301\u8f83\u9ad8\u7684\u9884\u6d4b\u51c6\u786e\u6027\u3002\u5728\u673a\u5668\u4eba\u5bfc\u822a\u9886\u57df\uff0c\u81ea\u9002\u5e94\u7ec8\u6b62\u673a\u5236\u53ef\u4ee5\u901a\u8fc7\u5f15\u5165\u73af\u5883\u611f\u77e5\u6280\u672f\u6765\u63d0\u5347\u5176\u5728\u9ad8\u52a8\u6001\u73af\u5883\u4e2d\u7684\u7a33\u5b9a\u6027\u3002\u4f8b\u5982\uff0c\u901a\u8fc7\u5b9e\u65f6\u611f\u77e5\u73af\u5883\u53d8\u5316\uff0c\u52a8\u6001\u8c03\u6574\u641c\u7d22\u7b56\u7565\uff0c\u53ef\u4ee5\u5728\u73af\u5883\u53d8\u5316\u8fc5\u901f\u7684\u60c5\u51b5\u4e0b\uff0c\u4f9d\u7136\u4fdd\u6301\u8def\u5f84\u89c4\u5212\u7684\u51c6\u786e\u6027\u3002\u5728\u65e0\u4eba\u673a\u8def\u5f84\u89c4\u5212\u9886\u57df\uff0c\u4f18\u5316UCB\u516c\u5f0f\u4e2d\u7684\u8d85\u53c2\u6570\u53ef\u4ee5\u901a\u8fc7\u5f15\u5165\u81ea\u52a8\u5316\u8c03\u4f18\u6280\u672f\u6765\u51cf\u5c11\u7cfb\u7edf\u7684\u590d\u6742\u6027\u3002\u4f8b\u5982\uff0c\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u81ea\u52a8\u8c03\u6574\u8d85\u53c2\u6570\uff0c\u53ef\u4ee5\u5728\u65e0\u4eba\u673a\u96c6\u7fa4\u4efb\u52a1\u4e2d\uff0c\u4f9d\u7136\u4fdd\u6301\u8def\u5f84\u89c4\u5212\u7684\u51c6\u786e\u6027\u3002\u5728\u89c6\u9891\u63a8\u8350\u7cfb\u7edf\u9886\u57df\uff0c\u51cf\u5c11\u6bcf\u6b21\u8fed\u4ee3\u7684\u91c7\u6837\u6570\u636e\u91cf\u53ef\u4ee5\u901a\u8fc7\u5f15\u5165\u7528\u6237\u5174\u8da3\u9884\u6d4b\u6a21\u578b\u6765\u63d0\u5347\u5176\u7a33\u5b9a\u6027\u3002\u4f8b\u5982\uff0c\u901a\u8fc7\u5b9e\u65f6\u9884\u6d4b\u7528\u6237\u5174\u8da3\u53d8\u5316\uff0c\u52a8\u6001\u8c03\u6574\u91c7\u6837\u6570\u636e\u91cf\uff0c\u53ef\u4ee5\u5728\u7528\u6237\u5174\u8da3\u5feb\u901f\u53d8\u5316\u7684\u60c5\u51b5\u4e0b\uff0c\u4f9d\u7136\u4fdd\u6301\u63a8\u8350\u5185\u5bb9\u7684\u51c6\u786e\u6027\u3002", "ref_task_step_id": ""}, "__type__": "task_step_node"}, "a2b27547-c4b3-4bf8-ae68-c913858a2752": {"__data__": {"id_": "a2b27547-c4b3-4bf8-ae68-c913858a2752", "metadata": {}, "relationships": {}, "hash": "", "start_task_context": "MCTS\u5728PRM\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u7684\u5e94\u7528\u63a2\u7d22\u7efc\u8ff0", "aemo_representation_context": "### Step 1: \u5206\u6790\u8fd1\u51e0\u5e74\u7814\u7a76\u9886\u57df\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba\n\n**\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09**\uff1a\n- **\u57fa\u672c\u539f\u7406**\uff1aMCTS\u662f\u4e00\u79cd\u7528\u4e8e\u51b3\u7b56\u8fc7\u7a0b\u7684\u542f\u53d1\u5f0f\u641c\u7d22\u7b97\u6cd5\uff0c\u5e7f\u6cdb\u5e94\u7528\u4e8e\u6e38\u620fAI\u3001\u8def\u5f84\u89c4\u5212\u7b49\u9886\u57df\u3002\u5176\u6838\u5fc3\u601d\u60f3\u662f\u901a\u8fc7\u6a21\u62df\u591a\u6b21\u968f\u673a\u6e38\u620f\u6765\u8bc4\u4f30\u4e0d\u540c\u51b3\u7b56\u7684\u4f18\u52a3\u3002\n- **\u6280\u672f\u6846\u67b6**\uff1a\u4e3b\u8981\u5305\u62ec\u9009\u62e9\uff08Selection\uff09\u3001\u6269\u5c55\uff08Expansion\uff09\u3001\u6a21\u62df\uff08Simulation\uff09\u548c\u56de\u6eaf\uff08Backpropagation\uff09\u56db\u4e2a\u6b65\u9aa4\u3002\n\n**\u504f\u597d\u7b56\u7565\u6a21\u578b\uff08PRM\uff09**\uff1a\n- **\u57fa\u672c\u539f\u7406**\uff1aPRM\u65e8\u5728\u901a\u8fc7\u5b66\u4e60\u7528\u6237\u7684\u504f\u597d\u6765\u4f18\u5316\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5e38\u7528\u4e8e\u63a8\u8350\u7cfb\u7edf\u3001\u4e2a\u6027\u5316\u670d\u52a1\u7b49\u3002\n- **\u6280\u672f\u6846\u67b6**\uff1a\u901a\u5e38\u5305\u62ec\u504f\u597d\u5efa\u6a21\u3001\u7b56\u7565\u4f18\u5316\u548c\u6a21\u578b\u5fae\u8c03\u7b49\u73af\u8282\u3002\n\n**\u65b9\u6cd5\u8bba**\uff1a\n- **\u96c6\u6210\u5b66\u4e60**\uff1a\u5c06MCTS\u4e0ePRM\u7ed3\u5408\uff0c\u5229\u7528MCTS\u7684\u641c\u7d22\u80fd\u529b\u4f18\u5316PRM\u7684\u51b3\u7b56\u8fc7\u7a0b\u3002\n- **\u5f3a\u5316\u5b66\u4e60**\uff1a\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u673a\u5236\u4e0d\u65ad\u8c03\u6574PRM\u7684\u53c2\u6570\uff0c\u4ee5\u9002\u5e94\u52a8\u6001\u73af\u5883\u3002\n\n### Step 2: \u7814\u7a76\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u4e3b\u8981\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0e\u53d8\u4f53\n\n**\u5e94\u7528\u573a\u666f**\uff1a\n- **\u6e38\u620fAI**\uff1aMCTS\u4e0ePRM\u7ed3\u5408\u7528\u4e8e\u6e38\u620f\u4e2d\u7684\u667a\u80fd\u51b3\u7b56\uff0c\u5982AlphaGo\u3002\n- **\u8def\u5f84\u89c4\u5212**\uff1a\u5728\u673a\u5668\u4eba\u5bfc\u822a\u548c\u81ea\u52a8\u9a7e\u9a76\u4e2d\uff0c\u5229\u7528MCTS\u4f18\u5316\u8def\u5f84\u9009\u62e9\u3002\n- **\u63a8\u8350\u7cfb\u7edf**\uff1a\u901a\u8fc7PRM\u5b66\u4e60\u7528\u6237\u504f\u597d\uff0c\u7ed3\u5408MCTS\u8fdb\u884c\u66f4\u7cbe\u51c6\u7684\u63a8\u8350\u3002\n\n**\u53d8\u4f53**\uff1a\n- **\u6df1\u5ea6MCTS**\uff1a\u7ed3\u5408\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff0c\u63d0\u5347MCTS\u7684\u51b3\u7b56\u80fd\u529b\u3002\n- **\u591a\u6a21\u6001PRM**\uff1a\u878d\u5408\u591a\u79cd\u6570\u636e\u6e90\uff08\u5982\u6587\u672c\u3001\u56fe\u50cf\uff09\uff0c\u589e\u5f3aPRM\u7684\u6cdb\u5316\u80fd\u529b\u3002\n\n### Step 3: \u8bc4\u4f30\u5b66\u672f\u754c\u7684\u6280\u672f\u8fdb\u6b65\u4e0e\u5c40\u9650\u6027\n\n**\u6280\u672f\u8fdb\u6b65**\uff1a\n- **\u6027\u80fd\u63d0\u5347**\uff1aMCTS\u4e0ePRM\u7ed3\u5408\u663e\u8457\u63d0\u5347\u4e86\u51b3\u7b56\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002\n- **\u6cdb\u5316\u80fd\u529b**\uff1a\u591a\u6a21\u6001PRM\u5728\u5904\u7406\u591a\u6837\u5316\u6570\u636e\u65f6\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002\n\n**\u5c40\u9650\u6027**\uff1a\n- **\u8ba1\u7b97\u590d\u6742\u5ea6**\uff1aMCTS\u5728\u5927\u89c4\u6a21\u95ee\u9898\u4e2d\u8ba1\u7b97\u91cf\u5927\uff0c\u6548\u7387\u6709\u5f85\u63d0\u5347\u3002\n- **\u6570\u636e\u4f9d\u8d56**\uff1aPRM\u7684\u6027\u80fd\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u9ad8\u8d28\u91cf\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u6570\u636e\u504f\u5dee\u95ee\u9898\u4f9d\u7136\u5b58\u5728\u3002\n\n### Step 4: \u63a2\u8ba8\u8ba1\u7b97\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\n\n**\u9002\u7528\u6027**\uff1a\n- **\u591a\u9886\u57df\u5e94\u7528**\uff1aMCTS+PRM\u6846\u67b6\u5728\u6e38\u620f\u3001\u8def\u5f84\u89c4\u5212\u3001\u63a8\u8350\u7cfb\u7edf\u7b49\u591a\u4e2a\u9886\u57df\u8868\u73b0\u51fa\u826f\u597d\u7684\u9002\u7528\u6027\u3002\n- **\u591a\u6a21\u6001\u6570\u636e**\uff1a\u591a\u6a21\u6001PRM\u80fd\u591f\u5904\u7406\u591a\u79cd\u7c7b\u578b\u7684\u6570\u636e\uff0c\u9002\u7528\u4e8e\u590d\u6742\u5e94\u7528\u573a\u666f\u3002\n\n**\u6cdb\u5316\u80fd\u529b**\uff1a\n- **\u8de8\u9886\u57df\u6cdb\u5316**\uff1a\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\uff0cMCTS+PRM\u6846\u67b6\u5728\u4e0d\u540c\u9886\u57df\u95f4\u5c55\u73b0\u51fa\u8f83\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002\n- **\u52a8\u6001\u73af\u5883\u9002\u5e94**\uff1a\u5f3a\u5316\u5b66\u4e60\u673a\u5236\u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u9002\u5e94\u52a8\u6001\u53d8\u5316\u7684\u73af\u5883\u3002\n\n### Step 5: \u5206\u6790\u6700\u65b0\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027\n\n**\u7a33\u5b9a\u6027**\uff1a\n- **\u7b97\u6cd5\u4f18\u5316**\uff1a\u901a\u8fc7\u6539\u8fdbMCTS\u7684\u9009\u62e9\u7b56\u7565\u548cPRM\u7684\u53c2\u6570\u8c03\u6574\u673a\u5236\uff0c\u63d0\u5347\u4e86\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u3002\n- **\u9c81\u68d2\u6027\u589e\u5f3a**\uff1a\u5f15\u5165\u5bf9\u6297\u8bad\u7ec3\u548c\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\uff0c\u63d0\u9ad8\u6a21\u578b\u5728\u590d\u6742\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u6027\u3002\n\n**\u5bb9\u9519\u6027**\uff1a\n- **\u9519\u8bef\u5904\u7406**\uff1a\u5728\u6a21\u62df\u548c\u56de\u6eaf\u9636\u6bb5\u5f15\u5165\u5bb9\u9519\u673a\u5236\uff0c\u51cf\u5c11\u9519\u8bef\u51b3\u7b56\u7684\u5f71\u54cd\u3002\n- **\u5927\u89c4\u6a21\u6570\u636e\u9002\u5e94\u6027**\uff1a\u4f18\u5316\u7b97\u6cd5\u7ed3\u6784\uff0c\u63d0\u5347\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\u7684\u5904\u7406\u80fd\u529b\u3002\n\n### Step 6: \u8bc4\u4f30\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218\n\n**\u672a\u6765\u7814\u7a76\u65b9\u5411**\uff1a\n- **\u9ad8\u6548\u8ba1\u7b97**\uff1a\u7814\u7a76\u66f4\u9ad8\u6548\u7684MCTS\u7b97\u6cd5\uff0c\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002\n- **\u6570\u636e\u589e\u5f3a**\uff1a\u63a2\u7d22\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u51cf\u5c11PRM\u5bf9\u9ad8\u8d28\u91cf\u6570\u636e\u7684\u4f9d\u8d56\u3002\n\n**\u6311\u6218**\uff1a\n- **\u8de8\u6a21\u6001\u878d\u5408**\uff1a\u5982\u4f55\u6709\u6548\u878d\u5408\u591a\u79cd\u6a21\u6001\u6570\u636e\uff0c\u63d0\u5347\u6a21\u578b\u7684\u7efc\u5408\u6027\u80fd\u3002\n- **\u52a8\u6001\u73af\u5883\u9002\u5e94**\uff1a\u5728\u52a8\u6001\u73af\u5883\u4e0b\uff0c\u5982\u4f55\u4fdd\u6301\u6a21\u578b\u7684\u7a33\u5b9a\u6027\u548c\u9ad8\u6548\u6027\u3002\n\n**\u63a8\u52a8\u540e\u7eed\u7814\u7a76**\uff1a\n- **\u65b0\u95ee\u9898\u63d0\u51fa**\uff1a\u5982\u591a\u6a21\u6001\u6570\u636e\u4e0b\u7684\u504f\u597d\u5efa\u6a21\u3001\u52a8\u6001\u73af\u5883\u4e0b\u7684\u51b3\u7b56\u4f18\u5316\u7b49\u3002\n- **\u7814\u7a76\u5207\u5165\u70b9**\uff1a\u4ece\u7b97\u6cd5\u4f18\u5316\u3001\u6570\u636e\u589e\u5f3a\u3001\u8de8\u6a21\u6001\u878d\u5408\u7b49\u591a\u4e2a\u89d2\u5ea6\u5207\u5165\uff0c\u63a8\u52a8\u6280\u672f\u8fdb\u6b65\u3002\n\n### \u603b\u7ed3\n\n\u8fd1\u5e74\u6765\uff0cMCTS\u5728PRM\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u7684\u5e94\u7528\u63a2\u7d22\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4e3b\u8981\u4f53\u73b0\u5728\u6280\u672f\u6846\u67b6\u7684\u5b8c\u5584\u3001\u5e94\u7528\u573a\u666f\u7684\u62d3\u5c55\u3001\u6027\u80fd\u7684\u63d0\u5347\u4ee5\u53ca\u7a33\u5b9a\u6027\u548c\u5bb9\u9519\u6027\u7684\u589e\u5f3a\u3002\u7136\u800c\uff0c\u4ecd\u5b58\u5728\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u3001\u6570\u636e\u4f9d\u8d56\u6027\u5f3a\u7b49\u5c40\u9650\u6027\u3002\u672a\u6765\u7684\u7814\u7a76\u5e94\u5173\u6ce8\u9ad8\u6548\u8ba1\u7b97\u3001\u6570\u636e\u589e\u5f3a\u548c\u8de8\u6a21\u6001\u878d\u5408\u7b49\u65b9\u5411\uff0c\u4ee5\u5e94\u5bf9\u52a8\u6001\u73af\u5883\u4e0b\u7684\u51b3\u7b56\u4f18\u5316\u7b49\u6311\u6218\uff0c\u63a8\u52a8\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002", "task_step_name": "\u5bb9\u9519\u6027", "task_step_description": "\u5728\u6a21\u62df\u548c\u56de\u6eaf\u9636\u6bb5\u5f15\u5165\u5bb9\u9519\u673a\u5236\uff0c\u51cf\u5c11\u9519\u8bef\u51b3\u7b56\u7684\u5f71\u54cd\u3002", "task_step_level": "4>2", "task_step_question": "### \u95ee\u9898\u63d0\u51fa\n\n\u5728MCTS\u4e0ePRM\u7ed3\u5408\u7684\u6846\u67b6\u4e2d\uff0c\u6a21\u62df\u548c\u56de\u6eaf\u9636\u6bb5\u7684\u5bb9\u9519\u673a\u5236\u5bf9\u4e8e\u51cf\u5c11\u9519\u8bef\u51b3\u7b56\u7684\u5f71\u54cd\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u5bb9\u9519\u673a\u5236\u53ef\u80fd\u9762\u4e34\u4ee5\u4e0b\u6311\u6218\uff1a\n\n1. **\u5bb9\u9519\u673a\u5236\u7684\u590d\u6742\u6027**\uff1a\u5982\u4f55\u5728\u4fdd\u6301\u7b97\u6cd5\u6548\u7387\u7684\u540c\u65f6\uff0c\u8bbe\u8ba1\u51fa\u6709\u6548\u7684\u5bb9\u9519\u673a\u5236\uff0c\u4ee5\u5e94\u5bf9\u6a21\u62df\u548c\u56de\u6eaf\u9636\u6bb5\u53ef\u80fd\u51fa\u73b0\u7684\u9519\u8bef\u51b3\u7b56\uff1f\n2. **\u52a8\u6001\u73af\u5883\u9002\u5e94\u6027**\uff1a\u5728\u52a8\u6001\u53d8\u5316\u7684\u73af\u5883\u4e2d\uff0c\u5bb9\u9519\u673a\u5236\u5982\u4f55\u5feb\u901f\u9002\u5e94\u5e76\u8c03\u6574\uff0c\u4ee5\u786e\u4fdd\u6a21\u578b\u7684\u7a33\u5b9a\u6027\u548c\u51b3\u7b56\u7684\u51c6\u786e\u6027\uff1f\n3. **\u591a\u6a21\u6001\u6570\u636e\u878d\u5408**\uff1a\u5728\u591a\u6a21\u6001\u6570\u636e\uff08\u5982\u6587\u672c\u3001\u56fe\u50cf\uff09\u7684\u80cc\u666f\u4e0b\uff0c\u5bb9\u9519\u673a\u5236\u5982\u4f55\u6709\u6548\u878d\u5408\u4e0d\u540c\u6a21\u6001\u7684\u4fe1\u606f\uff0c\u4ee5\u63d0\u5347\u6a21\u578b\u7684\u6574\u4f53\u5bb9\u9519\u80fd\u529b\uff1f\n\n### \u95ee\u9898\u7ec6\u5316\n\n\u57fa\u4e8e\u4e0a\u8ff0\u6311\u6218\uff0c\u53ef\u4ee5\u8fdb\u4e00\u6b65\u7ec6\u5316\u95ee\u9898\uff1a\n\n- **\u5bb9\u9519\u673a\u5236\u7684\u8bbe\u8ba1\u4e0e\u4f18\u5316**\uff1a\u5728MCTS\u7684\u6a21\u62df\u548c\u56de\u6eaf\u9636\u6bb5\uff0c\u5982\u4f55\u8bbe\u8ba1\u5e76\u4f18\u5316\u5bb9\u9519\u673a\u5236\uff0c\u4ee5\u6700\u5c0f\u5316\u9519\u8bef\u51b3\u7b56\u7684\u5f71\u54cd\uff0c\u540c\u65f6\u4fdd\u6301\u7b97\u6cd5\u7684\u8ba1\u7b97\u6548\u7387\uff1f\n- **\u52a8\u6001\u73af\u5883\u4e0b\u7684\u5bb9\u9519\u7b56\u7565**\uff1a\u5728\u52a8\u6001\u53d8\u5316\u7684\u73af\u5883\u4e2d\uff0c\u5bb9\u9519\u673a\u5236\u5e94\u5982\u4f55\u8bbe\u8ba1\uff0c\u4ee5\u786e\u4fdd\u6a21\u578b\u80fd\u591f\u5feb\u901f\u9002\u5e94\u73af\u5883\u53d8\u5316\uff0c\u5e76\u51cf\u5c11\u9519\u8bef\u51b3\u7b56\u7684\u7d2f\u79ef\u6548\u5e94\uff1f\n- **\u591a\u6a21\u6001\u6570\u636e\u4e0b\u7684\u5bb9\u9519\u878d\u5408**\uff1a\u5728\u591a\u6a21\u6001\u6570\u636e\u7684\u80cc\u666f\u4e0b\uff0c\u5bb9\u9519\u673a\u5236\u5982\u4f55\u6709\u6548\u878d\u5408\u4e0d\u540c\u6a21\u6001\u7684\u4fe1\u606f\uff0c\u4ee5\u63d0\u5347\u6a21\u578b\u5728\u590d\u6742\u573a\u666f\u4e0b\u7684\u5bb9\u9519\u80fd\u529b\u548c\u51b3\u7b56\u51c6\u786e\u6027\uff1f\n\n### \u7814\u7a76\u5207\u5165\u70b9\n\n\u9488\u5bf9\u4e0a\u8ff0\u95ee\u9898\uff0c\u53ef\u4ee5\u4ece\u4ee5\u4e0b\u51e0\u4e2a\u89d2\u5ea6\u5207\u5165\u7814\u7a76\uff1a\n\n1. **\u7b97\u6cd5\u4f18\u5316**\uff1a\u63a2\u7d22\u65b0\u7684\u5bb9\u9519\u7b97\u6cd5\u6216\u6539\u8fdb\u73b0\u6709\u7b97\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u5bb9\u9519\u673a\u5236\u7684\u6548\u7387\u548c\u6548\u679c\u3002\n2. **\u52a8\u6001\u73af\u5883\u5efa\u6a21**\uff1a\u7814\u7a76\u52a8\u6001\u73af\u5883\u4e0b\u7684\u5bb9\u9519\u7b56\u7565\uff0c\u5982\u5f15\u5165\u81ea\u9002\u5e94\u673a\u5236\u6216\u5b9e\u65f6\u8c03\u6574\u7b56\u7565\uff0c\u4ee5\u589e\u5f3a\u6a21\u578b\u7684\u9002\u5e94\u6027\u3002\n3. **\u591a\u6a21\u6001\u878d\u5408\u6280\u672f**\uff1a\u5f00\u53d1\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u7684\u5bb9\u9519\u6280\u672f\uff0c\u5982\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u673a\u5236\u6216\u591a\u6a21\u6001\u7279\u5f81\u63d0\u53d6\uff0c\u4ee5\u63d0\u5347\u6a21\u578b\u7684\u7efc\u5408\u5bb9\u9519\u80fd\u529b\u3002\n\n### \u9884\u671f\u6210\u679c\n\n\u901a\u8fc7\u4e0a\u8ff0\u7814\u7a76\uff0c\u9884\u671f\u80fd\u591f\uff1a\n\n- \u63d0\u51fa\u5e76\u9a8c\u8bc1\u6709\u6548\u7684\u5bb9\u9519\u673a\u5236\uff0c\u663e\u8457\u51cf\u5c11MCTS\u4e0ePRM\u7ed3\u5408\u6846\u67b6\u4e2d\u7684\u9519\u8bef\u51b3\u7b56\u5f71\u54cd\u3002\n- \u589e\u5f3a\u6a21\u578b\u5728\u52a8\u6001\u73af\u5883\u4e0b\u7684\u7a33\u5b9a\u6027\u548c\u51b3\u7b56\u51c6\u786e\u6027\uff0c\u63d0\u5347\u5176\u5728\u590d\u6742\u5e94\u7528\u573a\u666f\u4e2d\u7684\u9002\u7528\u6027\u3002\n- \u63d0\u5347\u591a\u6a21\u6001\u6570\u636e\u4e0b\u7684\u5bb9\u9519\u80fd\u529b\uff0c\u4e3a\u8de8\u6a21\u6001\u51b3\u7b56\u4f18\u5316\u63d0\u4f9b\u65b0\u7684\u6280\u672f\u652f\u6301\u548c\u7406\u8bba\u4f9d\u636e\u3002\n\n### \u7ed3\u8bba\n\n\u5bb9\u9519\u673a\u5236\u5728MCTS\u4e0ePRM\u7ed3\u5408\u6846\u67b6\u4e2d\u626e\u6f14\u7740\u81f3\u5173\u91cd\u8981\u7684\u89d2\u8272\u3002\u901a\u8fc7\u6df1\u5165\u7814\u7a76\u5bb9\u9519\u673a\u5236\u7684\u8bbe\u8ba1\u4e0e\u4f18\u5316\u3001\u52a8\u6001\u73af\u5883\u4e0b\u7684\u5bb9\u9519\u7b56\u7565\u4ee5\u53ca\u591a\u6a21\u6001\u6570\u636e\u4e0b\u7684\u5bb9\u9519\u878d\u5408\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u6a21\u578b\u7684\u7a33\u5b9a\u6027\u548c\u51b3\u7b56\u51c6\u786e\u6027\uff0c\u63a8\u52a8\u8be5\u9886\u57df\u7684\u6280\u672f\u8fdb\u6b65\u548c\u5e94\u7528\u62d3\u5c55\u3002", "task_step_question_context": [{"ref_id": "454845771530662550", "chunk_id": "1", "score": 0.310546875, "text": "# 2 RELATED WORK\nThe full version of the related work is in Appendix A, we briefly introduce several highly related works here. In general, model-based RL for solving decision-making problems can be divided into three perspectives: model learning, policy learning, and decision-making. Moreover, optimal control theory also concerns the decision-making problem and is deeply related to model-based RL.  \n\nModel learning: How to learn a good model to support decision-making is crucial in model-based RL. There are two main aspects of the work: the model structure designing (Chua et al., 2018; Zhang  \n\net al., 2021; 2020; Hafner et al., 2021; Chen et al., 2022) and the loss designing (D\u2019Oro et al., 2020;   \nFarahmand et al., 2017; Li et al., 2021).  \n\nPolicy learning: Two methods are always used to learn the policy by using the learned model. One is to serve the learned model as a black-box simulator to generate the data (Janner et al., 2019b; Yu et al., 2020; Lee et al., 2020). Another way is to use the learned model to calculate the policy gradient (Heess et al., 2015b; Clavera et al., 2019; Amos et al., 2021).  \n\nDecision-making: When making the decision, we need to generate the actions that can achieve our goal. Many of the model-based RL methods make the decision by using the learned policy solely (Hafner et al., 2021). Similar to our paper, some works also try to make decisions by using the learned model, but the majority only focus on the discrete action space. The well-known MCTS method achieves a lot of success. For example, the well-known Alpha Zero (Silver et al., 2017), MuZero (Schrittwieser et al., 2020). There are only a few works that study the continuous action space, such as the Continuous UCT (Cou\u00a8etoux et al., 2011), the sampled MuZero (Hubert et al., 2021), the TreePI (Springenberg et al., 2020), and the TD-MPC (Hansen et al., 2022a).  \n\nOptimal control theory: Beyond deep RL, optimal control also considers the decision-making problem but rather relies on the known and continuous transition model. In modern optimal control, Model Predictive Control (MPC) (Camacho & Alba, 2013) framework is always adopted when the environment is highly non-linear. In MPC, the action is planned during the execution by using the model, and such a procedure is called trajectory optimization. Plenty of previous works (Byravan et al., 2021; Chua et al., 2018; Pinneri et al., 2021; Nagabandi et al., 2020) use MPC framework to solve the continuous control tasks, but most of them are based on zero-order or sample-based method to do the planning. The most relevant works are DDP (Murray & Yakowitz, 1984), iLQR (Li & Todorov, 2004), and iLQG (Todorov & Li, 2005; Tassa et al., 2012). We discuss the detailed differences between our method and these methods in Appendix A.  \n\nSince our planning algorithm relies on the learned model and learned policy, we build our algorithm based on these works on model learning and policy learning . Our POMP algorithm tries to solve a more challenging task compared to the related work on decision-making : efficiently optimize the trajectory in continuous action space when the environment model is unknown. Different from our works, the MPC with DDP as trajectory optimizer from optimal control theory requires the known environment model, and also requires the hessian matrix for online optimization from scratch.\n\n# 3 PRELIMINARIES\nReinforcement Le onsider discrete-time Marko Decision Process (M $\\mathcal{M}$ the tuple ($(\\mathcal{X},\\mathcal{A},f,r,\\gamma)$ XA $\\mathcal{X}$ state space, A is the action space, $f\\,:\\,x_{t+1}\\,=$   \n$f(x_{t},a_{t})$ is the transition model, $r:\\mathcal{X}\\times\\mathcal{A}\\to\\mathbb{R}$ X \u00d7 A \u2192 is the reward function, $\\gamma$ is the discount factor. $t$ $\\begin{array}{r}{R_{t}=\\sum_{t^{\\prime}=t}^{\\infty}\\gamma^{t^{\\prime}-t}r_{t^{\\prime}}}\\end{array}$ , and Reinforcement Learn  \n$\\begin{array}{r}{\\operatorname*{max}_{\\theta}J(\\theta)=\\operatorname*{max}_{\\theta}\\mathbb{E}_{\\pi_{\\theta}}R_{t}=\\operatorname*{max}_{\\theta}\\mathbb{E}_{\\pi_{\\theta}}\\Big[\\sum_{t^{\\prime}=t}^{\\infty}\\gamma^{t^{\\prime}-t}r(x_{t^{\\prime}},a_{t^{\\prime}})\\Big].}\\end{array}$ ing (RL) aims to find a policy $\\pi_{\\theta}:\\mathcal{X}\\times\\mathcal{A}\\rightarrow\\mathbb{R}^{+}$ X \u00d7 A \u2192 h P that can maximize the expected return .$J$ . where  \n\n$\\begin{array}{r}{\\operatorname*{max}_{a_{t}}\\mathbb{E}\\Big[r(x_{t},a_{t}|x_{t}\\,=\\,x)+\\gamma V^{*}(x_{t+1})\\Big]}\\end{array}$ value function obeys an important identity known as the Bellman optimality equation Bellman Equation. hWe define the optimal value function i . The idea behind this equation is that if we know the $V^{*}(x)=\\operatorname*{max}\\mathbb{E}[R_{t}|x_{t}=x]$ |. The optimal $V^{*}(x)\\;=\\;$ $r(x_{t},a_{t})$ for any $a_{t}$ and next step value function $V^{*}(x_{t+1})$ for any $s_{t+1}$ , we can recursively select the action $a_{t}$ which m $r(x_{t},a_{t}|x_{t}=x)+\\gamma V^{*}(x_{t+1})$ milarly, we can denote the optimal action-value function $Q^{*}(x,a)\\;=\\;\\operatorname*{max}\\mathbb{E}[R_{t}|x_{t}\\;=\\;x,a_{t}\\;=\\;a]$ |], and it obeys a similar Bellman optimility equation $\\begin{array}{r}{Q^{*}(x,a)=\\operatorname*{max}_{a_{t+1}}\\mathbb{E}\\Big[r(x_{t},a_{t}|x_{t}=x,a_{t}=a)+\\gamma Q^{*}(x_{t+1},a_{t+1})\\Big].}\\end{array}$ .  \n\nModel-based RL. Model-based RL method distinguishes itself from model-free counterparts by using the data to learn a transition model. Following Janner et al. (2019a) and Clavera et al. (2019), we use parametric neural networks to approximate the transition function, reward function, policy function and $\\mathrm{^Q}$ -value function with the following objective function to be optimized  $J_{f}(\\psi)\\,=\\,\\mathbb{E}\\big[\\log f(x_{t+1}|x_{t},a_{t})\\big]$ '', $J_{r}(\\omega)\\,=\\,\\mathbb{E}\\big[\\log r(r_{t}|x_{t},a_{t})\\big]$ '', $\\begin{array}{r}{\\bar{J_{\\pi}}(\\theta)\\,=\\,\\mathbb{E}\\bigl[\\sum_{t=0}^{H-1}\\gamma^{t}r(\\bar{x}_{t},a_{t})\\,+\\,}\\end{array}$ ' P$\\gamma^{H}Q(x_{H},a_{H})]$ 'and $J_{Q}\\,=\\,\\mathbb{E}\\bigl[\\|Q(x_{t},a_{t})-(r+\\tilde{Q}(x_{t+1},a_{t+1}))\\|_{2}\\bigr]$ '\u2225\u2212\u2225', respectively. In ${\\cal J}_{\\pi}(\\theta)$ , we truncate the trajectory in horizon Hto avoid long time model rollout.  \n\nNotations. For one-dimensional state and action case, we denote the partial differentiation of function by using its output with subscripts, e.g. ,$\\begin{array}{r}{r_{x}\\ \\triangleq\\ \\frac{\\partial r(x,a)}{\\partial x},\\ r_{a}\\ \\triangleq\\ \\frac{\\bigtriangleup r(x,a)}{\\partial a},\\ f_{x}\\ \\triangleq\\ \\frac{\\partial f(x,a)}{\\partial x}}\\end{array}$ ,$f_{a}\\triangleq{\\frac{\\partial f(x,a)}{\\partial a}}$ ,$\\begin{array}{r}{Q_{x}\\triangleq\\frac{\\partial Q(x,a)}{\\partial x}}\\end{array}$ and $\\begin{array}{r}{Q_{a}\\triangleq\\frac{\\partial Q(x,a)}{\\partial a}}\\end{array}$ . See Appendix E for the multi-dimension case."}, {"ref_id": "454848282814999732", "chunk_id": "2", "score": 0.248046875, "text": "# 1. Introduction\nA problem that has been used to solve a large variety of real-world questions is the model counting problem (#Sat ) [ 1 ,2 ,3 ,4 ,5 ,6 ,7 ,8 ,9 ]. It asks to compute the number of solutions of a Boolean formula [ 10 ] and is theoretically of high worstcase complexity ( #\u00b7P-complete [ 11 ,12 ]). Lately, both #Sat and its approximate version have received renewed attention in theory and practice [ 13 ,4 ,14 ,15 ]. A concept that allows very natural abstractions of data and query results is projection. Projection has wide applications in databases [ 16 ] and declarative problem modeling. The problem projected model counting (PMC ) asks to count solutions of a Boolean formula with respect to a given set of projection variables , where multiple solutions that are identical when restricted to the projection variables count as only one solution. If all variables of the formula are projection variables, then PMC is the #Sat problem and if there are no projection variables then it is simply the Sat problem. Projected variables allow for solving problems where one needs to introduce auxiliary variables, in particular, if these variables are functionally independent of the variables of interest, in the problem encoding, e.g., [ 17 ,18 ]. Projected model counting is a fundamental problem in artificial intelligence and was also subject to a dedicated track in the first model counting competition [ 19 ]. It turns out that there are plenty of use cases and applications for PMC , ranging from a variety of real-world questions in modern society, artificial intelligence [ 20 ], reliability estimation [ 4 ] and combinatorics [ 21 ]. Variants of this problem are relevant to problems in probabilistic and quantitative reasoning, e.g., [ 2 ,3 ,9 ] and Bayesian reasoning [ 8 ]. This work also inspired follow-up work, as extensions of projected model counting as well as generalizations for logic programming and quantified Boolean formulas have been presented recently, e.g., [22, 23, 24].  \n\nWhen we consider the computational complexity of PMC it turns out that under standard assumptions the problem is even harder than #Sat , more precisely, complete for the class #\u00b7NP [25 ]. Even though there is a PMC solver [ 21 ]and an ASP solver that implements projected enumeration [ 26 ], PMC has received very little attention in parameterized algorithmics so far. Parameterized algorithms [ 27 ,28 ,29 ,30 ] tackle computationally hard problems by directly exploiting certain structural properties (parameter) of the input instance to solve the problem faster, preferably in polynomial-time for a fixed parameter value. In this paper, we consider the treewidth of graphs associated with the given input formula as parameter, namely the primal graph [ 31 ]. Roughly speaking, small treewidth of a graph measures its tree-likeness and sparsity. Treewidth is defined in terms of tree decompositions (TDs) , which are arrangements of graphs into trees. When we take advantage of small treewidth, we usually take a TD and evaluate the considered problem in parts, via dynamic programming ( $^{D P}$ )on the TD. This dynamic programming technique utilizes tree decompositions, where a tree decomposition is traversed in post-order, i.e., from the leaves towards the root, and thereby for each node of the TD tables are computed such that a problem is solved by cracking smaller (partial) problems.  \n\nIn this work we apply tree decompositions for projected model counting and study precise runtime dependency on treewidth . While there are also related works on properties for efficient counting algorithms, e.g., [ 32 ,33 ,34 ], even for treewidth, precise runtime dependency for projected model counting has been left open. We design a novel algorithm that runs in double exponential time $^{1}$ in the treewidth, but it is quadratic in the number of variables of a given formula. Later, we also establish a conditional lower bound showing that under reasonable assumptions it is quite unlikely that one can significantly improve this algorithm.  \n\nNaturally, it is expected that our proposed PMC algorithm can be only competitive for instances where the treewidth is very low. Still, despite our new theoretical result, it turns out that in practice there is a way to efficiently implement dynamic programming and tree decompositions for solving PMC .However, most of the existing systems based on dynamic programming guided along a tree decomposition are suffering from maintaining large tables, since the size of these tables (and thus the computational efforts required) are bounded by a function in the treewidth of the instance. Although dedicated competitions [ 35 ]for treewidth advanced the state-of-the-art for efficiently computing treewidth and TDs [ 36 ,37 ], these systems and approaches reach their limits when instances have higher treewidth. Indeed, such approaches based on dynamic programming reach their limits when instances have higher treewidth; a situation which can even occur in structured real-world instances [ 38 ]. Nevertheless in the area of Boolean satisfiability, this approach proved to be successful for counting problems, such as, e.g., (weighted) model counting [39, 40, 31].  \n\nTo further increase the practical applicability of dynamic programming for PMC , novel techniques are required, where we rely on certain simplifications of a graph, which we call abstraction 2 . Thereby, we (a) rely on different levels of abstraction of the instance at hand; (b) treat subproblems orginating in the abstraction by standard solvers whenever widths appear too high; and (c) use highly sophisticated data management in order to store and process tables obtained by dynamic programming.  \n\nContributions. In more details, we provide the following contributions.  \n\n1. We introduce a novel algorithm to solve projected model counting in time $O(2^{2^{k+4}}n^{2})$ where $k$ is the treewidth of the primal graph of the instance and $n$ is the size of the input instance. Similar to recent DP algorithms for problems on the second level of the polynomial hierarchy [ 41 ], our algorithm traverses the given tree decomposition multiple times (multipass). In the first traversal, we run a dynamic programming algorithm on tree decompositions to solve Sat [31 ]. In a second traversal, we construct equivalence classes on top of the previous computation to obtain model counts with respect to the projection variables by exploiting combinatorial properties of intersections.  \n\n2. Then, we establish that our runtime bounds are asymptotically tight under the exponential time hypothesis (ETH) [42 ] using a recent result by Lampis and Mitsou [ 43 ], who established lower bounds for the problem $\\exists\\forall$ -Sat assuming ETH. Intuitively, ETH states a complexity theoretical lower bound on how fast satisfiability problems can be solved. More precisely, one cannot solve 3 -Sat in time $2^{s\\cdot n}\\cdot n^{{\\mathcal{O}}(1)}$ for some $s>0$ and number $n$ of variables.  \n\n3. Finally, we also provide an implementation for PMC that efficiently utilizes treewidth and is highly competitive with state-of-the-art solvers. In more details, we treat above aspects (a), (b), and (c) as follows.  \n\n(a) To tame the beast of high treewidth, we propose nested dynamic programming , where only parts of some abstraction of a graph are decomposed. Then, each TD node also needs to solve a subproblem residing in the graph, but may involve vertices outside the abstraction. In turn, for solving such subproblems, the idea of nested DP is to subsequently repeat decomposing and solving more fine-grained graph abstractions in a nested fashion.While candidates for obtaining such abstractions often naturally originate from the problem PMC , nested DP may require computing those during nesting, for which we even present a generic solution.  \n\n(b) To further improve the capability of handling high treewidth, we show how to apply nested DP in the context of hybrid solving , where established, standard solvers (e.g., Sat solvers) and caching are incorporated in nested DP such that the best of two worlds are combined. Thereby, we solve counting problems like PMC , where we apply DP to parts of the problem instance that are subject to counting , while depending on the existence of a solution for certain subproblems. Those subproblems that are subject to searching for the existence of a solution reside in the abstraction only and are solved via standard solvers.  \n\n(c) We implemented a system based on a recently published tool [ 39 ] for using database management systems (DBMS) to efficiently perform table manipulation operations needed during DP. Our system is called nestHDB $_3$ and uses and significantly extends this tool in order to perform hybrid solving, thereby combining nested DP and standard solvers. As a result, we use DBMS for efficiently implementing the handling of tables needed by nested DP. Preliminary experiments indicate that nested DP with hybrid solving can be fruitful, where we are capable of solving instances, whose treewidth upper bounds are beyond 200.  \n\nThis paper combines research of work that is published at the 21st International Conference on Satisfiability (SAT 2018) [ 44 ] and research that was presented at the 23rd International Conference on Satisfiability (SAT 2020) [ 45 ]. In addition to these conference versions, we added detailed proofs, further examples, and significantly improved the presentation throughout the document."}, {"ref_id": "454845757390876126", "chunk_id": "5", "score": 0.2333984375, "text": "# A RELATED WORK\nModel-based RL methods for solving decision-making problems focus on three key perspectives: how to learn the model? how to use the learned model to learn the policy? And how to make the decision using the learned model and policy? Besides, decision-making that relies on the model is also investigated in the optimal control theory field which is deeply related to model-based RL.  \n\nModel learning: How to learn a good model to support decision-making is a crucial problem in model-based RL. There are two main aspects of the work: the model structure designing and the loss designing. For model structure designing, ensemble-based model (Chua et al., 2018), dropout mechanisms (Zhang et al., 2021), auto-regressive structure (Zhang et al., 2020), stochastic hidden model (Hafner et al., 2021), and transformer based model (Chen et al., 2022) are always considered to improve the model robustness and prediction accuracy. For loss designing, decision awareness (D\u2019Oro et al., 2020; Farahmand et al., 2017) and gradient awareness (Li et al., 2021) are always considered to reduce the gap between model learning and model utilization.  \n\nPolicy learning: Two methods are always used to learn the policy by using the learned model. One is to serve the learned model as a black-box simulator to generate the data. Janner et al. (2019b) is a representing work of this line. Yu et al. (2020), Lee et al. (2020) also follow such a manner by extending it to offline-RL setting. Another way is to use the learned model to calculate the policy gradient. Heess et al. (2015b) presents an algorithm to calculate the policy gradient by backpropagating through the model. Clavera et al. (2019) and Amos et al. (2021) share similar methods but use promising actor and critic learning strategy to achieve better performance.  \n\nDecision-making: When making the decision, we need to generate the actions that can achieve our goal. Most of the model-based RL methods make the decision by using the learned policy solely (Janner et al., 2019b; Yu et al., 2020; Clavera et al., 2019; Hafner et al., 2021). Similar to our paper, some works also try to make decisions by using the learned model, but the majority only focus on the discrete action space. For example, the well-known Alpha Zero system (Silver et al., 2017) uses MCTS to derive the action by using the known model. In MuZero and (Schrittwieser et al., 2020), the authors propose to use a learned model combined with an MCTS planner to achieve significant performances in a broad range of tasks within discrete action space. There are only a few works that study the continuous action space. Cou\u00a8etoux et al. (2011) extends the MCTS framework to continuous action space but also needs to know the real model and handle the model. In Hubert et al. (2021), the author proposed a sampled MuZero algorithm to handle the complex action space by planning over sampled actions. In Hansen et al. (2022a), the authors propose to learn a value function that can be used as long term return in the Cross-Entropy (CE) method for planning.  \n\nOptimal control: Beyond deep RL, optimal control also considers the decision-making problem but rather relies on the known and continuous transition model. In modern optimal control theory, Model Predictive Control (MPC) (Camacho & Alba, 2013) framework is always adopted when the environment is highly non-linear. In MPC, the action is planned during the execution by using the model, and such a procedure is called trajectory optimization. There are plenty of previous works that use the MPC framework to solve continuous control tasks. For example, Byravan et al. (2021) proposes to use sampling-based MPC for high-dimensional continuous control tasks with learned models and a learned policy as a proposal distribution. Pinneri et al. (2021) proposes an improved version of the Cross-Entropy Method for efficient planning. Nagabandi et al. (2020) proposes a PDDM method that uses a gradient-free planner algorithm combined with online MPC method to learn flexible contact-rich dexterous manipulation skills.  \n\nDifferential Dynamical Programming: The most relevant works are DDP (Murray & Yakowitz, 1984), iLQR (Li & Todorov, 2004), and iLQG (Tassa et al., 2012). Differentiable Dynamic Programming (DDP) (Tassa et al., 2012) employs the Bellman equation structure (Murray & Yakowitz, 1984; Pantoja, 1988; Aoyama et al., 2021) and has fast convergence property. It becomes more and more popular in the control field. iLQR (Li & Todorov, 2004), and iLQG (Tassa et al., 2012; Todorov & Li, 2005) are two variants of the DDP. In iLQR and iLQG, the second-order derivative of the environment model is ignored (set as zero). Therefore, iLQR and iLQG are more computationally efficient compared to the original DDP method. Since both iLQG and our D3P planner are motivated by DDP, they look similar naturally. But our method has several key differences compared with theirs, and these differences are well-designed to incorporate the neural network model. (1) DDP, iLQR, and iLQG are both pure planning algorithms that require a known environment model. (2) Computing the second-order derivative of the neural network based model is computationally costly (Hessian matrix). In our method, we only rely on the first-order derivative of the model. (3) The previous methods use the second-order Talyor expansion of the Q-value function to handle the local optimization problem. But it is hard to guarantee that the hessian matrix is a negative definite matrix, which is a necessary condition for convergence. Here, we construct an auxiliary target function $D$ and use a first-order Talyor expansion for the $Q$ function inside of the $D$ function to guarantee the non-positive definite matrix."}], "task_step_question_answer": "a2b27547-c4b3-4bf8-ae68-c913858a2752:\u300c### \u95ee\u9898\u63d0\u51fa\n\n\u5728MCTS\u4e0ePRM\u7ed3\u5408\u7684\u6846\u67b6\u4e2d\uff0c\u6a21\u62df\u548c\u56de\u6eaf\u9636\u6bb5\u7684\u5bb9\u9519\u673a\u5236\u5bf9\u4e8e\u51cf\u5c11\u9519\u8bef\u51b3\u7b56\u7684\u5f71\u54cd\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u5bb9\u9519\u673a\u5236\u53ef\u80fd\u9762\u4e34\u4ee5\u4e0b\u6311\u6218\uff1a\n\n1. **\u5bb9\u9519\u673a\u5236\u7684\u590d\u6742\u6027**\uff1a\u5982\u4f55\u5728\u4fdd\u6301\u7b97\u6cd5\u6548\u7387\u7684\u540c\u65f6\uff0c\u8bbe\u8ba1\u51fa\u6709\u6548\u7684\u5bb9\u9519\u673a\u5236\uff0c\u4ee5\u5e94\u5bf9\u6a21\u62df\u548c\u56de\u6eaf\u9636\u6bb5\u53ef\u80fd\u51fa\u73b0\u7684\u9519\u8bef\u51b3\u7b56\uff1f\n2. **\u52a8\u6001\u73af\u5883\u9002\u5e94\u6027**\uff1a\u5728\u52a8\u6001\u53d8\u5316\u7684\u73af\u5883\u4e2d\uff0c\u5bb9\u9519\u673a\u5236\u5982\u4f55\u5feb\u901f\u9002\u5e94\u5e76\u8c03\u6574\uff0c\u4ee5\u786e\u4fdd\u6a21\u578b\u7684\u7a33\u5b9a\u6027\u548c\u51b3\u7b56\u7684\u51c6\u786e\u6027\uff1f\n3. **\u591a\u6a21\u6001\u6570\u636e\u878d\u5408**\uff1a\u5728\u591a\u6a21\u6001\u6570\u636e\uff08\u5982\u6587\u672c\u3001\u56fe\u50cf\uff09\u7684\u80cc\u666f\u4e0b\uff0c\u5bb9\u9519\u673a\u5236\u5982\u4f55\u6709\u6548\u878d\u5408\u4e0d\u540c\u6a21\u6001\u7684\u4fe1\u606f\uff0c\u4ee5\u63d0\u5347\u6a21\u578b\u7684\u6574\u4f53\u5bb9\u9519\u80fd\u529b\uff1f\n\n### \u95ee\u9898\u7ec6\u5316\n\n\u57fa\u4e8e\u4e0a\u8ff0\u6311\u6218\uff0c\u53ef\u4ee5\u8fdb\u4e00\u6b65\u7ec6\u5316\u95ee\u9898\uff1a\n\n- **\u5bb9\u9519\u673a\u5236\u7684\u8bbe\u8ba1\u4e0e\u4f18\u5316**\uff1a\u5728MCTS\u7684\u6a21\u62df\u548c\u56de\u6eaf\u9636\u6bb5\uff0c\u5982\u4f55\u8bbe\u8ba1\u5e76\u4f18\u5316\u5bb9\u9519\u673a\u5236\uff0c\u4ee5\u6700\u5c0f\u5316\u9519\u8bef\u51b3\u7b56\u7684\u5f71\u54cd\uff0c\u540c\u65f6\u4fdd\u6301\u7b97\u6cd5\u7684\u8ba1\u7b97\u6548\u7387\uff1f\n- **\u52a8\u6001\u73af\u5883\u4e0b\u7684\u5bb9\u9519\u7b56\u7565**\uff1a\u5728\u52a8\u6001\u53d8\u5316\u7684\u73af\u5883\u4e2d\uff0c\u5bb9\u9519\u673a\u5236\u5e94\u5982\u4f55\u8bbe\u8ba1\uff0c\u4ee5\u786e\u4fdd\u6a21\u578b\u80fd\u591f\u5feb\u901f\u9002\u5e94\u73af\u5883\u53d8\u5316\uff0c\u5e76\u51cf\u5c11\u9519\u8bef\u51b3\u7b56\u7684\u7d2f\u79ef\u6548\u5e94\uff1f\n- **\u591a\u6a21\u6001\u6570\u636e\u4e0b\u7684\u5bb9\u9519\u878d\u5408**\uff1a\u5728\u591a\u6a21\u6001\u6570\u636e\u7684\u80cc\u666f\u4e0b\uff0c\u5bb9\u9519\u673a\u5236\u5982\u4f55\u6709\u6548\u878d\u5408\u4e0d\u540c\u6a21\u6001\u7684\u4fe1\u606f\uff0c\u4ee5\u63d0\u5347\u6a21\u578b\u5728\u590d\u6742\u573a\u666f\u4e0b\u7684\u5bb9\u9519\u80fd\u529b\u548c\u51b3\u7b56\u51c6\u786e\u6027\uff1f\n\n### \u7814\u7a76\u5207\u5165\u70b9\n\n\u9488\u5bf9\u4e0a\u8ff0\u95ee\u9898\uff0c\u53ef\u4ee5\u4ece\u4ee5\u4e0b\u51e0\u4e2a\u89d2\u5ea6\u5207\u5165\u7814\u7a76\uff1a\n\n1. **\u7b97\u6cd5\u4f18\u5316**\uff1a\u63a2\u7d22\u65b0\u7684\u5bb9\u9519\u7b97\u6cd5\u6216\u6539\u8fdb\u73b0\u6709\u7b97\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u5bb9\u9519\u673a\u5236\u7684\u6548\u7387\u548c\u6548\u679c\u3002\n2. **\u52a8\u6001\u73af\u5883\u5efa\u6a21**\uff1a\u7814\u7a76\u52a8\u6001\u73af\u5883\u4e0b\u7684\u5bb9\u9519\u7b56\u7565\uff0c\u5982\u5f15\u5165\u81ea\u9002\u5e94\u673a\u5236\u6216\u5b9e\u65f6\u8c03\u6574\u7b56\u7565\uff0c\u4ee5\u589e\u5f3a\u6a21\u578b\u7684\u9002\u5e94\u6027\u3002\n3. **\u591a\u6a21\u6001\u878d\u5408\u6280\u672f**\uff1a\u5f00\u53d1\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u7684\u5bb9\u9519\u6280\u672f\uff0c\u5982\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u673a\u5236\u6216\u591a\u6a21\u6001\u7279\u5f81\u63d0\u53d6\uff0c\u4ee5\u63d0\u5347\u6a21\u578b\u7684\u7efc\u5408\u5bb9\u9519\u80fd\u529b\u3002\n\n### \u9884\u671f\u6210\u679c\n\n\u901a\u8fc7\u4e0a\u8ff0\u7814\u7a76\uff0c\u9884\u671f\u80fd\u591f\uff1a\n\n- \u63d0\u51fa\u5e76\u9a8c\u8bc1\u6709\u6548\u7684\u5bb9\u9519\u673a\u5236\uff0c\u663e\u8457\u51cf\u5c11MCTS\u4e0ePRM\u7ed3\u5408\u6846\u67b6\u4e2d\u7684\u9519\u8bef\u51b3\u7b56\u5f71\u54cd\u3002\n- \u589e\u5f3a\u6a21\u578b\u5728\u52a8\u6001\u73af\u5883\u4e0b\u7684\u7a33\u5b9a\u6027\u548c\u51b3\u7b56\u51c6\u786e\u6027\uff0c\u63d0\u5347\u5176\u5728\u590d\u6742\u5e94\u7528\u573a\u666f\u4e2d\u7684\u9002\u7528\u6027\u3002\n- \u63d0\u5347\u591a\u6a21\u6001\u6570\u636e\u4e0b\u7684\u5bb9\u9519\u80fd\u529b\uff0c\u4e3a\u8de8\u6a21\u6001\u51b3\u7b56\u4f18\u5316\u63d0\u4f9b\u65b0\u7684\u6280\u672f\u652f\u6301\u548c\u7406\u8bba\u4f9d\u636e\u3002\n\n### \u7ed3\u8bba\n\n\u5bb9\u9519\u673a\u5236\u5728MCTS\u4e0ePRM\u7ed3\u5408\u6846\u67b6\u4e2d\u626e\u6f14\u7740\u81f3\u5173\u91cd\u8981\u7684\u89d2\u8272\u3002\u901a\u8fc7\u6df1\u5165\u7814\u7a76\u5bb9\u9519\u673a\u5236\u7684\u8bbe\u8ba1\u4e0e\u4f18\u5316\u3001\u52a8\u6001\u73af\u5883\u4e0b\u7684\u5bb9\u9519\u7b56\u7565\u4ee5\u53ca\u591a\u6a21\u6001\u6570\u636e\u4e0b\u7684\u5bb9\u9519\u878d\u5408\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u6a21\u578b\u7684\u7a33\u5b9a\u6027\u548c\u51b3\u7b56\u51c6\u786e\u6027\uff0c\u63a8\u52a8\u8be5\u9886\u57df\u7684\u6280\u672f\u8fdb\u6b65\u548c\u5e94\u7528\u62d3\u5c55\u3002\u300d", "ref_task_step_id": ""}, "__type__": "task_step_node"}, "ac68fbc2-cec5-4dc0-9a52-1d0003ba3731": {"__data__": {"id_": "ac68fbc2-cec5-4dc0-9a52-1d0003ba3731", "metadata": {}, "relationships": {}, "hash": "", "start_task_context": "MCTS\u5728PRM\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u7684\u5e94\u7528\u63a2\u7d22\u7efc\u8ff0", "aemo_representation_context": "### Step 1: \u5206\u6790\u8fd1\u51e0\u5e74\u7814\u7a76\u9886\u57df\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba\n\n**\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09**\uff1a\n- **\u57fa\u672c\u539f\u7406**\uff1aMCTS\u662f\u4e00\u79cd\u7528\u4e8e\u51b3\u7b56\u8fc7\u7a0b\u7684\u542f\u53d1\u5f0f\u641c\u7d22\u7b97\u6cd5\uff0c\u5e7f\u6cdb\u5e94\u7528\u4e8e\u6e38\u620fAI\u3001\u8def\u5f84\u89c4\u5212\u7b49\u9886\u57df\u3002\u5176\u6838\u5fc3\u601d\u60f3\u662f\u901a\u8fc7\u6a21\u62df\u591a\u6b21\u968f\u673a\u6e38\u620f\u6765\u8bc4\u4f30\u4e0d\u540c\u51b3\u7b56\u7684\u4f18\u52a3\u3002\n- **\u6280\u672f\u6846\u67b6**\uff1a\u4e3b\u8981\u5305\u62ec\u9009\u62e9\uff08Selection\uff09\u3001\u6269\u5c55\uff08Expansion\uff09\u3001\u6a21\u62df\uff08Simulation\uff09\u548c\u56de\u6eaf\uff08Backpropagation\uff09\u56db\u4e2a\u6b65\u9aa4\u3002\n\n**\u504f\u597d\u7b56\u7565\u6a21\u578b\uff08PRM\uff09**\uff1a\n- **\u57fa\u672c\u539f\u7406**\uff1aPRM\u65e8\u5728\u901a\u8fc7\u5b66\u4e60\u7528\u6237\u7684\u504f\u597d\u6765\u4f18\u5316\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5e38\u7528\u4e8e\u63a8\u8350\u7cfb\u7edf\u3001\u4e2a\u6027\u5316\u670d\u52a1\u7b49\u3002\n- **\u6280\u672f\u6846\u67b6**\uff1a\u901a\u5e38\u5305\u62ec\u504f\u597d\u5efa\u6a21\u3001\u7b56\u7565\u4f18\u5316\u548c\u6a21\u578b\u5fae\u8c03\u7b49\u73af\u8282\u3002\n\n**\u65b9\u6cd5\u8bba**\uff1a\n- **\u96c6\u6210\u5b66\u4e60**\uff1a\u5c06MCTS\u4e0ePRM\u7ed3\u5408\uff0c\u5229\u7528MCTS\u7684\u641c\u7d22\u80fd\u529b\u4f18\u5316PRM\u7684\u51b3\u7b56\u8fc7\u7a0b\u3002\n- **\u5f3a\u5316\u5b66\u4e60**\uff1a\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u673a\u5236\u4e0d\u65ad\u8c03\u6574PRM\u7684\u53c2\u6570\uff0c\u4ee5\u9002\u5e94\u52a8\u6001\u73af\u5883\u3002\n\n### Step 2: \u7814\u7a76\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u4e3b\u8981\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0e\u53d8\u4f53\n\n**\u5e94\u7528\u573a\u666f**\uff1a\n- **\u6e38\u620fAI**\uff1aMCTS\u4e0ePRM\u7ed3\u5408\u7528\u4e8e\u6e38\u620f\u4e2d\u7684\u667a\u80fd\u51b3\u7b56\uff0c\u5982AlphaGo\u3002\n- **\u8def\u5f84\u89c4\u5212**\uff1a\u5728\u673a\u5668\u4eba\u5bfc\u822a\u548c\u81ea\u52a8\u9a7e\u9a76\u4e2d\uff0c\u5229\u7528MCTS\u4f18\u5316\u8def\u5f84\u9009\u62e9\u3002\n- **\u63a8\u8350\u7cfb\u7edf**\uff1a\u901a\u8fc7PRM\u5b66\u4e60\u7528\u6237\u504f\u597d\uff0c\u7ed3\u5408MCTS\u8fdb\u884c\u66f4\u7cbe\u51c6\u7684\u63a8\u8350\u3002\n\n**\u53d8\u4f53**\uff1a\n- **\u6df1\u5ea6MCTS**\uff1a\u7ed3\u5408\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff0c\u63d0\u5347MCTS\u7684\u51b3\u7b56\u80fd\u529b\u3002\n- **\u591a\u6a21\u6001PRM**\uff1a\u878d\u5408\u591a\u79cd\u6570\u636e\u6e90\uff08\u5982\u6587\u672c\u3001\u56fe\u50cf\uff09\uff0c\u589e\u5f3aPRM\u7684\u6cdb\u5316\u80fd\u529b\u3002\n\n### Step 3: \u8bc4\u4f30\u5b66\u672f\u754c\u7684\u6280\u672f\u8fdb\u6b65\u4e0e\u5c40\u9650\u6027\n\n**\u6280\u672f\u8fdb\u6b65**\uff1a\n- **\u6027\u80fd\u63d0\u5347**\uff1aMCTS\u4e0ePRM\u7ed3\u5408\u663e\u8457\u63d0\u5347\u4e86\u51b3\u7b56\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002\n- **\u6cdb\u5316\u80fd\u529b**\uff1a\u591a\u6a21\u6001PRM\u5728\u5904\u7406\u591a\u6837\u5316\u6570\u636e\u65f6\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002\n\n**\u5c40\u9650\u6027**\uff1a\n- **\u8ba1\u7b97\u590d\u6742\u5ea6**\uff1aMCTS\u5728\u5927\u89c4\u6a21\u95ee\u9898\u4e2d\u8ba1\u7b97\u91cf\u5927\uff0c\u6548\u7387\u6709\u5f85\u63d0\u5347\u3002\n- **\u6570\u636e\u4f9d\u8d56**\uff1aPRM\u7684\u6027\u80fd\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u9ad8\u8d28\u91cf\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u6570\u636e\u504f\u5dee\u95ee\u9898\u4f9d\u7136\u5b58\u5728\u3002\n\n### Step 4: \u63a2\u8ba8\u8ba1\u7b97\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\n\n**\u9002\u7528\u6027**\uff1a\n- **\u591a\u9886\u57df\u5e94\u7528**\uff1aMCTS+PRM\u6846\u67b6\u5728\u6e38\u620f\u3001\u8def\u5f84\u89c4\u5212\u3001\u63a8\u8350\u7cfb\u7edf\u7b49\u591a\u4e2a\u9886\u57df\u8868\u73b0\u51fa\u826f\u597d\u7684\u9002\u7528\u6027\u3002\n- **\u591a\u6a21\u6001\u6570\u636e**\uff1a\u591a\u6a21\u6001PRM\u80fd\u591f\u5904\u7406\u591a\u79cd\u7c7b\u578b\u7684\u6570\u636e\uff0c\u9002\u7528\u4e8e\u590d\u6742\u5e94\u7528\u573a\u666f\u3002\n\n**\u6cdb\u5316\u80fd\u529b**\uff1a\n- **\u8de8\u9886\u57df\u6cdb\u5316**\uff1a\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\uff0cMCTS+PRM\u6846\u67b6\u5728\u4e0d\u540c\u9886\u57df\u95f4\u5c55\u73b0\u51fa\u8f83\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002\n- **\u52a8\u6001\u73af\u5883\u9002\u5e94**\uff1a\u5f3a\u5316\u5b66\u4e60\u673a\u5236\u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u9002\u5e94\u52a8\u6001\u53d8\u5316\u7684\u73af\u5883\u3002\n\n### Step 5: \u5206\u6790\u6700\u65b0\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027\n\n**\u7a33\u5b9a\u6027**\uff1a\n- **\u7b97\u6cd5\u4f18\u5316**\uff1a\u901a\u8fc7\u6539\u8fdbMCTS\u7684\u9009\u62e9\u7b56\u7565\u548cPRM\u7684\u53c2\u6570\u8c03\u6574\u673a\u5236\uff0c\u63d0\u5347\u4e86\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u3002\n- **\u9c81\u68d2\u6027\u589e\u5f3a**\uff1a\u5f15\u5165\u5bf9\u6297\u8bad\u7ec3\u548c\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\uff0c\u63d0\u9ad8\u6a21\u578b\u5728\u590d\u6742\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u6027\u3002\n\n**\u5bb9\u9519\u6027**\uff1a\n- **\u9519\u8bef\u5904\u7406**\uff1a\u5728\u6a21\u62df\u548c\u56de\u6eaf\u9636\u6bb5\u5f15\u5165\u5bb9\u9519\u673a\u5236\uff0c\u51cf\u5c11\u9519\u8bef\u51b3\u7b56\u7684\u5f71\u54cd\u3002\n- **\u5927\u89c4\u6a21\u6570\u636e\u9002\u5e94\u6027**\uff1a\u4f18\u5316\u7b97\u6cd5\u7ed3\u6784\uff0c\u63d0\u5347\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\u7684\u5904\u7406\u80fd\u529b\u3002\n\n### Step 6: \u8bc4\u4f30\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218\n\n**\u672a\u6765\u7814\u7a76\u65b9\u5411**\uff1a\n- **\u9ad8\u6548\u8ba1\u7b97**\uff1a\u7814\u7a76\u66f4\u9ad8\u6548\u7684MCTS\u7b97\u6cd5\uff0c\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002\n- **\u6570\u636e\u589e\u5f3a**\uff1a\u63a2\u7d22\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u51cf\u5c11PRM\u5bf9\u9ad8\u8d28\u91cf\u6570\u636e\u7684\u4f9d\u8d56\u3002\n\n**\u6311\u6218**\uff1a\n- **\u8de8\u6a21\u6001\u878d\u5408**\uff1a\u5982\u4f55\u6709\u6548\u878d\u5408\u591a\u79cd\u6a21\u6001\u6570\u636e\uff0c\u63d0\u5347\u6a21\u578b\u7684\u7efc\u5408\u6027\u80fd\u3002\n- **\u52a8\u6001\u73af\u5883\u9002\u5e94**\uff1a\u5728\u52a8\u6001\u73af\u5883\u4e0b\uff0c\u5982\u4f55\u4fdd\u6301\u6a21\u578b\u7684\u7a33\u5b9a\u6027\u548c\u9ad8\u6548\u6027\u3002\n\n**\u63a8\u52a8\u540e\u7eed\u7814\u7a76**\uff1a\n- **\u65b0\u95ee\u9898\u63d0\u51fa**\uff1a\u5982\u591a\u6a21\u6001\u6570\u636e\u4e0b\u7684\u504f\u597d\u5efa\u6a21\u3001\u52a8\u6001\u73af\u5883\u4e0b\u7684\u51b3\u7b56\u4f18\u5316\u7b49\u3002\n- **\u7814\u7a76\u5207\u5165\u70b9**\uff1a\u4ece\u7b97\u6cd5\u4f18\u5316\u3001\u6570\u636e\u589e\u5f3a\u3001\u8de8\u6a21\u6001\u878d\u5408\u7b49\u591a\u4e2a\u89d2\u5ea6\u5207\u5165\uff0c\u63a8\u52a8\u6280\u672f\u8fdb\u6b65\u3002\n\n### \u603b\u7ed3\n\n\u8fd1\u5e74\u6765\uff0cMCTS\u5728PRM\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u7684\u5e94\u7528\u63a2\u7d22\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4e3b\u8981\u4f53\u73b0\u5728\u6280\u672f\u6846\u67b6\u7684\u5b8c\u5584\u3001\u5e94\u7528\u573a\u666f\u7684\u62d3\u5c55\u3001\u6027\u80fd\u7684\u63d0\u5347\u4ee5\u53ca\u7a33\u5b9a\u6027\u548c\u5bb9\u9519\u6027\u7684\u589e\u5f3a\u3002\u7136\u800c\uff0c\u4ecd\u5b58\u5728\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u3001\u6570\u636e\u4f9d\u8d56\u6027\u5f3a\u7b49\u5c40\u9650\u6027\u3002\u672a\u6765\u7684\u7814\u7a76\u5e94\u5173\u6ce8\u9ad8\u6548\u8ba1\u7b97\u3001\u6570\u636e\u589e\u5f3a\u548c\u8de8\u6a21\u6001\u878d\u5408\u7b49\u65b9\u5411\uff0c\u4ee5\u5e94\u5bf9\u52a8\u6001\u73af\u5883\u4e0b\u7684\u51b3\u7b56\u4f18\u5316\u7b49\u6311\u6218\uff0c\u63a8\u52a8\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002", "task_step_name": "\u8bc4\u4f30\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218", "task_step_description": "\u7814\u7a76\u66f4\u9ad8\u6548\u7684MCTS\u7b97\u6cd5\uff0c\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "task_step_level": "5", "task_step_question": "### \u95ee\u9898\u63d0\u51fa\n\n\u5728**\u8bc4\u4f30\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218**\u7684\u80cc\u666f\u4e0b\uff0c\u7ed3\u5408**\u7814\u7a76\u66f4\u9ad8\u6548\u7684MCTS\u7b97\u6cd5\uff0c\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6**\u7684\u4efb\u52a1\u6b65\u9aa4\uff0c\u63d0\u51fa\u4ee5\u4e0b\u95ee\u9898\uff1a\n\n**\u95ee\u9898**\uff1a  \n\u5728MCTS\u4e0ePRM\u7ed3\u5408\u7684\u5e94\u7528\u4e2d\uff0c\u5982\u4f55\u8bbe\u8ba1\u66f4\u9ad8\u6548\u7684MCTS\u7b97\u6cd5\u4ee5\u663e\u8457\u964d\u4f4e\u5176\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u51b3\u7b56\u7684\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff1f\u5177\u4f53\u800c\u8a00\uff0c\u6709\u54ea\u4e9b\u6f5c\u5728\u7684\u7b97\u6cd5\u4f18\u5316\u7b56\u7565\uff08\u5982\u5e76\u884c\u8ba1\u7b97\u3001\u526a\u679d\u6280\u672f\u3001\u542f\u53d1\u5f0f\u641c\u7d22\u7b49\uff09\u53ef\u4ee5\u5728\u4e0d\u540c\u5e94\u7528\u573a\u666f\u4e2d\u6709\u6548\u51cf\u5c11\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\uff0c\u5e76\u786e\u4fdd\u6a21\u578b\u5728\u52a8\u6001\u73af\u5883\u4e0b\u7684\u7a33\u5b9a\u6027\u548c\u9002\u5e94\u6027\uff1f\n\n**\u80cc\u666f\u4e0e\u610f\u4e49**\uff1a  \nMCTS\u5728\u590d\u6742\u51b3\u7b56\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u8ba1\u7b97\u590d\u6742\u5ea6\u8f83\u9ad8\uff0c\u5c24\u5176\u662f\u5728\u5927\u89c4\u6a21\u95ee\u9898\u4e2d\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u5e7f\u6cdb\u90e8\u7f72\u3002\u901a\u8fc7\u7814\u7a76\u66f4\u9ad8\u6548\u7684MCTS\u7b97\u6cd5\uff0c\u4e0d\u4ec5\u53ef\u4ee5\u63d0\u5347\u6a21\u578b\u7684\u5b9e\u65f6\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u8fd8\u80fd\u4e3aPRM\u504f\u597d\u7b56\u7565\u6a21\u578b\u7684\u5fae\u8c03\u63d0\u4f9b\u66f4\u5f3a\u5927\u7684\u652f\u6301\uff0c\u4ece\u800c\u63a8\u52a8\u8be5\u6280\u672f\u5728\u6e38\u620fAI\u3001\u8def\u5f84\u89c4\u5212\u3001\u63a8\u8350\u7cfb\u7edf\u7b49\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u5e94\u7528\u3002", "task_step_question_context": [{"ref_id": "455026805307867280", "chunk_id": "0", "score": 0.52734375, "text": "# 2.2 Acceleration of MCTS\nMCTS-based methods have proved their strong capability of solving complex games or tasks. However, the high computational cost of MCTS hinders its application to some real-time and more general scenarios. Therefore, numerous works are devoted to accelerating MCTS. For example, to make the selection stage more effective, some heuristic pruning methods [ 14 ,33 ,29 ,1 ,2 ] aim to reduce the width and depth of the search tree with some heuristic functions. Furthermore, for more efficient evaluations, Lorentz [ 22 ] proposed early playout termination of MCTS (MCTS-EPT) to stop the random playouts early and use an evaluation function to assess win or loss. Moreover, Hsueh et al. [18 ] applied MCTS-EPT to the Chinese dark chess and proved its effectiveness. Afterward, similar ideas have been applied in the evaluation stage of AlphaGoZero [ 32 ] and later MCTS-based methods [31 ,27 ,34 ]. They evaluate the $Q$ -values through a learnable evaluation network instead of running playouts to the end. Grill et al. [15 ] propose a novel regularized policy optimization method based on AlphaZero to decrease the search budget of MCTS, which is from the optimization perspective. Danihelka et al. [10 ] propose a policy improvement algorithm based on sampling actions without replacement, named Gumbel trick to achieve better performance when planning with few simulations. However, these methods mentioned above focus on the specific stage of the search iteration or reduce the total budget through pruning and optimization methods, which are orthogonal to us. And few works targets at the search loop. Lan et al. [21 ] propose DS-MCTS, which defines the uncertainty of MCTS and approximates it by extra DNNs with specific features for board games in training. During the evaluation, DS-MCTS will check periodically and stop the search if the state is certain.\n\n# 3 Background\nThe AlphaGo series of work [ 30 ,32 ,31 ,27 ] are all MCTS-based reinforcement learning algorithms. Those algorithms assume the environment transition dynamics are known or learn the environment dynamics. Based on the dynamics, they use the Monte-Carlo tree search (MCTS) as the policy improvement operator. I.e., taking in the current policy, MCTS returns a better policy with the search algorithm. The systematic search allows the MCTS-based RL algorithm to quickly improve the policy and perform much better in the setting where heavy reasoning is required.\n\n# 3.1 MCTS\nThis part briefly introduces the MCTS method implemented in reinforcement learning applications. As mentioned in the related works, modern MCTS-based RL algorithms include four stages in the search loop, namely selection, expansion, evaluation, and backpropagation.  \n\nMCTS takes in the current states and generates a policy after the search loop of $N$ iterations. Here $N$ is a constant number of iterations set by the designer, regarded as the total budget. In the selection stage of each iteration, an action will be selected by maximizing over UCB. Specifically, AlphaZero [31 ] and MuZero [ 27 ] are developed based on a variant of UCB, P-UCT [ 25 ] and have achieved great success on board games and Atari games. The formula of P-UCT is the Eq (1):  \n\n$$\na^{k}=\\arg\\operatorname*{max}_{a\\in\\mathcal{A}}Q(s,a)+P(s,a)\\frac{\\sqrt{\\sum_{b\\in\\mathcal{A}}N(s,b)}}{1+N(s,a)}(c_{1}+\\log((\\sum_{b\\in\\mathcal{A}}N(s,b)+c_{2}+1)/c_{2})),\n$$  \n\nwhere $k$ is the index of iteration, $\\boldsymbol{\\mathcal{A}}$ is the acti $Q(s,a)$ is the estimated Q-value, $P(s,a)$ is the policy prior obtained from neural networks, $N(s,a)$ is the visitations to select the action a from the state $s$ and $c_{1},c_{2}$ are hyper-parameters. The output of MCTS is the visitation of each action of the root node. After $N$ search iterations, the final policy $\\pi(s)$ is defined as the normalized root visitation distribution simplification, we use $\\pi_{N}(s)$ $\\pi_{k}$ in place of , where $\\begin{array}{r}{\\pi_{k}(s,a)=N(s,a)/\\sum_{b\\in\\mathcal{A}}N(s,b)=N(s,a)/k,a\\in\\mathcal{A}}\\end{array}$ $\\pi_{k}(s)$ sometimes. And the detailed procedure of MCTS is \u2208A . For introduced in Appendix. In our method, we propose to approximate the final policy $\\pi_{N}(s)$ with $\\hat{\\pi}_{k}(s)$ ,which we name as a virtual expanded policy, through a new expansion method and a termination rule. In this way, the number of iterations in MCTS can be reduced from $N$ to $k$ .\n\n# 3.2 Computation Requirement\nMost of the computations in MCTS-based RL are in the MCTS procedure. Each action taken by MCTS requires $N$ times neural network evaluations, where $N$ is a constant number of iterations in the search loop. Traditional RL algorithms, such as PPO [ 28 ] or DQN [ 23 ], only need a single neural network evaluation per action. Thus, MCTS-based RL is roughly $N$ times computationally more expensive than traditional RL algorithms. In practice, training a single Atari game needs 12 hours of computation time on 40 TPUs [ 27 ]. The computation need is roughly two orders of magnitude more than traditional RL algorithms [28], although the final performance of MuZero is much better.\n\n# 4 Method\nWe aim to spend more search time on harder states and less on easier states. Intuitively, human knows when to make a quick decision or a slow decision under different circumstances. Unfortunately, this situation-aware behavior is absent in current MCTS algorithms. Therefore, we propose an MCTS variant that terminates the search iteration adaptively. It consists of two components: a novel expansion method named virtual expansion to estimate the final visitation based on the current partial tree; a termination rule that decides when to terminate based on the hardness of the current scenario. And we will display the adaptive mechanism through visualizations in Section 5.5."}, {"ref_id": "454848282814999732", "chunk_id": "2", "score": 0.48046875, "text": "# Spending Thinking Time Wisely: Accelerating MCTS with Virtual Expansions\nWeirui $\\mathbf{Ye}^{*}$ Pieter Abbeel \u2020Yang Gao $\\ast\\ddag\\S$ \u2217Tsinghua University, \u2020UC Berkeley, \u00a7Shanghai Qi Zhi Institute\n\n# Abstract\nOne of the most important AI research questions is to trade off computation versus performance since \u201cperfect rationality\" exists in theory but is impossible to achieve in practice. Recently, Monte-Carlo tree search (MCTS) has attracted considerable attention due to the significant performance improvement in various challenging domains. However, the expensive time cost during search severely restricts its scope for applications. This paper proposes the Virtual MCTS (V-MCTS), a variant of MCTS that spends more search time on harder states and less search time on simpler states adaptively. We give theoretical bounds of the proposed method and evaluate the performance and computations on $9\\times9$ Go board games and Atari games. Experiments show that our method can achieve comparable performances to the original search algorithm while requiring less than $50\\%$ search time on average. We believe that this approach is a viable alternative for tasks under limited time and resources. The code is available at https://github.com/YeWR/V-MCTS.git .\n\n# 1 Introduction\nWhen artificial intelligence was first studied in the 1950s, researchers have sought to find the solution to the question \u201cHow to build an agent with perfect rationality\". The term \u201cperfect rationality\" [7 ,24 ,26 ] here refers to the decision made with infinite amounts of computations. However, one can only solve small-scale problems without considering the practical computation time since classical search algorithms usually exhibit exponential running time. Therefore, recent AI research would no longer seek to achieve \u201cperfect rationality\", but instead carefully trade-off computation versus the level of rationality. People have developed computational models like \u201cbounded optimality\" to model these settings [ 26 ]. The increasing level of rationality under the same computational budget has given us a lot of AI successes. Algorithms include the Monte-Carlo sampling algorithms, the variational inference algorithms, and using DNNs as universal function approximators [9, 8, 13, 30, 17].  \n\nRecently, MCTS-based RL algorithms have achieved much success, mainly on board games. The most notable achievement is that AlphaGo beats Hui Fan in 2015 [ 30 ]. It is the first time a computer program beat a human professional Go player. Afterward, AlphaGo beats two top-ranking human players, Lee Sedol in 2016 and Jie Ke in 2017, the latter of which ranked first worldwide at the time. Later, MCTS-based RL algorithms were further extended to other board games and Atari games [ 27 ]. EfficientZero [ 34 ] significantly improves the sample efficiency of MCTS-based RL algorithms, shedding light on its future applications in the real world like robotics and self-driving.  \n\nDespite the impressive performance of MCTS-based RL algorithms, they require massive amounts of computation to train and evaluate. For example, MuZero [ 27 ] used 1000 TPUs trained for 12 hours to learn the game of Go, and for a single Atari game, it needs 40 TPUs to train 12 hours. Compared to previous algorithms on the Atari games benchmark, it needs around two orders of magnitude more compute. This prohibitively large computational requirement has slowed down both the further development of MCTS-based RL algorithms as well as its practical use.  \n\nUnder the hood, MCTS-based RL algorithms imagine the futures when taking different future action sequences. However, this imaging process for the current method is not computationally efficient. For example, AlphaGo needs to look ahead 1600 game states to place a single stone. On the contrary, top human professional players can only think through around 100-200 game states per minute [ 30 ]. Apart from the inefficiency, the current MCTS algorithm deals with easy and challenging cases with the same computational budget. However, human knows to use their time when it is most needed.  \n\nIn this paper, we aim to design new algorithms that save the computational time of the MCTSbased RL methods. We make three key contributions : (1) We present Virtual MCTS, a variant of MCTS, to approximate the vanilla MCTS search policies with less computation. Moreover, unlike previous pruning-based methods that focus on the selection or evaluation stage in MCTS, our method improves the search loop. It terminates the search iterations earlier adaptively when current states are simpler; (2) Theoretically, we provide some error bounds of the proposed method. Furthermore, the visualization results indicate that Virtual MCTS has a better computation and performance trade-off than vanilla MCTS; (3) Empirically, our method can save more than $50\\%$ of search times on the challenging game Go $9\\times9$ and more than $60\\%$ on the visually complex Atari games while keeping comparable performances to those of vanilla MCTS.\n\n# 2 Related Work\n\n# 2.1 Reinforcement Learning with MCTS\nFor a long time, Computer Go has been regarded as a remarkably challenging game [ 3 ,6 ]. Researchers attempt to use Monte-Carlo techniques that evaluate the value of the node state through random playouts [ 4 ,11 ,12 ,30 ]. Afterward, UCT algorithms have generally been applied in Monte-Carlo tree search (MCTS) algorithms, which use UCB1 to select action at each node of the tree [ 20 ]. Recently, MCTS-based RL methods [ 30 ,32 ,31 ,27 ] have become increasingly popular and achieved super-human performances on board games because of their strong ability to search.  \n\nModern MCTS-based RL algorithms include four stages in the search loop : selection, expansion, evaluation, and backpropagation. The computation bottlenecks in vanilla MCTS come from the search loop, especially for the evaluation stage and the selection stage of each iteration. The selection stage is time-consuming when the search tree becomes wider and deeper. The evaluation stage is quite expensive because people attempt to evaluate the node value by random playouts to the end in previous researches. Due to the search loop, MCTS-based algorithms have multiple model inferences compared to other model-free RL methods like PPO [28] and SAC [16]."}, {"ref_id": "455026805323333778", "chunk_id": "1", "score": 0.4140625, "text": "# 5 Experiment\nTo examine the performance of MCTS-VS, we conduct experiments on different tasks, including synthetic functions, NAS-bench problems and MuJoCo locomotion tasks, to compare MCTS-VS with other black-box optimization methods. For MCTS-VS, we use the same hyper-parameters except $C_{p}$ , which is used for calculating UCB in Eq. (1). For Dropout and embedding-based methods, we set the parameter $d$ to the number of valid dimensions for synthetic functions, and a reasonable value for real-world problems. The hyper-parameters of the same components of different methods are set to the same. We use five identical random seeds (2021\u20132025) for all problems and methods. More details about the settings can be found in Appendix C. Our code is available at https://github.com/lamda-bbo/MCTS-VS ."}], "task_step_question_answer": "ac68fbc2-cec5-4dc0-9a52-1d0003ba3731:\u300c\u5728MCTS\u4e0ePRM\u7ed3\u5408\u7684\u5e94\u7528\u4e2d\uff0c\u8bbe\u8ba1\u66f4\u9ad8\u6548\u7684MCTS\u7b97\u6cd5\u4ee5\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u5e76\u4fdd\u6301\u6216\u63d0\u5347\u51b3\u7b56\u7684\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u7814\u7a76\u65b9\u5411\u3002\u4ee5\u4e0b\u662f\u4e00\u4e9b\u6f5c\u5728\u7684\u7b97\u6cd5\u4f18\u5316\u7b56\u7565\uff1a\n\n1. **\u5e76\u884c\u8ba1\u7b97**\uff1a\u901a\u8fc7\u5e76\u884c\u5316MCTS\u7684\u641c\u7d22\u8fc7\u7a0b\uff0c\u53ef\u4ee5\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u65f6\u95f4\u3002\u4f8b\u5982\uff0c\u53ef\u4ee5\u5c06\u4e0d\u540c\u7684\u641c\u7d22\u6811\u5206\u652f\u5206\u914d\u7ed9\u4e0d\u540c\u7684\u5904\u7406\u5668\u6216\u8ba1\u7b97\u5355\u5143\uff0c\u4ece\u800c\u52a0\u901f\u6574\u4f53\u641c\u7d22\u8fc7\u7a0b\u3002\n\n2. **\u526a\u679d\u6280\u672f**\uff1a\u5728\u641c\u7d22\u8fc7\u7a0b\u4e2d\u5e94\u7528\u526a\u679d\u6280\u672f\uff0c\u53ef\u4ee5\u6709\u6548\u5730\u51cf\u5c11\u641c\u7d22\u6811\u7684\u5bbd\u5ea6\u548c\u6df1\u5ea6\u3002\u4f8b\u5982\uff0c\u4f7f\u7528\u542f\u53d1\u5f0f\u51fd\u6570\u6765\u8bc4\u4f30\u8282\u70b9\u7684\u6f5c\u5728\u4ef7\u503c\uff0c\u4ece\u800c\u63d0\u524d\u526a\u9664\u90a3\u4e9b\u4e0d\u592a\u53ef\u80fd\u5e26\u6765\u9ad8\u56de\u62a5\u7684\u5206\u652f\u3002\n\n3. **\u542f\u53d1\u5f0f\u641c\u7d22**\uff1a\u5f15\u5165\u542f\u53d1\u5f0f\u641c\u7d22\u7b56\u7565\uff0c\u53ef\u4ee5\u5728\u641c\u7d22\u8fc7\u7a0b\u4e2d\u4f18\u5148\u63a2\u7d22\u90a3\u4e9b\u66f4\u6709\u6f5c\u529b\u7684\u8282\u70b9\u3002\u4f8b\u5982\uff0c\u4f7f\u7528\u57fa\u4e8e\u7ecf\u9a8c\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u6765\u6307\u5bfc\u641c\u7d22\u65b9\u5411\uff0c\u4ece\u800c\u51cf\u5c11\u4e0d\u5fc5\u8981\u7684\u8ba1\u7b97\u3002\n\n4. **\u865a\u62df\u6269\u5c55\u4e0e\u81ea\u9002\u5e94\u7ec8\u6b62**\uff1a\u5982V-MCTS\u6240\u63d0\u51fa\u7684\uff0c\u901a\u8fc7\u865a\u62df\u6269\u5c55\u548c\u81ea\u9002\u5e94\u7ec8\u6b62\u7b56\u7565\uff0c\u53ef\u4ee5\u5728\u8f83\u7b80\u5355\u7684\u72b6\u6001\u4e0b\u51cf\u5c11\u641c\u7d22\u65f6\u95f4\uff0c\u800c\u5728\u8f83\u590d\u6742\u7684\u72b6\u6001\u4e0b\u589e\u52a0\u641c\u7d22\u65f6\u95f4\uff0c\u4ece\u800c\u5b9e\u73b0\u8ba1\u7b97\u8d44\u6e90\u7684\u4f18\u5316\u5206\u914d\u3002\n\n5. **\u5b66\u4e60\u8bc4\u4f30\u7f51\u7edc**\uff1a\u4f7f\u7528\u5b66\u4e60\u8bc4\u4f30\u7f51\u7edc\u6765\u66ff\u4ee3\u4f20\u7edf\u7684\u968f\u673a\u6a21\u62df\uff0c\u53ef\u4ee5\u5728\u8bc4\u4f30\u9636\u6bb5\u51cf\u5c11\u8ba1\u7b97\u91cf\u3002\u4f8b\u5982\uff0cAlphaGoZero\u548cMuZero\u4e2d\u4f7f\u7528\u7684\u8bc4\u4f30\u7f51\u7edc\uff0c\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u76f4\u63a5\u9884\u6d4b\u8282\u70b9\u7684\u4ef7\u503c\uff0c\u4ece\u800c\u907f\u514d\u4e86\u5927\u91cf\u7684\u968f\u673a\u6a21\u62df\u3002\n\n\u8fd9\u4e9b\u7b56\u7565\u5728\u4e0d\u540c\u5e94\u7528\u573a\u666f\u4e2d\u90fd\u53ef\u4ee5\u6709\u6548\u51cf\u5c11\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\uff0c\u5e76\u786e\u4fdd\u6a21\u578b\u5728\u52a8\u6001\u73af\u5883\u4e0b\u7684\u7a33\u5b9a\u6027\u548c\u9002\u5e94\u6027\u3002\u901a\u8fc7\u7ed3\u5408\u8fd9\u4e9b\u4f18\u5316\u7b56\u7565\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u5347MCTS\u7b97\u6cd5\u7684\u6548\u7387\uff0c\u4ece\u800c\u63a8\u52a8\u5176\u5728\u6e38\u620fAI\u3001\u8def\u5f84\u89c4\u5212\u3001\u63a8\u8350\u7cfb\u7edf\u7b49\u9886\u57df\u7684\u5e7f\u6cdb\u5e94\u7528\u3002\u300d", "ref_task_step_id": ""}, "__type__": "task_step_node"}, "7895a383-ce04-42fa-9517-e0cea2b11f50": {"__data__": {"id_": "7895a383-ce04-42fa-9517-e0cea2b11f50", "metadata": {}, "relationships": {}, "hash": "", "start_task_context": "MCTS\u5728PRM\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u7684\u5e94\u7528\u63a2\u7d22\u7efc\u8ff0", "aemo_representation_context": "### Step 1: \u5206\u6790\u8fd1\u51e0\u5e74\u7814\u7a76\u9886\u57df\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba\n\n**\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09**\uff1a\n- **\u57fa\u672c\u539f\u7406**\uff1aMCTS\u662f\u4e00\u79cd\u7528\u4e8e\u51b3\u7b56\u8fc7\u7a0b\u7684\u542f\u53d1\u5f0f\u641c\u7d22\u7b97\u6cd5\uff0c\u5e7f\u6cdb\u5e94\u7528\u4e8e\u6e38\u620fAI\u3001\u8def\u5f84\u89c4\u5212\u7b49\u9886\u57df\u3002\u5176\u6838\u5fc3\u601d\u60f3\u662f\u901a\u8fc7\u6a21\u62df\u591a\u6b21\u968f\u673a\u6e38\u620f\u6765\u8bc4\u4f30\u4e0d\u540c\u51b3\u7b56\u7684\u4f18\u52a3\u3002\n- **\u6280\u672f\u6846\u67b6**\uff1a\u4e3b\u8981\u5305\u62ec\u9009\u62e9\uff08Selection\uff09\u3001\u6269\u5c55\uff08Expansion\uff09\u3001\u6a21\u62df\uff08Simulation\uff09\u548c\u56de\u6eaf\uff08Backpropagation\uff09\u56db\u4e2a\u6b65\u9aa4\u3002\n\n**\u504f\u597d\u7b56\u7565\u6a21\u578b\uff08PRM\uff09**\uff1a\n- **\u57fa\u672c\u539f\u7406**\uff1aPRM\u65e8\u5728\u901a\u8fc7\u5b66\u4e60\u7528\u6237\u7684\u504f\u597d\u6765\u4f18\u5316\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5e38\u7528\u4e8e\u63a8\u8350\u7cfb\u7edf\u3001\u4e2a\u6027\u5316\u670d\u52a1\u7b49\u3002\n- **\u6280\u672f\u6846\u67b6**\uff1a\u901a\u5e38\u5305\u62ec\u504f\u597d\u5efa\u6a21\u3001\u7b56\u7565\u4f18\u5316\u548c\u6a21\u578b\u5fae\u8c03\u7b49\u73af\u8282\u3002\n\n**\u65b9\u6cd5\u8bba**\uff1a\n- **\u96c6\u6210\u5b66\u4e60**\uff1a\u5c06MCTS\u4e0ePRM\u7ed3\u5408\uff0c\u5229\u7528MCTS\u7684\u641c\u7d22\u80fd\u529b\u4f18\u5316PRM\u7684\u51b3\u7b56\u8fc7\u7a0b\u3002\n- **\u5f3a\u5316\u5b66\u4e60**\uff1a\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u673a\u5236\u4e0d\u65ad\u8c03\u6574PRM\u7684\u53c2\u6570\uff0c\u4ee5\u9002\u5e94\u52a8\u6001\u73af\u5883\u3002\n\n### Step 2: \u7814\u7a76\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u4e3b\u8981\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0e\u53d8\u4f53\n\n**\u5e94\u7528\u573a\u666f**\uff1a\n- **\u6e38\u620fAI**\uff1aMCTS\u4e0ePRM\u7ed3\u5408\u7528\u4e8e\u6e38\u620f\u4e2d\u7684\u667a\u80fd\u51b3\u7b56\uff0c\u5982AlphaGo\u3002\n- **\u8def\u5f84\u89c4\u5212**\uff1a\u5728\u673a\u5668\u4eba\u5bfc\u822a\u548c\u81ea\u52a8\u9a7e\u9a76\u4e2d\uff0c\u5229\u7528MCTS\u4f18\u5316\u8def\u5f84\u9009\u62e9\u3002\n- **\u63a8\u8350\u7cfb\u7edf**\uff1a\u901a\u8fc7PRM\u5b66\u4e60\u7528\u6237\u504f\u597d\uff0c\u7ed3\u5408MCTS\u8fdb\u884c\u66f4\u7cbe\u51c6\u7684\u63a8\u8350\u3002\n\n**\u53d8\u4f53**\uff1a\n- **\u6df1\u5ea6MCTS**\uff1a\u7ed3\u5408\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff0c\u63d0\u5347MCTS\u7684\u51b3\u7b56\u80fd\u529b\u3002\n- **\u591a\u6a21\u6001PRM**\uff1a\u878d\u5408\u591a\u79cd\u6570\u636e\u6e90\uff08\u5982\u6587\u672c\u3001\u56fe\u50cf\uff09\uff0c\u589e\u5f3aPRM\u7684\u6cdb\u5316\u80fd\u529b\u3002\n\n### Step 3: \u8bc4\u4f30\u5b66\u672f\u754c\u7684\u6280\u672f\u8fdb\u6b65\u4e0e\u5c40\u9650\u6027\n\n**\u6280\u672f\u8fdb\u6b65**\uff1a\n- **\u6027\u80fd\u63d0\u5347**\uff1aMCTS\u4e0ePRM\u7ed3\u5408\u663e\u8457\u63d0\u5347\u4e86\u51b3\u7b56\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002\n- **\u6cdb\u5316\u80fd\u529b**\uff1a\u591a\u6a21\u6001PRM\u5728\u5904\u7406\u591a\u6837\u5316\u6570\u636e\u65f6\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002\n\n**\u5c40\u9650\u6027**\uff1a\n- **\u8ba1\u7b97\u590d\u6742\u5ea6**\uff1aMCTS\u5728\u5927\u89c4\u6a21\u95ee\u9898\u4e2d\u8ba1\u7b97\u91cf\u5927\uff0c\u6548\u7387\u6709\u5f85\u63d0\u5347\u3002\n- **\u6570\u636e\u4f9d\u8d56**\uff1aPRM\u7684\u6027\u80fd\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u9ad8\u8d28\u91cf\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u6570\u636e\u504f\u5dee\u95ee\u9898\u4f9d\u7136\u5b58\u5728\u3002\n\n### Step 4: \u63a2\u8ba8\u8ba1\u7b97\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\n\n**\u9002\u7528\u6027**\uff1a\n- **\u591a\u9886\u57df\u5e94\u7528**\uff1aMCTS+PRM\u6846\u67b6\u5728\u6e38\u620f\u3001\u8def\u5f84\u89c4\u5212\u3001\u63a8\u8350\u7cfb\u7edf\u7b49\u591a\u4e2a\u9886\u57df\u8868\u73b0\u51fa\u826f\u597d\u7684\u9002\u7528\u6027\u3002\n- **\u591a\u6a21\u6001\u6570\u636e**\uff1a\u591a\u6a21\u6001PRM\u80fd\u591f\u5904\u7406\u591a\u79cd\u7c7b\u578b\u7684\u6570\u636e\uff0c\u9002\u7528\u4e8e\u590d\u6742\u5e94\u7528\u573a\u666f\u3002\n\n**\u6cdb\u5316\u80fd\u529b**\uff1a\n- **\u8de8\u9886\u57df\u6cdb\u5316**\uff1a\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\uff0cMCTS+PRM\u6846\u67b6\u5728\u4e0d\u540c\u9886\u57df\u95f4\u5c55\u73b0\u51fa\u8f83\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002\n- **\u52a8\u6001\u73af\u5883\u9002\u5e94**\uff1a\u5f3a\u5316\u5b66\u4e60\u673a\u5236\u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u9002\u5e94\u52a8\u6001\u53d8\u5316\u7684\u73af\u5883\u3002\n\n### Step 5: \u5206\u6790\u6700\u65b0\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027\n\n**\u7a33\u5b9a\u6027**\uff1a\n- **\u7b97\u6cd5\u4f18\u5316**\uff1a\u901a\u8fc7\u6539\u8fdbMCTS\u7684\u9009\u62e9\u7b56\u7565\u548cPRM\u7684\u53c2\u6570\u8c03\u6574\u673a\u5236\uff0c\u63d0\u5347\u4e86\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u3002\n- **\u9c81\u68d2\u6027\u589e\u5f3a**\uff1a\u5f15\u5165\u5bf9\u6297\u8bad\u7ec3\u548c\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\uff0c\u63d0\u9ad8\u6a21\u578b\u5728\u590d\u6742\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u6027\u3002\n\n**\u5bb9\u9519\u6027**\uff1a\n- **\u9519\u8bef\u5904\u7406**\uff1a\u5728\u6a21\u62df\u548c\u56de\u6eaf\u9636\u6bb5\u5f15\u5165\u5bb9\u9519\u673a\u5236\uff0c\u51cf\u5c11\u9519\u8bef\u51b3\u7b56\u7684\u5f71\u54cd\u3002\n- **\u5927\u89c4\u6a21\u6570\u636e\u9002\u5e94\u6027**\uff1a\u4f18\u5316\u7b97\u6cd5\u7ed3\u6784\uff0c\u63d0\u5347\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\u7684\u5904\u7406\u80fd\u529b\u3002\n\n### Step 6: \u8bc4\u4f30\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218\n\n**\u672a\u6765\u7814\u7a76\u65b9\u5411**\uff1a\n- **\u9ad8\u6548\u8ba1\u7b97**\uff1a\u7814\u7a76\u66f4\u9ad8\u6548\u7684MCTS\u7b97\u6cd5\uff0c\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002\n- **\u6570\u636e\u589e\u5f3a**\uff1a\u63a2\u7d22\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u51cf\u5c11PRM\u5bf9\u9ad8\u8d28\u91cf\u6570\u636e\u7684\u4f9d\u8d56\u3002\n\n**\u6311\u6218**\uff1a\n- **\u8de8\u6a21\u6001\u878d\u5408**\uff1a\u5982\u4f55\u6709\u6548\u878d\u5408\u591a\u79cd\u6a21\u6001\u6570\u636e\uff0c\u63d0\u5347\u6a21\u578b\u7684\u7efc\u5408\u6027\u80fd\u3002\n- **\u52a8\u6001\u73af\u5883\u9002\u5e94**\uff1a\u5728\u52a8\u6001\u73af\u5883\u4e0b\uff0c\u5982\u4f55\u4fdd\u6301\u6a21\u578b\u7684\u7a33\u5b9a\u6027\u548c\u9ad8\u6548\u6027\u3002\n\n**\u63a8\u52a8\u540e\u7eed\u7814\u7a76**\uff1a\n- **\u65b0\u95ee\u9898\u63d0\u51fa**\uff1a\u5982\u591a\u6a21\u6001\u6570\u636e\u4e0b\u7684\u504f\u597d\u5efa\u6a21\u3001\u52a8\u6001\u73af\u5883\u4e0b\u7684\u51b3\u7b56\u4f18\u5316\u7b49\u3002\n- **\u7814\u7a76\u5207\u5165\u70b9**\uff1a\u4ece\u7b97\u6cd5\u4f18\u5316\u3001\u6570\u636e\u589e\u5f3a\u3001\u8de8\u6a21\u6001\u878d\u5408\u7b49\u591a\u4e2a\u89d2\u5ea6\u5207\u5165\uff0c\u63a8\u52a8\u6280\u672f\u8fdb\u6b65\u3002\n\n### \u603b\u7ed3\n\n\u8fd1\u5e74\u6765\uff0cMCTS\u5728PRM\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u7684\u5e94\u7528\u63a2\u7d22\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4e3b\u8981\u4f53\u73b0\u5728\u6280\u672f\u6846\u67b6\u7684\u5b8c\u5584\u3001\u5e94\u7528\u573a\u666f\u7684\u62d3\u5c55\u3001\u6027\u80fd\u7684\u63d0\u5347\u4ee5\u53ca\u7a33\u5b9a\u6027\u548c\u5bb9\u9519\u6027\u7684\u589e\u5f3a\u3002\u7136\u800c\uff0c\u4ecd\u5b58\u5728\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u3001\u6570\u636e\u4f9d\u8d56\u6027\u5f3a\u7b49\u5c40\u9650\u6027\u3002\u672a\u6765\u7684\u7814\u7a76\u5e94\u5173\u6ce8\u9ad8\u6548\u8ba1\u7b97\u3001\u6570\u636e\u589e\u5f3a\u548c\u8de8\u6a21\u6001\u878d\u5408\u7b49\u65b9\u5411\uff0c\u4ee5\u5e94\u5bf9\u52a8\u6001\u73af\u5883\u4e0b\u7684\u51b3\u7b56\u4f18\u5316\u7b49\u6311\u6218\uff0c\u63a8\u52a8\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002", "task_step_name": "\u672a\u6765\u7814\u7a76\u65b9\u5411", "task_step_description": "\u7814\u7a76\u66f4\u9ad8\u6548\u7684MCTS\u7b97\u6cd5\uff0c\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "task_step_level": "5>1", "task_step_question": "### \u95ee\u9898\u63d0\u51fa\n\n\u5728\u63a2\u7d22MCTS\u5728PRM\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u7684\u5e94\u7528\u4e2d\uff0c\u5982\u4f55\u8bbe\u8ba1\u66f4\u9ad8\u6548\u7684MCTS\u7b97\u6cd5\u4ee5\u663e\u8457\u964d\u4f4e\u5176\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u5176\u5728\u590d\u6742\u51b3\u7b56\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff1f\u5177\u4f53\u6765\u8bf4\uff0c\u6709\u54ea\u4e9b\u6f5c\u5728\u7684\u7b97\u6cd5\u4f18\u5316\u7b56\u7565\uff08\u5982\u5e76\u884c\u8ba1\u7b97\u3001\u526a\u679d\u6280\u672f\u3001\u542f\u53d1\u5f0f\u641c\u7d22\u7b49\uff09\u53ef\u4ee5\u5728\u4e0d\u540c\u5e94\u7528\u573a\u666f\u4e2d\u6709\u6548\u51cf\u5c11MCTS\u7684\u8ba1\u7b97\u8d1f\u62c5\uff0c\u5e76\u786e\u4fdd\u5176\u5728\u52a8\u6001\u73af\u5883\u4e0b\u7684\u7a33\u5b9a\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff1f", "task_step_question_context": [{"ref_id": "454984236379937352", "chunk_id": "11", "score": 0.380859375, "text": "# 2.2 Acceleration of MCTS\nMCTS-based methods have proved their strong capability of solving complex games or tasks. However, the high computational cost of MCTS hinders its application to some real-time and more general scenarios. Therefore, numerous works are devoted to accelerating MCTS. For example, to make the selection stage more effective, some heuristic pruning methods [ 14 ,33 ,29 ,1 ,2 ] aim to reduce the width and depth of the search tree with some heuristic functions. Furthermore, for more efficient evaluations, Lorentz [ 22 ] proposed early playout termination of MCTS (MCTS-EPT) to stop the random playouts early and use an evaluation function to assess win or loss. Moreover, Hsueh et al. [18 ] applied MCTS-EPT to the Chinese dark chess and proved its effectiveness. Afterward, similar ideas have been applied in the evaluation stage of AlphaGoZero [ 32 ] and later MCTS-based methods [31 ,27 ,34 ]. They evaluate the $Q$ -values through a learnable evaluation network instead of running playouts to the end. Grill et al. [15 ] propose a novel regularized policy optimization method based on AlphaZero to decrease the search budget of MCTS, which is from the optimization perspective. Danihelka et al. [10 ] propose a policy improvement algorithm based on sampling actions without replacement, named Gumbel trick to achieve better performance when planning with few simulations. However, these methods mentioned above focus on the specific stage of the search iteration or reduce the total budget through pruning and optimization methods, which are orthogonal to us. And few works targets at the search loop. Lan et al. [21 ] propose DS-MCTS, which defines the uncertainty of MCTS and approximates it by extra DNNs with specific features for board games in training. During the evaluation, DS-MCTS will check periodically and stop the search if the state is certain.\n\n# 3 Background\nThe AlphaGo series of work [ 30 ,32 ,31 ,27 ] are all MCTS-based reinforcement learning algorithms. Those algorithms assume the environment transition dynamics are known or learn the environment dynamics. Based on the dynamics, they use the Monte-Carlo tree search (MCTS) as the policy improvement operator. I.e., taking in the current policy, MCTS returns a better policy with the search algorithm. The systematic search allows the MCTS-based RL algorithm to quickly improve the policy and perform much better in the setting where heavy reasoning is required.\n\n# 3.1 MCTS\nThis part briefly introduces the MCTS method implemented in reinforcement learning applications. As mentioned in the related works, modern MCTS-based RL algorithms include four stages in the search loop, namely selection, expansion, evaluation, and backpropagation.  \n\nMCTS takes in the current states and generates a policy after the search loop of $N$ iterations. Here $N$ is a constant number of iterations set by the designer, regarded as the total budget. In the selection stage of each iteration, an action will be selected by maximizing over UCB. Specifically, AlphaZero [31 ] and MuZero [ 27 ] are developed based on a variant of UCB, P-UCT [ 25 ] and have achieved great success on board games and Atari games. The formula of P-UCT is the Eq (1):  \n\n$$\na^{k}=\\arg\\operatorname*{max}_{a\\in\\mathcal{A}}Q(s,a)+P(s,a)\\frac{\\sqrt{\\sum_{b\\in\\mathcal{A}}N(s,b)}}{1+N(s,a)}(c_{1}+\\log((\\sum_{b\\in\\mathcal{A}}N(s,b)+c_{2}+1)/c_{2})),\n$$  \n\nwhere $k$ is the index of iteration, $\\boldsymbol{\\mathcal{A}}$ is the acti $Q(s,a)$ is the estimated Q-value, $P(s,a)$ is the policy prior obtained from neural networks, $N(s,a)$ is the visitations to select the action a from the state $s$ and $c_{1},c_{2}$ are hyper-parameters. The output of MCTS is the visitation of each action of the root node. After $N$ search iterations, the final policy $\\pi(s)$ is defined as the normalized root visitation distribution simplification, we use $\\pi_{N}(s)$ $\\pi_{k}$ in place of , where $\\begin{array}{r}{\\pi_{k}(s,a)=N(s,a)/\\sum_{b\\in\\mathcal{A}}N(s,b)=N(s,a)/k,a\\in\\mathcal{A}}\\end{array}$ $\\pi_{k}(s)$ sometimes. And the detailed procedure of MCTS is \u2208A . For introduced in Appendix. In our method, we propose to approximate the final policy $\\pi_{N}(s)$ with $\\hat{\\pi}_{k}(s)$ ,which we name as a virtual expanded policy, through a new expansion method and a termination rule. In this way, the number of iterations in MCTS can be reduced from $N$ to $k$ .\n\n# 3.2 Computation Requirement\nMost of the computations in MCTS-based RL are in the MCTS procedure. Each action taken by MCTS requires $N$ times neural network evaluations, where $N$ is a constant number of iterations in the search loop. Traditional RL algorithms, such as PPO [ 28 ] or DQN [ 23 ], only need a single neural network evaluation per action. Thus, MCTS-based RL is roughly $N$ times computationally more expensive than traditional RL algorithms. In practice, training a single Atari game needs 12 hours of computation time on 40 TPUs [ 27 ]. The computation need is roughly two orders of magnitude more than traditional RL algorithms [28], although the final performance of MuZero is much better.\n\n# 4 Method\nWe aim to spend more search time on harder states and less on easier states. Intuitively, human knows when to make a quick decision or a slow decision under different circumstances. Unfortunately, this situation-aware behavior is absent in current MCTS algorithms. Therefore, we propose an MCTS variant that terminates the search iteration adaptively. It consists of two components: a novel expansion method named virtual expansion to estimate the final visitation based on the current partial tree; a termination rule that decides when to terminate based on the hardness of the current scenario. And we will display the adaptive mechanism through visualizations in Section 5.5."}, {"ref_id": "455026805323333778", "chunk_id": "1", "score": 0.380859375, "text": "# Spending Thinking Time Wisely: Accelerating MCTS with Virtual Expansions\nWeirui $\\mathbf{Ye}^{*}$ Pieter Abbeel \u2020Yang Gao $\\ast\\ddag\\S$ \u2217Tsinghua University, \u2020UC Berkeley, \u00a7Shanghai Qi Zhi Institute\n\n# Abstract\nOne of the most important AI research questions is to trade off computation versus performance since \u201cperfect rationality\" exists in theory but is impossible to achieve in practice. Recently, Monte-Carlo tree search (MCTS) has attracted considerable attention due to the significant performance improvement in various challenging domains. However, the expensive time cost during search severely restricts its scope for applications. This paper proposes the Virtual MCTS (V-MCTS), a variant of MCTS that spends more search time on harder states and less search time on simpler states adaptively. We give theoretical bounds of the proposed method and evaluate the performance and computations on $9\\times9$ Go board games and Atari games. Experiments show that our method can achieve comparable performances to the original search algorithm while requiring less than $50\\%$ search time on average. We believe that this approach is a viable alternative for tasks under limited time and resources. The code is available at https://github.com/YeWR/V-MCTS.git .\n\n# 1 Introduction\nWhen artificial intelligence was first studied in the 1950s, researchers have sought to find the solution to the question \u201cHow to build an agent with perfect rationality\". The term \u201cperfect rationality\" [7 ,24 ,26 ] here refers to the decision made with infinite amounts of computations. However, one can only solve small-scale problems without considering the practical computation time since classical search algorithms usually exhibit exponential running time. Therefore, recent AI research would no longer seek to achieve \u201cperfect rationality\", but instead carefully trade-off computation versus the level of rationality. People have developed computational models like \u201cbounded optimality\" to model these settings [ 26 ]. The increasing level of rationality under the same computational budget has given us a lot of AI successes. Algorithms include the Monte-Carlo sampling algorithms, the variational inference algorithms, and using DNNs as universal function approximators [9, 8, 13, 30, 17].  \n\nRecently, MCTS-based RL algorithms have achieved much success, mainly on board games. The most notable achievement is that AlphaGo beats Hui Fan in 2015 [ 30 ]. It is the first time a computer program beat a human professional Go player. Afterward, AlphaGo beats two top-ranking human players, Lee Sedol in 2016 and Jie Ke in 2017, the latter of which ranked first worldwide at the time. Later, MCTS-based RL algorithms were further extended to other board games and Atari games [ 27 ]. EfficientZero [ 34 ] significantly improves the sample efficiency of MCTS-based RL algorithms, shedding light on its future applications in the real world like robotics and self-driving.  \n\nDespite the impressive performance of MCTS-based RL algorithms, they require massive amounts of computation to train and evaluate. For example, MuZero [ 27 ] used 1000 TPUs trained for 12 hours to learn the game of Go, and for a single Atari game, it needs 40 TPUs to train 12 hours. Compared to previous algorithms on the Atari games benchmark, it needs around two orders of magnitude more compute. This prohibitively large computational requirement has slowed down both the further development of MCTS-based RL algorithms as well as its practical use.  \n\nUnder the hood, MCTS-based RL algorithms imagine the futures when taking different future action sequences. However, this imaging process for the current method is not computationally efficient. For example, AlphaGo needs to look ahead 1600 game states to place a single stone. On the contrary, top human professional players can only think through around 100-200 game states per minute [ 30 ]. Apart from the inefficiency, the current MCTS algorithm deals with easy and challenging cases with the same computational budget. However, human knows to use their time when it is most needed.  \n\nIn this paper, we aim to design new algorithms that save the computational time of the MCTSbased RL methods. We make three key contributions : (1) We present Virtual MCTS, a variant of MCTS, to approximate the vanilla MCTS search policies with less computation. Moreover, unlike previous pruning-based methods that focus on the selection or evaluation stage in MCTS, our method improves the search loop. It terminates the search iterations earlier adaptively when current states are simpler; (2) Theoretically, we provide some error bounds of the proposed method. Furthermore, the visualization results indicate that Virtual MCTS has a better computation and performance trade-off than vanilla MCTS; (3) Empirically, our method can save more than $50\\%$ of search times on the challenging game Go $9\\times9$ and more than $60\\%$ on the visually complex Atari games while keeping comparable performances to those of vanilla MCTS.\n\n# 2 Related Work\n\n# 2.1 Reinforcement Learning with MCTS\nFor a long time, Computer Go has been regarded as a remarkably challenging game [ 3 ,6 ]. Researchers attempt to use Monte-Carlo techniques that evaluate the value of the node state through random playouts [ 4 ,11 ,12 ,30 ]. Afterward, UCT algorithms have generally been applied in Monte-Carlo tree search (MCTS) algorithms, which use UCB1 to select action at each node of the tree [ 20 ]. Recently, MCTS-based RL methods [ 30 ,32 ,31 ,27 ] have become increasingly popular and achieved super-human performances on board games because of their strong ability to search.  \n\nModern MCTS-based RL algorithms include four stages in the search loop : selection, expansion, evaluation, and backpropagation. The computation bottlenecks in vanilla MCTS come from the search loop, especially for the evaluation stage and the selection stage of each iteration. The selection stage is time-consuming when the search tree becomes wider and deeper. The evaluation stage is quite expensive because people attempt to evaluate the node value by random playouts to the end in previous researches. Due to the search loop, MCTS-based algorithms have multiple model inferences compared to other model-free RL methods like PPO [28] and SAC [16]."}, {"ref_id": "454848282814999732", "chunk_id": "2", "score": 0.294921875, "text": "# 4.2 The MCTS Framework for Strategy Synthesis\nWe instantiate MCTS for this optimal strategy search problem. We use UCT as the tree policy in the selection phase and rollout randomly in the rollout phase. Notably, in the backup phase, we apply the max-backup rule [Sabharwal et al. , 2012; Sun et al. , 2023]. This approach updates the action values with the best return observed, rather than the average. It encourages more aggressive exploitation towards the bestperforming strategy observed, aligning with our goal. Therefore, in each MCTS simulation, the agent explores and assesses a single strategy, continually updating and retaining the best strategy seen so far. The process continues until a predetermined number of simulations have been run. At the conclusion of this process, the strategy that has achieved the highest reward $R_{T}$ is selected and presented as the synthesized SMT strategy for the specified instance set $P$ .  \n\nFigure 2 illustrates our basic MCTS framework, using a simplified CFG $G^{\\prime}$ for illustrative purposes. $G^{\\prime}$ is defined as $\\mathrm{~S~}\\dot{\\rightarrow}\\mathrm{~\\tiny~T~S~}|$ symbolize variables for strategy and tactic, respectively. smt and $\\mathrm{~S~}\\rightarrow$ Tsimplify |aig , where S and T  \n\nThe primary challenge in synthesizing strategies through MCTS is the extensive time required to evaluate each strategy, which involves calling an SMT solver on all instances in $P$ . This situation leads to a very limited exploration of potential paths, particularly given the immense search space created by the rich strategy language. To address this issue, we first add additional rules restricting valid actions based on domain knowledge. For example, no tactic could be applied sequentially following a solver tactic such as smt . We refer readers to the Appendix for a comprehensive list of such rules. More importantly, we have introduced two heuristic methods, namely layered search and the staged search methods, into conventional MCTS, facilitating a deeper and more effective exploration of the strategy space.\n\n# 4.3 Layered Search\nTo solve the above-mentioned challenge, we propose a layered search method to optimize the tactic parameters within strategy synthesis. As shown in our CFG $G$ , each tactic can be paired with multiple parameters. In traditional MCTS, the selection of each candidate value for a parameter is represented by one production rule in $G$ , and the agent needs to make sequential production-rule decisions to configure all parameters for a given tactic, leading to exponential growth in the problem search tree.  \n\n  \nFigure 3: Comparison of the conventional MCTS and the layersearch in treating tactic parameter tuning  \n\nTo address this issue, our layered search method approaches the tuning of each tactic parameter as a separate Multi-Armed Bandit (MAB) problem [Bubeck et al. , 2012]. In an MAB scenario, an agent repeatedly chooses from multiple actions (arms) with unknown rewards. The major difference between MAB and MDP is that, in MAB, an action does not affect subsequent state transitions. For these MABs, we select actions according to UCB1 [Auer et al. , 2002].  \n\nIn the layered search, we build an individual MAB for each parameter to be tuned, with each arm representing one candidate value. The MABs are associated with a tree edge (tactic) but themselves are not part of the tree. They are engaged to select parameter values when their associated tree edge is traversed, and they are updated based on the episode reward during the Backup phase. However, such MABs do not expand the MCTS search tree after the parameter configuration, since they are separate components from the main search tree. This is in contrast to conventional MCTS, which also employs MAB principles to select among child nodes to explore, where these nodes are part of the search tree.  \n\nThis concept is well explained in Figure 3 with an example based on the simplified grammar $G^{\\prime}$ . Here, the tactic simplify has two Boolean parameters, namely som and flat to be tuned. As shown in Figure 3(a), the search tree is expanded fourfold to accommodate all possible combinations of these two parameters in the conventional MCTS framework. In contrast, in the layered search framework, MABs for the two parameters are isolated from the search tree, creating no additional branches in the tree.  \n\nThe rationale behind the layer search is twofold. Firstly, tactics such as simplify may have dozens of parameters, and it is common for a tactic to be used multiple times within a strategy. Thus, navigating a search space that is fully expanded by all possible parameter combinations becomes impractical, especially given the time-intensive nature of strategy evaluation. Secondly, we argue that parameter tuning, although important, serves more as an auxiliary task in comparison to the tasks of tactic selection and sequencing. By employing the layered search method, we maintain the primary focus on more influential tasks. At the same time, the isolated MABs efficiently optimize the parameters, thus effectively managing these secondary tasks without overwhelming the main search process."}], "task_step_question_answer": "7895a383-ce04-42fa-9517-e0cea2b11f50:\u300c### \u95ee\u9898\u5206\u6790\u4e0e\u4f18\u5316\u7b56\u7565\n\n\u5728\u63a2\u7d22MCTS\uff08\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff09\u5728PRM\uff08\u504f\u597d\u7b56\u7565\u6a21\u578b\uff09\u5fae\u8c03\u4e2d\u7684\u5e94\u7528\u65f6\uff0c\u8bbe\u8ba1\u66f4\u9ad8\u6548\u7684MCTS\u7b97\u6cd5\u4ee5\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u5e76\u4fdd\u6301\u6216\u63d0\u5347\u6027\u80fd\uff0c\u53ef\u4ee5\u4ece\u4ee5\u4e0b\u51e0\u4e2a\u65b9\u9762\u5165\u624b\uff1a\n\n#### 1. **\u5e76\u884c\u8ba1\u7b97**\n   - **\u7b56\u7565**\uff1a\u5229\u7528\u591a\u6838\u5904\u7406\u5668\u6216\u5206\u5e03\u5f0f\u8ba1\u7b97\u8d44\u6e90\uff0c\u5c06MCTS\u7684\u641c\u7d22\u8fc7\u7a0b\u5e76\u884c\u5316\u3002\u4f8b\u5982\uff0c\u5c06\u4e0d\u540c\u7684\u641c\u7d22\u6811\u5206\u652f\u5206\u914d\u7ed9\u4e0d\u540c\u7684\u8ba1\u7b97\u5355\u5143\uff0c\u540c\u65f6\u8fdb\u884c\u63a2\u7d22\u548c\u8bc4\u4f30\u3002\n   - **\u4f18\u52bf**\uff1a\u663e\u8457\u51cf\u5c11\u641c\u7d22\u65f6\u95f4\uff0c\u5c24\u5176\u662f\u5728\u5927\u89c4\u6a21\u590d\u6742\u51b3\u7b56\u4efb\u52a1\u4e2d\uff0c\u80fd\u591f\u6709\u6548\u5229\u7528\u8ba1\u7b97\u8d44\u6e90\u3002\n   - **\u6311\u6218**\uff1a\u9700\u8981\u89e3\u51b3\u5e76\u884c\u5316\u5e26\u6765\u7684\u540c\u6b65\u548c\u901a\u4fe1\u5f00\u9500\u95ee\u9898\uff0c\u786e\u4fdd\u5404\u8ba1\u7b97\u5355\u5143\u4e4b\u95f4\u7684\u6570\u636e\u4e00\u81f4\u6027\u3002\n\n#### 2. **\u526a\u679d\u6280\u672f**\n   - **\u7b56\u7565**\uff1a\u5f15\u5165\u542f\u53d1\u5f0f\u526a\u679d\u65b9\u6cd5\uff0c\u51cf\u5c11\u641c\u7d22\u6811\u7684\u5bbd\u5ea6\u548c\u6df1\u5ea6\u3002\u4f8b\u5982\uff0c\u901a\u8fc7\u8bc4\u4f30\u8282\u70b9\u7684\u6f5c\u5728\u4ef7\u503c\uff0c\u63d0\u524d\u526a\u9664\u4f4e\u6982\u7387\u6216\u4f4e\u56de\u62a5\u7684\u5206\u652f\u3002\n   - **\u4f18\u52bf**\uff1a\u51cf\u5c11\u4e0d\u5fc5\u8981\u7684\u8ba1\u7b97\uff0c\u805a\u7126\u4e8e\u9ad8\u56de\u62a5\u7684\u641c\u7d22\u8def\u5f84\uff0c\u4ece\u800c\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002\n   - **\u6311\u6218**\uff1a\u9700\u8981\u8bbe\u8ba1\u9ad8\u6548\u7684\u542f\u53d1\u5f0f\u51fd\u6570\uff0c\u786e\u4fdd\u526a\u679d\u4e0d\u4f1a\u9057\u6f0f\u91cd\u8981\u7684\u51b3\u7b56\u8def\u5f84\u3002\n\n#### 3. **\u542f\u53d1\u5f0f\u641c\u7d22**\n   - **\u7b56\u7565**\uff1a\u7ed3\u5408\u9886\u57df\u77e5\u8bc6\u6216\u5b66\u4e60\u6a21\u578b\uff0c\u5f15\u5165\u542f\u53d1\u5f0f\u641c\u7d22\u7b56\u7565\uff0c\u6307\u5bfcMCTS\u7684\u641c\u7d22\u65b9\u5411\u3002\u4f8b\u5982\uff0c\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u9884\u6d4b\u8282\u70b9\u7684\u4ef7\u503c\uff0c\u4f18\u5148\u63a2\u7d22\u9ad8\u4ef7\u503c\u533a\u57df\u3002\n   - **\u4f18\u52bf**\uff1a\u63d0\u9ad8\u641c\u7d22\u6548\u7387\uff0c\u51cf\u5c11\u968f\u673a\u63a2\u7d22\u7684\u76f2\u76ee\u6027\uff0c\u5c24\u5176\u662f\u5728\u52a8\u6001\u73af\u5883\u4e2d\u80fd\u591f\u5feb\u901f\u9002\u5e94\u53d8\u5316\u3002\n   - **\u6311\u6218**\uff1a\u9700\u8981\u786e\u4fdd\u542f\u53d1\u5f0f\u6a21\u578b\u7684\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u907f\u514d\u8fc7\u5ea6\u4f9d\u8d56\u7279\u5b9a\u573a\u666f\u3002\n\n#### 4. **\u865a\u62df\u6269\u5c55\u4e0e\u81ea\u9002\u5e94\u7ec8\u6b62**\n   - **\u7b56\u7565**\uff1a\u91c7\u7528\u865a\u62df\u6269\u5c55\u6280\u672f\uff0c\u901a\u8fc7\u90e8\u5206\u641c\u7d22\u6811\u4f30\u8ba1\u6700\u7ec8\u7b56\u7565\uff0c\u5e76\u7ed3\u5408\u81ea\u9002\u5e94\u7ec8\u6b62\u89c4\u5219\uff0c\u6839\u636e\u5f53\u524d\u72b6\u6001\u7684\u590d\u6742\u5ea6\u52a8\u6001\u8c03\u6574\u641c\u7d22\u6df1\u5ea6\u3002\n   - **\u4f18\u52bf**\uff1a\u5728\u7b80\u5355\u72b6\u6001\u4e0b\u5feb\u901f\u51b3\u7b56\uff0c\u5728\u590d\u6742\u72b6\u6001\u4e0b\u6df1\u5165\u641c\u7d22\uff0c\u5b9e\u73b0\u8ba1\u7b97\u8d44\u6e90\u7684\u5408\u7406\u5206\u914d\u3002\n   - **\u6311\u6218**\uff1a\u9700\u8981\u8bbe\u8ba1\u6709\u6548\u7684\u7ec8\u6b62\u89c4\u5219\uff0c\u786e\u4fdd\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u90fd\u80fd\u4fdd\u6301\u51b3\u7b56\u7684\u7a33\u5b9a\u6027\u3002\n\n#### 5. **\u5206\u5c42\u641c\u7d22\u4e0e\u53c2\u6570\u4f18\u5316**\n   - **\u7b56\u7565**\uff1a\u5c06\u53c2\u6570\u4f18\u5316\u4e0e\u7b56\u7565\u641c\u7d22\u5206\u79bb\uff0c\u91c7\u7528\u5206\u5c42\u641c\u7d22\u65b9\u6cd5\uff0c\u901a\u8fc7\u72ec\u7acb\u7684MAB\uff08\u591a\u81c2\u8d4c\u535a\u673a\uff09\u6a21\u578b\u4f18\u5316\u6218\u672f\u53c2\u6570\uff0c\u51cf\u5c11\u641c\u7d22\u6811\u7684\u590d\u6742\u5ea6\u3002\n   - **\u4f18\u52bf**\uff1a\u805a\u7126\u4e8e\u6838\u5fc3\u7b56\u7565\u641c\u7d22\uff0c\u540c\u65f6\u9ad8\u6548\u4f18\u5316\u8f85\u52a9\u53c2\u6570\uff0c\u907f\u514d\u641c\u7d22\u7a7a\u95f4\u7684\u6307\u6570\u7ea7\u81a8\u80c0\u3002\n   - **\u6311\u6218**\uff1a\u9700\u8981\u786e\u4fdd\u53c2\u6570\u4f18\u5316\u4e0e\u7b56\u7565\u641c\u7d22\u7684\u534f\u540c\u6027\uff0c\u907f\u514d\u53c2\u6570\u4f18\u5316\u5f71\u54cd\u6574\u4f53\u7b56\u7565\u7684\u7a33\u5b9a\u6027\u3002\n\n### \u7ed3\u8bba\n\u901a\u8fc7\u7ed3\u5408\u5e76\u884c\u8ba1\u7b97\u3001\u526a\u679d\u6280\u672f\u3001\u542f\u53d1\u5f0f\u641c\u7d22\u3001\u865a\u62df\u6269\u5c55\u4e0e\u81ea\u9002\u5e94\u7ec8\u6b62\u4ee5\u53ca\u5206\u5c42\u641c\u7d22\u7b49\u4f18\u5316\u7b56\u7565\uff0c\u53ef\u4ee5\u663e\u8457\u964d\u4f4eMCTS\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u5176\u5728\u590d\u6742\u51b3\u7b56\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002\u8fd9\u4e9b\u7b56\u7565\u5728\u4e0d\u540c\u5e94\u7528\u573a\u666f\u4e2d\u5177\u6709\u5e7f\u6cdb\u7684\u9002\u7528\u6027\uff0c\u80fd\u591f\u6709\u6548\u5e94\u5bf9\u52a8\u6001\u73af\u5883\u4e0b\u7684\u6311\u6218\uff0c\u5e76\u786e\u4fdd\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002\u300d", "ref_task_step_id": ""}, "__type__": "task_step_node"}, "595b81fe-b868-420f-a43e-e551a8cbcce1": {"__data__": {"id_": "595b81fe-b868-420f-a43e-e551a8cbcce1", "metadata": {}, "relationships": {}, "hash": "", "start_task_context": "MCTS\u5728PRM\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u7684\u5e94\u7528\u63a2\u7d22\u7efc\u8ff0", "aemo_representation_context": "### Step 1: \u5206\u6790\u8fd1\u51e0\u5e74\u7814\u7a76\u9886\u57df\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba\n\n**\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09**\uff1a\n- **\u57fa\u672c\u539f\u7406**\uff1aMCTS\u662f\u4e00\u79cd\u7528\u4e8e\u51b3\u7b56\u8fc7\u7a0b\u7684\u542f\u53d1\u5f0f\u641c\u7d22\u7b97\u6cd5\uff0c\u5e7f\u6cdb\u5e94\u7528\u4e8e\u6e38\u620fAI\u3001\u8def\u5f84\u89c4\u5212\u7b49\u9886\u57df\u3002\u5176\u6838\u5fc3\u601d\u60f3\u662f\u901a\u8fc7\u6a21\u62df\u591a\u6b21\u968f\u673a\u6e38\u620f\u6765\u8bc4\u4f30\u4e0d\u540c\u51b3\u7b56\u7684\u4f18\u52a3\u3002\n- **\u6280\u672f\u6846\u67b6**\uff1a\u4e3b\u8981\u5305\u62ec\u9009\u62e9\uff08Selection\uff09\u3001\u6269\u5c55\uff08Expansion\uff09\u3001\u6a21\u62df\uff08Simulation\uff09\u548c\u56de\u6eaf\uff08Backpropagation\uff09\u56db\u4e2a\u6b65\u9aa4\u3002\n\n**\u504f\u597d\u7b56\u7565\u6a21\u578b\uff08PRM\uff09**\uff1a\n- **\u57fa\u672c\u539f\u7406**\uff1aPRM\u65e8\u5728\u901a\u8fc7\u5b66\u4e60\u7528\u6237\u7684\u504f\u597d\u6765\u4f18\u5316\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5e38\u7528\u4e8e\u63a8\u8350\u7cfb\u7edf\u3001\u4e2a\u6027\u5316\u670d\u52a1\u7b49\u3002\n- **\u6280\u672f\u6846\u67b6**\uff1a\u901a\u5e38\u5305\u62ec\u504f\u597d\u5efa\u6a21\u3001\u7b56\u7565\u4f18\u5316\u548c\u6a21\u578b\u5fae\u8c03\u7b49\u73af\u8282\u3002\n\n**\u65b9\u6cd5\u8bba**\uff1a\n- **\u96c6\u6210\u5b66\u4e60**\uff1a\u5c06MCTS\u4e0ePRM\u7ed3\u5408\uff0c\u5229\u7528MCTS\u7684\u641c\u7d22\u80fd\u529b\u4f18\u5316PRM\u7684\u51b3\u7b56\u8fc7\u7a0b\u3002\n- **\u5f3a\u5316\u5b66\u4e60**\uff1a\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u673a\u5236\u4e0d\u65ad\u8c03\u6574PRM\u7684\u53c2\u6570\uff0c\u4ee5\u9002\u5e94\u52a8\u6001\u73af\u5883\u3002\n\n### Step 2: \u7814\u7a76\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u4e3b\u8981\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0e\u53d8\u4f53\n\n**\u5e94\u7528\u573a\u666f**\uff1a\n- **\u6e38\u620fAI**\uff1aMCTS\u4e0ePRM\u7ed3\u5408\u7528\u4e8e\u6e38\u620f\u4e2d\u7684\u667a\u80fd\u51b3\u7b56\uff0c\u5982AlphaGo\u3002\n- **\u8def\u5f84\u89c4\u5212**\uff1a\u5728\u673a\u5668\u4eba\u5bfc\u822a\u548c\u81ea\u52a8\u9a7e\u9a76\u4e2d\uff0c\u5229\u7528MCTS\u4f18\u5316\u8def\u5f84\u9009\u62e9\u3002\n- **\u63a8\u8350\u7cfb\u7edf**\uff1a\u901a\u8fc7PRM\u5b66\u4e60\u7528\u6237\u504f\u597d\uff0c\u7ed3\u5408MCTS\u8fdb\u884c\u66f4\u7cbe\u51c6\u7684\u63a8\u8350\u3002\n\n**\u53d8\u4f53**\uff1a\n- **\u6df1\u5ea6MCTS**\uff1a\u7ed3\u5408\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff0c\u63d0\u5347MCTS\u7684\u51b3\u7b56\u80fd\u529b\u3002\n- **\u591a\u6a21\u6001PRM**\uff1a\u878d\u5408\u591a\u79cd\u6570\u636e\u6e90\uff08\u5982\u6587\u672c\u3001\u56fe\u50cf\uff09\uff0c\u589e\u5f3aPRM\u7684\u6cdb\u5316\u80fd\u529b\u3002\n\n### Step 3: \u8bc4\u4f30\u5b66\u672f\u754c\u7684\u6280\u672f\u8fdb\u6b65\u4e0e\u5c40\u9650\u6027\n\n**\u6280\u672f\u8fdb\u6b65**\uff1a\n- **\u6027\u80fd\u63d0\u5347**\uff1aMCTS\u4e0ePRM\u7ed3\u5408\u663e\u8457\u63d0\u5347\u4e86\u51b3\u7b56\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002\n- **\u6cdb\u5316\u80fd\u529b**\uff1a\u591a\u6a21\u6001PRM\u5728\u5904\u7406\u591a\u6837\u5316\u6570\u636e\u65f6\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002\n\n**\u5c40\u9650\u6027**\uff1a\n- **\u8ba1\u7b97\u590d\u6742\u5ea6**\uff1aMCTS\u5728\u5927\u89c4\u6a21\u95ee\u9898\u4e2d\u8ba1\u7b97\u91cf\u5927\uff0c\u6548\u7387\u6709\u5f85\u63d0\u5347\u3002\n- **\u6570\u636e\u4f9d\u8d56**\uff1aPRM\u7684\u6027\u80fd\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u9ad8\u8d28\u91cf\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u6570\u636e\u504f\u5dee\u95ee\u9898\u4f9d\u7136\u5b58\u5728\u3002\n\n### Step 4: \u63a2\u8ba8\u8ba1\u7b97\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\n\n**\u9002\u7528\u6027**\uff1a\n- **\u591a\u9886\u57df\u5e94\u7528**\uff1aMCTS+PRM\u6846\u67b6\u5728\u6e38\u620f\u3001\u8def\u5f84\u89c4\u5212\u3001\u63a8\u8350\u7cfb\u7edf\u7b49\u591a\u4e2a\u9886\u57df\u8868\u73b0\u51fa\u826f\u597d\u7684\u9002\u7528\u6027\u3002\n- **\u591a\u6a21\u6001\u6570\u636e**\uff1a\u591a\u6a21\u6001PRM\u80fd\u591f\u5904\u7406\u591a\u79cd\u7c7b\u578b\u7684\u6570\u636e\uff0c\u9002\u7528\u4e8e\u590d\u6742\u5e94\u7528\u573a\u666f\u3002\n\n**\u6cdb\u5316\u80fd\u529b**\uff1a\n- **\u8de8\u9886\u57df\u6cdb\u5316**\uff1a\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\uff0cMCTS+PRM\u6846\u67b6\u5728\u4e0d\u540c\u9886\u57df\u95f4\u5c55\u73b0\u51fa\u8f83\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002\n- **\u52a8\u6001\u73af\u5883\u9002\u5e94**\uff1a\u5f3a\u5316\u5b66\u4e60\u673a\u5236\u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u9002\u5e94\u52a8\u6001\u53d8\u5316\u7684\u73af\u5883\u3002\n\n### Step 5: \u5206\u6790\u6700\u65b0\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027\n\n**\u7a33\u5b9a\u6027**\uff1a\n- **\u7b97\u6cd5\u4f18\u5316**\uff1a\u901a\u8fc7\u6539\u8fdbMCTS\u7684\u9009\u62e9\u7b56\u7565\u548cPRM\u7684\u53c2\u6570\u8c03\u6574\u673a\u5236\uff0c\u63d0\u5347\u4e86\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u3002\n- **\u9c81\u68d2\u6027\u589e\u5f3a**\uff1a\u5f15\u5165\u5bf9\u6297\u8bad\u7ec3\u548c\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\uff0c\u63d0\u9ad8\u6a21\u578b\u5728\u590d\u6742\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u6027\u3002\n\n**\u5bb9\u9519\u6027**\uff1a\n- **\u9519\u8bef\u5904\u7406**\uff1a\u5728\u6a21\u62df\u548c\u56de\u6eaf\u9636\u6bb5\u5f15\u5165\u5bb9\u9519\u673a\u5236\uff0c\u51cf\u5c11\u9519\u8bef\u51b3\u7b56\u7684\u5f71\u54cd\u3002\n- **\u5927\u89c4\u6a21\u6570\u636e\u9002\u5e94\u6027**\uff1a\u4f18\u5316\u7b97\u6cd5\u7ed3\u6784\uff0c\u63d0\u5347\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\u7684\u5904\u7406\u80fd\u529b\u3002\n\n### Step 6: \u8bc4\u4f30\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218\n\n**\u672a\u6765\u7814\u7a76\u65b9\u5411**\uff1a\n- **\u9ad8\u6548\u8ba1\u7b97**\uff1a\u7814\u7a76\u66f4\u9ad8\u6548\u7684MCTS\u7b97\u6cd5\uff0c\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002\n- **\u6570\u636e\u589e\u5f3a**\uff1a\u63a2\u7d22\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u51cf\u5c11PRM\u5bf9\u9ad8\u8d28\u91cf\u6570\u636e\u7684\u4f9d\u8d56\u3002\n\n**\u6311\u6218**\uff1a\n- **\u8de8\u6a21\u6001\u878d\u5408**\uff1a\u5982\u4f55\u6709\u6548\u878d\u5408\u591a\u79cd\u6a21\u6001\u6570\u636e\uff0c\u63d0\u5347\u6a21\u578b\u7684\u7efc\u5408\u6027\u80fd\u3002\n- **\u52a8\u6001\u73af\u5883\u9002\u5e94**\uff1a\u5728\u52a8\u6001\u73af\u5883\u4e0b\uff0c\u5982\u4f55\u4fdd\u6301\u6a21\u578b\u7684\u7a33\u5b9a\u6027\u548c\u9ad8\u6548\u6027\u3002\n\n**\u63a8\u52a8\u540e\u7eed\u7814\u7a76**\uff1a\n- **\u65b0\u95ee\u9898\u63d0\u51fa**\uff1a\u5982\u591a\u6a21\u6001\u6570\u636e\u4e0b\u7684\u504f\u597d\u5efa\u6a21\u3001\u52a8\u6001\u73af\u5883\u4e0b\u7684\u51b3\u7b56\u4f18\u5316\u7b49\u3002\n- **\u7814\u7a76\u5207\u5165\u70b9**\uff1a\u4ece\u7b97\u6cd5\u4f18\u5316\u3001\u6570\u636e\u589e\u5f3a\u3001\u8de8\u6a21\u6001\u878d\u5408\u7b49\u591a\u4e2a\u89d2\u5ea6\u5207\u5165\uff0c\u63a8\u52a8\u6280\u672f\u8fdb\u6b65\u3002\n\n### \u603b\u7ed3\n\n\u8fd1\u5e74\u6765\uff0cMCTS\u5728PRM\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u7684\u5e94\u7528\u63a2\u7d22\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4e3b\u8981\u4f53\u73b0\u5728\u6280\u672f\u6846\u67b6\u7684\u5b8c\u5584\u3001\u5e94\u7528\u573a\u666f\u7684\u62d3\u5c55\u3001\u6027\u80fd\u7684\u63d0\u5347\u4ee5\u53ca\u7a33\u5b9a\u6027\u548c\u5bb9\u9519\u6027\u7684\u589e\u5f3a\u3002\u7136\u800c\uff0c\u4ecd\u5b58\u5728\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u3001\u6570\u636e\u4f9d\u8d56\u6027\u5f3a\u7b49\u5c40\u9650\u6027\u3002\u672a\u6765\u7684\u7814\u7a76\u5e94\u5173\u6ce8\u9ad8\u6548\u8ba1\u7b97\u3001\u6570\u636e\u589e\u5f3a\u548c\u8de8\u6a21\u6001\u878d\u5408\u7b49\u65b9\u5411\uff0c\u4ee5\u5e94\u5bf9\u52a8\u6001\u73af\u5883\u4e0b\u7684\u51b3\u7b56\u4f18\u5316\u7b49\u6311\u6218\uff0c\u63a8\u52a8\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002", "task_step_name": "\u6311\u6218", "task_step_description": "\u5982\u4f55\u6709\u6548\u878d\u5408\u591a\u79cd\u6a21\u6001\u6570\u636e\uff0c\u63d0\u5347\u6a21\u578b\u7684\u7efc\u5408\u6027\u80fd\u3002", "task_step_level": "5>2", "task_step_question": "### \u95ee\u9898\u63d0\u51fa\n\n\u5728MCTS\u4e0ePRM\u7ed3\u5408\u7684\u5e94\u7528\u4e2d\uff0c\u5982\u4f55\u6709\u6548\u878d\u5408\u591a\u79cd\u6a21\u6001\u6570\u636e\uff08\u5982\u6587\u672c\u3001\u56fe\u50cf\u3001\u97f3\u9891\u7b49\uff09\u4ee5\u63d0\u5347\u6a21\u578b\u7684\u7efc\u5408\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u52a8\u6001\u73af\u5883\u4e0b\u4fdd\u6301\u6a21\u578b\u7684\u7a33\u5b9a\u6027\u548c\u9ad8\u6548\u6027\uff1f\u5177\u4f53\u6765\u8bf4\uff0c\u6709\u54ea\u4e9b\u6280\u672f\u6216\u65b9\u6cd5\u53ef\u4ee5\u7528\u4e8e\u8de8\u6a21\u6001\u6570\u636e\u7684\u878d\u5408\uff0c\u4ee5\u53ca\u8fd9\u4e9b\u65b9\u6cd5\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u53ef\u80fd\u9762\u4e34\u7684\u6311\u6218\u548c\u89e3\u51b3\u65b9\u6848\u662f\u4ec0\u4e48\uff1f", "task_step_question_context": [{"ref_id": "454846130529033370", "chunk_id": "4", "score": 0.328125, "text": "# 2. Related Works\n\n# 2.1. Imbalanced multimodal learning\nSeveral recent studies [ 5 ,42 ,43 ] have shown that many multimodal DNNs cannot achieve better performance compared to the best single-modal DNNs. Wang et al . [ 42 ]found that different modalities overfit and generalize at different rates and thus obtain suboptimal solutions when jointly training them using a unified optimization strategy. Peng et al . [ 31 ] proposed that the better-performing modality will dominate the gradient update while suppressing the learning process of the other modality. Furthermore, it has been reported that multimodal DNNs can exploit modal bias in the data, which is inconsistent with the expectation of exploiting cross-modal interactions in VQA [ 8 ,12 ,43 ].  \n\nSeveral approaches have been proposed recently to deal with the modal imbalance problem [ 5 ,31 ,42 ,46 ]. Wang et al . [ 42 ] used additional classifiers for each modality and its fusion modality and then optimized the gradient mixing problem they introduced to obtain better weights for each branch. Du et al . [ 5 ] tried to improve uni-modal performance by distilling knowledge from well-trained models. However, these approaches introduce more model structure and computational effort, which makes the training process more complex and expensive. Xiao et al . [ 46 ] proposed DropPathway, which randomly drops the audio pathway during training as a regularization technique to adjust the learning paces between visual and audio pathways. Peng et al . [ 31 ] chose to slow down the learning rate of the mighty modality by online modulation to lessen the inhibitory effect on the other modality. Although a certain degree of improvement is achieved, such approaches do not impose the intrinsic motivation of improvement on the slow-learning modality, making the improvement of this modality a passive rather than an active behavior. Besides, the interference from other modalities will hinder the improvement by modulation based on the fused modality data. Furthermore, the application scenarios of these methods are limited by fusion methods or model structures. In this paper, we aim to power the slow-learning modality to reduce the gap with the dominant one and allow it to be used in conjunction with various fusion methods and model architectures.\n\n# 2.2. Prototypical network\nPrototypical networks were originally proposed to solve few-shot or zero-shot classification problems [ 4 ,7 ,34 ,37 ], based on the idea that there is an embedding, defined as a prototype, which is surrounded by points from the same class. Recently, this approach has been widely used to address long-tail recognition [ 47 ], domain adaptation [ 3 ,29 ], and facilitate unsupervised learning [ 20 ], since prototypes can be used to represent general features of a class. In [20 ,37 ], prototypes were interpreted as non-parametric prototypical classifiers that perform on par or even better than parametric linear classifiers. In this paper, we leverage prototypes to build non-parametric classifiers to evaluate the performance of each modality feature.  \n\n  \nFigure 2. (a) Performance of each modality and their fusion. (b) Training accuracy of multimodal with different modulations. Baseline means no extra modulation. Acc increases the gradient magnitude of the slow-learning modality. OMG-GE [ 31 ] reduces the gradient magnitude of better modality. (c) The gradient direction discrepancy (angle) between uni-modal and multimodal on the baseline. The results were acquired from CREMA-D.\n\n# 3. Modality Imbalance Analysis\nWithout loss of generality, we consider two input modalities as $m_{0}$ and $m_{1}$ . The dataset is denoted as $\\mathcal{D}$ ,hconsists of instances and their corresponding labels $(\\pmb{x},y)$ ,where ${\\pmb x}~=~({\\pmb x}^{m_{0}},{\\pmb x}^{m_{1}},y)~=~\\{x_{i}^{m_{0}},x_{i}^{m_{1}},y_{i}\\}_{i=1,2,\\dots,N},\\nonumber$ ,$y=\\{1,2,...,M\\}$ , and $M$ is the number of categories . The goal is to train a model with this data to predict $y$ from $\\textbf{\\em x}$ .  \n\nWe use a multimodal DNN with two uni-modal branches for prediction. Each branch has an encoder, denoted as $\\phi^{0},\\;\\phi^{1}$ , to extract features of respective modal data $\\pmb{x}^{m_{0}}$ and $\\pmb{x}^{m_{1}}$ . The representation outputs of encoders are deted a $z^{0}=\\phi^{0}\\,\\,\\overline{{(\\theta^{0},x^{m_{0}})}}$ \u0001and $z^{\\bar{1}}=\\phi^{1}\\left(\\theta^{1},{\\pmb x}^{m_{1}}\\right)$ \u0001, where $\\theta^{0}$ and \u03b8$\\theta^{1}$ are the parameters of encoders. The two unimodal encoders are connected through the representations by some kind of fusion, which is prevalent in multimodal learning [ 19 ,23 ,48 ]. In this work, we have tried some different fusion methods. For simplicity, we use $[\\cdot,\\cdot]$ to denote fusion operation. Let $W\\,\\in\\,\\mathbb{R}^{M\\times\\left(d_{z^{0}}+d_{z^{1}}\\right)}$ and $b\\,\\in\\,\\mathbb{R}^{M}$ denote the parameters of the linear classifier to produce the logits output:  \n\n$$\nf\\left(\\pmb{x}\\right)=W\\left[\\phi^{0}\\left(\\theta^{0},\\pmb{x}^{m_{0}}\\right);\\phi^{1}\\left(\\theta^{1},\\pmb{x}^{m_{1}}\\right)\\right]+b\n$$  \n\nThe cross-entropy loss of the multimodal model is  \n\n$$\n\\mathcal{L}_{C E}=-\\frac{1}{N}\\sum_{i=1}^{N}\\log\\frac{e^{f(x_{i})_{y_{i}}}}{\\sum_{k=1}^{M}e^{f(x_{i})_{k}}}\n$$  \n\nThe gradient of the softmax logits output with true label $y^{i}$ should be:  \n\n$$\n\\frac{\\partial\\mathcal{L}_{C E}}{\\partial f\\left(x_{i}\\right)_{y_{i}}}=\\frac{e^{\\left(W\\left[\\phi^{0};\\phi^{1}\\right]+b\\right)_{y_{i}}}}{\\sum_{k=1}^{M}e^{\\left(W\\left[\\phi^{0};\\phi^{1}\\right]+b\\right)_{k}}}-1\n$$  \n\nFor convenience, we simplify $\\phi^{0}\\left(\\theta^{0},x^{m_{0}}\\right)$ \u0001and $\\phi^{1}\\left(\\theta^{1},x^{m_{1}}\\right)$ \u0001as $\\phi^{0}$ and $\\phi^{1}$ .According to Eq. ( ), the final gradient is influenced by the performance of the fused modality. However, we cannot directly judge the contribution of the two modalities. To do it, we take a simple fusion method, summation, as the example here:  \n\n$$\n\\begin{array}{r}{f\\left(\\pmb{x}\\right)=W^{0}\\cdot\\phi^{0}\\left(\\theta^{0},\\pmb{x}^{m_{0}}\\right)+b^{0}}\\\\ {+W^{1}\\cdot\\phi^{1}\\left(\\theta^{1},\\pmb{x}^{m_{1}}\\right)+b^{1}}\\end{array}\n$$  \n\nwhere $W^{0}\\,\\in\\,\\mathbb R^{M\\times d_{z^{0}}}$ ,$W^{1}\\,\\in\\,\\mathbb{R}^{M\\times d_{z^{1}}}$ and $b^{0},b^{1}\\,\\in\\,\\mathbb{R}^{M}$ are the parameters for individual modal classifier. Therefore, we use the logits output of the ground truth as the performance for each modality and their summation fusion:  \n\n$$\n\\begin{array}{r l}&{s^{0}=\\operatorname{softmax}\\left(W^{0}\\cdot\\phi^{0}+b^{0}\\right)_{y}}\\\\ &{s^{1}=\\operatorname{softmax}\\left(W^{1}\\cdot\\phi^{1}+b^{1}\\right)_{y}}\\\\ &{s^{f u}=\\operatorname{softmax}\\left(W^{0}\\cdot\\phi^{0}+b^{0}+W^{1}\\cdot\\phi^{1}+b^{1}\\right)_{y}}\\end{array}\n$$  \n\nAs shown in Fig. 2a , the performance of the audio modal is very similar to the multimodal during training and the visual modal is much worse in CREMA-D [ 2 ], which means better modality contributes more to the gradient because of higher performance similarity with their fusion. Moreover, the visual modal is severely suppressed in the multimodal learning process because of the excessive dominance of gradient updates by the audio modal. Therefore, we have to mitigate the inhibition on visual modal to fully exploit visual features. A straightforward approach can be to increase the magnitude of the gradient. We test a similar way with OGM-GE [ 31 ], named Acc, to increase the gradient magnitude of the slow-learning modality instead of lower the gradient magnitude of the better modality in OGM-GE. The results are shown in Figs. 2b and 2c .  \n\nAs demonstrated in Fig. 2b , increasing the gradient magnitude of the slow-learning modality (visual) could improve the validation accuracy a little bit but not as obviously as OGM-GE does. To find the reason for the phenomenon, we use the uni-modal output $s^{0}$ and $s^{1}$ to calculate the gradient for each modal branch additionally with CE loss and illustrate the direction discrepancy of gradients between each uni-modal and multimodal $s^{f u}$ , as shown in Fig. 2c . The angle between the actual gradient update direction (from the multimodal output) and each modal\u2019s guidance direction (from the uni-modal output) increases dramatically during training, in the meantime, remaining acute. Therefore, the two modalities influence each other, resulting in a larger gap between the gradient update direction obtained by the fused modality and the expected direction of each modality. That means the slow-learning modality cannot fully exploit its feature with the disturbance from other modalities, ultimately making the method of modulating the gradient amplitude limited, as illustrated in Fig. 1 .  \n\n  \nFigure 3. The pipeline of modality modulation with prototypical modal rebalance strategy."}, {"ref_id": "454847538467043982", "chunk_id": "7", "score": 0.26171875, "text": "# 5. Conclusion\nIn this work, we proposed a meta learning-based framework (MCRES) to improve the generalization performance of RES models, especially when handling novel compositions of learned concepts. By constructing a virtual training set and multiple virtual testing sets w.r.t. various levels of novel compositions and then optimizing the model via meta optimization, our framework can effectively improve model generalization performance. Extensive experiments show that our framework achieves superior performance on widely used benchmarks. Moreover, our framework is flexible, and can be seamlessly applied on various models with different architectures to enhance their performance.  \n\nAcknowledgement. This work is supported by MOE AcRF Tier 2 (Proposal ID: T2EP20222-0035), National Research Foundation Singapore under its AI Singapore Programme (AISG-100E-2020-065), and SUTD SKI Project (SKI 2021 02 06)."}, {"ref_id": "454984236293691964", "chunk_id": "5", "score": 0.2578125, "text": "# 4.3.2 Effectiveness of Proposed Methods\nIn Table 6 , we provide results using different pretraining tasks and masking strategies to demonstrate the effectiveness of our proposed modules.  \n\nComparing #1 and #2 in Table 6 , we observe that WWM brings significant performance improvements on all datasets. The reason is that it increases the difficulty of the MLM task, so we can obtain a stronger language model. We also find that LAM can also brings consistent improvements on all dataset because LAM can force the model to learn better representations for layout information, which is beneficial to downstream tasks.  \n\nComparing #2 to #4 and #3 to #5, it is observed that the MPM task also brings considerable improvements on all datasets. MPM works as an auxiliary task to help the MLM task and can increase the pre-training difficulty, contributing to learning better and more robust layout representations.  \n\nMoreover, the full-version LayoutMask (#5) outperforms the naive version (#1) by a large margin $\\operatorname{FUNSD}\\!+\\!3.18\\%$ , CORD $+0.67\\%$ ,$\\mathrm{SROIE}{+}1.11\\%$ ,and $\\mathbf{RV}\\mathrm{L-}\\mathrm{CDIP+}1.09\\%)$ , demonstrating the effectiveness of our proposed modules when working together. To better illustrate the effectiveness of our model design, we list category-level accuracy improvements on RVL-CDIP dataset and provide detailed discussions in Section Bof the Appendix.\n\n# 5 Conclusion\nIn this paper, we propose LayoutMask, a novel multi-modal pre-training model, to solve the reading order issues in VrDU tasks. LayoutMask adopts local 1D position as layout input and can generate adaptive and robust multi-modal representations. In LayoutMask, we equip the MLM task with two masking strategies and design a novel pretraining objective, Masked Position Modeling, to enhance the text-layout interactions and layout representation learning. With only using text and layout modalities, our method can achieve excellent results and significantly outperforms many SOTA methods in VrDU tasks.\n\n# Limitations\nOur method has the following limitations:  \n\nDatasets: In multi-modal pre-training, we rely on downstream datasets to evaluate the performance of pre-trained models. The commonly used entity extraction datasets are relatively small and lack diversity, so the proposed method may not generalize well to real word scenarios.  \n\nLack of Image Modality: In LayoutMask, we focus on text-layout interactions, leaving the image modality unexplored. However, documents in the real world contain many elements that can not be described by text and layout modalities, like figures and lines, so incorporating image modality is important in building a universal multi-modal pre-training model for document understanding.\n\n\n\n# A Ablation Study of Masking Probabilities\nWe compare LayoutMask using different $\\mathrm{P_{mlm}}$ and $\\mathrm{P_{mpm}}$ , and the results are in Figure 4 . We first find the best $\\mathrm{P_{mlm}}$ without using the MPM task, and the optimal value is $25\\%$ . Then we fix such optimal $\\mathrm{P_{mlm}}$ to find the best $\\mathrm{P_{mpm}}$ , which is $15\\%$ as the results show.\n\n# BAblation Study on RVL-CDIP\nTo further understand the effectiveness of our model design, we list the detailed classification results on RVL-CDIP dataset with the naive version and the full version in Table 7 . It is observed that the major performance improvements come from three categories: presentation $(+3.36\\%)$ , ad"}], "task_step_question_answer": "\u5728\u8de8\u6a21\u6001\u6570\u636e\u878d\u5408\u7684\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u6a21\u6001\u4e0d\u5e73\u8861\u548c\u6570\u636e\u566a\u58f0\u4e0e\u7f3a\u5931\u662f\u4e24\u4e2a\u4e3b\u8981\u6311\u6218\u3002\u6a21\u6001\u4e0d\u5e73\u8861\u53ef\u4ee5\u901a\u8fc7\u68af\u5ea6\u8c03\u5236\u548c\u968f\u673a\u4e22\u5f03\u8def\u5f84\u6765\u89e3\u51b3\uff0c\u5177\u4f53\u6765\u8bf4\uff0c\u68af\u5ea6\u8c03\u5236\u53ef\u4ee5\u901a\u8fc7\u8c03\u6574\u4e0d\u540c\u6a21\u6001\u7684\u68af\u5ea6\u66f4\u65b0\u5e45\u5ea6\u6765\u5e73\u8861\u6a21\u6001\u5b66\u4e60\uff0c\u4f8b\u5982OGM-GE\u65b9\u6cd5\u901a\u8fc7\u964d\u4f4e\u5f3a\u52bf\u6a21\u6001\u7684\u68af\u5ea6\u5e45\u5ea6\u6765\u589e\u5f3a\u5f31\u52bf\u6a21\u6001\u7684\u8868\u73b0\u3002\u968f\u673a\u4e22\u5f03\u8def\u5f84\u5219\u53ef\u4ee5\u901a\u8fc7DropPathway\u65b9\u6cd5\uff0c\u968f\u673a\u9009\u62e9\u4e22\u5f03\u67d0\u4e9b\u6a21\u6001\u7684\u8def\u5f84\uff0c\u4f5c\u4e3a\u6b63\u5219\u5316\u6280\u672f\uff0c\u8c03\u6574\u4e0d\u540c\u6a21\u6001\u7684\u5b66\u4e60\u8282\u594f\u3002\u6570\u636e\u566a\u58f0\u4e0e\u7f3a\u5931\u5219\u53ef\u4ee5\u901a\u8fc7\u9c81\u68d2\u6027\u8bbe\u8ba1\u548c\u81ea\u9002\u5e94\u878d\u5408\u7b56\u7565\u6765\u5e94\u5bf9\uff0c\u4f8b\u5982\u4f7f\u7528\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GAN\uff09\u586b\u8865\u7f3a\u5931\u6570\u636e\uff0c\u6216\u901a\u8fc7\u591a\u4efb\u52a1\u5b66\u4e60\u589e\u5f3a\u6a21\u578b\u7684\u6297\u566a\u80fd\u529b\u3002\u81ea\u9002\u5e94\u878d\u5408\u7b56\u7565\u53ef\u4ee5\u52a8\u6001\u8c03\u6574\u878d\u5408\u7b56\u7565\uff0c\u51cf\u5c11\u5bf9\u4e0d\u53ef\u9760\u6a21\u6001\u7684\u4f9d\u8d56\u3002\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\u7684\u63d0\u5347\u53ef\u4ee5\u901a\u8fc7\u8f7b\u91cf\u5316\u6a21\u578b\u8bbe\u8ba1\u3001\u5206\u5e03\u5f0f\u8ba1\u7b97\u3001\u5143\u5b66\u4e60\u548c\u591a\u6a21\u6001\u9884\u8bad\u7ec3\u6765\u5b9e\u73b0\uff0c\u4f8b\u5982\u4f7f\u7528\u77e5\u8bc6\u84b8\u998f\u6280\u672f\u6216\u526a\u679d\u6280\u672f\u6765\u51cf\u5c11\u6a21\u578b\u7684\u8ba1\u7b97\u8d1f\u62c5\uff0c\u5c06\u4e0d\u540c\u6a21\u6001\u7684\u5904\u7406\u4efb\u52a1\u5206\u914d\u5230\u4e0d\u540c\u7684\u8ba1\u7b97\u8282\u70b9\uff0c\u63d0\u5347\u6574\u4f53\u6548\u7387\u3002\u5143\u5b66\u4e60\u548c\u591a\u6a21\u6001\u9884\u8bad\u7ec3\u53ef\u4ee5\u901a\u8fc7\u6784\u5efa\u865a\u62df\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6\uff0c\u4f18\u5316\u6a21\u578b\u7684\u6cdb\u5316\u6027\u80fd\u3002\u672a\u6765\u65b9\u5411\u7684\u63a2\u7d22\u53ef\u4ee5\u901a\u8fc7\u5f15\u5165\u56fe\u50cf\u6a21\u6001\u548c\u8de8\u6a21\u6001\u4ea4\u4e92\u589e\u5f3a\u6765\u8fdb\u4e00\u6b65\u4f18\u5316\uff0c\u4f8b\u5982\u901a\u8fc7\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u6216\u8de8\u6a21\u6001\u751f\u6210\u6a21\u578b\uff0c\u8fdb\u4e00\u6b65\u63d0\u5347\u878d\u5408\u6548\u679c\u3002", "ref_task_step_id": ""}, "__type__": "task_step_node"}}, "task_step_store/ref_task_step_info": {"": {"node_ids": ["276fc94b-2aa5-4712-ab27-c1e4f31af69f", "3fc9a567-9ad0-439c-bd1f-3954fb72323f", "372a76b7-8c9e-42aa-b783-cf530fb1852d", "d6916989-efb3-47e8-978b-9f9e079e1eaf", "8a1e6e8e-b71e-46d2-aa53-8f3d04b1d842", "0f8fcb2e-9570-4314-99ab-0d3606636375", "5e563bfc-4301-43c2-9c34-d92ea2c5783a", "589d43ac-52eb-46d5-a6e1-797b6c59a64e", "64b86178-229d-4430-83ef-e052788e3896", "6dba87e8-762d-4764-9182-3555b4b50a86", "d338c352-b6ad-45ff-b9c3-0418aa6cccc4", "edaf9c7c-601d-4068-b117-92e5eaa45af5", "29d7d9b0-9dd0-42e2-a847-f9e6d0c0b37f", "c438cc1b-a630-4359-b8d9-abeaf797d240", "a2b27547-c4b3-4bf8-ae68-c913858a2752", "ac68fbc2-cec5-4dc0-9a52-1d0003ba3731", "7895a383-ce04-42fa-9517-e0cea2b11f50", "595b81fe-b868-420f-a43e-e551a8cbcce1", "8a1e6e8e-b71e-46d2-aa53-8f3d04b1d842", "3fc9a567-9ad0-439c-bd1f-3954fb72323f", "276fc94b-2aa5-4712-ab27-c1e4f31af69f", "0f8fcb2e-9570-4314-99ab-0d3606636375", "d6916989-efb3-47e8-978b-9f9e079e1eaf", "372a76b7-8c9e-42aa-b783-cf530fb1852d", "8a1e6e8e-b71e-46d2-aa53-8f3d04b1d842", "3fc9a567-9ad0-439c-bd1f-3954fb72323f", "276fc94b-2aa5-4712-ab27-c1e4f31af69f", "0f8fcb2e-9570-4314-99ab-0d3606636375", "d6916989-efb3-47e8-978b-9f9e079e1eaf", "372a76b7-8c9e-42aa-b783-cf530fb1852d", "d6916989-efb3-47e8-978b-9f9e079e1eaf", "276fc94b-2aa5-4712-ab27-c1e4f31af69f", "8a1e6e8e-b71e-46d2-aa53-8f3d04b1d842", "3fc9a567-9ad0-439c-bd1f-3954fb72323f", "0f8fcb2e-9570-4314-99ab-0d3606636375", "372a76b7-8c9e-42aa-b783-cf530fb1852d", "d6916989-efb3-47e8-978b-9f9e079e1eaf", "d6916989-efb3-47e8-978b-9f9e079e1eaf", "5e563bfc-4301-43c2-9c34-d92ea2c5783a", "372a76b7-8c9e-42aa-b783-cf530fb1852d", "372a76b7-8c9e-42aa-b783-cf530fb1852d", "0f8fcb2e-9570-4314-99ab-0d3606636375", "0f8fcb2e-9570-4314-99ab-0d3606636375", "64b86178-229d-4430-83ef-e052788e3896", "5e563bfc-4301-43c2-9c34-d92ea2c5783a", "589d43ac-52eb-46d5-a6e1-797b6c59a64e", "64b86178-229d-4430-83ef-e052788e3896", "589d43ac-52eb-46d5-a6e1-797b6c59a64e", "3fc9a567-9ad0-439c-bd1f-3954fb72323f", "3fc9a567-9ad0-439c-bd1f-3954fb72323f", "6dba87e8-762d-4764-9182-3555b4b50a86", "8a1e6e8e-b71e-46d2-aa53-8f3d04b1d842", "8a1e6e8e-b71e-46d2-aa53-8f3d04b1d842", "276fc94b-2aa5-4712-ab27-c1e4f31af69f", "276fc94b-2aa5-4712-ab27-c1e4f31af69f", "d338c352-b6ad-45ff-b9c3-0418aa6cccc4", "6dba87e8-762d-4764-9182-3555b4b50a86", "d338c352-b6ad-45ff-b9c3-0418aa6cccc4", "edaf9c7c-601d-4068-b117-92e5eaa45af5", "edaf9c7c-601d-4068-b117-92e5eaa45af5", "589d43ac-52eb-46d5-a6e1-797b6c59a64e", "64b86178-229d-4430-83ef-e052788e3896", "5e563bfc-4301-43c2-9c34-d92ea2c5783a", "6dba87e8-762d-4764-9182-3555b4b50a86", "d338c352-b6ad-45ff-b9c3-0418aa6cccc4", "edaf9c7c-601d-4068-b117-92e5eaa45af5", "589d43ac-52eb-46d5-a6e1-797b6c59a64e", "589d43ac-52eb-46d5-a6e1-797b6c59a64e", "29d7d9b0-9dd0-42e2-a847-f9e6d0c0b37f", "29d7d9b0-9dd0-42e2-a847-f9e6d0c0b37f", "5e563bfc-4301-43c2-9c34-d92ea2c5783a", "5e563bfc-4301-43c2-9c34-d92ea2c5783a", "64b86178-229d-4430-83ef-e052788e3896", "64b86178-229d-4430-83ef-e052788e3896", "c438cc1b-a630-4359-b8d9-abeaf797d240", "c438cc1b-a630-4359-b8d9-abeaf797d240", "a2b27547-c4b3-4bf8-ae68-c913858a2752", "a2b27547-c4b3-4bf8-ae68-c913858a2752", "6dba87e8-762d-4764-9182-3555b4b50a86", "6dba87e8-762d-4764-9182-3555b4b50a86", "edaf9c7c-601d-4068-b117-92e5eaa45af5", "edaf9c7c-601d-4068-b117-92e5eaa45af5", "7895a383-ce04-42fa-9517-e0cea2b11f50", "ac68fbc2-cec5-4dc0-9a52-1d0003ba3731", "7895a383-ce04-42fa-9517-e0cea2b11f50", "ac68fbc2-cec5-4dc0-9a52-1d0003ba3731", "d338c352-b6ad-45ff-b9c3-0418aa6cccc4", "d338c352-b6ad-45ff-b9c3-0418aa6cccc4", "595b81fe-b868-420f-a43e-e551a8cbcce1", "595b81fe-b868-420f-a43e-e551a8cbcce1", "c438cc1b-a630-4359-b8d9-abeaf797d240", "29d7d9b0-9dd0-42e2-a847-f9e6d0c0b37f", "a2b27547-c4b3-4bf8-ae68-c913858a2752", "595b81fe-b868-420f-a43e-e551a8cbcce1", "ac68fbc2-cec5-4dc0-9a52-1d0003ba3731", "7895a383-ce04-42fa-9517-e0cea2b11f50", "c438cc1b-a630-4359-b8d9-abeaf797d240", "c438cc1b-a630-4359-b8d9-abeaf797d240", "595b81fe-b868-420f-a43e-e551a8cbcce1"], "metadata": {}}}, "task_step_store/metadata": {"276fc94b-2aa5-4712-ab27-c1e4f31af69f": {"task_step_hash": "", "ref_task_step_id": ""}, "3fc9a567-9ad0-439c-bd1f-3954fb72323f": {"task_step_hash": "", "ref_task_step_id": ""}, "372a76b7-8c9e-42aa-b783-cf530fb1852d": {"task_step_hash": "", "ref_task_step_id": ""}, "d6916989-efb3-47e8-978b-9f9e079e1eaf": {"task_step_hash": "", "ref_task_step_id": ""}, "8a1e6e8e-b71e-46d2-aa53-8f3d04b1d842": {"task_step_hash": "", "ref_task_step_id": ""}, "0f8fcb2e-9570-4314-99ab-0d3606636375": {"task_step_hash": "", "ref_task_step_id": ""}, "5e563bfc-4301-43c2-9c34-d92ea2c5783a": {"task_step_hash": "", "ref_task_step_id": ""}, "589d43ac-52eb-46d5-a6e1-797b6c59a64e": {"task_step_hash": "", "ref_task_step_id": ""}, "64b86178-229d-4430-83ef-e052788e3896": {"task_step_hash": "", "ref_task_step_id": ""}, "6dba87e8-762d-4764-9182-3555b4b50a86": {"task_step_hash": "", "ref_task_step_id": ""}, "d338c352-b6ad-45ff-b9c3-0418aa6cccc4": {"task_step_hash": "", "ref_task_step_id": ""}, "edaf9c7c-601d-4068-b117-92e5eaa45af5": {"task_step_hash": "", "ref_task_step_id": ""}, "29d7d9b0-9dd0-42e2-a847-f9e6d0c0b37f": {"task_step_hash": "", "ref_task_step_id": ""}, "c438cc1b-a630-4359-b8d9-abeaf797d240": {"task_step_hash": "", "ref_task_step_id": ""}, "a2b27547-c4b3-4bf8-ae68-c913858a2752": {"task_step_hash": "", "ref_task_step_id": ""}, "ac68fbc2-cec5-4dc0-9a52-1d0003ba3731": {"task_step_hash": "", "ref_task_step_id": ""}, "7895a383-ce04-42fa-9517-e0cea2b11f50": {"task_step_hash": "", "ref_task_step_id": ""}, "595b81fe-b868-420f-a43e-e551a8cbcce1": {"task_step_hash": "", "ref_task_step_id": ""}}}