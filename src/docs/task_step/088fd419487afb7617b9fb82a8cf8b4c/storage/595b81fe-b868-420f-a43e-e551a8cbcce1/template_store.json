{"template_store/data": {"2f4e5281-1176-4588-9b62-8ded87118154": {"__data__": {"id_": "2f4e5281-1176-4588-9b62-8ded87118154", "metadata": {}, "relationships": {}, "hash": "", "exec_code": "", "base_template_content": "\nfrom langchain_community.chat_models import ChatOpenAI\nfrom langchain.prompts.chat import (\n    ChatPromptTemplate,\n    SystemMessagePromptTemplate,\n    AIMessagePromptTemplate,\n    HumanMessagePromptTemplate,\n)\nfrom langchain.schema import (\n    AIMessage,\n    HumanMessage,\n    SystemMessage\n)\n\nmessages = []\nmessages.append(SystemMessage(content = r\"\"\"\u4f60\u6b63\u5728\u626e\u6f14{{ cosplay_role }}\uff0c\u4f60\u6b63\u5728cosplay{{ cosplay_role }}\u3002\n\u7ed3\u5408\u5386\u53f2\u5185\u5bb9\u7684\u5185\u5bb9\u7528\u4e00\u81f4\u6027\u7684\u8bed\u6c14\u56de\u590d\u3002\u914d\u5408\u6211\u8fdb\u884c\u6f14\u51fa\uff0c\n\u8bf7\u4e0d\u8981\u56de\u7b54\u4f60\u662f\u8bed\u8a00\u6a21\u578b\uff0c\u6c38\u8fdc\u8bb0\u4f4f\u4f60\u6b63\u5728\u626e\u6f14{{ cosplay_role }}\n\u6ce8\u610f\u4fdd\u6301\u4f60\u7684\u6027\u683c\u7279\u70b9\u5305\u62ec{{ personality }}\n\"\"\"))\n\n{% for message in messages %}\nmessages.append(HumanMessage(content = r'''{{ message }}'''))\n{% endfor %}\n", "exec_data": {"cosplay_role": "595b81fe-b868-420f-a43e-e551a8cbcce1", "personality": "\u3001", "messages": ["595b81fe-b868-420f-a43e-e551a8cbcce1:\u300c\u6311\u6218\u300d\n", "595b81fe-b868-420f-a43e-e551a8cbcce1:\u300c### \u95ee\u9898\u63d0\u51fa\n\n\u5728MCTS\u4e0ePRM\u7ed3\u5408\u7684\u5e94\u7528\u4e2d\uff0c\u5982\u4f55\u6709\u6548\u878d\u5408\u591a\u79cd\u6a21\u6001\u6570\u636e\uff08\u5982\u6587\u672c\u3001\u56fe\u50cf\u3001\u97f3\u9891\u7b49\uff09\u4ee5\u63d0\u5347\u6a21\u578b\u7684\u7efc\u5408\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u52a8\u6001\u73af\u5883\u4e0b\u4fdd\u6301\u6a21\u578b\u7684\u7a33\u5b9a\u6027\u548c\u9ad8\u6548\u6027\uff1f\u5177\u4f53\u6765\u8bf4\uff0c\u6709\u54ea\u4e9b\u6280\u672f\u6216\u65b9\u6cd5\u53ef\u4ee5\u7528\u4e8e\u8de8\u6a21\u6001\u6570\u636e\u7684\u878d\u5408\uff0c\u4ee5\u53ca\u8fd9\u4e9b\u65b9\u6cd5\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u53ef\u80fd\u9762\u4e34\u7684\u6311\u6218\u548c\u89e3\u51b3\u65b9\u6848\u662f\u4ec0\u4e48\uff1f\u300d\n", "595b81fe-b868-420f-a43e-e551a8cbcce1:\u300cref_ids: 454846130529033370, chunk_ids: 4, Score: 0.3281, Text: # 2. Related Works\n\n# 2.1. Imbalanced multimodal learning\nSeveral recent studies [ 5 ,42 ,43 ] have shown that many multimodal DNNs cannot achieve better performance compared to the best single-modal DNNs. Wang et al . [ 42 ]found that different modalities overfit and generalize at different rates and thus obtain suboptimal solutions when jointly training them using a unified optimization strategy. Peng et al . [ 31 ] proposed that the better-performing modality will dominate the gradient update while suppressing the learning process of the other modality. Furthermore, it has been reported that multimodal DNNs can exploit modal bias in the data, which is inconsistent with the expectation of exploiting cross-modal interactions in VQA [ 8 ,12 ,43 ].  \n\nSeveral approaches have been proposed recently to deal with the modal imbalance problem [ 5 ,31 ,42 ,46 ]. Wang et al . [ 42 ] used additional classifiers for each modality and its fusion modality and then optimized the gradient mixing problem they introduced to obtain better weights for each branch. Du et al . [ 5 ] tried to improve uni-modal performance by distilling knowledge from well-trained models. However, these approaches introduce more model structure and computational effort, which makes the training process more complex and expensive. Xiao et al . [ 46 ] proposed DropPathway, which randomly drops the audio pathway during training as a regularization technique to adjust the learning paces between visual and audio pathways. Peng et al . [ 31 ] chose to slow down the learning rate of the mighty modality by online modulation to lessen the inhibitory effect on the other modality. Although a certain degree of improvement is achieved, such approaches do not impose the intrinsic motivation of improvement on the slow-learning modality, making the improvement of this modality a passive rather than an active behavior. Besides, the interference from other modalities will hinder the improvement by modulation based on the fused modality data. Furthermore, the application scenarios of these methods are limited by fusion methods or model structures. In this paper, we aim to power the slow-learning modality to reduce the gap with the dominant one and allow it to be used in conjunction with various fusion methods and model architectures.\n\n# 2.2. Prototypical network\nPrototypical networks were originally proposed to solve few-shot or zero-shot classification problems [ 4 ,7 ,34 ,37 ], based on the idea that there is an embedding, defined as a prototype, which is surrounded by points from the same class. Recently, this approach has been widely used to address long-tail recognition [ 47 ], domain adaptation [ 3 ,29 ], and facilitate unsupervised learning [ 20 ], since prototypes can be used to represent general features of a class. In [20 ,37 ], prototypes were interpreted as non-parametric prototypical classifiers that perform on par or even better than parametric linear classifiers. In this paper, we leverage prototypes to build non-parametric classifiers to evaluate the performance of each modality feature.  \n\n  \nFigure 2. (a) Performance of each modality and their fusion. (b) Training accuracy of multimodal with different modulations. Baseline means no extra modulation. Acc increases the gradient magnitude of the slow-learning modality. OMG-GE [ 31 ] reduces the gradient magnitude of better modality. (c) The gradient direction discrepancy (angle) between uni-modal and multimodal on the baseline. The results were acquired from CREMA-D.\n\n# 3. Modality Imbalance Analysis\nWithout loss of generality, we consider two input modalities as $m_{0}$ and $m_{1}$ . The dataset is denoted as $\\\\mathcal{D}$ ,hconsists of instances and their corresponding labels $(\\\\pmb{x},y)$ ,where ${\\\\pmb x}~=~({\\\\pmb x}^{m_{0}},{\\\\pmb x}^{m_{1}},y)~=~\\\\{x_{i}^{m_{0}},x_{i}^{m_{1}},y_{i}\\\\}_{i=1,2,\\\\dots,N},\\\\nonumber$ ,$y=\\\\{1,2,...,M\\\\}$ , and $M$ is the number of categories . The goal is to train a model with this data to predict $y$ from $\\\\textbf{\\\\em x}$ .  \n\nWe use a multimodal DNN with two uni-modal branches for prediction. Each branch has an encoder, denoted as $\\\\phi^{0},\\\\;\\\\phi^{1}$ , to extract features of respective modal data $\\\\pmb{x}^{m_{0}}$ and $\\\\pmb{x}^{m_{1}}$ . The representation outputs of encoders are deted a $z^{0}=\\\\phi^{0}\\\\,\\\\,\\\\overline{{(\\\\theta^{0},x^{m_{0}})}}$ \u0001and $z^{\\\\bar{1}}=\\\\phi^{1}\\\\left(\\\\theta^{1},{\\\\pmb x}^{m_{1}}\\\\right)$ \u0001, where $\\\\theta^{0}$ and \u03b8$\\\\theta^{1}$ are the parameters of encoders. The two unimodal encoders are connected through the representations by some kind of fusion, which is prevalent in multimodal learning [ 19 ,23 ,48 ]. In this work, we have tried some different fusion methods. For simplicity, we use $[\\\\cdot,\\\\cdot]$ to denote fusion operation. Let $W\\\\,\\\\in\\\\,\\\\mathbb{R}^{M\\\\times\\\\left(d_{z^{0}}+d_{z^{1}}\\\\right)}$ and $b\\\\,\\\\in\\\\,\\\\mathbb{R}^{M}$ denote the parameters of the linear classifier to produce the logits output:  \n\n$$\nf\\\\left(\\\\pmb{x}\\\\right)=W\\\\left[\\\\phi^{0}\\\\left(\\\\theta^{0},\\\\pmb{x}^{m_{0}}\\\\right);\\\\phi^{1}\\\\left(\\\\theta^{1},\\\\pmb{x}^{m_{1}}\\\\right)\\\\right]+b\n$$  \n\nThe cross-entropy loss of the multimodal model is  \n\n$$\n\\\\mathcal{L}_{C E}=-\\\\frac{1}{N}\\\\sum_{i=1}^{N}\\\\log\\\\frac{e^{f(x_{i})_{y_{i}}}}{\\\\sum_{k=1}^{M}e^{f(x_{i})_{k}}}\n$$  \n\nThe gradient of the softmax logits output with true label $y^{i}$ should be:  \n\n$$\n\\\\frac{\\\\partial\\\\mathcal{L}_{C E}}{\\\\partial f\\\\left(x_{i}\\\\right)_{y_{i}}}=\\\\frac{e^{\\\\left(W\\\\left[\\\\phi^{0};\\\\phi^{1}\\\\right]+b\\\\right)_{y_{i}}}}{\\\\sum_{k=1}^{M}e^{\\\\left(W\\\\left[\\\\phi^{0};\\\\phi^{1}\\\\right]+b\\\\right)_{k}}}-1\n$$  \n\nFor convenience, we simplify $\\\\phi^{0}\\\\left(\\\\theta^{0},x^{m_{0}}\\\\right)$ \u0001and $\\\\phi^{1}\\\\left(\\\\theta^{1},x^{m_{1}}\\\\right)$ \u0001as $\\\\phi^{0}$ and $\\\\phi^{1}$ .According to Eq. ( ), the final gradient is influenced by the performance of the fused modality. However, we cannot directly judge the contribution of the two modalities. To do it, we take a simple fusion method, summation, as the example here:  \n\n$$\n\\\\begin{array}{r}{f\\\\left(\\\\pmb{x}\\\\right)=W^{0}\\\\cdot\\\\phi^{0}\\\\left(\\\\theta^{0},\\\\pmb{x}^{m_{0}}\\\\right)+b^{0}}\\\\\\\\ {+W^{1}\\\\cdot\\\\phi^{1}\\\\left(\\\\theta^{1},\\\\pmb{x}^{m_{1}}\\\\right)+b^{1}}\\\\end{array}\n$$  \n\nwhere $W^{0}\\\\,\\\\in\\\\,\\\\mathbb R^{M\\\\times d_{z^{0}}}$ ,$W^{1}\\\\,\\\\in\\\\,\\\\mathbb{R}^{M\\\\times d_{z^{1}}}$ and $b^{0},b^{1}\\\\,\\\\in\\\\,\\\\mathbb{R}^{M}$ are the parameters for individual modal classifier. Therefore, we use the logits output of the ground truth as the performance for each modality and their summation fusion:  \n\n$$\n\\\\begin{array}{r l}&{s^{0}=\\\\operatorname{softmax}\\\\left(W^{0}\\\\cdot\\\\phi^{0}+b^{0}\\\\right)_{y}}\\\\\\\\ &{s^{1}=\\\\operatorname{softmax}\\\\left(W^{1}\\\\cdot\\\\phi^{1}+b^{1}\\\\right)_{y}}\\\\\\\\ &{s^{f u}=\\\\operatorname{softmax}\\\\left(W^{0}\\\\cdot\\\\phi^{0}+b^{0}+W^{1}\\\\cdot\\\\phi^{1}+b^{1}\\\\right)_{y}}\\\\end{array}\n$$  \n\nAs shown in Fig. 2a , the performance of the audio modal is very similar to the multimodal during training and the visual modal is much worse in CREMA-D [ 2 ], which means better modality contributes more to the gradient because of higher performance similarity with their fusion. Moreover, the visual modal is severely suppressed in the multimodal learning process because of the excessive dominance of gradient updates by the audio modal. Therefore, we have to mitigate the inhibition on visual modal to fully exploit visual features. A straightforward approach can be to increase the magnitude of the gradient. We test a similar way with OGM-GE [ 31 ], named Acc, to increase the gradient magnitude of the slow-learning modality instead of lower the gradient magnitude of the better modality in OGM-GE. The results are shown in Figs. 2b and 2c .  \n\nAs demonstrated in Fig. 2b , increasing the gradient magnitude of the slow-learning modality (visual) could improve the validation accuracy a little bit but not as obviously as OGM-GE does. To find the reason for the phenomenon, we use the uni-modal output $s^{0}$ and $s^{1}$ to calculate the gradient for each modal branch additionally with CE loss and illustrate the direction discrepancy of gradients between each uni-modal and multimodal $s^{f u}$ , as shown in Fig. 2c . The angle between the actual gradient update direction (from the multimodal output) and each modal\u2019s guidance direction (from the uni-modal output) increases dramatically during training, in the meantime, remaining acute. Therefore, the two modalities influence each other, resulting in a larger gap between the gradient update direction obtained by the fused modality and the expected direction of each modality. That means the slow-learning modality cannot fully exploit its feature with the disturbance from other modalities, ultimately making the method of modulating the gradient amplitude limited, as illustrated in Fig. 1 .  \n\n  \nFigure 3. The pipeline of modality modulation with prototypical modal rebalance strategy.\u300d\n", "595b81fe-b868-420f-a43e-e551a8cbcce1:\u300cref_ids: 454847538467043982, chunk_ids: 7, Score: 0.2617, Text: # 5. Conclusion\nIn this work, we proposed a meta learning-based framework (MCRES) to improve the generalization performance of RES models, especially when handling novel compositions of learned concepts. By constructing a virtual training set and multiple virtual testing sets w.r.t. various levels of novel compositions and then optimizing the model via meta optimization, our framework can effectively improve model generalization performance. Extensive experiments show that our framework achieves superior performance on widely used benchmarks. Moreover, our framework is flexible, and can be seamlessly applied on various models with different architectures to enhance their performance.  \n\nAcknowledgement. This work is supported by MOE AcRF Tier 2 (Proposal ID: T2EP20222-0035), National Research Foundation Singapore under its AI Singapore Programme (AISG-100E-2020-065), and SUTD SKI Project (SKI 2021 02 06).\u300d\n", "595b81fe-b868-420f-a43e-e551a8cbcce1:\u300cref_ids: 454984236293691964, chunk_ids: 5, Score: 0.2578, Text: # 4.3.2 Effectiveness of Proposed Methods\nIn Table 6 , we provide results using different pretraining tasks and masking strategies to demonstrate the effectiveness of our proposed modules.  \n\nComparing #1 and #2 in Table 6 , we observe that WWM brings significant performance improvements on all datasets. The reason is that it increases the difficulty of the MLM task, so we can obtain a stronger language model. We also find that LAM can also brings consistent improvements on all dataset because LAM can force the model to learn better representations for layout information, which is beneficial to downstream tasks.  \n\nComparing #2 to #4 and #3 to #5, it is observed that the MPM task also brings considerable improvements on all datasets. MPM works as an auxiliary task to help the MLM task and can increase the pre-training difficulty, contributing to learning better and more robust layout representations.  \n\nMoreover, the full-version LayoutMask (#5) outperforms the naive version (#1) by a large margin $\\\\operatorname{FUNSD}\\\\!+\\\\!3.18\\\\%$ , CORD $+0.67\\\\%$ ,$\\\\mathrm{SROIE}{+}1.11\\\\%$ ,and $\\\\mathbf{RV}\\\\mathrm{L-}\\\\mathrm{CDIP+}1.09\\\\%)$ , demonstrating the effectiveness of our proposed modules when working together. To better illustrate the effectiveness of our model design, we list category-level accuracy improvements on RVL-CDIP dataset and provide detailed discussions in Section Bof the Appendix.\n\n# 5 Conclusion\nIn this paper, we propose LayoutMask, a novel multi-modal pre-training model, to solve the reading order issues in VrDU tasks. LayoutMask adopts local 1D position as layout input and can generate adaptive and robust multi-modal representations. In LayoutMask, we equip the MLM task with two masking strategies and design a novel pretraining objective, Masked Position Modeling, to enhance the text-layout interactions and layout representation learning. With only using text and layout modalities, our method can achieve excellent results and significantly outperforms many SOTA methods in VrDU tasks.\n\n# Limitations\nOur method has the following limitations:  \n\nDatasets: In multi-modal pre-training, we rely on downstream datasets to evaluate the performance of pre-trained models. The commonly used entity extraction datasets are relatively small and lack diversity, so the proposed method may not generalize well to real word scenarios.  \n\nLack of Image Modality: In LayoutMask, we focus on text-layout interactions, leaving the image modality unexplored. However, documents in the real world contain many elements that can not be described by text and layout modalities, like figures and lines, so incorporating image modality is important in building a universal multi-modal pre-training model for document understanding.\n\n\n\n# A Ablation Study of Masking Probabilities\nWe compare LayoutMask using different $\\\\mathrm{P_{mlm}}$ and $\\\\mathrm{P_{mpm}}$ , and the results are in Figure 4 . We first find the best $\\\\mathrm{P_{mlm}}$ without using the MPM task, and the optimal value is $25\\\\%$ . Then we fix such optimal $\\\\mathrm{P_{mlm}}$ to find the best $\\\\mathrm{P_{mpm}}$ , which is $15\\\\%$ as the results show.\n\n# BAblation Study on RVL-CDIP\nTo further understand the effectiveness of our model design, we list the detailed classification results on RVL-CDIP dataset with the naive version and the full version in Table 7 . It is observed that the major performance improvements come from three categories: presentation $(+3.36\\\\%)$ , ad\u300d\n"]}}, "__type__": "1"}}, "template_store/metadata": {"2f4e5281-1176-4588-9b62-8ded87118154": {"template_hash": ""}}}