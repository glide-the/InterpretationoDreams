角色,内容,分镜
d338c352-b6ad-45ff-b9c3-0418aa6cccc4,适用性,3>1
d338c352-b6ad-45ff-b9c3-0418aa6cccc4,"### 问题提出

在MCTS+PRM框架的适用性分析中，该框架在游戏、路径规划、推荐系统等多个领域表现出良好的适用性。然而，不同领域的应用场景和需求存在显著差异。例如，游戏AI需要快速响应和实时决策，而推荐系统则更注重长期用户偏好的学习和预测。因此，如何针对不同领域的特点，优化MCTS+PRM框架的具体实现，以最大化其适用性和性能，是一个值得深入研究的问题。

**具体问题**：在MCTS+PRM框架中，如何根据不同应用领域（如游戏AI、路径规划、推荐系统）的特点，调整和优化MCTS的选择策略和PRM的偏好建模机制，以提升框架在该领域的适用性和性能？",3>1
d338c352-b6ad-45ff-b9c3-0418aa6cccc4,"ref_ids: 454845941740251190, chunk_ids: 9, Score: 0.3613, Text: # Related Works
Two streams of research are particularly relevant to our work: learnable (lifelong) MAPF methods and utilizing MCTS for multi-agent systems and MAPF in particular. Next, we review both of these domains.  

Learnable (L)MAPF Solvers Among the recent works dedicated to MAPF, one of the first ones that were specifically dedicated to creating a learning-based MAPF solver was (Sartoretti et al. 2019). A combination of reinforcement learning and learning from expert demonstrations was used to create a learnable policy called Primal, tailored to solve conventional MAPF problems. Later in (Damani et al. 2021), an enhanced version of this solver, Primal2, was introduced. The latter was equipped with special corridor reasoning techniques, aiming at avoiding the deadlocks in narrow corridors, and it supported lifelong MAPF setting (therefore, we choose Primal2 as one of the baselines we compare our method to). Among the other learnable MAPF solvers that use reinforcement learning to obtain a decision-making policy, one can name (Riviere et al. 2020; Wang et al. 2020). The learnable methods introduced in (Li et al. 2020; Ma, Luo, and Ma 2021; Li et al. 2022) add communication capabilities to the agents, i.e., allow the agents to communicate to resolve deadlocks and avoid congestion. In this work, we compare with one of the most recent communication-based methods, i.e., SCRIMP (Wang et al. 2023). However, it is worth noting that our method does not rely on agent communication.  

MCTS for MAPF Initially, Monte Carlo Tree Search (MCTS) algorithms demonstrated their effectiveness in competitive games with complete information, such as chess or Go (Silver et al. 2017). More recent versions of MCTS utilize deep neural networks to approximate the values of game states instead of relying solely on simulations. These approaches have also shown promising results in singleagent scenarios, where agents can learn a model of the environment and play Atari games (Schrittwieser et al. 2020; Ye et al. 2021). Besides gaming, MCTS methods have found applications in other domains, such as matrix multiplication optimization (Fawzi et al. 2022) and theorem proving using the Hyper Tree approach (Lample et al. 2022). Additionally, MCTS techniques have demonstrated applicability in robotics (Best et al. 2019; Dam et al. 2022).  

Despite the growing interest in utilizing MCTS for multiagent tasks, there have been limited applications of MCTS for MAPF. In their work (Zerbel and Yliniemi 2019), the authors propose a multi-agent MCTS for Anonymous MAPF in a grid-world environment. Their environment has a dense reward signal (the agent who reached any goal on the map received a reward and ended the episode), and there are no obstacles, making collision avoidance easier. The authors build a separate tree for each agent using a classical algorithm. They then jointly apply the best actions (forming a plan) from the trees in the simulator to receive true scores of the solution and update the trees on that difference. This approach performs well even with a large number of agents.  

A recent paper (Skrynnik et al. 2021) proposed a more sophisticated approach for multi-agent planning that combines RL and MCTS. The authors suggested a two-part scheme that includes a goal achievement module and a conflict resolution module. The latter was trained using MCTS. The construction of the search tree for each of the agents was also performed independently, and actions for other agents were selected using the currently trained policy. This work used MCTS only during training to train the conflict resolution policy.",3>1
d338c352-b6ad-45ff-b9c3-0418aa6cccc4,"ref_ids: 454848213962351312, chunk_ids: 1, Score: 0.3125, Text: # 2.2 Acceleration of MCTS
MCTS-based methods have proved their strong capability of solving complex games or tasks. However, the high computational cost of MCTS hinders its application to some real-time and more general scenarios. Therefore, numerous works are devoted to accelerating MCTS. For example, to make the selection stage more effective, some heuristic pruning methods [ 14 ,33 ,29 ,1 ,2 ] aim to reduce the width and depth of the search tree with some heuristic functions. Furthermore, for more efficient evaluations, Lorentz [ 22 ] proposed early playout termination of MCTS (MCTS-EPT) to stop the random playouts early and use an evaluation function to assess win or loss. Moreover, Hsueh et al. [18 ] applied MCTS-EPT to the Chinese dark chess and proved its effectiveness. Afterward, similar ideas have been applied in the evaluation stage of AlphaGoZero [ 32 ] and later MCTS-based methods [31 ,27 ,34 ]. They evaluate the $Q$ -values through a learnable evaluation network instead of running playouts to the end. Grill et al. [15 ] propose a novel regularized policy optimization method based on AlphaZero to decrease the search budget of MCTS, which is from the optimization perspective. Danihelka et al. [10 ] propose a policy improvement algorithm based on sampling actions without replacement, named Gumbel trick to achieve better performance when planning with few simulations. However, these methods mentioned above focus on the specific stage of the search iteration or reduce the total budget through pruning and optimization methods, which are orthogonal to us. And few works targets at the search loop. Lan et al. [21 ] propose DS-MCTS, which defines the uncertainty of MCTS and approximates it by extra DNNs with specific features for board games in training. During the evaluation, DS-MCTS will check periodically and stop the search if the state is certain.

# 3 Background
The AlphaGo series of work [ 30 ,32 ,31 ,27 ] are all MCTS-based reinforcement learning algorithms. Those algorithms assume the environment transition dynamics are known or learn the environment dynamics. Based on the dynamics, they use the Monte-Carlo tree search (MCTS) as the policy improvement operator. I.e., taking in the current policy, MCTS returns a better policy with the search algorithm. The systematic search allows the MCTS-based RL algorithm to quickly improve the policy and perform much better in the setting where heavy reasoning is required.

# 3.1 MCTS
This part briefly introduces the MCTS method implemented in reinforcement learning applications. As mentioned in the related works, modern MCTS-based RL algorithms include four stages in the search loop, namely selection, expansion, evaluation, and backpropagation.  

MCTS takes in the current states and generates a policy after the search loop of $N$ iterations. Here $N$ is a constant number of iterations set by the designer, regarded as the total budget. In the selection stage of each iteration, an action will be selected by maximizing over UCB. Specifically, AlphaZero [31 ] and MuZero [ 27 ] are developed based on a variant of UCB, P-UCT [ 25 ] and have achieved great success on board games and Atari games. The formula of P-UCT is the Eq (1):  

$$
a^{k}=\\arg\\operatorname*{max}_{a\\in\\mathcal{A}}Q(s,a)+P(s,a)\\frac{\\sqrt{\\sum_{b\\in\\mathcal{A}}N(s,b)}}{1+N(s,a)}(c_{1}+\\log((\\sum_{b\\in\\mathcal{A}}N(s,b)+c_{2}+1)/c_{2})),
$$  

where $k$ is the index of iteration, $\\boldsymbol{\\mathcal{A}}$ is the acti $Q(s,a)$ is the estimated Q-value, $P(s,a)$ is the policy prior obtained from neural networks, $N(s,a)$ is the visitations to select the action a from the state $s$ and $c_{1},c_{2}$ are hyper-parameters. The output of MCTS is the visitation of each action of the root node. After $N$ search iterations, the final policy $\\pi(s)$ is defined as the normalized root visitation distribution simplification, we use $\\pi_{N}(s)$ $\\pi_{k}$ in place of , where $\\begin{array}{r}{\\pi_{k}(s,a)=N(s,a)/\\sum_{b\\in\\mathcal{A}}N(s,b)=N(s,a)/k,a\\in\\mathcal{A}}\\end{array}$ $\\pi_{k}(s)$ sometimes. And the detailed procedure of MCTS is ∈A . For introduced in Appendix. In our method, we propose to approximate the final policy $\\pi_{N}(s)$ with $\\hat{\\pi}_{k}(s)$ ,which we name as a virtual expanded policy, through a new expansion method and a termination rule. In this way, the number of iterations in MCTS can be reduced from $N$ to $k$ .

# 3.2 Computation Requirement
Most of the computations in MCTS-based RL are in the MCTS procedure. Each action taken by MCTS requires $N$ times neural network evaluations, where $N$ is a constant number of iterations in the search loop. Traditional RL algorithms, such as PPO [ 28 ] or DQN [ 23 ], only need a single neural network evaluation per action. Thus, MCTS-based RL is roughly $N$ times computationally more expensive than traditional RL algorithms. In practice, training a single Atari game needs 12 hours of computation time on 40 TPUs [ 27 ]. The computation need is roughly two orders of magnitude more than traditional RL algorithms [28], although the final performance of MuZero is much better.

# 4 Method
We aim to spend more search time on harder states and less on easier states. Intuitively, human knows when to make a quick decision or a slow decision under different circumstances. Unfortunately, this situation-aware behavior is absent in current MCTS algorithms. Therefore, we propose an MCTS variant that terminates the search iteration adaptively. It consists of two components: a novel expansion method named virtual expansion to estimate the final visitation based on the current partial tree; a termination rule that decides when to terminate based on the hardness of the current scenario. And we will display the adaptive mechanism through visualizations in Section 5.5.",3>1
d338c352-b6ad-45ff-b9c3-0418aa6cccc4,"ref_ids: 454984236281633338, chunk_ids: 4, Score: 0.2480, Text: # 2 RELATED WORK
The full version of the related work is in Appendix A, we briefly introduce several highly related works here. In general, model-based RL for solving decision-making problems can be divided into three perspectives: model learning, policy learning, and decision-making. Moreover, optimal control theory also concerns the decision-making problem and is deeply related to model-based RL.  

Model learning: How to learn a good model to support decision-making is crucial in model-based RL. There are two main aspects of the work: the model structure designing (Chua et al., 2018; Zhang  

et al., 2021; 2020; Hafner et al., 2021; Chen et al., 2022) and the loss designing (D’Oro et al., 2020;   
Farahmand et al., 2017; Li et al., 2021).  

Policy learning: Two methods are always used to learn the policy by using the learned model. One is to serve the learned model as a black-box simulator to generate the data (Janner et al., 2019b; Yu et al., 2020; Lee et al., 2020). Another way is to use the learned model to calculate the policy gradient (Heess et al., 2015b; Clavera et al., 2019; Amos et al., 2021).  

Decision-making: When making the decision, we need to generate the actions that can achieve our goal. Many of the model-based RL methods make the decision by using the learned policy solely (Hafner et al., 2021). Similar to our paper, some works also try to make decisions by using the learned model, but the majority only focus on the discrete action space. The well-known MCTS method achieves a lot of success. For example, the well-known Alpha Zero (Silver et al., 2017), MuZero (Schrittwieser et al., 2020). There are only a few works that study the continuous action space, such as the Continuous UCT (Cou¨etoux et al., 2011), the sampled MuZero (Hubert et al., 2021), the TreePI (Springenberg et al., 2020), and the TD-MPC (Hansen et al., 2022a).  

Optimal control theory: Beyond deep RL, optimal control also considers the decision-making problem but rather relies on the known and continuous transition model. In modern optimal control, Model Predictive Control (MPC) (Camacho & Alba, 2013) framework is always adopted when the environment is highly non-linear. In MPC, the action is planned during the execution by using the model, and such a procedure is called trajectory optimization. Plenty of previous works (Byravan et al., 2021; Chua et al., 2018; Pinneri et al., 2021; Nagabandi et al., 2020) use MPC framework to solve the continuous control tasks, but most of them are based on zero-order or sample-based method to do the planning. The most relevant works are DDP (Murray & Yakowitz, 1984), iLQR (Li & Todorov, 2004), and iLQG (Todorov & Li, 2005; Tassa et al., 2012). We discuss the detailed differences between our method and these methods in Appendix A.  

Since our planning algorithm relies on the learned model and learned policy, we build our algorithm based on these works on model learning and policy learning . Our POMP algorithm tries to solve a more challenging task compared to the related work on decision-making : efficiently optimize the trajectory in continuous action space when the environment model is unknown. Different from our works, the MPC with DDP as trajectory optimizer from optimal control theory requires the known environment model, and also requires the hessian matrix for online optimization from scratch.

# 3 PRELIMINARIES
Reinforcement Le onsider discrete-time Marko Decision Process (M $\\mathcal{M}$ the tuple ($(\\mathcal{X},\\mathcal{A},f,r,\\gamma)$ XA $\\mathcal{X}$ state space, A is the action space, $f\\,:\\,x_{t+1}\\,=$   
$f(x_{t},a_{t})$ is the transition model, $r:\\mathcal{X}\\times\\mathcal{A}\\to\\mathbb{R}$ X × A → is the reward function, $\\gamma$ is the discount factor. $t$ $\\begin{array}{r}{R_{t}=\\sum_{t^{\\prime}=t}^{\\infty}\\gamma^{t^{\\prime}-t}r_{t^{\\prime}}}\\end{array}$ , and Reinforcement Learn  
$\\begin{array}{r}{\\operatorname*{max}_{\\theta}J(\\theta)=\\operatorname*{max}_{\\theta}\\mathbb{E}_{\\pi_{\\theta}}R_{t}=\\operatorname*{max}_{\\theta}\\mathbb{E}_{\\pi_{\\theta}}\\Big[\\sum_{t^{\\prime}=t}^{\\infty}\\gamma^{t^{\\prime}-t}r(x_{t^{\\prime}},a_{t^{\\prime}})\\Big].}\\end{array}$ ing (RL) aims to find a policy $\\pi_{\\theta}:\\mathcal{X}\\times\\mathcal{A}\\rightarrow\\mathbb{R}^{+}$ X × A → h P that can maximize the expected return .$J$ . where  

$\\begin{array}{r}{\\operatorname*{max}_{a_{t}}\\mathbb{E}\\Big[r(x_{t},a_{t}|x_{t}\\,=\\,x)+\\gamma V^{*}(x_{t+1})\\Big]}\\end{array}$ value function obeys an important identity known as the Bellman optimality equation Bellman Equation. hWe define the optimal value function i . The idea behind this equation is that if we know the $V^{*}(x)=\\operatorname*{max}\\mathbb{E}[R_{t}|x_{t}=x]$ |. The optimal $V^{*}(x)\\;=\\;$ $r(x_{t},a_{t})$ for any $a_{t}$ and next step value function $V^{*}(x_{t+1})$ for any $s_{t+1}$ , we can recursively select the action $a_{t}$ which m $r(x_{t},a_{t}|x_{t}=x)+\\gamma V^{*}(x_{t+1})$ milarly, we can denote the optimal action-value function $Q^{*}(x,a)\\;=\\;\\operatorname*{max}\\mathbb{E}[R_{t}|x_{t}\\;=\\;x,a_{t}\\;=\\;a]$ |], and it obeys a similar Bellman optimility equation $\\begin{array}{r}{Q^{*}(x,a)=\\operatorname*{max}_{a_{t+1}}\\mathbb{E}\\Big[r(x_{t},a_{t}|x_{t}=x,a_{t}=a)+\\gamma Q^{*}(x_{t+1},a_{t+1})\\Big].}\\end{array}$ .  

Model-based RL. Model-based RL method distinguishes itself from model-free counterparts by using the data to learn a transition model. Following Janner et al. (2019a) and Clavera et al. (2019), we use parametric neural networks to approximate the transition function, reward function, policy function and $\\mathrm{^Q}$ -value function with the following objective function to be optimized  $J_{f}(\\psi)\\,=\\,\\mathbb{E}\\big[\\log f(x_{t+1}|x_{t},a_{t})\\big]$ '', $J_{r}(\\omega)\\,=\\,\\mathbb{E}\\big[\\log r(r_{t}|x_{t},a_{t})\\big]$ '', $\\begin{array}{r}{\\bar{J_{\\pi}}(\\theta)\\,=\\,\\mathbb{E}\\bigl[\\sum_{t=0}^{H-1}\\gamma^{t}r(\\bar{x}_{t},a_{t})\\,+\\,}\\end{array}$ ' P$\\gamma^{H}Q(x_{H},a_{H})]$ 'and $J_{Q}\\,=\\,\\mathbb{E}\\bigl[\\|Q(x_{t},a_{t})-(r+\\tilde{Q}(x_{t+1},a_{t+1}))\\|_{2}\\bigr]$ '∥−∥', respectively. In ${\\cal J}_{\\pi}(\\theta)$ , we truncate the trajectory in horizon Hto avoid long time model rollout.  

Notations. For one-dimensional state and action case, we denote the partial differentiation of function by using its output with subscripts, e.g. ,$\\begin{array}{r}{r_{x}\\ \\triangleq\\ \\frac{\\partial r(x,a)}{\\partial x},\\ r_{a}\\ \\triangleq\\ \\frac{\\bigtriangleup r(x,a)}{\\partial a},\\ f_{x}\\ \\triangleq\\ \\frac{\\partial f(x,a)}{\\partial x}}\\end{array}$ ,$f_{a}\\triangleq{\\frac{\\partial f(x,a)}{\\partial a}}$ ,$\\begin{array}{r}{Q_{x}\\triangleq\\frac{\\partial Q(x,a)}{\\partial x}}\\end{array}$ and $\\begin{array}{r}{Q_{a}\\triangleq\\frac{\\partial Q(x,a)}{\\partial a}}\\end{array}$ . See Appendix E for the multi-dimension case.",3>1
