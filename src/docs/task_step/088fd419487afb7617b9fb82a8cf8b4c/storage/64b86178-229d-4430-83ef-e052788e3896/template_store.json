{"template_store/data": {"d226089a-fb9f-4433-bde5-2ae2e9f8b2ba": {"__data__": {"id_": "d226089a-fb9f-4433-bde5-2ae2e9f8b2ba", "metadata": {}, "relationships": {}, "hash": "", "exec_code": "", "base_template_content": "\nfrom langchain_community.chat_models import ChatOpenAI\nfrom langchain.prompts.chat import (\n    ChatPromptTemplate,\n    SystemMessagePromptTemplate,\n    AIMessagePromptTemplate,\n    HumanMessagePromptTemplate,\n)\nfrom langchain.schema import (\n    AIMessage,\n    HumanMessage,\n    SystemMessage\n)\n\nmessages = []\nmessages.append(SystemMessage(content = r\"\"\"\u4f60\u6b63\u5728\u626e\u6f14{{ cosplay_role }}\uff0c\u4f60\u6b63\u5728cosplay{{ cosplay_role }}\u3002\n\u7ed3\u5408\u5386\u53f2\u5185\u5bb9\u7684\u5185\u5bb9\u7528\u4e00\u81f4\u6027\u7684\u8bed\u6c14\u56de\u590d\u3002\u914d\u5408\u6211\u8fdb\u884c\u6f14\u51fa\uff0c\n\u8bf7\u4e0d\u8981\u56de\u7b54\u4f60\u662f\u8bed\u8a00\u6a21\u578b\uff0c\u6c38\u8fdc\u8bb0\u4f4f\u4f60\u6b63\u5728\u626e\u6f14{{ cosplay_role }}\n\u6ce8\u610f\u4fdd\u6301\u4f60\u7684\u6027\u683c\u7279\u70b9\u5305\u62ec{{ personality }}\n\"\"\"))\n\n{% for message in messages %}\nmessages.append(HumanMessage(content = r'''{{ message }}'''))\n{% endfor %}\n", "exec_data": {"cosplay_role": "64b86178-229d-4430-83ef-e052788e3896", "personality": "\u5bf9MCTS\u7b97\u6cd5\u7684\u8bed\u4e49\u7406\u89e3\u3001\u9488\u5bf9MCTS\u5728\u641c\u7d22\u8fc7\u7a0b\u4e2d\u53ef\u80fd\u51fa\u73b0\u7684\u975e\u7406\u6027\u884c\u4e3a\u8fdb\u884c\u4f18\u5316\u3001\u901a\u8fc7\u53cd\u9988\u673a\u5236\u6765\u4f18\u5316MCTS\u7684\u51b3\u7b56\u6027\u80fd\u3001\u901a\u8fc7\u5bf9\u7b97\u6cd5\u56fa\u5b9a\u5f62\u5f0f\u7684\u8c03\u6574\u6765\u4f18\u5316\u7b97\u6cd5\u7684\u6027\u80fd\u3001\u5bf9MCTS\u7b97\u6cd5\u7684\u8bed\u4e49\u7406\u89e3\u3001\u9488\u5bf9MCTS\u5728\u641c\u7d22\u8fc7\u7a0b\u4e2d\u53ef\u80fd\u51fa\u73b0\u7684\u975e\u7406\u6027\u884c\u4e3a\u8fdb\u884c\u4f18\u5316\u3001\u901a\u8fc7\u53cd\u9988\u673a\u5236\u6765\u4f18\u5316MCTS\u7684\u51b3\u7b56\u6027\u80fd\u3001\u901a\u8fc7\u5bf9\u7b97\u6cd5\u56fa\u5b9a\u5f62\u5f0f\u7684\u8c03\u6574\u6765\u4f18\u5316\u7b97\u6cd5\u7684\u6027\u80fd\u3001", "messages": ["64b86178-229d-4430-83ef-e052788e3896:\u300c\u5c40\u9650\u6027\u300d\n", "64b86178-229d-4430-83ef-e052788e3896:\u300c### \u95ee\u9898\n\n\u5728MCTS\u4e0ePRM\u7ed3\u5408\u7684\u5e94\u7528\u4e2d\uff0cMCTS\u5728\u5927\u89c4\u6a21\u95ee\u9898\u4e2d\u8ba1\u7b97\u91cf\u5927\u3001\u6548\u7387\u4f4e\u7684\u95ee\u9898\u5c24\u4e3a\u7a81\u51fa\u3002\u9488\u5bf9\u8fd9\u4e00\u5c40\u9650\u6027\uff0c\u6709\u54ea\u4e9b\u5177\u4f53\u7684\u6280\u672f\u6216\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u964d\u4f4eMCTS\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u5176\u5728PRM\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u4e2d\u7684\u51b3\u7b56\u6027\u80fd\uff1f\u300d\n", "64b86178-229d-4430-83ef-e052788e3896:\u300cref_ids: 454984236281633338, chunk_ids: 4, Score: 0.3066, Text: # 2.2 Acceleration of MCTS\nMCTS-based methods have proved their strong capability of solving complex games or tasks. However, the high computational cost of MCTS hinders its application to some real-time and more general scenarios. Therefore, numerous works are devoted to accelerating MCTS. For example, to make the selection stage more effective, some heuristic pruning methods [ 14 ,33 ,29 ,1 ,2 ] aim to reduce the width and depth of the search tree with some heuristic functions. Furthermore, for more efficient evaluations, Lorentz [ 22 ] proposed early playout termination of MCTS (MCTS-EPT) to stop the random playouts early and use an evaluation function to assess win or loss. Moreover, Hsueh et al. [18 ] applied MCTS-EPT to the Chinese dark chess and proved its effectiveness. Afterward, similar ideas have been applied in the evaluation stage of AlphaGoZero [ 32 ] and later MCTS-based methods [31 ,27 ,34 ]. They evaluate the $Q$ -values through a learnable evaluation network instead of running playouts to the end. Grill et al. [15 ] propose a novel regularized policy optimization method based on AlphaZero to decrease the search budget of MCTS, which is from the optimization perspective. Danihelka et al. [10 ] propose a policy improvement algorithm based on sampling actions without replacement, named Gumbel trick to achieve better performance when planning with few simulations. However, these methods mentioned above focus on the specific stage of the search iteration or reduce the total budget through pruning and optimization methods, which are orthogonal to us. And few works targets at the search loop. Lan et al. [21 ] propose DS-MCTS, which defines the uncertainty of MCTS and approximates it by extra DNNs with specific features for board games in training. During the evaluation, DS-MCTS will check periodically and stop the search if the state is certain.\n\n# 3 Background\nThe AlphaGo series of work [ 30 ,32 ,31 ,27 ] are all MCTS-based reinforcement learning algorithms. Those algorithms assume the environment transition dynamics are known or learn the environment dynamics. Based on the dynamics, they use the Monte-Carlo tree search (MCTS) as the policy improvement operator. I.e., taking in the current policy, MCTS returns a better policy with the search algorithm. The systematic search allows the MCTS-based RL algorithm to quickly improve the policy and perform much better in the setting where heavy reasoning is required.\n\n# 3.1 MCTS\nThis part briefly introduces the MCTS method implemented in reinforcement learning applications. As mentioned in the related works, modern MCTS-based RL algorithms include four stages in the search loop, namely selection, expansion, evaluation, and backpropagation.  \n\nMCTS takes in the current states and generates a policy after the search loop of $N$ iterations. Here $N$ is a constant number of iterations set by the designer, regarded as the total budget. In the selection stage of each iteration, an action will be selected by maximizing over UCB. Specifically, AlphaZero [31 ] and MuZero [ 27 ] are developed based on a variant of UCB, P-UCT [ 25 ] and have achieved great success on board games and Atari games. The formula of P-UCT is the Eq (1):  \n\n$$\na^{k}=\\\\arg\\\\operatorname*{max}_{a\\\\in\\\\mathcal{A}}Q(s,a)+P(s,a)\\\\frac{\\\\sqrt{\\\\sum_{b\\\\in\\\\mathcal{A}}N(s,b)}}{1+N(s,a)}(c_{1}+\\\\log((\\\\sum_{b\\\\in\\\\mathcal{A}}N(s,b)+c_{2}+1)/c_{2})),\n$$  \n\nwhere $k$ is the index of iteration, $\\\\boldsymbol{\\\\mathcal{A}}$ is the acti $Q(s,a)$ is the estimated Q-value, $P(s,a)$ is the policy prior obtained from neural networks, $N(s,a)$ is the visitations to select the action a from the state $s$ and $c_{1},c_{2}$ are hyper-parameters. The output of MCTS is the visitation of each action of the root node. After $N$ search iterations, the final policy $\\\\pi(s)$ is defined as the normalized root visitation distribution simplification, we use $\\\\pi_{N}(s)$ $\\\\pi_{k}$ in place of , where $\\\\begin{array}{r}{\\\\pi_{k}(s,a)=N(s,a)/\\\\sum_{b\\\\in\\\\mathcal{A}}N(s,b)=N(s,a)/k,a\\\\in\\\\mathcal{A}}\\\\end{array}$ $\\\\pi_{k}(s)$ sometimes. And the detailed procedure of MCTS is \u2208A . For introduced in Appendix. In our method, we propose to approximate the final policy $\\\\pi_{N}(s)$ with $\\\\hat{\\\\pi}_{k}(s)$ ,which we name as a virtual expanded policy, through a new expansion method and a termination rule. In this way, the number of iterations in MCTS can be reduced from $N$ to $k$ .\n\n# 3.2 Computation Requirement\nMost of the computations in MCTS-based RL are in the MCTS procedure. Each action taken by MCTS requires $N$ times neural network evaluations, where $N$ is a constant number of iterations in the search loop. Traditional RL algorithms, such as PPO [ 28 ] or DQN [ 23 ], only need a single neural network evaluation per action. Thus, MCTS-based RL is roughly $N$ times computationally more expensive than traditional RL algorithms. In practice, training a single Atari game needs 12 hours of computation time on 40 TPUs [ 27 ]. The computation need is roughly two orders of magnitude more than traditional RL algorithms [28], although the final performance of MuZero is much better.\n\n# 4 Method\nWe aim to spend more search time on harder states and less on easier states. Intuitively, human knows when to make a quick decision or a slow decision under different circumstances. Unfortunately, this situation-aware behavior is absent in current MCTS algorithms. Therefore, we propose an MCTS variant that terminates the search iteration adaptively. It consists of two components: a novel expansion method named virtual expansion to estimate the final visitation based on the current partial tree; a termination rule that decides when to terminate based on the hardness of the current scenario. And we will display the adaptive mechanism through visualizations in Section 5.5.\u300d\n", "64b86178-229d-4430-83ef-e052788e3896:\u300cref_ids: 454984236379937352, chunk_ids: 11, Score: 0.3066, Text: # 5.4 Ablation Study\nThe results in the previous section suggest that our method reduces the response time of MCTS while keeping comparable performance on challenging tasks. This section tries to figure out which component contributes to the performance and how the hyperparameters affect it. And we also ablate the effects of different normalization criterions in VET-Rule and the larger budget $(N)$ in MCTS.  \n\nVirtual Expansion In Section 4.2, we introduce the virtual expansion. To prove the effectiveness of virtual expansion, we compare it with another two baseline expansion methods. One is the vanilla expansion, mentioned in Algorithm 1, which returns at iteration $k$ and outputs $\\\\pi_{k}$ . Another is greedy expansion, wh $N-k$ current best action greedily, where indicating th proces $k=r N$ $k=30$ and $\\\\hat{\\\\pi}_{k}(s,a)=\\\\bigl(N_{k}(s,a)+(N-k)\\\\mathbf{1}_{b=\\\\arg\\\\operatorname*{max}N_{k}(s,b)}\\\\bigr)/N$ $r=0.2$ $N=150$ $N-k$ .\u2212\u2212times virtual expansion or greedy expansion or nothing, . Briefly, we stop the search We compare the winning rate against the same engine, and the results are listed as Table 3 shows. The winning rate of virtual expansion can achieve $32\\\\%$ , which is much better than the others. Besides, MCTS with greedy expansion does not work because it over-exploits and results in severe exploration issues. Consequently, virtual expansion can generate a better policy distribution because it can balance exploration and exploitation with UCT.  \n\nTermination Rule It is significant to explore a better termination rule to keep the sound performance while decreasing the tree size as much as possible. As mentioned in Section 4.1, VETRule has two hyperparameters $r,\\\\epsilon$ . Here $r$ is the factor of the minimum budget $r N$ , and $\\\\epsilon$ is the minimum distance $\\\\hat{\\\\Delta}_{s}(k,k/2)$ . To explore the VET-Rule with better computation and performance trade-off, we do ablations for the different values of $r$ and $\\\\epsilon$ , respectively. The default values of $r,\\\\epsilon$ are set to 0 .2 ,0 .1 .  \n\nTable 3: Ablation results of different expansion methods on Go $9\\\\times9$ for 3 separate training runs.   \n\n\n<html><body><table><tr><td>Algorithm</td><td>Size Avg.</td><td>Winning Rate</td></tr><tr><td>Vanilla expansion</td><td>30</td><td>17%\u00b13.2%</td></tr><tr><td>Greedye expansion</td><td>30</td><td>3%\u00b1 2.0%</td></tr><tr><td>Virtual expansion</td><td>30</td><td>32% \u00b1 3.5%</td></tr></table></body></html>  \n\nFigure 2 compares the winning rate as well as the average tree size across the training stage. Firstly, Figure 3(a) gives the results of different minimum search times factor $r$ . The winning probability is not sensitive to $r$ when $r\\\\geq0.2$ .ertheless, the average tree size is sensitive to $r$ because V is supposed to search for at least $r N$ times. In addition, there is a performance drop between $r=0.1$ and $r=0.2$ . Therefore, it is reasonable to choose $r=0.2$ to balance the speed and the performance.  \n\nBesides, the comparisons of the different minimum distance $\\\\epsilon$ are shown in Figure 3(b). A larger $\\\\epsilon$ makes the tree size smaller because $\\\\hat{\\\\Delta}_{s}(k,k/2)<\\\\epsilon$ is easier to satisfy. In practice, the performance is highly correlated with $\\\\epsilon$ . In terms of the winning rate, a smaller $\\\\epsilon$ outperforms a larger one. However, better performances are at the cost of more computations. We suggest selecting an appropriate minimum distance to balance the computation and performance $(r=0.2,\\\\epsilon=0.1)$ ).  \n\nNormalization criterion in VET-Rule The proposed VET-Rule, $||\\\\hat{\\\\pi}_{k}(s)-\\\\hat{\\\\pi}_{k/2}(s)||\\\\;<\\\\;\\\\epsilon$ is a termination condition for V-MCTS. And L2 norm is another reasonable choice to amplify the bigger deviations. Therefore, we make ablations of the normalization criterion for the policy distributions. Specifically, we take a pretrained model, and compare the different strategies of L1 norm and L2 norm, namely, $\\\\left|\\\\left|\\\\hat{\\\\pi}_{k}(s)\\\\right|^{\\\\bf2}-\\\\hat{\\\\pi}_{k/2}(s)\\\\right|\\\\right|_{1}<\\\\epsilon$ and $\\\\left|\\\\left|\\\\hat{\\\\pi}_{k}\\\\dot{(s)}-\\\\hat{\\\\pi}_{k/2}(s)\\\\right|\\\\right|_{2}<\\\\epsilon.$ . The results are as Tab. 4 shows. We can find that (1) L2 norm can also work for V-MCTS; (2) L1 norm is better than L2 norm. And we attribute this to the formulation of ucb scores. Because the ucb scores have already taken into account the difference in the visitations (see the $\\\\mathbf{N}(\\\\mathbf{s},\\\\mathbf{a})$ in Eq (1)). Therefore, amplifying the deviations may result in some bias.  \n\nTable 4: Comparison of the winning rate and the average budget with different norm strategies in VETRule. L1 Norm means $\\\\left|\\\\left|\\\\hat{\\\\pi}_{k}(s)-\\\\check{\\\\hat{\\\\pi}}_{k/2}(s)\\\\right|\\\\right|_{1}<\\\\epsilon$ and L2 Norm means $\\\\left|\\\\left|\\\\hat{\\\\pi}_{k}(s)-\\\\hat{\\\\pi}_{k/2}(s)\\\\right|\\\\right|_{2}<\\\\epsilon$ .  \n\n\n<html><body><table><tr><td></td><td>Average budget</td><td>Winningrate</td></tr><tr><td>MCTS (N = 150)</td><td>150</td><td>82.0%</td></tr><tr><td>V-MCTS L1 Norm, N = 150,r = 0.2,E= 0.1</td><td>96.2</td><td>81.5%</td></tr><tr><td>V-MCTS L2 Norm, N = 150,r= 0.2,E= 0.1</td><td>97.1</td><td>79.8%</td></tr><tr><td>V-MCTS L2 Norm, N = 150, r = 0.2, = 0.05</td><td>119.3</td><td>81.0%</td></tr></table></body></html>  \n\nLarger budget $(N)$ in MCTS To investigate whether our method still holds with larger amounts of MCTS expansions, we take a pretrained model and compare two strategies: (1) vanilla expansion with $\\\\mathrm{N{=}150/400/600/800}$ nodes in MCTS (2) virtual expanded policy with $N=800,r=0.2,\\\\epsilon=0.1$ .The results are listed in Tab. 5. The result shows that (1) V-MCTS $\\\\mathit{\\\\Omega}^{N}=800,r=0.2,\\\\epsilon=0.1)$ is better than MCTS ( $N=600)$ ) in both the average budget and the winning rate, (2) V-MCTS can achieve comparable performance to the oracle MCTS( $N=800)$ ) while keeping much less average budget. Therefore, V-MCTS works with a larger amount of MCTS expansions.  \n\nTable 5: Comparison of the winning rate and the average budget with larger amounts of MCTS expansions. Here the hyper-parameters of our method are $N=800,r=0.2,\\\\epsilon=0.1$ .  \n\n\n<html><body><table><tr><td>MCTS</td><td>N = 150</td><td>N = 400</td><td>N=600</td><td>N=800</td><td>Ours</td></tr><tr><td>Average budget Winningrate</td><td>150 82.0%</td><td>400 84.5%</td><td>600 84.9%</td><td>800 85.9%</td><td>431.1 85.0%</td></tr></table></body></html>  \n\n  \nFigure 3: Heatmap of policy distributions from the MCTS ( $N=150)$ ) and the V-MCTS. The agent play as Black in (a) and White in (b) against the GnuGo (level 10). Our agent wins in both of the games. A darker red color represents larger visitations of the corresponding action. The V-MCTS will terminate with different search times $k$ according to the situations and generate a near-oracle policy distribution.\u300d\n", "64b86178-229d-4430-83ef-e052788e3896:\u300cref_ids: 454984236248078902, chunk_ids: 2, Score: 0.2363, Text: # DSensitivity Analysis of Hyper-parameters of MCTS-VS\nWe provide further studies to examine the influence of the hyper-parameters of MCTS-VS, including the employed optimization algorithm for optimizing the selected variables in each iteration, the \u201cfillin\u201d strategy, the hyper-parameter $k$ used in the best$k$ strategy, the hyper-parameter $C_{p}$ for calculating UCB in Eq. (1), the number $2\\\\,\\\\times\\\\,N_{v}\\\\,\\\\times\\\\,N_{s}$ sampled data in each iteration, the threshold $N_{b a d}$ for re-initializing a tree, and the threshold $N_{s p l i t}$ for splitting a tree node.  \n\nThe optimization algorithm is employed by MCTS-VS to optimize the selected variables in each iteration. We compare three different optimization algorithms, i.e., random search (RS), BO and TuRBO. First, we conduct experiments similar to \u201cEffectiveness of Variable Selection\u201d in Section 5.1, to show the effectiveness of MCTS-VS even when equipped with RS. Figure 6 shows that MCTSVS-RS is better than Dropout-RS and RS, revealing the advantage of MCTS-VS.  \n\n  \nFigure 6: Effectiveness of MCTS-VS when equipped with RS.  \n\nNext we compare the performance of MCTS-VS equipped with RS, BO and TuRBO, by experiments on the Hartmann functions with increasing ratio of valid variables. Hartmann 6 _500 has 6 valid variables. Hartmann 6 _5 _500 is generated by mixing 5 Hartmann 6 functions as Hartmann 6 $(\\\\pmb{x}_{1:6})+$ Hartmann 6 $\\\\backslash(\\\\pmb{x}_{7:12})+\\\\cdot\\\\cdot\\\\cdot+\\\\mathrm{Hartmann6}(\\\\pmb{x}_{25:30})$ , and appending 470 unrelated dimensions, where $\\\\pmb{x}_{i:j}$ denotes the $i$ -th to j-th variables. Hartmann 6 _10 _500 is generated alike. Thus, Hartmann 6 _5 _500 and Hartmann 6 _10 _500 have 30 and 60 valid variables, respectively. The results in Figure 7 show that as the ratio of valid variables increases, MCTS-VS-TuRBO gradually surpasses MCTS-VS-RS and MCTS-VS-BO, while MCTS-VS-RS becomes worse and worse. This is expected. If the ratio of valid variables is high, MCTS-VS is more likely to select the valid variables, so it is worth to use the expensive optimization algorithm, e.g., TuRBO, to optimize the selected variables. If the ratio is low, unrelated variables are more likely to be selected most of the time, so using a cheap optimization algorithm would be better. These observations also give us some guidance on selecting optimization algorithms in practice.  \n\n\u201cFill-in\u201d strategy is a basic component of variable selection methods, which influences the quality of the value of unselected variables. We compare the employed best$k$ strategy $(k=20)$ ) with the average best$k$ strategy and the random strategy. The average best$k$ strategy uses the average of the best $k$ data points for the unselected variables, and the random strategy samples the value of an unselected variable from its domain randomly. As shown in Figure 8(a), the random strategy leads to the poor performance of MCTS-VS-BO, which may be because it does not utilize the historical information and leads to over-exploration. The best${\\\\cdot k}$ strategy utilizes the historical points that have high objective values to fill in the unselected variables, thus behaving much better. The performance of the average strategy is between the best$k$ and random strategies. We recommend using the best$k$ strategy in practice.  \n\nThe hyper-parameter $k$ used in the best$k$ strategy controls the degree of exploitation for the unselected variables. As shown in Figure 8(b), a smaller $k$ encourages exploitation, which results in better performance in the early stage, but easily leads to premature convergence. A larger $k$ encourages exploration and behaves worse in the early stage, but may converge to a better value. We recommend using a larger $k$ if allowing enough evaluations.  \n\n  \nFigure 7: Sensitivity analysis of the optimization algorithm.  \n\n  \nFigure 8: Sensitivity analysis of the \u201cfill-in\u201d strategy and the hyper-parameter $k$ of the best$k$ strategy, using MCTS-VS-BO on Hartmann 6 _300 .  \n\nThe hyper-parameter $C_{p}$ for calculating UCB in Eq. (1) balances the exploration and exploitation of MCTS. As shown in Figure 9, a too small $C_{p}$ leads to relatively worse performance, highlighting the importance of exploration. A too large $C_{p}$ may also lead to over-exploration. But overall MCTSVS is not very sensitive to $C_{p}$ . We recommend setting $C_{p}$ between $1\\\\%$ and $10\\\\%$ of the optimum (i.e., max $f({\\\\boldsymbol{x}}))$ ), which is consistent with that for LA-MCTS [40].  \n\n  \nFigure 9: Sensitivity analysis of the hyper-parameter $C_{p}$ for calculating UCB in Eq. (1), using MCTS-VS-BO on Levy and Hartmann.  \n\nThe number $2\\\\,\\\\times\\\\,N_{v}\\\\,\\\\times\\\\,N_{s}$ of sampled data in ch iteration depends on the batch size $N_{v}$ of variable index subset and the sample batch size $N_{s}$ , and will influence the accuracy of estimating the variable score vector in Eq. (2). If we increase $N_{v}$ and $N_{s}$ , we can calculate the variable score more accurately, but also need more evaluations. Figure 10(a) shows that given the same number of evaluations, MCTS-VS-BO achieves the best performance when $N_{v}=2$ and $N_{s}=3$ . Thus, this setting may be a good choice to balance the accuracy of variable score and the number of evaluations, which is also used throughout the experiments.  \n\nThe threshold $N_{b a d}$ for re-initializing a tree controls the tolerance of selecting bad tree nodes (i.e., nodes containing unimportant variables). A smaller $N_{b a d}$ leads to frequent re-initialization, which can adjust quickly but may cause under-exploitation of the tree. A larger $N_{b a d}$ can make full use of the tree, but may optimize too much on unimportant variables. Figure 10(b) shows that MCTS-VS achieves the best performance when $N_{b a d}=5$ . Thus, we recommend to use this setting, to balance the re-initialization and exploitation of the tree.  \n\nThe threshold $N_{s p l i t}$ for splitting a node. If the number of variables in a node is larger than $N_{s p l i t}$ ,the node can be further partitioned. That is, the parameter $N_{s p l i t}$ controls the least number of variables in a leaf node and thus affects the number of selected variables, which has a direct influence on the wall clock time. Note that MCTS-VS selects a leaf node and optimizes the variables contained by this node in each iteration. The smaller $N_{s p l i t}$ , the shorter the time. Figure 10(c) shows that $N_{s p l i t}$ has little influence on the performance of MCTS-VS-BO, and thus we recommend to set $N_{s p l i t}=3$ to reduce the wall clock time.  \n\n  \nFigure 10: S vity analysis of the number $2\\\\,\\\\times\\\\,N_{v}\\\\,\\\\times\\\\,N_{s}$ pled data in each iteration, the threshold $N_{b a d}$ for re-initializing a tree and the threshold $N_{s p l i t}$ for splitting a node, using MCTS-VS-BO on Hartmann 6 _300 .  \n\nInfluence of the hyper-parameters on the runtime of MCTS-VS. We also provide some intuitive explanation about the influence of the hyper-parameters on the runtime. The threshold $N_{s p l i t}$ for splitting a node has a direct impact on the runtime, because it controls the least number of variables to be optimized in a leaf node. That is, the runtime will increase with $N_{s p l i t}$ . Other parameters may affect the depth of the tree and thus the runtime. For the threshold $N_{b a d}$ for re-initializing a tree, if it is set to a small value, MCTS-VS will re-build the tree frequently and the depth of the tree is small. The shallow nodes have more variables, leading to more runtime to optimize. For the hyper-parameter $C_{p}$ for calculating UCB, if it is set to a large value, the exploration is preferred and MCTS-VS will tend to select the right node (regarded as containing unimportant variables). The tree thus will be re-built freq tly, ding to more runtime. For the number $2\\\\,\\\\times\\\\,N_{v}\\\\,\\\\times\\\\,N_{s}$ of sampled data at each iteration, if $N_{v}$ and $N_{s}$ are set to large values, the depth of the tree will be small given the total number of evaluations, and thus lead to more runtime.\u300d\n"]}}, "__type__": "1"}}, "template_store/metadata": {"d226089a-fb9f-4433-bde5-2ae2e9f8b2ba": {"template_hash": ""}}}