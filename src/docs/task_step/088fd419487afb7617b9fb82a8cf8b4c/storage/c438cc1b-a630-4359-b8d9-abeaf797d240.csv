角色,内容,分镜
c438cc1b-a630-4359-b8d9-abeaf797d240,稳定性,4>1
c438cc1b-a630-4359-b8d9-abeaf797d240,"### 问题提出

在MCTS与PRM结合的应用中，如何通过改进MCTS的选择策略和PRM的参数调整机制来进一步提升算法的稳定性？具体来说，有哪些具体的技术手段或方法可以用于优化MCTS的选择策略和PRM的参数调整，以确保在动态和复杂环境下，模型能够保持较高的稳定性和鲁棒性？此外，这些改进措施在实际应用中（如游戏AI、路径规划或推荐系统）的效果如何，是否存在潜在的局限性或挑战？",4>1
c438cc1b-a630-4359-b8d9-abeaf797d240,"ref_ids: 455026805323333778, chunk_ids: 1, Score: 0.3457, Text: # 2.2 Acceleration of MCTS
MCTS-based methods have proved their strong capability of solving complex games or tasks. However, the high computational cost of MCTS hinders its application to some real-time and more general scenarios. Therefore, numerous works are devoted to accelerating MCTS. For example, to make the selection stage more effective, some heuristic pruning methods [ 14 ,33 ,29 ,1 ,2 ] aim to reduce the width and depth of the search tree with some heuristic functions. Furthermore, for more efficient evaluations, Lorentz [ 22 ] proposed early playout termination of MCTS (MCTS-EPT) to stop the random playouts early and use an evaluation function to assess win or loss. Moreover, Hsueh et al. [18 ] applied MCTS-EPT to the Chinese dark chess and proved its effectiveness. Afterward, similar ideas have been applied in the evaluation stage of AlphaGoZero [ 32 ] and later MCTS-based methods [31 ,27 ,34 ]. They evaluate the $Q$ -values through a learnable evaluation network instead of running playouts to the end. Grill et al. [15 ] propose a novel regularized policy optimization method based on AlphaZero to decrease the search budget of MCTS, which is from the optimization perspective. Danihelka et al. [10 ] propose a policy improvement algorithm based on sampling actions without replacement, named Gumbel trick to achieve better performance when planning with few simulations. However, these methods mentioned above focus on the specific stage of the search iteration or reduce the total budget through pruning and optimization methods, which are orthogonal to us. And few works targets at the search loop. Lan et al. [21 ] propose DS-MCTS, which defines the uncertainty of MCTS and approximates it by extra DNNs with specific features for board games in training. During the evaluation, DS-MCTS will check periodically and stop the search if the state is certain.

# 3 Background
The AlphaGo series of work [ 30 ,32 ,31 ,27 ] are all MCTS-based reinforcement learning algorithms. Those algorithms assume the environment transition dynamics are known or learn the environment dynamics. Based on the dynamics, they use the Monte-Carlo tree search (MCTS) as the policy improvement operator. I.e., taking in the current policy, MCTS returns a better policy with the search algorithm. The systematic search allows the MCTS-based RL algorithm to quickly improve the policy and perform much better in the setting where heavy reasoning is required.

# 3.1 MCTS
This part briefly introduces the MCTS method implemented in reinforcement learning applications. As mentioned in the related works, modern MCTS-based RL algorithms include four stages in the search loop, namely selection, expansion, evaluation, and backpropagation.  

MCTS takes in the current states and generates a policy after the search loop of $N$ iterations. Here $N$ is a constant number of iterations set by the designer, regarded as the total budget. In the selection stage of each iteration, an action will be selected by maximizing over UCB. Specifically, AlphaZero [31 ] and MuZero [ 27 ] are developed based on a variant of UCB, P-UCT [ 25 ] and have achieved great success on board games and Atari games. The formula of P-UCT is the Eq (1):  

$$
a^{k}=\\arg\\operatorname*{max}_{a\\in\\mathcal{A}}Q(s,a)+P(s,a)\\frac{\\sqrt{\\sum_{b\\in\\mathcal{A}}N(s,b)}}{1+N(s,a)}(c_{1}+\\log((\\sum_{b\\in\\mathcal{A}}N(s,b)+c_{2}+1)/c_{2})),
$$  

where $k$ is the index of iteration, $\\boldsymbol{\\mathcal{A}}$ is the acti $Q(s,a)$ is the estimated Q-value, $P(s,a)$ is the policy prior obtained from neural networks, $N(s,a)$ is the visitations to select the action a from the state $s$ and $c_{1},c_{2}$ are hyper-parameters. The output of MCTS is the visitation of each action of the root node. After $N$ search iterations, the final policy $\\pi(s)$ is defined as the normalized root visitation distribution simplification, we use $\\pi_{N}(s)$ $\\pi_{k}$ in place of , where $\\begin{array}{r}{\\pi_{k}(s,a)=N(s,a)/\\sum_{b\\in\\mathcal{A}}N(s,b)=N(s,a)/k,a\\in\\mathcal{A}}\\end{array}$ $\\pi_{k}(s)$ sometimes. And the detailed procedure of MCTS is ∈A . For introduced in Appendix. In our method, we propose to approximate the final policy $\\pi_{N}(s)$ with $\\hat{\\pi}_{k}(s)$ ,which we name as a virtual expanded policy, through a new expansion method and a termination rule. In this way, the number of iterations in MCTS can be reduced from $N$ to $k$ .

# 3.2 Computation Requirement
Most of the computations in MCTS-based RL are in the MCTS procedure. Each action taken by MCTS requires $N$ times neural network evaluations, where $N$ is a constant number of iterations in the search loop. Traditional RL algorithms, such as PPO [ 28 ] or DQN [ 23 ], only need a single neural network evaluation per action. Thus, MCTS-based RL is roughly $N$ times computationally more expensive than traditional RL algorithms. In practice, training a single Atari game needs 12 hours of computation time on 40 TPUs [ 27 ]. The computation need is roughly two orders of magnitude more than traditional RL algorithms [28], although the final performance of MuZero is much better.

# 4 Method
We aim to spend more search time on harder states and less on easier states. Intuitively, human knows when to make a quick decision or a slow decision under different circumstances. Unfortunately, this situation-aware behavior is absent in current MCTS algorithms. Therefore, we propose an MCTS variant that terminates the search iteration adaptively. It consists of two components: a novel expansion method named virtual expansion to estimate the final visitation based on the current partial tree; a termination rule that decides when to terminate based on the hardness of the current scenario. And we will display the adaptive mechanism through visualizations in Section 5.5.",4>1
c438cc1b-a630-4359-b8d9-abeaf797d240,"ref_ids: 454984236379937352, chunk_ids: 11, Score: 0.2988, Text: # 5 Experiment
To examine the performance of MCTS-VS, we conduct experiments on different tasks, including synthetic functions, NAS-bench problems and MuJoCo locomotion tasks, to compare MCTS-VS with other black-box optimization methods. For MCTS-VS, we use the same hyper-parameters except $C_{p}$ , which is used for calculating UCB in Eq. (1). For Dropout and embedding-based methods, we set the parameter $d$ to the number of valid dimensions for synthetic functions, and a reasonable value for real-world problems. The hyper-parameters of the same components of different methods are set to the same. We use five identical random seeds (2021–2025) for all problems and methods. More details about the settings can be found in Appendix C. Our code is available at https://github.com/lamda-bbo/MCTS-VS .",4>1
c438cc1b-a630-4359-b8d9-abeaf797d240,"ref_ids: 454984278060050946, chunk_ids: 3, Score: 0.2734, Text: # DSensitivity Analysis of Hyper-parameters of MCTS-VS
We provide further studies to examine the influence of the hyper-parameters of MCTS-VS, including the employed optimization algorithm for optimizing the selected variables in each iteration, the “fillin” strategy, the hyper-parameter $k$ used in the best$k$ strategy, the hyper-parameter $C_{p}$ for calculating UCB in Eq. (1), the number $2\\,\\times\\,N_{v}\\,\\times\\,N_{s}$ sampled data in each iteration, the threshold $N_{b a d}$ for re-initializing a tree, and the threshold $N_{s p l i t}$ for splitting a tree node.  

The optimization algorithm is employed by MCTS-VS to optimize the selected variables in each iteration. We compare three different optimization algorithms, i.e., random search (RS), BO and TuRBO. First, we conduct experiments similar to “Effectiveness of Variable Selection” in Section 5.1, to show the effectiveness of MCTS-VS even when equipped with RS. Figure 6 shows that MCTSVS-RS is better than Dropout-RS and RS, revealing the advantage of MCTS-VS.  

  
Figure 6: Effectiveness of MCTS-VS when equipped with RS.  

Next we compare the performance of MCTS-VS equipped with RS, BO and TuRBO, by experiments on the Hartmann functions with increasing ratio of valid variables. Hartmann 6 _500 has 6 valid variables. Hartmann 6 _5 _500 is generated by mixing 5 Hartmann 6 functions as Hartmann 6 $(\\pmb{x}_{1:6})+$ Hartmann 6 $\\backslash(\\pmb{x}_{7:12})+\\cdot\\cdot\\cdot+\\mathrm{Hartmann6}(\\pmb{x}_{25:30})$ , and appending 470 unrelated dimensions, where $\\pmb{x}_{i:j}$ denotes the $i$ -th to j-th variables. Hartmann 6 _10 _500 is generated alike. Thus, Hartmann 6 _5 _500 and Hartmann 6 _10 _500 have 30 and 60 valid variables, respectively. The results in Figure 7 show that as the ratio of valid variables increases, MCTS-VS-TuRBO gradually surpasses MCTS-VS-RS and MCTS-VS-BO, while MCTS-VS-RS becomes worse and worse. This is expected. If the ratio of valid variables is high, MCTS-VS is more likely to select the valid variables, so it is worth to use the expensive optimization algorithm, e.g., TuRBO, to optimize the selected variables. If the ratio is low, unrelated variables are more likely to be selected most of the time, so using a cheap optimization algorithm would be better. These observations also give us some guidance on selecting optimization algorithms in practice.  

“Fill-in” strategy is a basic component of variable selection methods, which influences the quality of the value of unselected variables. We compare the employed best$k$ strategy $(k=20)$ ) with the average best$k$ strategy and the random strategy. The average best$k$ strategy uses the average of the best $k$ data points for the unselected variables, and the random strategy samples the value of an unselected variable from its domain randomly. As shown in Figure 8(a), the random strategy leads to the poor performance of MCTS-VS-BO, which may be because it does not utilize the historical information and leads to over-exploration. The best${\\cdot k}$ strategy utilizes the historical points that have high objective values to fill in the unselected variables, thus behaving much better. The performance of the average strategy is between the best$k$ and random strategies. We recommend using the best$k$ strategy in practice.  

The hyper-parameter $k$ used in the best$k$ strategy controls the degree of exploitation for the unselected variables. As shown in Figure 8(b), a smaller $k$ encourages exploitation, which results in better performance in the early stage, but easily leads to premature convergence. A larger $k$ encourages exploration and behaves worse in the early stage, but may converge to a better value. We recommend using a larger $k$ if allowing enough evaluations.  

  
Figure 7: Sensitivity analysis of the optimization algorithm.  

  
Figure 8: Sensitivity analysis of the “fill-in” strategy and the hyper-parameter $k$ of the best$k$ strategy, using MCTS-VS-BO on Hartmann 6 _300 .  

The hyper-parameter $C_{p}$ for calculating UCB in Eq. (1) balances the exploration and exploitation of MCTS. As shown in Figure 9, a too small $C_{p}$ leads to relatively worse performance, highlighting the importance of exploration. A too large $C_{p}$ may also lead to over-exploration. But overall MCTSVS is not very sensitive to $C_{p}$ . We recommend setting $C_{p}$ between $1\\%$ and $10\\%$ of the optimum (i.e., max $f({\\boldsymbol{x}}))$ ), which is consistent with that for LA-MCTS [40].  

  
Figure 9: Sensitivity analysis of the hyper-parameter $C_{p}$ for calculating UCB in Eq. (1), using MCTS-VS-BO on Levy and Hartmann.  

The number $2\\,\\times\\,N_{v}\\,\\times\\,N_{s}$ of sampled data in ch iteration depends on the batch size $N_{v}$ of variable index subset and the sample batch size $N_{s}$ , and will influence the accuracy of estimating the variable score vector in Eq. (2). If we increase $N_{v}$ and $N_{s}$ , we can calculate the variable score more accurately, but also need more evaluations. Figure 10(a) shows that given the same number of evaluations, MCTS-VS-BO achieves the best performance when $N_{v}=2$ and $N_{s}=3$ . Thus, this setting may be a good choice to balance the accuracy of variable score and the number of evaluations, which is also used throughout the experiments.  

The threshold $N_{b a d}$ for re-initializing a tree controls the tolerance of selecting bad tree nodes (i.e., nodes containing unimportant variables). A smaller $N_{b a d}$ leads to frequent re-initialization, which can adjust quickly but may cause under-exploitation of the tree. A larger $N_{b a d}$ can make full use of the tree, but may optimize too much on unimportant variables. Figure 10(b) shows that MCTS-VS achieves the best performance when $N_{b a d}=5$ . Thus, we recommend to use this setting, to balance the re-initialization and exploitation of the tree.  

The threshold $N_{s p l i t}$ for splitting a node. If the number of variables in a node is larger than $N_{s p l i t}$ ,the node can be further partitioned. That is, the parameter $N_{s p l i t}$ controls the least number of variables in a leaf node and thus affects the number of selected variables, which has a direct influence on the wall clock time. Note that MCTS-VS selects a leaf node and optimizes the variables contained by this node in each iteration. The smaller $N_{s p l i t}$ , the shorter the time. Figure 10(c) shows that $N_{s p l i t}$ has little influence on the performance of MCTS-VS-BO, and thus we recommend to set $N_{s p l i t}=3$ to reduce the wall clock time.  

  
Figure 10: S vity analysis of the number $2\\,\\times\\,N_{v}\\,\\times\\,N_{s}$ pled data in each iteration, the threshold $N_{b a d}$ for re-initializing a tree and the threshold $N_{s p l i t}$ for splitting a node, using MCTS-VS-BO on Hartmann 6 _300 .  

Influence of the hyper-parameters on the runtime of MCTS-VS. We also provide some intuitive explanation about the influence of the hyper-parameters on the runtime. The threshold $N_{s p l i t}$ for splitting a node has a direct impact on the runtime, because it controls the least number of variables to be optimized in a leaf node. That is, the runtime will increase with $N_{s p l i t}$ . Other parameters may affect the depth of the tree and thus the runtime. For the threshold $N_{b a d}$ for re-initializing a tree, if it is set to a small value, MCTS-VS will re-build the tree frequently and the depth of the tree is small. The shallow nodes have more variables, leading to more runtime to optimize. For the hyper-parameter $C_{p}$ for calculating UCB, if it is set to a large value, the exploration is preferred and MCTS-VS will tend to select the right node (regarded as containing unimportant variables). The tree thus will be re-built freq tly, ding to more runtime. For the number $2\\,\\times\\,N_{v}\\,\\times\\,N_{s}$ of sampled data at each iteration, if $N_{v}$ and $N_{s}$ are set to large values, the depth of the tree will be small given the total number of evaluations, and thus lead to more runtime.",4>1
