角色,内容,分镜
9cfb30a6-9a28-45f1-a55b-2f78b05406c7,评估学术界的技术进步与局限性,2
9cfb30a6-9a28-45f1-a55b-2f78b05406c7,将 MCTS 应用于 PRM 偏好策略模型微调在提升路径搜索效率和准确性方面有一定进步，但存在模型性能提升瓶颈，那么如何具体解决 MCTS 计算复杂度高、PRM 采样策略覆盖不足以及模型对环境数据依赖强这些局限性问题呢？ ,2
9cfb30a6-9a28-45f1-a55b-2f78b05406c7,"ref_ids: 454984236281633338, chunk_ids: 4, Score: 0.3320, Text: # Monte Carlo Tree Search in the Presence of Transition Uncertainty
Farnaz Kohankhaki , Kiarash Aghakasiri , Hongming Zhang 1 , Ting-Han Wei 1 , Chao Gao 2 ,Martin M¨uller 1  

1 University of Alberta, 2 Edmonton Research Center, Huawei Canada {kohankha, aghakasi, hongmin2, tinghan, mmueller }@ualberta.ca, cgao3 $@$ outlook.com

# Abstract
Monte Carlo Tree Search (MCTS) is an immensely popular search-based framework used for decision making. It is traditionally applied to domains where a perfect simulation model of the environment is available. We study and improve MCTS in the context where the environment model is given but imperfect. We show that the discrepancy between the model and the actual environment can lead to significant performance degradation with standard MCTS. We therefore develop Uncertainty Adapted MCTS (UA-MCTS), a more robust algorithm within the MCTS framework. We estimate the transition uncertainty in the given model, and direct the search towards more certain transitions in the state space. We modify all four MCTS phases to improve the search behavior by considering these estimates. We prove, in the corrupted bandit case, that adding uncertainty information to adapt UCB leads to tighter regret bound than standard UCB. Empirically, we evaluate UA-MCTS and its individual components on the deterministic domains from the MinAtar test suite. Our results demonstrate that UA-MCTS strongly improves MCTS in the presence of model transition errors.

# 1 Introduction
The Monte Carlo Tree Search (MCTS) framework (Browne et al. 2012) approaches sequential decision-making problems by selective lookahead search. It manages the balance of exploration and exploitation with techniques such as UCT (Kocsis, Szepesv´ari, and Willemson 2006). Often combined with machine learning, it has been enormously successful in both games (Silver et al. 2016; Banerjee 2020; Arneson, Hayward, and Henderson 2010; Saffidine 2008; Nijssen and Winands 2010) and non-game applications (Lu et al. 2016; Mansley, Weinstein, and Littman 2011; Sabharwal, Samulowitz, and Reddy 2012; Cazenave 2010). In these applications, a perfect simulation model allows for efficient lookahead search. However, in many practical applications, only an imperfect model is available to the agent. Yet lookahead using such a model can still be useful. We improve MCTS for this setting.  

One research area that studies imperfect models of the environment is model-based reinforcement learning (MBRL).  

Here, an agent builds its own model through limited real world interactions. The resulting learned model, when used for lookahead search, can either be for planning or for producing more accurate training targets (Silver, Sutton, and M¨uller 2008). It can also be used to generate simulated training samples for better sample efficiency (Sutton and Barto 2018). The learned model may be inaccurate for many reasons, including stochasticity of the environment, insufficient training, insufficient capacity, non stationary environments, etc. Consequently, there is a rich body of research on uncertainty in MBRL (Abbas et al. 2020; Xiao et al. 2019; Buckman et al. 2018).  

While previous approaches to using search with imperfect models exist (Vemula et al. 2020; Vemula, Bagnell, and Likhachev 2021), to the best of our knowledge, there is no prior work that directly adapts MCTS to deal with model uncertainty. In our work, we define transition uncertainty as a measure of difference between the state transitions in the perfect model and in the model that is available to the agent. We use a neural network to estimate this uncertainty.  

Our Uncertainty Adapted MCTS (UA-MCTS) approach implements the main components of the MCTS framework in a way that guides the search away from states with high uncertainty. We compare the performance of our proposed methods with MCTS baselines in three deterministic MinAtar environments (Young and Tian 2019). In each case the search agent “believes” it is playing the real game. However, the rules of the game itself have changed, and the agent only learns about this change slowly when it acts in the real environment. The results show that UA-MCTS is able to outperform the baseline MCTS with an imperfect model.  

Our approach is inspired by the work of (Vemula et al. 2020) where a robotic arm has to solve tasks despite being handicapped, e.g. by a broken motor or by an unmodeled weight restriction. To show how an agent should adapt UCB-based exploration strategy in the presence of environment uncertainties, we first consider a case of stochastic bandits (Lattimore and Szepesv´ari 2020) along with corrupted feedback. We prove that incorporating uncertainty information can enhance the performance of UCB, yielding a regret bound that is more constrained compared to the standard UCB. We also prove that in the general case of tree search, with similar modification of UCT, our UA-MCTS approach maintains its completeness property, ensuring that as the number of iterations goes to infinity, all nodes will be consistently explored. To further motivate our approach, we compare the scenarios of learning to improve the transition function, using MCTS, directly against the easier task of just learning a transition uncertainty function with UA-MCTS. In both cases, learning occurs online; the former is used with MCTS while the latter is used with UA-MCTS. Our results show that learning the transition function is much harder than learning transition uncertainty, which justifies the use of UA-MCTS in such settings.",2
9cfb30a6-9a28-45f1-a55b-2f78b05406c7,"ref_ids: 455026805323333778, chunk_ids: 1, Score: 0.2266, Text: # Monte Carlo Tree Search based Variable Selection for High Dimensional Bayesian Optimization
Lei Song , Ke Xue , Xiaobin Huang, Chao Qian †State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing 210023, China {songl, xuek, huangxb, qianc}@lamda.nju.edu.cn

# Abstract
Bayesian optimization (BO) is a class of popular methods for expensive black-box optimization, and has been widely applied to many scenarios. However, BO suffers from the curse of dimensionality, and scaling it to high-dimensional problems is still a challenge. In this paper, we propose a variable selection method MCTS-VS based on Monte Carlo tree search (MCTS), to iteratively select and optimize a subset of variables. That is, MCTS-VS constructs a low-dimensional subspace via MCTS and optimizes in the subspace with any BO algorithm. We give a theoretical analysis of the general variable selection method to reveal how it can work. Experiments on high-dimensional synthetic functions and real-world problems (i.e., NAS-bench problems and MuJoCo locomotion tasks) show that MCTS-VS equipped with a proper BO optimizer can achieve state-of-the-art performance.

# 1 Introduction
In many real-world tasks such as neural architecture search (NAS) [ 41 ] and policy search in reinforcement learning (RL) [ 6 ], one often needs to solve the expensive black-box optimization problems. Bayesian optimization (BO) [ 2 ,11 ,23 ,32 ] is a sample-efficient algorithm for solving such problems. It iteratively fits a surrogate model, typically Gaussian process (GP), and maximizes an acquisition function to obtain the next point to evaluate. While BO has been employed in a wide variety of settings, successful applications are often limited to low-dimensional problems.  

Recently, scaling BO to high-dimensional problems has received a lot of interest. Decompositionbased methods [ 13 ,15 ,17 ,26 ,31 ] assume that the high-dimensional function to be optimized has a certain structure, typically the additive structure. By decomposing the original high-dimensional function into the sum of several low-dimensional functions, they optimize each low-dimensional function to obtain the point in the high-dimensional space. However, it is not easy to decide whether a decomposition exists as well as to learn the decomposition.  

Other methods often assume that the original high-dimensional function with dimension $D$ has a low-dimensional subspace with dimension $d\\ll D$ , and then perform the optimization in the low-dimensional subspace and project the low-dimensional point back for evaluation. For example, embedding-based methods [ 20 ,27 ,42 ] use a random matrix to embed the original space into the lowdimensional subspace. Another way is to select a subset of variables directly, which can even avoid the time-consuming matrix operations of embedding-based methods. For example, Dropout [ 21 ]selects $d$ variables randomly in each iteration. Note that for both embedding and variable selection methods, the parameter $d$ can have a large influence on the performance, which is, however, difficult to set in real-world problems.  

In this paper, we propose a new Variable Selection method using Monte Carlo Tree Search (MCTS), called MCTS-VS. MCTS is employed to partition the variables into important and unimportant ones, and only those selected important variables are optimized via any black-box optimization algorithm, e.g., vanilla BO [ 32 ] or TuRBO [ 10 ]. The values of unimportant variables are sampled using historical information. Compared with Dropout-BO, MCTS-VS can select important variables automatically.  

We also provide regret and computational complexity analyses of general variable selection methods, showing that variable selection can reduce the computational complexity while increasing the cumulative regret. Our regret bound generalizes that of GP-UCB [ 38 ] which always selects all variables, as well as that of Dropout [ 21 ] which selects $d$ variables randomly in each iteration. The results suggest that a good variable selection method should select as important variables as possible.  

Experiments on high-dimensional synthetic functions and real-world problems (i.e., NAS and RL problems) show that MCTS-VS is better than the previous variable selection method Dropout [ 21 ], and can also achieve the competitive performance to state-of-the-art BO algorithms. Furthermore, its running time is small due to the advantage of variable selection. We also observe that MCTS-VS can select important variables, explaining its good performance based on our theoretical analysis.

# 2 Background

# 2.1 Bayesian Optimization
We consider the problem $\\operatorname*{max}_{\\pmb{x}\\in\\mathcal{X}}f(\\pmb{x})$ , where $f$ is a black-box function and $\\mathcal{X}\\subseteq\\mathbb{R}^{D}$ is the domain. The basic framework of BO contains two critical components: a surrogate model and an acquisition functi ost p te model. Given the sampled data point $\\{(\\mathbfit{x^{i}},\\dot{y}^{i})\\}_{i=1}^{t-1}$ ,$f\\sim\\mathcal{G P}(\\mu(\\cdot),k(\\cdot,\\cdot)+\\eta^{2}\\mathbf{I})$ ∼GP $y^{i}=f(\\pmb{x}^{i})+\\epsilon^{i}$ ···$\\epsilon^{i}\\sim\\mathcal{N}(0,\\eta^{2})$ , specified by the mean ∼N is the obs $\\mu(\\cdot)$ ·tion noise, GP at iterati and covariance kernel $k(\\cdot,\\cdot)$ $t$ ··eeks to i , where I is er the identity matrix of size D. After that, an acquisition function, e.g., Probability of Improvement (PI) [ 19 ], Expected Improvement (EI) [ 16 ] or Upper Confidence Bound (UCB) [ 38 ], is used to determine the next query point $\\pmb{x}^{t}$ while balancing exploitation and exploration.",2
9cfb30a6-9a28-45f1-a55b-2f78b05406c7,"ref_ids: 454984236379937352, chunk_ids: 11, Score: 0.2227, Text: # 2.2 Acceleration of MCTS
MCTS-based methods have proved their strong capability of solving complex games or tasks. However, the high computational cost of MCTS hinders its application to some real-time and more general scenarios. Therefore, numerous works are devoted to accelerating MCTS. For example, to make the selection stage more effective, some heuristic pruning methods [ 14 ,33 ,29 ,1 ,2 ] aim to reduce the width and depth of the search tree with some heuristic functions. Furthermore, for more efficient evaluations, Lorentz [ 22 ] proposed early playout termination of MCTS (MCTS-EPT) to stop the random playouts early and use an evaluation function to assess win or loss. Moreover, Hsueh et al. [18 ] applied MCTS-EPT to the Chinese dark chess and proved its effectiveness. Afterward, similar ideas have been applied in the evaluation stage of AlphaGoZero [ 32 ] and later MCTS-based methods [31 ,27 ,34 ]. They evaluate the $Q$ -values through a learnable evaluation network instead of running playouts to the end. Grill et al. [15 ] propose a novel regularized policy optimization method based on AlphaZero to decrease the search budget of MCTS, which is from the optimization perspective. Danihelka et al. [10 ] propose a policy improvement algorithm based on sampling actions without replacement, named Gumbel trick to achieve better performance when planning with few simulations. However, these methods mentioned above focus on the specific stage of the search iteration or reduce the total budget through pruning and optimization methods, which are orthogonal to us. And few works targets at the search loop. Lan et al. [21 ] propose DS-MCTS, which defines the uncertainty of MCTS and approximates it by extra DNNs with specific features for board games in training. During the evaluation, DS-MCTS will check periodically and stop the search if the state is certain.

# 3 Background
The AlphaGo series of work [ 30 ,32 ,31 ,27 ] are all MCTS-based reinforcement learning algorithms. Those algorithms assume the environment transition dynamics are known or learn the environment dynamics. Based on the dynamics, they use the Monte-Carlo tree search (MCTS) as the policy improvement operator. I.e., taking in the current policy, MCTS returns a better policy with the search algorithm. The systematic search allows the MCTS-based RL algorithm to quickly improve the policy and perform much better in the setting where heavy reasoning is required.

# 3.1 MCTS
This part briefly introduces the MCTS method implemented in reinforcement learning applications. As mentioned in the related works, modern MCTS-based RL algorithms include four stages in the search loop, namely selection, expansion, evaluation, and backpropagation.  

MCTS takes in the current states and generates a policy after the search loop of $N$ iterations. Here $N$ is a constant number of iterations set by the designer, regarded as the total budget. In the selection stage of each iteration, an action will be selected by maximizing over UCB. Specifically, AlphaZero [31 ] and MuZero [ 27 ] are developed based on a variant of UCB, P-UCT [ 25 ] and have achieved great success on board games and Atari games. The formula of P-UCT is the Eq (1):  

$$
a^{k}=\\arg\\operatorname*{max}_{a\\in\\mathcal{A}}Q(s,a)+P(s,a)\\frac{\\sqrt{\\sum_{b\\in\\mathcal{A}}N(s,b)}}{1+N(s,a)}(c_{1}+\\log((\\sum_{b\\in\\mathcal{A}}N(s,b)+c_{2}+1)/c_{2})),
$$  

where $k$ is the index of iteration, $\\boldsymbol{\\mathcal{A}}$ is the acti $Q(s,a)$ is the estimated Q-value, $P(s,a)$ is the policy prior obtained from neural networks, $N(s,a)$ is the visitations to select the action a from the state $s$ and $c_{1},c_{2}$ are hyper-parameters. The output of MCTS is the visitation of each action of the root node. After $N$ search iterations, the final policy $\\pi(s)$ is defined as the normalized root visitation distribution simplification, we use $\\pi_{N}(s)$ $\\pi_{k}$ in place of , where $\\begin{array}{r}{\\pi_{k}(s,a)=N(s,a)/\\sum_{b\\in\\mathcal{A}}N(s,b)=N(s,a)/k,a\\in\\mathcal{A}}\\end{array}$ $\\pi_{k}(s)$ sometimes. And the detailed procedure of MCTS is ∈A . For introduced in Appendix. In our method, we propose to approximate the final policy $\\pi_{N}(s)$ with $\\hat{\\pi}_{k}(s)$ ,which we name as a virtual expanded policy, through a new expansion method and a termination rule. In this way, the number of iterations in MCTS can be reduced from $N$ to $k$ .

# 3.2 Computation Requirement
Most of the computations in MCTS-based RL are in the MCTS procedure. Each action taken by MCTS requires $N$ times neural network evaluations, where $N$ is a constant number of iterations in the search loop. Traditional RL algorithms, such as PPO [ 28 ] or DQN [ 23 ], only need a single neural network evaluation per action. Thus, MCTS-based RL is roughly $N$ times computationally more expensive than traditional RL algorithms. In practice, training a single Atari game needs 12 hours of computation time on 40 TPUs [ 27 ]. The computation need is roughly two orders of magnitude more than traditional RL algorithms [28], although the final performance of MuZero is much better.

# 4 Method
We aim to spend more search time on harder states and less on easier states. Intuitively, human knows when to make a quick decision or a slow decision under different circumstances. Unfortunately, this situation-aware behavior is absent in current MCTS algorithms. Therefore, we propose an MCTS variant that terminates the search iteration adaptively. It consists of two components: a novel expansion method named virtual expansion to estimate the final visitation based on the current partial tree; a termination rule that decides when to terminate based on the hardness of the current scenario. And we will display the adaptive mechanism through visualizations in Section 5.5.",2
