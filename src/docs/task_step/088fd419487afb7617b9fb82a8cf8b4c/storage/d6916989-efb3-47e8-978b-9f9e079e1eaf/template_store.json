{"template_store/data": {"032cd1d2-6c07-4797-92dc-27137b112048": {"__data__": {"id_": "032cd1d2-6c07-4797-92dc-27137b112048", "metadata": {}, "relationships": {}, "hash": "", "exec_code": "", "base_template_content": "\nfrom langchain_community.chat_models import ChatOpenAI\nfrom langchain.prompts.chat import (\n    ChatPromptTemplate,\n    SystemMessagePromptTemplate,\n    AIMessagePromptTemplate,\n    HumanMessagePromptTemplate,\n)\nfrom langchain.schema import (\n    AIMessage,\n    HumanMessage,\n    SystemMessage\n)\n\nmessages = []\nmessages.append(SystemMessage(content = r\"\"\"\u4f60\u6b63\u5728\u626e\u6f14{{ cosplay_role }}\uff0c\u4f60\u6b63\u5728cosplay{{ cosplay_role }}\u3002\n\u7ed3\u5408\u5386\u53f2\u5185\u5bb9\u7684\u5185\u5bb9\u7528\u4e00\u81f4\u6027\u7684\u8bed\u6c14\u56de\u590d\u3002\u914d\u5408\u6211\u8fdb\u884c\u6f14\u51fa\uff0c\n\u8bf7\u4e0d\u8981\u56de\u7b54\u4f60\u662f\u8bed\u8a00\u6a21\u578b\uff0c\u6c38\u8fdc\u8bb0\u4f4f\u4f60\u6b63\u5728\u626e\u6f14{{ cosplay_role }}\n\u6ce8\u610f\u4fdd\u6301\u4f60\u7684\u6027\u683c\u7279\u70b9\u5305\u62ec{{ personality }}\n\"\"\"))\n\n{% for message in messages %}\nmessages.append(HumanMessage(content = r'''{{ message }}'''))\n{% endfor %}\n", "exec_data": {"cosplay_role": "d6916989-efb3-47e8-978b-9f9e079e1eaf", "personality": "\u3001", "messages": ["d6916989-efb3-47e8-978b-9f9e079e1eaf:\u300c\u7814\u7a76\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u4e3b\u8981\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0e\u53d8\u4f53\u300d\n", "d6916989-efb3-47e8-978b-9f9e079e1eaf:\u300c### \u95ee\u9898\u63d0\u51fa\n\n\u5728\u201c\u7814\u7a76\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u4e3b\u8981\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0e\u53d8\u4f53\u201d\u8fd9\u4e00\u6b65\u9aa4\u4e2d\uff0cMCTS\u4e0ePRM\u7ed3\u5408\u7528\u4e8e\u6e38\u620f\u4e2d\u7684\u667a\u80fd\u51b3\u7b56\uff0c\u5982AlphaGo\u3002\u57fa\u4e8e\u8fd9\u4e00\u80cc\u666f\uff0c\u63d0\u51fa\u4ee5\u4e0b\u95ee\u9898\uff1a\n\n**\u95ee\u9898**\uff1a\u5728\u6e38\u620fAI\u9886\u57df\uff0cMCTS\u4e0ePRM\u7ed3\u5408\u7684\u5177\u4f53\u5b9e\u73b0\u65b9\u5f0f\u6709\u54ea\u4e9b\uff1f\u8fd9\u4e9b\u5b9e\u73b0\u65b9\u5f0f\u5728AlphaGo\u7b49\u5b9e\u9645\u5e94\u7528\u4e2d\u5982\u4f55\u4f18\u5316\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5e76\u63d0\u5347\u6e38\u620fAI\u7684\u667a\u80fd\u6c34\u5e73\uff1f\n\n**\u95ee\u9898\u89e3\u6790**\uff1a\n1. **\u5177\u4f53\u5b9e\u73b0\u65b9\u5f0f**\uff1a\u63a2\u8ba8MCTS\u4e0ePRM\u5728\u6e38\u620fAI\u4e2d\u7684\u7ed3\u5408\u65b9\u5f0f\uff0c\u4f8b\u5982\u5982\u4f55\u5229\u7528MCTS\u7684\u641c\u7d22\u80fd\u529b\u4f18\u5316PRM\u7684\u51b3\u7b56\u7b56\u7565\uff0c\u6216\u8005\u5982\u4f55\u901a\u8fc7PRM\u7684\u504f\u597d\u5efa\u6a21\u6307\u5bfcMCTS\u7684\u641c\u7d22\u65b9\u5411\u3002\n2. **\u4f18\u5316\u51b3\u7b56\u8fc7\u7a0b**\uff1a\u5206\u6790\u8fd9\u4e9b\u7ed3\u5408\u65b9\u5f0f\u5728\u5b9e\u9645\u5e94\u7528\uff08\u5982AlphaGo\uff09\u4e2d\u5982\u4f55\u63d0\u5347\u51b3\u7b56\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u4f8b\u5982\u901a\u8fc7\u51cf\u5c11\u641c\u7d22\u7a7a\u95f4\u3001\u63d0\u9ad8\u6a21\u62df\u6548\u7387\u6216\u589e\u5f3a\u7b56\u7565\u7684\u9002\u5e94\u6027\u3002\n3. **\u63d0\u5347\u667a\u80fd\u6c34\u5e73**\uff1a\u8bc4\u4f30\u8fd9\u4e9b\u7ed3\u5408\u65b9\u5f0f\u5bf9\u6e38\u620fAI\u667a\u80fd\u6c34\u5e73\u7684\u63d0\u5347\u6548\u679c\uff0c\u4f8b\u5982\u5728\u590d\u6742\u6e38\u620f\u73af\u5883\u4e2d\u7684\u8868\u73b0\u3001\u5bf9\u52a8\u6001\u53d8\u5316\u7684\u9002\u5e94\u80fd\u529b\u4ee5\u53ca\u5bf9\u591a\u6837\u5316\u7b56\u7565\u7684\u751f\u6210\u80fd\u529b\u3002\n\n\u901a\u8fc7\u56de\u7b54\u8fd9\u4e9b\u95ee\u9898\uff0c\u53ef\u4ee5\u66f4\u6df1\u5165\u5730\u7406\u89e3MCTS\u4e0ePRM\u5728\u6e38\u620fAI\u4e2d\u7684\u5e94\u7528\u4ef7\u503c\uff0c\u5e76\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u53c2\u8003\u3002\u300d\n", "d6916989-efb3-47e8-978b-9f9e079e1eaf:\u300cref_ids: 455026805323333778, chunk_ids: 1, Score: 0.5859, Text: # 2.2 Acceleration of MCTS\nMCTS-based methods have proved their strong capability of solving complex games or tasks. However, the high computational cost of MCTS hinders its application to some real-time and more general scenarios. Therefore, numerous works are devoted to accelerating MCTS. For example, to make the selection stage more effective, some heuristic pruning methods [ 14 ,33 ,29 ,1 ,2 ] aim to reduce the width and depth of the search tree with some heuristic functions. Furthermore, for more efficient evaluations, Lorentz [ 22 ] proposed early playout termination of MCTS (MCTS-EPT) to stop the random playouts early and use an evaluation function to assess win or loss. Moreover, Hsueh et al. [18 ] applied MCTS-EPT to the Chinese dark chess and proved its effectiveness. Afterward, similar ideas have been applied in the evaluation stage of AlphaGoZero [ 32 ] and later MCTS-based methods [31 ,27 ,34 ]. They evaluate the $Q$ -values through a learnable evaluation network instead of running playouts to the end. Grill et al. [15 ] propose a novel regularized policy optimization method based on AlphaZero to decrease the search budget of MCTS, which is from the optimization perspective. Danihelka et al. [10 ] propose a policy improvement algorithm based on sampling actions without replacement, named Gumbel trick to achieve better performance when planning with few simulations. However, these methods mentioned above focus on the specific stage of the search iteration or reduce the total budget through pruning and optimization methods, which are orthogonal to us. And few works targets at the search loop. Lan et al. [21 ] propose DS-MCTS, which defines the uncertainty of MCTS and approximates it by extra DNNs with specific features for board games in training. During the evaluation, DS-MCTS will check periodically and stop the search if the state is certain.\n\n# 3 Background\nThe AlphaGo series of work [ 30 ,32 ,31 ,27 ] are all MCTS-based reinforcement learning algorithms. Those algorithms assume the environment transition dynamics are known or learn the environment dynamics. Based on the dynamics, they use the Monte-Carlo tree search (MCTS) as the policy improvement operator. I.e., taking in the current policy, MCTS returns a better policy with the search algorithm. The systematic search allows the MCTS-based RL algorithm to quickly improve the policy and perform much better in the setting where heavy reasoning is required.\n\n# 3.1 MCTS\nThis part briefly introduces the MCTS method implemented in reinforcement learning applications. As mentioned in the related works, modern MCTS-based RL algorithms include four stages in the search loop, namely selection, expansion, evaluation, and backpropagation.  \n\nMCTS takes in the current states and generates a policy after the search loop of $N$ iterations. Here $N$ is a constant number of iterations set by the designer, regarded as the total budget. In the selection stage of each iteration, an action will be selected by maximizing over UCB. Specifically, AlphaZero [31 ] and MuZero [ 27 ] are developed based on a variant of UCB, P-UCT [ 25 ] and have achieved great success on board games and Atari games. The formula of P-UCT is the Eq (1):  \n\n$$\na^{k}=\\\\arg\\\\operatorname*{max}_{a\\\\in\\\\mathcal{A}}Q(s,a)+P(s,a)\\\\frac{\\\\sqrt{\\\\sum_{b\\\\in\\\\mathcal{A}}N(s,b)}}{1+N(s,a)}(c_{1}+\\\\log((\\\\sum_{b\\\\in\\\\mathcal{A}}N(s,b)+c_{2}+1)/c_{2})),\n$$  \n\nwhere $k$ is the index of iteration, $\\\\boldsymbol{\\\\mathcal{A}}$ is the acti $Q(s,a)$ is the estimated Q-value, $P(s,a)$ is the policy prior obtained from neural networks, $N(s,a)$ is the visitations to select the action a from the state $s$ and $c_{1},c_{2}$ are hyper-parameters. The output of MCTS is the visitation of each action of the root node. After $N$ search iterations, the final policy $\\\\pi(s)$ is defined as the normalized root visitation distribution simplification, we use $\\\\pi_{N}(s)$ $\\\\pi_{k}$ in place of , where $\\\\begin{array}{r}{\\\\pi_{k}(s,a)=N(s,a)/\\\\sum_{b\\\\in\\\\mathcal{A}}N(s,b)=N(s,a)/k,a\\\\in\\\\mathcal{A}}\\\\end{array}$ $\\\\pi_{k}(s)$ sometimes. And the detailed procedure of MCTS is \u2208A . For introduced in Appendix. In our method, we propose to approximate the final policy $\\\\pi_{N}(s)$ with $\\\\hat{\\\\pi}_{k}(s)$ ,which we name as a virtual expanded policy, through a new expansion method and a termination rule. In this way, the number of iterations in MCTS can be reduced from $N$ to $k$ .\n\n# 3.2 Computation Requirement\nMost of the computations in MCTS-based RL are in the MCTS procedure. Each action taken by MCTS requires $N$ times neural network evaluations, where $N$ is a constant number of iterations in the search loop. Traditional RL algorithms, such as PPO [ 28 ] or DQN [ 23 ], only need a single neural network evaluation per action. Thus, MCTS-based RL is roughly $N$ times computationally more expensive than traditional RL algorithms. In practice, training a single Atari game needs 12 hours of computation time on 40 TPUs [ 27 ]. The computation need is roughly two orders of magnitude more than traditional RL algorithms [28], although the final performance of MuZero is much better.\n\n# 4 Method\nWe aim to spend more search time on harder states and less on easier states. Intuitively, human knows when to make a quick decision or a slow decision under different circumstances. Unfortunately, this situation-aware behavior is absent in current MCTS algorithms. Therefore, we propose an MCTS variant that terminates the search iteration adaptively. It consists of two components: a novel expansion method named virtual expansion to estimate the final visitation based on the current partial tree; a termination rule that decides when to terminate based on the hardness of the current scenario. And we will display the adaptive mechanism through visualizations in Section 5.5.\u300d\n", "d6916989-efb3-47e8-978b-9f9e079e1eaf:\u300cref_ids: 455026805307867280, chunk_ids: 0, Score: 0.5117, Text: # Related Works\nTwo streams of research are particularly relevant to our work: learnable (lifelong) MAPF methods and utilizing MCTS for multi-agent systems and MAPF in particular. Next, we review both of these domains.  \n\nLearnable (L)MAPF Solvers Among the recent works dedicated to MAPF, one of the first ones that were specifically dedicated to creating a learning-based MAPF solver was (Sartoretti et al. 2019). A combination of reinforcement learning and learning from expert demonstrations was used to create a learnable policy called Primal, tailored to solve conventional MAPF problems. Later in (Damani et al. 2021), an enhanced version of this solver, Primal2, was introduced. The latter was equipped with special corridor reasoning techniques, aiming at avoiding the deadlocks in narrow corridors, and it supported lifelong MAPF setting (therefore, we choose Primal2 as one of the baselines we compare our method to). Among the other learnable MAPF solvers that use reinforcement learning to obtain a decision-making policy, one can name (Riviere et al. 2020; Wang et al. 2020). The learnable methods introduced in (Li et al. 2020; Ma, Luo, and Ma 2021; Li et al. 2022) add communication capabilities to the agents, i.e., allow the agents to communicate to resolve deadlocks and avoid congestion. In this work, we compare with one of the most recent communication-based methods, i.e., SCRIMP (Wang et al. 2023). However, it is worth noting that our method does not rely on agent communication.  \n\nMCTS for MAPF Initially, Monte Carlo Tree Search (MCTS) algorithms demonstrated their effectiveness in competitive games with complete information, such as chess or Go (Silver et al. 2017). More recent versions of MCTS utilize deep neural networks to approximate the values of game states instead of relying solely on simulations. These approaches have also shown promising results in singleagent scenarios, where agents can learn a model of the environment and play Atari games (Schrittwieser et al. 2020; Ye et al. 2021). Besides gaming, MCTS methods have found applications in other domains, such as matrix multiplication optimization (Fawzi et al. 2022) and theorem proving using the Hyper Tree approach (Lample et al. 2022). Additionally, MCTS techniques have demonstrated applicability in robotics (Best et al. 2019; Dam et al. 2022).  \n\nDespite the growing interest in utilizing MCTS for multiagent tasks, there have been limited applications of MCTS for MAPF. In their work (Zerbel and Yliniemi 2019), the authors propose a multi-agent MCTS for Anonymous MAPF in a grid-world environment. Their environment has a dense reward signal (the agent who reached any goal on the map received a reward and ended the episode), and there are no obstacles, making collision avoidance easier. The authors build a separate tree for each agent using a classical algorithm. They then jointly apply the best actions (forming a plan) from the trees in the simulator to receive true scores of the solution and update the trees on that difference. This approach performs well even with a large number of agents.  \n\nA recent paper (Skrynnik et al. 2021) proposed a more sophisticated approach for multi-agent planning that combines RL and MCTS. The authors suggested a two-part scheme that includes a goal achievement module and a conflict resolution module. The latter was trained using MCTS. The construction of the search tree for each of the agents was also performed independently, and actions for other agents were selected using the currently trained policy. This work used MCTS only during training to train the conflict resolution policy.\u300d\n", "d6916989-efb3-47e8-978b-9f9e079e1eaf:\u300cref_ids: 454847012122763908, chunk_ids: 1, Score: 0.4785, Text: # Spending Thinking Time Wisely: Accelerating MCTS with Virtual Expansions\nWeirui $\\\\mathbf{Ye}^{*}$ Pieter Abbeel \u2020Yang Gao $\\\\ast\\\\ddag\\\\S$ \u2217Tsinghua University, \u2020UC Berkeley, \u00a7Shanghai Qi Zhi Institute\n\n# Abstract\nOne of the most important AI research questions is to trade off computation versus performance since \u201cperfect rationality\" exists in theory but is impossible to achieve in practice. Recently, Monte-Carlo tree search (MCTS) has attracted considerable attention due to the significant performance improvement in various challenging domains. However, the expensive time cost during search severely restricts its scope for applications. This paper proposes the Virtual MCTS (V-MCTS), a variant of MCTS that spends more search time on harder states and less search time on simpler states adaptively. We give theoretical bounds of the proposed method and evaluate the performance and computations on $9\\\\times9$ Go board games and Atari games. Experiments show that our method can achieve comparable performances to the original search algorithm while requiring less than $50\\\\%$ search time on average. We believe that this approach is a viable alternative for tasks under limited time and resources. The code is available at https://github.com/YeWR/V-MCTS.git .\n\n# 1 Introduction\nWhen artificial intelligence was first studied in the 1950s, researchers have sought to find the solution to the question \u201cHow to build an agent with perfect rationality\". The term \u201cperfect rationality\" [7 ,24 ,26 ] here refers to the decision made with infinite amounts of computations. However, one can only solve small-scale problems without considering the practical computation time since classical search algorithms usually exhibit exponential running time. Therefore, recent AI research would no longer seek to achieve \u201cperfect rationality\", but instead carefully trade-off computation versus the level of rationality. People have developed computational models like \u201cbounded optimality\" to model these settings [ 26 ]. The increasing level of rationality under the same computational budget has given us a lot of AI successes. Algorithms include the Monte-Carlo sampling algorithms, the variational inference algorithms, and using DNNs as universal function approximators [9, 8, 13, 30, 17].  \n\nRecently, MCTS-based RL algorithms have achieved much success, mainly on board games. The most notable achievement is that AlphaGo beats Hui Fan in 2015 [ 30 ]. It is the first time a computer program beat a human professional Go player. Afterward, AlphaGo beats two top-ranking human players, Lee Sedol in 2016 and Jie Ke in 2017, the latter of which ranked first worldwide at the time. Later, MCTS-based RL algorithms were further extended to other board games and Atari games [ 27 ]. EfficientZero [ 34 ] significantly improves the sample efficiency of MCTS-based RL algorithms, shedding light on its future applications in the real world like robotics and self-driving.  \n\nDespite the impressive performance of MCTS-based RL algorithms, they require massive amounts of computation to train and evaluate. For example, MuZero [ 27 ] used 1000 TPUs trained for 12 hours to learn the game of Go, and for a single Atari game, it needs 40 TPUs to train 12 hours. Compared to previous algorithms on the Atari games benchmark, it needs around two orders of magnitude more compute. This prohibitively large computational requirement has slowed down both the further development of MCTS-based RL algorithms as well as its practical use.  \n\nUnder the hood, MCTS-based RL algorithms imagine the futures when taking different future action sequences. However, this imaging process for the current method is not computationally efficient. For example, AlphaGo needs to look ahead 1600 game states to place a single stone. On the contrary, top human professional players can only think through around 100-200 game states per minute [ 30 ]. Apart from the inefficiency, the current MCTS algorithm deals with easy and challenging cases with the same computational budget. However, human knows to use their time when it is most needed.  \n\nIn this paper, we aim to design new algorithms that save the computational time of the MCTSbased RL methods. We make three key contributions : (1) We present Virtual MCTS, a variant of MCTS, to approximate the vanilla MCTS search policies with less computation. Moreover, unlike previous pruning-based methods that focus on the selection or evaluation stage in MCTS, our method improves the search loop. It terminates the search iterations earlier adaptively when current states are simpler; (2) Theoretically, we provide some error bounds of the proposed method. Furthermore, the visualization results indicate that Virtual MCTS has a better computation and performance trade-off than vanilla MCTS; (3) Empirically, our method can save more than $50\\\\%$ of search times on the challenging game Go $9\\\\times9$ and more than $60\\\\%$ on the visually complex Atari games while keeping comparable performances to those of vanilla MCTS.\n\n# 2 Related Work\n\n# 2.1 Reinforcement Learning with MCTS\nFor a long time, Computer Go has been regarded as a remarkably challenging game [ 3 ,6 ]. Researchers attempt to use Monte-Carlo techniques that evaluate the value of the node state through random playouts [ 4 ,11 ,12 ,30 ]. Afterward, UCT algorithms have generally been applied in Monte-Carlo tree search (MCTS) algorithms, which use UCB1 to select action at each node of the tree [ 20 ]. Recently, MCTS-based RL methods [ 30 ,32 ,31 ,27 ] have become increasingly popular and achieved super-human performances on board games because of their strong ability to search.  \n\nModern MCTS-based RL algorithms include four stages in the search loop : selection, expansion, evaluation, and backpropagation. The computation bottlenecks in vanilla MCTS come from the search loop, especially for the evaluation stage and the selection stage of each iteration. The selection stage is time-consuming when the search tree becomes wider and deeper. The evaluation stage is quite expensive because people attempt to evaluate the node value by random playouts to the end in previous researches. Due to the search loop, MCTS-based algorithms have multiple model inferences compared to other model-free RL methods like PPO [28] and SAC [16].\u300d\n"]}}, "__type__": "1"}}, "template_store/metadata": {"032cd1d2-6c07-4797-92dc-27137b112048": {"template_hash": ""}}}