{"task_step_store/data": {"49abb75e-21b0-4504-8500-80c6e0bd9a07": {"__data__": {"id_": "49abb75e-21b0-4504-8500-80c6e0bd9a07", "metadata": {}, "relationships": {}, "hash": "", "start_task_context": "MCTS\u5728PRM\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u7684\u5e94\u7528\u63a2\u7d22\u7efc\u8ff0", "aemo_representation_context": "### 1. \u5206\u6790\u8fd1\u51e0\u5e74\u7814\u7a76\u9886\u57df\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba\n\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u76f8\u5173\u7814\u7a76\u9886\u57df\uff0c\u9488\u5bf9\u8def\u5f84\u641c\u7d22\u4e0e\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u65b9\u9762\uff0c\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09\u4ee5\u53ca\u6982\u7387\u8def\u7ebf\u56fe\uff08PRM\uff09\u65b9\u6cd5\u53d7\u5230\u5173\u6ce8\u3002\u8fd1\u51e0\u5e74\u7684\u7814\u7a76\u56f4\u7ed5\u5982\u4f55\u5c06 MCTS \u4e0e PRM \u76f8\u7ed3\u5408\uff0c\u63a2\u7d22\u5728\u4e0d\u540c\u4efb\u52a1\u573a\u666f\u4e0b\u5bf9\u504f\u597d\u7b56\u7565\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u7684\u6709\u6548\u6280\u672f\u6846\u67b6\u3002\u65b9\u6cd5\u8bba\u4e0a\uff0c\u591a\u901a\u8fc7\u5b9e\u9a8c\u5bf9\u6bd4\u4e0d\u540c\u53c2\u6570\u8bbe\u7f6e\u548c\u6539\u8fdb\u7b56\u7565\u4e0b\u6a21\u578b\u7684\u6027\u80fd\u8868\u73b0\uff0c\u4ee5\u6b64\u6765\u4f18\u5316\u6280\u672f\u6846\u67b6\u3002\n\n### 2. \u7814\u7a76\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u4e3b\u8981\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0e\u53d8\u4f53\n - **\u4e3b\u8981\u6846\u67b6**\uff1aMCTS \u548c PRM \u662f\u6838\u5fc3\u6846\u67b6\u3002\n - **\u5e94\u7528**\uff1a\u5728\u8def\u5f84\u641c\u7d22\u4efb\u52a1\u4e2d\uff0cPRM \u7528\u4e8e\u6784\u5efa\u73af\u5883\u7684\u6982\u7387\u8def\u7ebf\u56fe\uff0c\u4e3a\u8def\u5f84\u641c\u7d22\u63d0\u4f9b\u57fa\u7840\u7ed3\u6784\uff1bMCTS \u5219\u57fa\u4e8e PRM \u751f\u6210\u7684\u56fe\u8fdb\u884c\u641c\u7d22\u7b56\u7565\u7684\u4f18\u5316\uff0c\u901a\u8fc7\u4e0d\u65ad\u6a21\u62df\u548c\u9009\u62e9\u8def\u5f84\u6765\u627e\u5230\u6700\u4f18\u6216\u8fd1\u4f3c\u6700\u4f18\u8def\u5f84\u3002\u5728\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u4efb\u52a1\u4e2d\uff0c\u5229\u7528 MCTS \u7684\u641c\u7d22\u80fd\u529b\u6765\u63a2\u7d22\u4e0d\u540c\u7684\u7b56\u7565\u7a7a\u95f4\uff0c\u4ee5\u627e\u5230\u80fd\u4f7f\u504f\u597d\u7b56\u7565\u6a21\u578b\u6027\u80fd\u63d0\u5347\u7684\u5fae\u8c03\u53c2\u6570\u3002\n - **\u53d8\u4f53**\uff1a\u4e00\u4e9b\u7814\u7a76\u5bf9 MCTS \u7684\u9009\u62e9\u3001\u6269\u5c55\u3001\u6a21\u62df\u548c\u53cd\u5411\u4f20\u64ad\u6b65\u9aa4\u8fdb\u884c\u6539\u8fdb\uff0c\u4f8b\u5982\u91c7\u7528\u4e0d\u540c\u7684\u9009\u62e9\u7b56\u7565\uff08\u5982 UCT \u53d8\u4f53\uff09\u6765\u63d0\u9ad8\u641c\u7d22\u6548\u7387\uff1b\u5bf9 PRM \u8fdb\u884c\u6539\u8fdb\uff0c\u5982\u8c03\u6574\u91c7\u6837\u7b56\u7565\u4ee5\u66f4\u597d\u5730\u9002\u5e94\u590d\u6742\u73af\u5883\uff0c\u4ece\u800c\u5f62\u6210\u9002\u7528\u4e8e\u4e0d\u540c\u4efb\u52a1\u7684\u53d8\u4f53\u6846\u67b6\u3002\n\n### 3. \u8bc4\u4f30\u5b66\u672f\u754c\u7684\u6280\u672f\u8fdb\u6b65\u4e0e\u5c40\u9650\u6027\n - **\u6280\u672f\u8fdb\u6b65**\uff1a\u901a\u8fc7\u5c06 MCTS \u5e94\u7528\u4e8e PRM \u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\uff0c\u5728\u8def\u5f84\u641c\u7d22\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u4e0a\u6709\u4e00\u5b9a\u63d0\u5347\u3002\u80fd\u591f\u5728\u590d\u6742\u73af\u5883\u4e2d\u66f4\u5feb\u5730\u627e\u5230\u53ef\u884c\u8def\u5f84\uff0c\u5e76\u4e14\u504f\u597d\u7b56\u7565\u6a21\u578b\u7ecf\u8fc7\u5fae\u8c03\u540e\u80fd\u66f4\u597d\u5730\u7b26\u5408\u7279\u5b9a\u4efb\u52a1\u9700\u6c42\u3002\n - **\u5c40\u9650\u6027**\uff1a\u6a21\u578b\u6027\u80fd\u63d0\u5347\u5b58\u5728\u74f6\u9888\u3002\u4e00\u65b9\u9762\uff0cMCTS \u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u8f83\u9ad8\uff0c\u5728\u5927\u89c4\u6a21\u73af\u5883\u6216\u590d\u6742\u4efb\u52a1\u4e2d\u641c\u7d22\u6548\u7387\u53d7\u9650\uff1b\u53e6\u4e00\u65b9\u9762\uff0cPRM \u7684\u91c7\u6837\u7b56\u7565\u53ef\u80fd\u65e0\u6cd5\u5b8c\u5168\u8986\u76d6\u6240\u6709\u6709\u6548\u8def\u5f84\uff0c\u5bfc\u81f4\u641c\u7d22\u7ed3\u679c\u5b58\u5728\u504f\u5dee\u3002\u540c\u65f6\uff0c\u6a21\u578b\u5bf9\u73af\u5883\u6570\u636e\u7684\u4f9d\u8d56\u8f83\u5f3a\uff0c\u5f53\u73af\u5883\u53d1\u751f\u53d8\u5316\u65f6\uff0c\u6a21\u578b\u7684\u9002\u5e94\u6027\u8f83\u5dee\uff0c\u5b58\u5728\u6a21\u578b\u504f\u5dee\u95ee\u9898\u3002\n\n### 4. \u63a2\u8ba8\u8ba1\u7b97\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\n - **\u9002\u7528\u6027**\uff1a\u5728\u7ed3\u6784\u5316\u7a0b\u5ea6\u8f83\u9ad8\u3001\u73af\u5883\u76f8\u5bf9\u7a33\u5b9a\u7684\u6570\u636e\u96c6\u548c\u5e94\u7528\u573a\u666f\u4e2d\uff0c\u57fa\u4e8e MCTS \u548c PRM \u7684\u6a21\u578b\u8868\u73b0\u8f83\u597d\uff0c\u80fd\u591f\u6709\u6548\u5730\u5b8c\u6210\u8def\u5f84\u641c\u7d22\u548c\u504f\u597d\u7b56\u7565\u5fae\u8c03\u4efb\u52a1\u3002\u4f8b\u5982\u5728\u7b80\u5355\u7684\u673a\u5668\u4eba\u8def\u5f84\u89c4\u5212\u573a\u666f\u4e2d\uff0c\u6a21\u578b\u53ef\u4ee5\u5feb\u901f\u751f\u6210\u53ef\u884c\u8def\u5f84\u3002\n - **\u6cdb\u5316\u80fd\u529b**\uff1a\u7136\u800c\uff0c\u5728\u591a\u9886\u57df\u3001\u591a\u6a21\u6001\u6570\u636e\u7684\u73b0\u5b9e\u5e94\u7528\u573a\u666f\u4e2d\uff0c\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002\u4f8b\u5982\u5728\u5305\u542b\u591a\u79cd\u4f20\u611f\u5668\u6570\u636e\uff08\u56fe\u50cf\u3001\u6fc0\u5149\u96f7\u8fbe\u7b49\uff09\u7684\u590d\u6742\u73af\u5883\u4e0b\uff0c\u6a21\u578b\u96be\u4ee5\u878d\u5408\u4e0d\u540c\u6a21\u6001\u7684\u6570\u636e\u8fdb\u884c\u6709\u6548\u51b3\u7b56\uff0c\u5bf9\u4e8e\u4e0d\u540c\u9886\u57df\u7684\u4efb\u52a1\uff08\u5982\u7269\u6d41\u8def\u5f84\u89c4\u5212\u548c\u533b\u7597\u5f71\u50cf\u5206\u6790\u4e2d\u7684\u8def\u5f84\u5f15\u5bfc\uff09\uff0c\u6a21\u578b\u9700\u8981\u5927\u91cf\u7684\u91cd\u65b0\u8bad\u7ec3\u548c\u8c03\u6574\u624d\u80fd\u9002\u5e94\u3002\n\n### 5. \u5206\u6790\u6700\u65b0\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027\n - **\u7a33\u5b9a\u6027**\uff1a\u90e8\u5206\u9488\u5bf9 MCTS \u7a33\u5b9a\u6027\u7684\u4f18\u5316\u63aa\u65bd\u88ab\u63d0\u51fa\uff0c\u4f8b\u5982\u6539\u8fdb\u641c\u7d22\u6811\u7684\u6784\u5efa\u548c\u66f4\u65b0\u7b56\u7565\uff0c\u4f7f\u5f97\u7b97\u6cd5\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u80fd\u591f\u5728\u590d\u6742\u73af\u5883\u4e0b\u4fdd\u6301\u76f8\u5bf9\u7a33\u5b9a\u7684\u6027\u80fd\u3002\u4f46\u5728\u52a8\u6001\u73af\u5883\u4e2d\uff0c\u7531\u4e8e\u73af\u5883\u7684\u5b9e\u65f6\u53d8\u5316\uff0c\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u641c\u7d22\u7ed3\u679c\u53ef\u80fd\u51fa\u73b0\u8f83\u5927\u6ce2\u52a8\u3002\n - **\u5bb9\u9519\u6027**\uff1a\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\uff0c\u7b97\u6cd5\u7684\u5bb9\u9519\u6027\u6709\u9650\u3002\u5f53\u6570\u636e\u5b58\u5728\u566a\u58f0\u6216\u4e0d\u5b8c\u6574\u65f6\uff0cMCTS \u548c PRM \u53ef\u80fd\u4f1a\u751f\u6210\u9519\u8bef\u7684\u8def\u5f84\u6216\u505a\u51fa\u4e0d\u5408\u7406\u7684\u504f\u597d\u7b56\u7565\u8c03\u6574\uff0c\u5f71\u54cd\u6a21\u578b\u7684\u6574\u4f53\u6027\u80fd\u3002\n\n### 6. \u8bc4\u4f30\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218\n - **\u672a\u6765\u7814\u7a76\u65b9\u5411**\uff1a\u63d0\u51fa\u4e86\u63a2\u7d22\u66f4\u9ad8\u6548\u7684 MCTS \u641c\u7d22\u7b97\u6cd5\uff0c\u4ee5\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff1b\u6539\u8fdb PRM \u7684\u91c7\u6837\u7b56\u7565\uff0c\u4f7f\u5176\u80fd\u66f4\u597d\u5730\u9002\u5e94\u590d\u6742\u548c\u52a8\u6001\u73af\u5883\uff1b\u7814\u7a76\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u65b9\u6cd5\uff0c\u63d0\u9ad8\u6a21\u578b\u5728\u591a\u9886\u57df\u3001\u591a\u6a21\u6001\u6570\u636e\u573a\u666f\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u3002\n - **\u6311\u6218**\uff1a\u65b0\u7684\u7814\u7a76\u95ee\u9898\u5305\u62ec\u5982\u4f55\u5728\u4fdd\u8bc1\u7b97\u6cd5\u6548\u7387\u7684\u540c\u65f6\u63d0\u9ad8\u5176\u5728\u590d\u6742\u73af\u5883\u4e0b\u7684\u7a33\u5b9a\u6027\u548c\u5bb9\u9519\u6027\uff1b\u5982\u4f55\u6709\u6548\u878d\u5408\u591a\u6a21\u6001\u6570\u636e\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u3002\u8fd9\u4e9b\u6311\u6218\u63a8\u52a8\u540e\u7eed\u7814\u7a76\u671d\u7740\u4f18\u5316\u7b97\u6cd5\u7ed3\u6784\u3001\u6539\u8fdb\u6570\u636e\u5904\u7406\u65b9\u5f0f\u4ee5\u53ca\u63a2\u7d22\u65b0\u7684\u6a21\u578b\u878d\u5408\u65b9\u6cd5\u7b49\u65b9\u5411\u53d1\u5c55\u3002\n\n### \u7814\u7a76\u6210\u679c\u3001\u65b9\u6cd5\u7684\u521b\u65b0\u6027\u4e0e\u5e94\u7528\u4ef7\u503c\n - **\u7814\u7a76\u6210\u679c**\uff1a\u660e\u786e\u4e86 MCTS \u5728 PRM \u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u4e2d\u7684\u5e94\u7528\u65b9\u5f0f\u53ca\u6548\u679c\uff0c\u5206\u6790\u4e86\u73b0\u6709\u6280\u672f\u7684\u8fdb\u6b65\u4e0e\u5c40\u9650\u3001\u6a21\u578b\u7684\u9002\u7528\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3001\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027\u7b49\u65b9\u9762\u3002\n - **\u65b9\u6cd5\u7684\u521b\u65b0\u6027**\uff1a\u5c06 MCTS \u4e0e PRM \u76f8\u7ed3\u5408\u7528\u4e8e\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\uff0c\u63a2\u7d22\u4e86\u4e0d\u540c\u7684\u6539\u8fdb\u7b56\u7565\u548c\u53d8\u4f53\u6846\u67b6\uff0c\u4e3a\u8def\u5f84\u641c\u7d22\u548c\u504f\u597d\u7b56\u7565\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u548c\u65b9\u6cd5\u3002\n - **\u5e94\u7528\u4ef7\u503c**\uff1a\u5728\u673a\u5668\u4eba\u8def\u5f84\u89c4\u5212\u3001\u7269\u6d41\u8def\u5f84\u89c4\u5212\u7b49\u9886\u57df\u6709\u6f5c\u5728\u5e94\u7528\u4ef7\u503c\uff0c\u901a\u8fc7\u4f18\u5316\u8def\u5f84\u641c\u7d22\u548c\u504f\u597d\u7b56\u7565\uff0c\u80fd\u591f\u63d0\u9ad8\u76f8\u5173\u7cfb\u7edf\u7684\u8fd0\u884c\u6548\u7387\u548c\u51b3\u7b56\u8d28\u91cf\u3002 ", "task_step_name": "\u5206\u6790\u8fd1\u51e0\u5e74\u7814\u7a76\u9886\u57df\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba", "task_step_description": "\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u76f8\u5173\u7814\u7a76\u9886\u57df\uff0c\u9488\u5bf9\u8def\u5f84\u641c\u7d22\u4e0e\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u65b9\u9762\uff0c\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09\u4ee5\u53ca\u6982\u7387\u8def\u7ebf\u56fe\uff08PRM\uff09\u65b9\u6cd5\u53d7\u5230\u5173\u6ce8\u3002\u8fd1\u51e0\u5e74\u7684\u7814\u7a76\u56f4\u7ed5\u5982\u4f55\u5c06 MCTS \u4e0e PRM \u76f8\u7ed3\u5408\uff0c\u63a2\u7d22\u5728\u4e0d\u540c\u4efb\u52a1\u573a\u666f\u4e0b\u5bf9\u504f\u597d\u7b56\u7565\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u7684\u6709\u6548\u6280\u672f\u6846\u67b6\u3002\u65b9\u6cd5\u8bba\u4e0a\uff0c\u591a\u901a\u8fc7\u5b9e\u9a8c\u5bf9\u6bd4\u4e0d\u540c\u53c2\u6570\u8bbe\u7f6e\u548c\u6539\u8fdb\u7b56\u7565\u4e0b\u6a21\u578b\u7684\u6027\u80fd\u8868\u73b0\uff0c\u4ee5\u6b64\u6765\u4f18\u5316\u6280\u672f\u6846\u67b6\u3002", "task_step_level": "0", "task_step_question": "\u8fd1\u51e0\u5e74\u5728\u5c06 MCTS \u4e0e PRM \u76f8\u7ed3\u5408\u63a2\u7d22\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u7684\u6709\u6548\u6280\u672f\u6846\u67b6\u65f6\uff0c\u5177\u4f53\u662f\u5982\u4f55\u901a\u8fc7\u5b9e\u9a8c\u5bf9\u6bd4\u4e0d\u540c\u53c2\u6570\u8bbe\u7f6e\u548c\u6539\u8fdb\u7b56\u7565\u6765\u4f18\u5316\u6280\u672f\u6846\u67b6\u7684\uff1f ", "task_step_question_context": [{"ref_id": "454984236281633338", "chunk_id": "4", "score": 0.37109375, "text": "# 5 Experiment\nTo examine the performance of MCTS-VS, we conduct experiments on different tasks, including synthetic functions, NAS-bench problems and MuJoCo locomotion tasks, to compare MCTS-VS with other black-box optimization methods. For MCTS-VS, we use the same hyper-parameters except $C_{p}$ , which is used for calculating UCB in Eq. (1). For Dropout and embedding-based methods, we set the parameter $d$ to the number of valid dimensions for synthetic functions, and a reasonable value for real-world problems. The hyper-parameters of the same components of different methods are set to the same. We use five identical random seeds (2021\u20132025) for all problems and methods. More details about the settings can be found in Appendix C. Our code is available at https://github.com/lamda-bbo/MCTS-VS ."}, {"ref_id": "454984236293691964", "chunk_id": "5", "score": 0.267578125, "text": "# DSensitivity Analysis of Hyper-parameters of MCTS-VS\nWe provide further studies to examine the influence of the hyper-parameters of MCTS-VS, including the employed optimization algorithm for optimizing the selected variables in each iteration, the \u201cfillin\u201d strategy, the hyper-parameter $k$ used in the best$k$ strategy, the hyper-parameter $C_{p}$ for calculating UCB in Eq. (1), the number $2\\,\\times\\,N_{v}\\,\\times\\,N_{s}$ sampled data in each iteration, the threshold $N_{b a d}$ for re-initializing a tree, and the threshold $N_{s p l i t}$ for splitting a tree node.  \n\nThe optimization algorithm is employed by MCTS-VS to optimize the selected variables in each iteration. We compare three different optimization algorithms, i.e., random search (RS), BO and TuRBO. First, we conduct experiments similar to \u201cEffectiveness of Variable Selection\u201d in Section 5.1, to show the effectiveness of MCTS-VS even when equipped with RS. Figure 6 shows that MCTSVS-RS is better than Dropout-RS and RS, revealing the advantage of MCTS-VS.  \n\n  \nFigure 6: Effectiveness of MCTS-VS when equipped with RS.  \n\nNext we compare the performance of MCTS-VS equipped with RS, BO and TuRBO, by experiments on the Hartmann functions with increasing ratio of valid variables. Hartmann 6 _500 has 6 valid variables. Hartmann 6 _5 _500 is generated by mixing 5 Hartmann 6 functions as Hartmann 6 $(\\pmb{x}_{1:6})+$ Hartmann 6 $\\backslash(\\pmb{x}_{7:12})+\\cdot\\cdot\\cdot+\\mathrm{Hartmann6}(\\pmb{x}_{25:30})$ , and appending 470 unrelated dimensions, where $\\pmb{x}_{i:j}$ denotes the $i$ -th to j-th variables. Hartmann 6 _10 _500 is generated alike. Thus, Hartmann 6 _5 _500 and Hartmann 6 _10 _500 have 30 and 60 valid variables, respectively. The results in Figure 7 show that as the ratio of valid variables increases, MCTS-VS-TuRBO gradually surpasses MCTS-VS-RS and MCTS-VS-BO, while MCTS-VS-RS becomes worse and worse. This is expected. If the ratio of valid variables is high, MCTS-VS is more likely to select the valid variables, so it is worth to use the expensive optimization algorithm, e.g., TuRBO, to optimize the selected variables. If the ratio is low, unrelated variables are more likely to be selected most of the time, so using a cheap optimization algorithm would be better. These observations also give us some guidance on selecting optimization algorithms in practice.  \n\n\u201cFill-in\u201d strategy is a basic component of variable selection methods, which influences the quality of the value of unselected variables. We compare the employed best$k$ strategy $(k=20)$ ) with the average best$k$ strategy and the random strategy. The average best$k$ strategy uses the average of the best $k$ data points for the unselected variables, and the random strategy samples the value of an unselected variable from its domain randomly. As shown in Figure 8(a), the random strategy leads to the poor performance of MCTS-VS-BO, which may be because it does not utilize the historical information and leads to over-exploration. The best${\\cdot k}$ strategy utilizes the historical points that have high objective values to fill in the unselected variables, thus behaving much better. The performance of the average strategy is between the best$k$ and random strategies. We recommend using the best$k$ strategy in practice.  \n\nThe hyper-parameter $k$ used in the best$k$ strategy controls the degree of exploitation for the unselected variables. As shown in Figure 8(b), a smaller $k$ encourages exploitation, which results in better performance in the early stage, but easily leads to premature convergence. A larger $k$ encourages exploration and behaves worse in the early stage, but may converge to a better value. We recommend using a larger $k$ if allowing enough evaluations.  \n\n  \nFigure 7: Sensitivity analysis of the optimization algorithm.  \n\n  \nFigure 8: Sensitivity analysis of the \u201cfill-in\u201d strategy and the hyper-parameter $k$ of the best$k$ strategy, using MCTS-VS-BO on Hartmann 6 _300 .  \n\nThe hyper-parameter $C_{p}$ for calculating UCB in Eq. (1) balances the exploration and exploitation of MCTS. As shown in Figure 9, a too small $C_{p}$ leads to relatively worse performance, highlighting the importance of exploration. A too large $C_{p}$ may also lead to over-exploration. But overall MCTSVS is not very sensitive to $C_{p}$ . We recommend setting $C_{p}$ between $1\\%$ and $10\\%$ of the optimum (i.e., max $f({\\boldsymbol{x}}))$ ), which is consistent with that for LA-MCTS [40].  \n\n  \nFigure 9: Sensitivity analysis of the hyper-parameter $C_{p}$ for calculating UCB in Eq. (1), using MCTS-VS-BO on Levy and Hartmann.  \n\nThe number $2\\,\\times\\,N_{v}\\,\\times\\,N_{s}$ of sampled data in ch iteration depends on the batch size $N_{v}$ of variable index subset and the sample batch size $N_{s}$ , and will influence the accuracy of estimating the variable score vector in Eq. (2). If we increase $N_{v}$ and $N_{s}$ , we can calculate the variable score more accurately, but also need more evaluations. Figure 10(a) shows that given the same number of evaluations, MCTS-VS-BO achieves the best performance when $N_{v}=2$ and $N_{s}=3$ . Thus, this setting may be a good choice to balance the accuracy of variable score and the number of evaluations, which is also used throughout the experiments.  \n\nThe threshold $N_{b a d}$ for re-initializing a tree controls the tolerance of selecting bad tree nodes (i.e., nodes containing unimportant variables). A smaller $N_{b a d}$ leads to frequent re-initialization, which can adjust quickly but may cause under-exploitation of the tree. A larger $N_{b a d}$ can make full use of the tree, but may optimize too much on unimportant variables. Figure 10(b) shows that MCTS-VS achieves the best performance when $N_{b a d}=5$ . Thus, we recommend to use this setting, to balance the re-initialization and exploitation of the tree.  \n\nThe threshold $N_{s p l i t}$ for splitting a node. If the number of variables in a node is larger than $N_{s p l i t}$ ,the node can be further partitioned. That is, the parameter $N_{s p l i t}$ controls the least number of variables in a leaf node and thus affects the number of selected variables, which has a direct influence on the wall clock time. Note that MCTS-VS selects a leaf node and optimizes the variables contained by this node in each iteration. The smaller $N_{s p l i t}$ , the shorter the time. Figure 10(c) shows that $N_{s p l i t}$ has little influence on the performance of MCTS-VS-BO, and thus we recommend to set $N_{s p l i t}=3$ to reduce the wall clock time.  \n\n  \nFigure 10: S vity analysis of the number $2\\,\\times\\,N_{v}\\,\\times\\,N_{s}$ pled data in each iteration, the threshold $N_{b a d}$ for re-initializing a tree and the threshold $N_{s p l i t}$ for splitting a node, using MCTS-VS-BO on Hartmann 6 _300 .  \n\nInfluence of the hyper-parameters on the runtime of MCTS-VS. We also provide some intuitive explanation about the influence of the hyper-parameters on the runtime. The threshold $N_{s p l i t}$ for splitting a node has a direct impact on the runtime, because it controls the least number of variables to be optimized in a leaf node. That is, the runtime will increase with $N_{s p l i t}$ . Other parameters may affect the depth of the tree and thus the runtime. For the threshold $N_{b a d}$ for re-initializing a tree, if it is set to a small value, MCTS-VS will re-build the tree frequently and the depth of the tree is small. The shallow nodes have more variables, leading to more runtime to optimize. For the hyper-parameter $C_{p}$ for calculating UCB, if it is set to a large value, the exploration is preferred and MCTS-VS will tend to select the right node (regarded as containing unimportant variables). The tree thus will be re-built freq tly, ding to more runtime. For the number $2\\,\\times\\,N_{v}\\,\\times\\,N_{s}$ of sampled data at each iteration, if $N_{v}$ and $N_{s}$ are set to large values, the depth of the tree will be small given the total number of evaluations, and thus lead to more runtime."}, {"ref_id": "454845604178965768", "chunk_id": "4", "score": 0.23828125, "text": "# 4.2 The MCTS Framework for Strategy Synthesis\nWe instantiate MCTS for this optimal strategy search problem. We use UCT as the tree policy in the selection phase and rollout randomly in the rollout phase. Notably, in the backup phase, we apply the max-backup rule [Sabharwal et al. , 2012; Sun et al. , 2023]. This approach updates the action values with the best return observed, rather than the average. It encourages more aggressive exploitation towards the bestperforming strategy observed, aligning with our goal. Therefore, in each MCTS simulation, the agent explores and assesses a single strategy, continually updating and retaining the best strategy seen so far. The process continues until a predetermined number of simulations have been run. At the conclusion of this process, the strategy that has achieved the highest reward $R_{T}$ is selected and presented as the synthesized SMT strategy for the specified instance set $P$ .  \n\nFigure 2 illustrates our basic MCTS framework, using a simplified CFG $G^{\\prime}$ for illustrative purposes. $G^{\\prime}$ is defined as $\\mathrm{~S~}\\dot{\\rightarrow}\\mathrm{~\\tiny~T~S~}|$ symbolize variables for strategy and tactic, respectively. smt and $\\mathrm{~S~}\\rightarrow$ Tsimplify |aig , where S and T  \n\nThe primary challenge in synthesizing strategies through MCTS is the extensive time required to evaluate each strategy, which involves calling an SMT solver on all instances in $P$ . This situation leads to a very limited exploration of potential paths, particularly given the immense search space created by the rich strategy language. To address this issue, we first add additional rules restricting valid actions based on domain knowledge. For example, no tactic could be applied sequentially following a solver tactic such as smt . We refer readers to the Appendix for a comprehensive list of such rules. More importantly, we have introduced two heuristic methods, namely layered search and the staged search methods, into conventional MCTS, facilitating a deeper and more effective exploration of the strategy space.\n\n# 4.3 Layered Search\nTo solve the above-mentioned challenge, we propose a layered search method to optimize the tactic parameters within strategy synthesis. As shown in our CFG $G$ , each tactic can be paired with multiple parameters. In traditional MCTS, the selection of each candidate value for a parameter is represented by one production rule in $G$ , and the agent needs to make sequential production-rule decisions to configure all parameters for a given tactic, leading to exponential growth in the problem search tree.  \n\n  \nFigure 3: Comparison of the conventional MCTS and the layersearch in treating tactic parameter tuning  \n\nTo address this issue, our layered search method approaches the tuning of each tactic parameter as a separate Multi-Armed Bandit (MAB) problem [Bubeck et al. , 2012]. In an MAB scenario, an agent repeatedly chooses from multiple actions (arms) with unknown rewards. The major difference between MAB and MDP is that, in MAB, an action does not affect subsequent state transitions. For these MABs, we select actions according to UCB1 [Auer et al. , 2002].  \n\nIn the layered search, we build an individual MAB for each parameter to be tuned, with each arm representing one candidate value. The MABs are associated with a tree edge (tactic) but themselves are not part of the tree. They are engaged to select parameter values when their associated tree edge is traversed, and they are updated based on the episode reward during the Backup phase. However, such MABs do not expand the MCTS search tree after the parameter configuration, since they are separate components from the main search tree. This is in contrast to conventional MCTS, which also employs MAB principles to select among child nodes to explore, where these nodes are part of the search tree.  \n\nThis concept is well explained in Figure 3 with an example based on the simplified grammar $G^{\\prime}$ . Here, the tactic simplify has two Boolean parameters, namely som and flat to be tuned. As shown in Figure 3(a), the search tree is expanded fourfold to accommodate all possible combinations of these two parameters in the conventional MCTS framework. In contrast, in the layered search framework, MABs for the two parameters are isolated from the search tree, creating no additional branches in the tree.  \n\nThe rationale behind the layer search is twofold. Firstly, tactics such as simplify may have dozens of parameters, and it is common for a tactic to be used multiple times within a strategy. Thus, navigating a search space that is fully expanded by all possible parameter combinations becomes impractical, especially given the time-intensive nature of strategy evaluation. Secondly, we argue that parameter tuning, although important, serves more as an auxiliary task in comparison to the tasks of tactic selection and sequencing. By employing the layered search method, we maintain the primary focus on more influential tasks. At the same time, the isolated MABs efficiently optimize the parameters, thus effectively managing these secondary tasks without overwhelming the main search process."}], "task_step_question_answer": "\u8fdb\u4e00\u6b65\u7684\u7814\u7a76\u53ef\u4ee5\u5173\u6ce8\u5982\u4f55\u5c06\u8fd9\u4e00\u4f18\u5316\u540e\u7684\u6280\u672f\u6846\u67b6\u5e94\u7528\u4e8e\u66f4\u5e7f\u6cdb\u7684\u9886\u57df\uff0c\u4f8b\u5982\u667a\u80fd\u5bb6\u5c45\u4e2d\u7684\u673a\u5668\u4eba\u670d\u52a1\u3001\u65e0\u4eba\u673a\u81ea\u4e3b\u98de\u884c\u7b49\u3002\u5728\u667a\u80fd\u5bb6\u5c45\u4e2d\uff0c\u673a\u5668\u4eba\u9700\u8981\u6839\u636e\u52a8\u6001\u53d8\u5316\u7684\u73af\u5883\u8fdb\u884c\u9ad8\u6548\u7684\u8def\u5f84\u89c4\u5212\uff0c\u4ee5\u786e\u4fdd\u670d\u52a1\u7684\u53ca\u65f6\u6027\u548c\u51c6\u786e\u6027\u3002\u65e0\u4eba\u673a\u81ea\u4e3b\u98de\u884c\u5219\u9700\u8981\u5728\u590d\u6742\u7684\u4e09\u7ef4\u7a7a\u95f4\u4e2d\u8fdb\u884c\u5b9e\u65f6\u8def\u5f84\u8c03\u6574\uff0c\u4ee5\u907f\u5f00\u969c\u788d\u7269\u5e76\u4f18\u5316\u98de\u884c\u8def\u5f84\u3002\u6b64\u5916\uff0c\u63a2\u7d22\u4e0e\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u7ed3\u5408\uff0c\u5982\u591a\u673a\u5668\u4eba\u534f\u540c\u4f5c\u4e1a\uff0c\u53ef\u80fd\u4f1a\u5e26\u6765\u65b0\u7684\u7a81\u7834\u3002\u901a\u8fc7\u8bbe\u8ba1\u591a\u667a\u80fd\u4f53\u534f\u540c\u7684\u5b9e\u9a8c\u573a\u666f\uff0c\u9a8c\u8bc1\u6a21\u578b\u5728\u591a\u673a\u5668\u4eba\u534f\u540c\u8def\u5f84\u89c4\u5212\u4e2d\u7684\u6027\u80fd\u8868\u73b0\u3002\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u8fd8\u9700\u8981\u8003\u8651\u7b97\u6cd5\u7684\u5b9e\u65f6\u6027\u548c\u80fd\u8017\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u4e2d\uff0c\u5982\u4f55\u5e73\u8861\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u80fd\u8017\u5c06\u6210\u4e3a\u4e00\u4e2a\u91cd\u8981\u7684\u7814\u7a76\u65b9\u5411\u3002\u901a\u8fc7\u4e0d\u65ad\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u548c\u5b9e\u9645\u5e94\u7528\u6d4b\u8bd5\uff0c\u53ef\u4ee5\u9010\u6b65\u5b8c\u5584\u8fd9\u4e00\u6280\u672f\u6846\u67b6\uff0c\u4f7f\u5176\u5728\u66f4\u591a\u590d\u6742\u573a\u666f\u4e2d\u53d1\u6325\u91cd\u8981\u4f5c\u7528\u3002", "ref_task_step_id": ""}, "__type__": "task_step_node"}, "03a69f17-fbfb-4e1c-93cb-b83ae52d8393": {"__data__": {"id_": "03a69f17-fbfb-4e1c-93cb-b83ae52d8393", "metadata": {}, "relationships": {}, "hash": "", "start_task_context": "MCTS\u5728PRM\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u7684\u5e94\u7528\u63a2\u7d22\u7efc\u8ff0", "aemo_representation_context": "### 1. \u5206\u6790\u8fd1\u51e0\u5e74\u7814\u7a76\u9886\u57df\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba\n\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u76f8\u5173\u7814\u7a76\u9886\u57df\uff0c\u9488\u5bf9\u8def\u5f84\u641c\u7d22\u4e0e\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u65b9\u9762\uff0c\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09\u4ee5\u53ca\u6982\u7387\u8def\u7ebf\u56fe\uff08PRM\uff09\u65b9\u6cd5\u53d7\u5230\u5173\u6ce8\u3002\u8fd1\u51e0\u5e74\u7684\u7814\u7a76\u56f4\u7ed5\u5982\u4f55\u5c06 MCTS \u4e0e PRM \u76f8\u7ed3\u5408\uff0c\u63a2\u7d22\u5728\u4e0d\u540c\u4efb\u52a1\u573a\u666f\u4e0b\u5bf9\u504f\u597d\u7b56\u7565\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u7684\u6709\u6548\u6280\u672f\u6846\u67b6\u3002\u65b9\u6cd5\u8bba\u4e0a\uff0c\u591a\u901a\u8fc7\u5b9e\u9a8c\u5bf9\u6bd4\u4e0d\u540c\u53c2\u6570\u8bbe\u7f6e\u548c\u6539\u8fdb\u7b56\u7565\u4e0b\u6a21\u578b\u7684\u6027\u80fd\u8868\u73b0\uff0c\u4ee5\u6b64\u6765\u4f18\u5316\u6280\u672f\u6846\u67b6\u3002\n\n### 2. \u7814\u7a76\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u4e3b\u8981\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0e\u53d8\u4f53\n - **\u4e3b\u8981\u6846\u67b6**\uff1aMCTS \u548c PRM \u662f\u6838\u5fc3\u6846\u67b6\u3002\n - **\u5e94\u7528**\uff1a\u5728\u8def\u5f84\u641c\u7d22\u4efb\u52a1\u4e2d\uff0cPRM \u7528\u4e8e\u6784\u5efa\u73af\u5883\u7684\u6982\u7387\u8def\u7ebf\u56fe\uff0c\u4e3a\u8def\u5f84\u641c\u7d22\u63d0\u4f9b\u57fa\u7840\u7ed3\u6784\uff1bMCTS \u5219\u57fa\u4e8e PRM \u751f\u6210\u7684\u56fe\u8fdb\u884c\u641c\u7d22\u7b56\u7565\u7684\u4f18\u5316\uff0c\u901a\u8fc7\u4e0d\u65ad\u6a21\u62df\u548c\u9009\u62e9\u8def\u5f84\u6765\u627e\u5230\u6700\u4f18\u6216\u8fd1\u4f3c\u6700\u4f18\u8def\u5f84\u3002\u5728\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u4efb\u52a1\u4e2d\uff0c\u5229\u7528 MCTS \u7684\u641c\u7d22\u80fd\u529b\u6765\u63a2\u7d22\u4e0d\u540c\u7684\u7b56\u7565\u7a7a\u95f4\uff0c\u4ee5\u627e\u5230\u80fd\u4f7f\u504f\u597d\u7b56\u7565\u6a21\u578b\u6027\u80fd\u63d0\u5347\u7684\u5fae\u8c03\u53c2\u6570\u3002\n - **\u53d8\u4f53**\uff1a\u4e00\u4e9b\u7814\u7a76\u5bf9 MCTS \u7684\u9009\u62e9\u3001\u6269\u5c55\u3001\u6a21\u62df\u548c\u53cd\u5411\u4f20\u64ad\u6b65\u9aa4\u8fdb\u884c\u6539\u8fdb\uff0c\u4f8b\u5982\u91c7\u7528\u4e0d\u540c\u7684\u9009\u62e9\u7b56\u7565\uff08\u5982 UCT \u53d8\u4f53\uff09\u6765\u63d0\u9ad8\u641c\u7d22\u6548\u7387\uff1b\u5bf9 PRM \u8fdb\u884c\u6539\u8fdb\uff0c\u5982\u8c03\u6574\u91c7\u6837\u7b56\u7565\u4ee5\u66f4\u597d\u5730\u9002\u5e94\u590d\u6742\u73af\u5883\uff0c\u4ece\u800c\u5f62\u6210\u9002\u7528\u4e8e\u4e0d\u540c\u4efb\u52a1\u7684\u53d8\u4f53\u6846\u67b6\u3002\n\n### 3. \u8bc4\u4f30\u5b66\u672f\u754c\u7684\u6280\u672f\u8fdb\u6b65\u4e0e\u5c40\u9650\u6027\n - **\u6280\u672f\u8fdb\u6b65**\uff1a\u901a\u8fc7\u5c06 MCTS \u5e94\u7528\u4e8e PRM \u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\uff0c\u5728\u8def\u5f84\u641c\u7d22\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u4e0a\u6709\u4e00\u5b9a\u63d0\u5347\u3002\u80fd\u591f\u5728\u590d\u6742\u73af\u5883\u4e2d\u66f4\u5feb\u5730\u627e\u5230\u53ef\u884c\u8def\u5f84\uff0c\u5e76\u4e14\u504f\u597d\u7b56\u7565\u6a21\u578b\u7ecf\u8fc7\u5fae\u8c03\u540e\u80fd\u66f4\u597d\u5730\u7b26\u5408\u7279\u5b9a\u4efb\u52a1\u9700\u6c42\u3002\n - **\u5c40\u9650\u6027**\uff1a\u6a21\u578b\u6027\u80fd\u63d0\u5347\u5b58\u5728\u74f6\u9888\u3002\u4e00\u65b9\u9762\uff0cMCTS \u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u8f83\u9ad8\uff0c\u5728\u5927\u89c4\u6a21\u73af\u5883\u6216\u590d\u6742\u4efb\u52a1\u4e2d\u641c\u7d22\u6548\u7387\u53d7\u9650\uff1b\u53e6\u4e00\u65b9\u9762\uff0cPRM \u7684\u91c7\u6837\u7b56\u7565\u53ef\u80fd\u65e0\u6cd5\u5b8c\u5168\u8986\u76d6\u6240\u6709\u6709\u6548\u8def\u5f84\uff0c\u5bfc\u81f4\u641c\u7d22\u7ed3\u679c\u5b58\u5728\u504f\u5dee\u3002\u540c\u65f6\uff0c\u6a21\u578b\u5bf9\u73af\u5883\u6570\u636e\u7684\u4f9d\u8d56\u8f83\u5f3a\uff0c\u5f53\u73af\u5883\u53d1\u751f\u53d8\u5316\u65f6\uff0c\u6a21\u578b\u7684\u9002\u5e94\u6027\u8f83\u5dee\uff0c\u5b58\u5728\u6a21\u578b\u504f\u5dee\u95ee\u9898\u3002\n\n### 4. \u63a2\u8ba8\u8ba1\u7b97\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\n - **\u9002\u7528\u6027**\uff1a\u5728\u7ed3\u6784\u5316\u7a0b\u5ea6\u8f83\u9ad8\u3001\u73af\u5883\u76f8\u5bf9\u7a33\u5b9a\u7684\u6570\u636e\u96c6\u548c\u5e94\u7528\u573a\u666f\u4e2d\uff0c\u57fa\u4e8e MCTS \u548c PRM \u7684\u6a21\u578b\u8868\u73b0\u8f83\u597d\uff0c\u80fd\u591f\u6709\u6548\u5730\u5b8c\u6210\u8def\u5f84\u641c\u7d22\u548c\u504f\u597d\u7b56\u7565\u5fae\u8c03\u4efb\u52a1\u3002\u4f8b\u5982\u5728\u7b80\u5355\u7684\u673a\u5668\u4eba\u8def\u5f84\u89c4\u5212\u573a\u666f\u4e2d\uff0c\u6a21\u578b\u53ef\u4ee5\u5feb\u901f\u751f\u6210\u53ef\u884c\u8def\u5f84\u3002\n - **\u6cdb\u5316\u80fd\u529b**\uff1a\u7136\u800c\uff0c\u5728\u591a\u9886\u57df\u3001\u591a\u6a21\u6001\u6570\u636e\u7684\u73b0\u5b9e\u5e94\u7528\u573a\u666f\u4e2d\uff0c\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002\u4f8b\u5982\u5728\u5305\u542b\u591a\u79cd\u4f20\u611f\u5668\u6570\u636e\uff08\u56fe\u50cf\u3001\u6fc0\u5149\u96f7\u8fbe\u7b49\uff09\u7684\u590d\u6742\u73af\u5883\u4e0b\uff0c\u6a21\u578b\u96be\u4ee5\u878d\u5408\u4e0d\u540c\u6a21\u6001\u7684\u6570\u636e\u8fdb\u884c\u6709\u6548\u51b3\u7b56\uff0c\u5bf9\u4e8e\u4e0d\u540c\u9886\u57df\u7684\u4efb\u52a1\uff08\u5982\u7269\u6d41\u8def\u5f84\u89c4\u5212\u548c\u533b\u7597\u5f71\u50cf\u5206\u6790\u4e2d\u7684\u8def\u5f84\u5f15\u5bfc\uff09\uff0c\u6a21\u578b\u9700\u8981\u5927\u91cf\u7684\u91cd\u65b0\u8bad\u7ec3\u548c\u8c03\u6574\u624d\u80fd\u9002\u5e94\u3002\n\n### 5. \u5206\u6790\u6700\u65b0\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027\n - **\u7a33\u5b9a\u6027**\uff1a\u90e8\u5206\u9488\u5bf9 MCTS \u7a33\u5b9a\u6027\u7684\u4f18\u5316\u63aa\u65bd\u88ab\u63d0\u51fa\uff0c\u4f8b\u5982\u6539\u8fdb\u641c\u7d22\u6811\u7684\u6784\u5efa\u548c\u66f4\u65b0\u7b56\u7565\uff0c\u4f7f\u5f97\u7b97\u6cd5\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u80fd\u591f\u5728\u590d\u6742\u73af\u5883\u4e0b\u4fdd\u6301\u76f8\u5bf9\u7a33\u5b9a\u7684\u6027\u80fd\u3002\u4f46\u5728\u52a8\u6001\u73af\u5883\u4e2d\uff0c\u7531\u4e8e\u73af\u5883\u7684\u5b9e\u65f6\u53d8\u5316\uff0c\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u641c\u7d22\u7ed3\u679c\u53ef\u80fd\u51fa\u73b0\u8f83\u5927\u6ce2\u52a8\u3002\n - **\u5bb9\u9519\u6027**\uff1a\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\uff0c\u7b97\u6cd5\u7684\u5bb9\u9519\u6027\u6709\u9650\u3002\u5f53\u6570\u636e\u5b58\u5728\u566a\u58f0\u6216\u4e0d\u5b8c\u6574\u65f6\uff0cMCTS \u548c PRM \u53ef\u80fd\u4f1a\u751f\u6210\u9519\u8bef\u7684\u8def\u5f84\u6216\u505a\u51fa\u4e0d\u5408\u7406\u7684\u504f\u597d\u7b56\u7565\u8c03\u6574\uff0c\u5f71\u54cd\u6a21\u578b\u7684\u6574\u4f53\u6027\u80fd\u3002\n\n### 6. \u8bc4\u4f30\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218\n - **\u672a\u6765\u7814\u7a76\u65b9\u5411**\uff1a\u63d0\u51fa\u4e86\u63a2\u7d22\u66f4\u9ad8\u6548\u7684 MCTS \u641c\u7d22\u7b97\u6cd5\uff0c\u4ee5\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff1b\u6539\u8fdb PRM \u7684\u91c7\u6837\u7b56\u7565\uff0c\u4f7f\u5176\u80fd\u66f4\u597d\u5730\u9002\u5e94\u590d\u6742\u548c\u52a8\u6001\u73af\u5883\uff1b\u7814\u7a76\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u65b9\u6cd5\uff0c\u63d0\u9ad8\u6a21\u578b\u5728\u591a\u9886\u57df\u3001\u591a\u6a21\u6001\u6570\u636e\u573a\u666f\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u3002\n - **\u6311\u6218**\uff1a\u65b0\u7684\u7814\u7a76\u95ee\u9898\u5305\u62ec\u5982\u4f55\u5728\u4fdd\u8bc1\u7b97\u6cd5\u6548\u7387\u7684\u540c\u65f6\u63d0\u9ad8\u5176\u5728\u590d\u6742\u73af\u5883\u4e0b\u7684\u7a33\u5b9a\u6027\u548c\u5bb9\u9519\u6027\uff1b\u5982\u4f55\u6709\u6548\u878d\u5408\u591a\u6a21\u6001\u6570\u636e\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u3002\u8fd9\u4e9b\u6311\u6218\u63a8\u52a8\u540e\u7eed\u7814\u7a76\u671d\u7740\u4f18\u5316\u7b97\u6cd5\u7ed3\u6784\u3001\u6539\u8fdb\u6570\u636e\u5904\u7406\u65b9\u5f0f\u4ee5\u53ca\u63a2\u7d22\u65b0\u7684\u6a21\u578b\u878d\u5408\u65b9\u6cd5\u7b49\u65b9\u5411\u53d1\u5c55\u3002\n\n### \u7814\u7a76\u6210\u679c\u3001\u65b9\u6cd5\u7684\u521b\u65b0\u6027\u4e0e\u5e94\u7528\u4ef7\u503c\n - **\u7814\u7a76\u6210\u679c**\uff1a\u660e\u786e\u4e86 MCTS \u5728 PRM \u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u4e2d\u7684\u5e94\u7528\u65b9\u5f0f\u53ca\u6548\u679c\uff0c\u5206\u6790\u4e86\u73b0\u6709\u6280\u672f\u7684\u8fdb\u6b65\u4e0e\u5c40\u9650\u3001\u6a21\u578b\u7684\u9002\u7528\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3001\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027\u7b49\u65b9\u9762\u3002\n - **\u65b9\u6cd5\u7684\u521b\u65b0\u6027**\uff1a\u5c06 MCTS \u4e0e PRM \u76f8\u7ed3\u5408\u7528\u4e8e\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\uff0c\u63a2\u7d22\u4e86\u4e0d\u540c\u7684\u6539\u8fdb\u7b56\u7565\u548c\u53d8\u4f53\u6846\u67b6\uff0c\u4e3a\u8def\u5f84\u641c\u7d22\u548c\u504f\u597d\u7b56\u7565\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u548c\u65b9\u6cd5\u3002\n - **\u5e94\u7528\u4ef7\u503c**\uff1a\u5728\u673a\u5668\u4eba\u8def\u5f84\u89c4\u5212\u3001\u7269\u6d41\u8def\u5f84\u89c4\u5212\u7b49\u9886\u57df\u6709\u6f5c\u5728\u5e94\u7528\u4ef7\u503c\uff0c\u901a\u8fc7\u4f18\u5316\u8def\u5f84\u641c\u7d22\u548c\u504f\u597d\u7b56\u7565\uff0c\u80fd\u591f\u63d0\u9ad8\u76f8\u5173\u7cfb\u7edf\u7684\u8fd0\u884c\u6548\u7387\u548c\u51b3\u7b56\u8d28\u91cf\u3002 ", "task_step_name": "\u7814\u7a76\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u4e3b\u8981\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0e\u53d8\u4f53", "task_step_description": "\u4e3b\u8981\u6846\u67b6\uff1aMCTS \u548c PRM \u662f\u6838\u5fc3\u6846\u67b6\u3002\u5e94\u7528\uff1a\u5728\u8def\u5f84\u641c\u7d22\u4efb\u52a1\u4e2d\uff0cPRM \u7528\u4e8e\u6784\u5efa\u73af\u5883\u7684\u6982\u7387\u8def\u7ebf\u56fe\uff0c\u4e3a\u8def\u5f84\u641c\u7d22\u63d0\u4f9b\u57fa\u7840\u7ed3\u6784\uff1bMCTS \u5219\u57fa\u4e8e PRM \u751f\u6210\u7684\u56fe\u8fdb\u884c\u641c\u7d22\u7b56\u7565\u7684\u4f18\u5316\uff0c\u901a\u8fc7\u4e0d\u65ad\u6a21\u62df\u548c\u9009\u62e9\u8def\u5f84\u6765\u627e\u5230\u6700\u4f18\u6216\u8fd1\u4f3c\u6700\u4f18\u8def\u5f84\u3002\u5728\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u4efb\u52a1\u4e2d\uff0c\u5229\u7528 MCTS \u7684\u641c\u7d22\u80fd\u529b\u6765\u63a2\u7d22\u4e0d\u540c\u7684\u7b56\u7565\u7a7a\u95f4\uff0c\u4ee5\u627e\u5230\u80fd\u4f7f\u504f\u597d\u7b56\u7565\u6a21\u578b\u6027\u80fd\u63d0\u5347\u7684\u5fae\u8c03\u53c2\u6570\u3002\u53d8\u4f53\uff1a\u4e00\u4e9b\u7814\u7a76\u5bf9 MCTS \u7684\u9009\u62e9\u3001\u6269\u5c55\u3001\u6a21\u62df\u548c\u53cd\u5411\u4f20\u64ad\u6b65\u9aa4\u8fdb\u884c\u6539\u8fdb\uff0c\u4f8b\u5982\u91c7\u7528\u4e0d\u540c\u7684\u9009\u62e9\u7b56\u7565\uff08\u5982 UCT \u53d8\u4f53\uff09\u6765\u63d0\u9ad8\u641c\u7d22\u6548\u7387\uff1b\u5bf9 PRM \u8fdb\u884c\u6539\u8fdb\uff0c\u5982\u8c03\u6574\u91c7\u6837\u7b56\u7565\u4ee5\u66f4\u597d\u5730\u9002\u5e94\u590d\u6742\u73af\u5883\uff0c\u4ece\u800c\u5f62\u6210\u9002\u7528\u4e8e\u4e0d\u540c\u4efb\u52a1\u7684\u53d8\u4f53\u6846\u67b6\u3002", "task_step_level": "1", "task_step_question": "\u5728\u8def\u5f84\u641c\u7d22\u4efb\u52a1\u548c\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u4efb\u52a1\u4e2d\uff0cMCTS\u548cPRM\u4f5c\u4e3a\u6838\u5fc3\u6846\u67b6\u5404\u81ea\u53d1\u6325\u4e86\u600e\u6837\u7684\u4f5c\u7528\uff0c\u4ee5\u53ca\u9488\u5bf9\u5b83\u4eec\u7684\u6539\u8fdb\u5f62\u6210\u4e86\u54ea\u4e9b\u9002\u7528\u4e8e\u4e0d\u540c\u4efb\u52a1\u7684\u53d8\u4f53\u6846\u67b6\uff1f  ", "task_step_question_context": [{"ref_id": "454845766417543398", "chunk_id": "4", "score": 0.2265625, "text": "# 4.2 RESULTS AND DISCUSSION\n\n# Question 1: How does TS-LLM perform in different generation tasks regarding Path@1? Does Path $@1$ provide a reasonable evaluation metric? (Sec. 3.2.2 and Sec. 3.4)\nIn Table 2, we first provide the comparison between TS-LLM variants with the CoT baseline regarding the Path $@1$ performance. Path $@1$ means each algorithm only generates one answer. TS-LLM variants generally outperform the baseline, underscoring the advantages of tree-search algorithms due to their capability to efficiently prune the search tree. However, notable exceptions are also seen in the RLHF task where MCTS, BFS, and DFS underperform. From our results, we want to highlight two things. Firstly, MCTS, BFS, and DFS generally perform better in shallow search problems (8 for GSM8K and 4 for Game24) while MCTS$\\alpha$ and MCTS-Rollout are dominant in deep search problems (15 for PrOntoQA and 64 for RLHF). Secondly, the the backward/backpropagation operation is quite important, since the first three methods\u2014MCTS, BFS, and DFS\u2014do not incorporate this operation and depend greedily on the value function. Note that the MCTS Path $@1$ also does not include the backward operation because it only happens after finding one complete path.  \n\nTable 5: Different value training for iterative update on GSM8k  \n\n  \nFigure 2: Different Value training on GSM8k   \nFigure 3: mean/max reward for RLHF alignment and the best results of 3 aggregations for the rest three tasks w.r.t. number of sequences on the 1st row and number of computations on the 2nd row.  \n\nDespite the superiority of TS-LLM, we argue that Path $@1$ is in fact not a reasonable evaluation metric. In table 2, we also include the number of computations used in Path $@1$ generation (number of tokens in sentence-level and number of forward computation in token-level). All TS-LLM variants consume much more computation than the CoT baseline. To enable a fair comparison, we provide additional baselines, CoT-SC and COT-SC-Tree with two aggregation methods: majorityvote (MAJ) and ORM-vote (denoted as ORM, and it utilizes the learned ORM in TS-LLM). We show results within a similar scale of computation consumption with TS-LLM variants. Under this situation, TS-LLM\u2019s advantages largely decrease when compared with $\\mathrm{CoT-SC_{\\mathrm{ORM}}}$ , especially on GSM8K (only BFS is better). We are surprised to see that such simple algorithms can also have outstanding performance when compared fairly. Despite this, most tree-search algorithms are still dominant in the rest three tasks given the same (CoT-SC-Tree) or larger search space (CoT-SC).\n\n# Question 2: How do different node constructions influence the performance? (Sec. 3.1)\nAs discussed in Sec. 3.1), the search space for sentence-level action nodes is limited. Thus, we investigate the possible influence introduced by different tree constructions on Game24 with different node sizes. Specifically, we set the number of maximal expanded node size as 6, 20, and 50. Table 3 lists the performance and the number of tokens generated comparing TS-LLM\u2019s variants, CoT-SC and CoT-SC-Tree. The almost doubled performance boost from 44.2 to 79.8 indicates the importance of appropriate expansion node size, improving TS-LLM\u2019s performance upper bound.\n\n# Question 3: Why do we need a learned value function and how to train that? (Sec. 3.2.1)\nIn Table 4, we provide a motivating example by prompting LLaMA2-7b-base as the value function and reward model for TS-LLM on Game24. The performance drop of BFS and MCTS$\\alpha$ indicates the incapability of small language models as reliable evaluators, which motivates us to substitute it with a learned value function and ORM as a general solution for any task and LLM with any size.  \n\nTable 6: Iterative update results. $\\theta_{0}$ is the old parameter while $\\theta_{1}$ is the new one after one iteration. We compare all combinations of policy and value on GSM8k (left) and RLHF alignment (right).   \n\n\n<html><body><table><tr><td>Method</td><td>Policy</td><td>Value</td><td>Performance(%) Method</td><td></td><td>Policy</td><td>Value</td><td>Performance(%)</td></tr><tr><td>Greedy</td><td>T00</td><td>1</td><td>41.4 \u00b1 0.0</td><td>Greedy</td><td>T00</td><td></td><td>0.39 \u00b1 0.0</td></tr><tr><td>Greedy</td><td>T01</td><td>1</td><td>47.9 \u00b1 0.0</td><td>Greedy</td><td>T01</td><td></td><td>1.87 \u00b1 0.0</td></tr><tr><td>Greedy</td><td>RFT k=50</td><td></td><td>47.0 \u00b1 0.0</td><td>Greedy</td><td>RFTN=5</td><td></td><td>1.16 \u00b1 0.0</td></tr><tr><td>Greedy</td><td>RFTk=100</td><td>-</td><td>47.5 \u00b1 0.0</td><td>Greedy</td><td>PPO</td><td></td><td>2.53\u00b10.0</td></tr><tr><td>MCTS-\u03b1</td><td>T00</td><td>{,r}00</td><td>51.9 \u00b1 0.6</td><td>MCTS-0</td><td>T00</td><td>{,r}00</td><td>2.221\u00b10.0</td></tr><tr><td>MCTS-\u03b1</td><td>T00</td><td>U,r}01</td><td>53.2\u00b10.3</td><td>MCTS-0</td><td>T00</td><td>U,r}01</td><td>2.482\u00b10.0</td></tr><tr><td>MCTS-\u03b1</td><td>T01</td><td>v,r}00</td><td>54.1 \u00b1 0.9</td><td>MCTS-0</td><td>T01</td><td>v,r}00</td><td>2.532\u00b10.0</td></tr><tr><td>MCTS-\u03b1</td><td>T01</td><td>\u03b8</td><td>56.5\u00b1 0.6</td><td>MCTS-\u03b1</td><td>T01</td><td>0</td><td>2.670 \u00b1 0.0</td></tr></table></body></html>  \n\nTherefore, we investigate data collection and training paradigms for value function and ORM in TSLLM. In Figure 2, we investigate the influence of data amount and diversity by training with mixed data uniformly sampled from checkpoints of all SFT epochs ( mixed ); data purely sampled from the last checkpoint ( pure ); 1/3 data of the pure setting $(p u r e,l e s s)$ . The results of CoT-SC ORM-vote $@10$ underscore the diversity of sampled data in learning a better ORM. The Path $@1$ results of 3 TSLLM variants show that the amount of sampled data is of great importance. We leave a detailed discussion of how value and reward function training is influenced in iterative training (Sec. 3.3) when answering Question 5. Our final conclusion is that collecting a set of diverse data as much as possible is always better for TS-LLM\u2019s value function and ORM training .\n\n# Question 4: How TS-LLM is improved by aggregating over multiple results? (Sec. 3.2.3)\nIn Fig. 3, we demonstrate the mean/max reward for the RLHF task and the best of 3 aggregation results for the rest three tasks. We measure the performance of aggregation w.r.t path number and token consumption. From the figure, we mainly summarize two conclusions: Firstly, Most TS-LLM variants benefit from aggregation and can show large strengths compared with other baselines. CoT-SC only beats TS-LLM in GSM8k with the same token size, mainly because of its larger search space. TS-LLM variants are still dominant when compared with CoT-SC-Tree. Secondly, treesearch algorithms\u2019 aggregation benefits less than CoT-SC in small-scale problems. In GSM8K and Game24, TS-LLM struggles to improve under large aggregation numbers. We believe this is because of: (1) The search space gap between CoT-SC and tree-search algorithms. Tree-search algorithms inherently explore fewer sentences, which is validated by comparing token consumption between CoT-SC-Tree $@50$ and $\\mathrm{CoT-SC}@50$ . (2) Different tree searches are not independent. The latter search might be influenced by the previous one, which decreases generation diversity."}, {"ref_id": "454898862265206278", "chunk_id": "2", "score": 0.15625, "text": "# 6 Search as a Meta-Strategy Operator\nMany MATAs produce a policy network $\\pmb{p}$ that maps directly from an infostate to a distribution over actions in a forward pass for every player. Recent work has found that leveraging computation at run-time and adding search to $\\textbf{\\emph{p}}$ can improve performance in large EFG domains [Silver et al. , 2018; Brown et al. , 2020; Schmid et al. , 2023]. As a case study for our meta-game evaluation framework, we apply it to investigate the effect of search as a general policy improver.  \n\nToward that end, we propose a heuristic search method for large EFGs based on information-set MCTS (IS-MCTS) [Cowling et al. , 2012] and Gumbel AlphaZero [Danihelka et al. , 2022]. Alg. 1 presents the procedure, Gumbel IS-MCTS ,in detail. Parameterized by a policy net $\\textbf{\\emph{p}}$ and a value net $\\pmb{v}$ ,Gumbel IS-MCTS conducts multiple passes over the gametree guided by $\\pmb{v}$ and $\\textbf{\\emph{p}}$ at an input infostate $s$ , and outputs an action $a$ for decision-making. We can apply this procedure to a variety of underlying MATAs, as a meta-strategy operator ters like simulation budget). The meta-stra adds run-time search to the output policy : transforming $\\mathcal{M}$ to $\\mathcal{M}^{\\prime}$ (with additional h pgy of $\\mathcal{M}^{\\prime}$ M. Unlike rparamein effect AlphaZero\u2014which uses the same MCTS method for training and run-time, with meta-strategy operators we can explore a variety of MATAs as training-time methods which produce $\\pmb{v}$ and $\\pmb{p}$ for search at test-time [Sokota et al. , 2024] (details in $\\S7.2)$ ).  \n\n<html><body><table><tr><td colspan=\"2\">Algorithm 1 Gumbel IS-MCTS</td></tr><tr><td></td><td>1: function Gumbel-Search(s, v,p)</td></tr><tr><td>2:</td><td>(s,a). R(s,a) \u2190 O, C(s,a) \u2190 0</td></tr><tr><td>3:</td><td>Va E A(s). sample g(a) ~ Gumbel(0),A < A(s)</td></tr><tr><td>4: 5:</td><td>repeat Sample a world state: h ~ Pr(h I s,p)</td></tr><tr><td>6:</td><td>while do</td></tr><tr><td>7:</td><td>if h is terminal then</td></tr><tr><td>8:</td><td></td></tr><tr><td></td><td>r \u2190payoffs of players Break</td></tr><tr><td>9:</td><td>else if  (h) is chance then</td></tr><tr><td>10:</td><td>a \u2190 sample according to chance</td></tr><tr><td>11:</td><td>else if si(h) not in search tree then</td></tr><tr><td>12:</td><td>Add si(h)to search tree</td></tr><tr><td>13:</td><td>r \u2190 v(si(h)). Break</td></tr><tr><td>14:</td><td>else if s(h) is root node s then</td></tr><tr><td>15:</td><td>a, A \u2190 one step of sequential halving (Alg. 3) based on GS(s, a) and remaining actions in A</td></tr><tr><td>16: 17:</td><td>else Select a according to Eq.(3) in App. B.2.</td></tr><tr><td>18:</td><td>end if</td></tr><tr><td></td><td>h\u2190ha</td></tr><tr><td>19:</td><td>end while</td></tr><tr><td>20:</td><td></td></tr><tr><td>21:</td><td>for (si, a) in this trajectory do Increment R(si, a) by ri, C(si, a) by 1.</td></tr><tr><td>22: 23:</td><td>end for</td></tr><tr><td>24:</td><td>until num_sim simulations done</td></tr><tr><td>25:</td><td>return Action a that remains in A</td></tr><tr><td></td><td></td></tr><tr><td></td><td>26: end function</td></tr></table></body></html>  \n\nJust like MCTS, IS-MCTS incrementally builds and traverses a search tree and aggregates statistics such as visit counts $C(s,a)$ and aggregated values $R(s,a)$ for visited $(s,a)$ pairs. During each simulation of the search (line 5), a world state is sampled from a posterior bel f$\\operatorname*{Pr}(h\\mid s,p)$ assuming the opponents played according to pprior to s. In our test domains, $\\operatorname*{Pr}(h\\mid s,p)$ can be computed exactly via Bayes\u2019s rule, where in larger domains, using particle filtering [Silver and Veness, 2010] or deep generative models $[\\mathrm{Hu}~e t$ al. , 2021; Li et al. , 2023] to approximate $\\operatorname*{Pr}(h\\mid s,p)$ are possible. Further technical details are provided in App. B.  \n\nThe key feature of Gumbel IS-MCTS is how it selects actions at the search nodes. At the beginning of the search (line 3), a Gumbel random variable, $g(a)$ , is sampled i.i.d. for each legal action $a$ of the root, for later use in action selection. At the root (line 15), the algorithm treats each legal action as an arm of a stochastic bandit, and uses a sequentialhalving algorithm [Pepels et al. , 2014] (Alg. 3) to distribute the simulation budget. Sequential-halving algorithms usually are designed for minimizing the simple regret [Bubeck et al. , 2009], which is the regret at the last-iteration action recommendation. By contrast, UCB-style algorithms are usually designed for minimizing the accumulated regret during an online learning process. For a game-playing search algorithm, minimizing simple regret makes more sense in terms of producing a single optimal action at a decision point.  \n\n  \nFigure 1: Example start of sequential bargaining game instance.  \n\nWe assign to each arm $a$ a Gumbel score $G S(s,a)\\;=\\;$ $g(a)+\\log\\!\\mathrm{it}\\,p(s,a)+G(\\hat{q}(s,a))$ . The second term is the logit of $a$ produced by $\\textbf{\\emph{p}}$ , and the third term is a monotone transformation of the action value $\\hat{q}(s,a)$ , which is estimated by $R(s,a),C(s,a)$ , and $\\pmb{v}$ . The intuition is that a high $\\hat{q}(s,a)$ value indicates a direction for policy improvement. Indeed, the improved policy $I m p(p)(s,a)\\triangleq\\operatorname{SoftMax}(\\log\\!\\mathrm{it}\\,p(s,a)+$ $G(\\hat{q}(s,a)))$ provably achieves higher expected values Danihelka et al. [2022, App. B]. The forms of $G$ and $\\hat{q}$ is detailed in App. B.1.  \n\nAdding Gumbel noise $g(a)$ implements the \u201cGumbel topK-trick\u201d: deterministically selecting the top $K$ actions according to $G S(s,a)$ is equivalent to sampling $K$ actions from $I m p(\\pmb{p})(s,a)$ without replacement [Huijben et al. , 2022]. The Gumbel score induces a low-variance non-deterministic action selection of the root node during the sequential halving process, which encourages exploration while distributing the simulation budget toward actions likely to yield higher expected values.  \n\nAt a non-root node (line 17), an action is selected to minimize the discrepancy between $I m p(p)$ and the produced visited frequency (details in App. B.2). At the end of the search, Gumbel IS-MCTS outputs the action $a$ that survives the sequential halving procedure."}, {"ref_id": "454984282436545430", "chunk_id": "1", "score": 0.1513671875, "text": "# 4.4 Finding Explanations with MCTS\nWe use the MCTS framework established in Silver et al. (2016 ), but we modify the search tree and the reward function to suite our purposes (see Figure 2 ). Each node in the tree represents an explanation $\\mathbf{E}=\\{\\mathbf{p}_{1},\\mathbf{p}_{2},\\ldots,\\mathbf{p}_{k}\\}$ . The root of the tree represents the whole text piece as a single phrase, Table 1: Performances of stress classifiers on the test set of D READDIT . While non-neural classifiers could not surpass $72\\%$ accuracy, the MentalRoBERTa FT model fine-tuned on the D READDIT train set yielded $82\\%$ accuracy. Here, the superscript FT denotes that the model was fine-tuned.  \n\n<html><body><table><tr><td>Model</td><td>Precision</td><td>Recall</td><td>F-1</td><td>Accuracy</td></tr><tr><td>BernoulliNB</td><td>0.69</td><td>0.84</td><td>0.75</td><td>0.72</td></tr><tr><td>MultinomialNB</td><td>0.68</td><td>0.87</td><td>0.76</td><td>0.72</td></tr><tr><td>SVM</td><td>0.71</td><td>0.77</td><td>0.74</td><td>0.72</td></tr><tr><td>MLP</td><td>0.71</td><td>0.74</td><td>0.73</td><td>0.71</td></tr><tr><td>MentalRoBERTa FI</td><td>0.78</td><td>0.90</td><td>0.84</td><td>0.82</td></tr></table></body></html>  \n\ni.e., $\\mathbf{E}_{r o o t}\\,=\\,\\{\\mathbf{t}\\}$ . When the search is at a given node in the tree, there are two options for expanding the next node: (i) remove the first or last token in any phrase, as long as the shortened phrase still contains at least $N_{l e n g t h}$ tokens, or (ii) remove a token in the middle of a phrase, thus breaking it into two phrases, as long as both resulting phrases have at least $N_{l e n g t h}$ tokens and the total number of phrases does not exceed $N_{p h r a s e s}$ .  \n\nThe search continues to expand nodes in the tree until either the current node cannot be expanded using either of the two rules above or the explanation at the current node contains too few tokens, i.e., $r(\\mathbf{E})\\leq r_{m i n}$ . This node serves as a leaf node and is given a reward equal to  \n\n$$\nR(\\mathbf{E})=S(\\mathbf{E})+I\\cdot\\alpha\\cdot H(\\mathbf{E})\n$$  \n\nsome $I\\in\\{-1,+1\\}$ and $\\alpha\\geq0$ . We use $I=$ $+1$ to select for high entropy (context-independent) explanations and $I=-1$ to select for low entropy (context-dependent) explanations. This reward is propagated back to all the nodes on the path from the root to this leaf node according to the update rules from Silver et al. (2016 ). After the search is complete, the best explanation $\\hat{\\bf E}$ is selected as  \n\n$$\n\\begin{array}{r}{\\hat{\\mathbf{E}}=\\underset{\\mathbf{E}}{\\mathrm{argmax}}\\;R(\\mathbf{E})\\;\\;\\mathrm{s.t.}\\;r(\\mathbf{E})\\leq r_{m a x},}\\end{array}\n$$  \n\nwhich means $\\hat{{\\bf E}}$ is the explanation in the search tree that maximizes the reward while satisfying the condition on the maximum proportion of tokens. The other interpretability conditions are guaranteed by the rules of the search tree expansion.\n\n# 5 Experiments\nAll of our experiments were run on the D READDIT dataset. We report results of our stress and context classification models and share findings of our MCTS explanation algorithm.  \n\n<html><body><table><tr><td>Model</td><td>Precision</td><td>Recall</td><td>F-1</td><td>Accuracy</td></tr><tr><td>BernoulliNB</td><td>0.81</td><td>0.75</td><td>0.76</td><td>0.80</td></tr><tr><td>MultinomialNB</td><td>0.77</td><td>0.75</td><td>0.75</td><td>0.79</td></tr><tr><td>SVM</td><td>0.76</td><td>0.72</td><td>0.74</td><td>0.76</td></tr><tr><td>MLP</td><td>0.78</td><td>0.78</td><td>0.78</td><td>0.79</td></tr><tr><td>MentalRoBERTa F1</td><td>0.85</td><td>0.86</td><td>0.86</td><td>0.87</td></tr></table></body></html>  \n\nTable 2: Performances of context classifiers. We restricted our focus to three subreddits: \u201canxiety,\u201d \u201cassistance,\u201d \u201crelationships.\u201d The fine-tuned MentalRoBERTa FT model yielded the best results with $87\\%$ accuracy.\n\n# 5.1 Classification\nAs Table 1 illustrates, basic stress classification models, such as Naive Bayes classifiers, SVMs, and MLPs, performed reasonably on the test set of D READDIT . The MentalRoBERTa FT model for stress fine-tuned on the training set of D READDIT for five epochs, however, was able to outperform all the other models, achieving an accuracy score of $82\\%$ and demonstrating the efficacy of the pretraining on mental health data 3 . Our results on the stress classification task are consistent with those of Turcan and McKeown (2019 ). Table 2 reports the performance of various models on the multiclass subreddit category classification. Here, we limited our attention to three categories, namely \u201canxiety,\u201d \u201cassistance,\u201d and \u201crelationships.\u201d The Reddit posts in these categories embody various distinct everyday, financial, and interpersonal stress factors, but at the same time, they seem to have common (context-independent) stress elements. In this context classification task, all models were able to go beyond the $75\\%$ accuracy level, but MentalRoBERTa FT yielded the highest accuracy.\n\n# 5.2 Explainability\nWe demonstrate our MCTS approach to explainability using the same three categories as above. We use stress and context classification models implemented with Multinomial NB, MLP, and MentalRoBERTa FT .For each of these models, we apply MCTS to identify explanations for each of the 166 test texts that is labeled as stressed and belongs to one of our three categories. We use the interpretability conditions $N_{p h r a s e s}\\,=\\,3$ ,$N_{l e n g t h}=5$ ,$r_{m i n}=0.2$ , and $r_{m a x}=0.5$ for all experiments 4 , and we use $\\alpha\\,=\\,10$ except where otherwise noted.  \n\n<html><body><table><tr><td></td><td></td><td>Original</td><td>Dependent</td><td>Independent</td></tr><tr><td rowspan=\"2\">MNB</td><td>S</td><td>0.850\u00b10.317</td><td>0.706\u00b10.190</td><td>0.617\u00b10.124</td></tr><tr><td>E</td><td>0.047 \u00b1 0.140</td><td>0.274 \u00b1 0.181</td><td>0.942 \u00b1 0.086</td></tr><tr><td rowspan=\"2\">MLP</td><td>S</td><td>0.725\u00b10.383</td><td>0.512\u00b10.194</td><td>0.546\u00b10.145</td></tr><tr><td>E</td><td>0.214 \u00b1 0.274</td><td>0.766 \u00b1 0.163</td><td>1.067\u00b10.022</td></tr><tr><td rowspan=\"2\">MRB</td><td>S</td><td>0.878\u00b10.324</td><td>0.830\u00b10.220</td><td>0.430\u00b10.273</td></tr><tr><td>E</td><td>0.042 \u00b1 0.124</td><td>0.019 \u00b1 0.018</td><td>0.640 \u00b1 0.171</td></tr></table></body></html>  \n\nTable 3: Stress (S) and context entropy (E) for original text, context-dependent explanation, and contextindependent explanation for the Multinomial Naive Bayes (MNB), Multilayer Perceptron (MLP), and Mental RoBERTa (MRB) models. Results were generated through MCTS with stress and context entropy averaged over the test set. The Wilcoxon signed rank test (Wilcoxon ,1945 ) between dependent and independent entropy is $p<0.0001$ for all models, indicating a very significant difference as desired.  \n\n  \n  \nFigure 3: Histogram of stress scores for the original text and for the context-dependent and contextindependent explanations extracted by our MCTS algorithm using an MLP model. Although stress is often higher in the original text than in the extracted explanations, the explanations still maintain a meaningful amount of stress.  \n\nWe quantitatively evaluate the explanations produced by MCTS. In Table 3 , we show the average stress and context entropy scores of the original text and of the context-dependent and contextindependent explanations. Our method is able to maintain a reasonably high and consistent level of stress across the explanations while modulating the context entropy appropriately for the two different types of explanations. This indicates that our approach can identify both context-dependent and context-independent sources of stress.  \n\nFigures 3 and 4 further illustrate this result for the MLP model by showing the full distribution of stress and context entropy scores across the test examples. Figures 5 ,6 , and 7 in the Appendix show the stress and context entropy distributions for all three models and for different values of $\\alpha$ .Lower $\\alpha$ increases stress but decreases the difference in entropy between the two types of explanations while higher $\\alpha$ decreases stress but increases the difference in entropy. This shows the flexibility of MCTS to select different types of explanations without retraining the classifiers.  \n\n  \nContext Entropy for Original Text and Explanations $(\\alpha=10.0)$   \nFigure 4: Histogram of context entropy for the original text and for the context-dependent and contextindependent explanations extracted by our MCTS algorithm using an MLP model. The context-independent explanations clearly have much higher context entropy than the context-dependent explanations as desired.  \n\nFurthermore, we qualitatively demonstrate our approach. Tables 4 ,5 , and 6 in the Appendix show examples from each of the three subreddits that illustrate how our method captures different underlying sources of stress in an interpretable manner."}], "task_step_question_answer": "\u5728RLHF\u4efb\u52a1\u4e2d\uff0cMCTS\u7684\u8868\u73b0\u6b20\u4f73\u4e3b\u8981\u662f\u7531\u4e8e\u5176\u641c\u7d22\u673a\u5236\u5728\u9ad8\u7ef4\u72b6\u6001\u7a7a\u95f4\u548c\u590d\u6742\u5956\u52b1\u51fd\u6570\u4e0b\u7684\u5c40\u9650\u6027\u3002RLHF\u4efb\u52a1\u901a\u5e38\u6d89\u53ca\u5927\u91cf\u7684\u72b6\u6001\u548c\u52a8\u4f5c\u7ec4\u5408\uff0cMCTS\u5728\u641c\u7d22\u8fc7\u7a0b\u4e2d\u9700\u8981\u904d\u5386\u5927\u91cf\u7684\u8282\u70b9\uff0c\u5bfc\u81f4\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u8fc7\u5927\u3002\u6b64\u5916\uff0cRLHF\u4efb\u52a1\u7684\u5956\u52b1\u51fd\u6570\u901a\u5e38\u662f\u975e\u7ebf\u6027\u7684\uff0cMCTS\u5728\u53cd\u5411\u4f20\u64ad\u8fc7\u7a0b\u4e2d\u96be\u4ee5\u51c6\u786e\u66f4\u65b0\u8282\u70b9\u4ef7\u503c\uff0c\u5bfc\u81f4\u641c\u7d22\u6548\u7387\u4f4e\u4e0b\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e9b\u95ee\u9898\uff0cMCTS\u03b1\u901a\u8fc7\u5f15\u5165\u66f4\u9ad8\u6548\u7684\u9009\u62e9\u7b56\u7565\u548c\u53cd\u5411\u4f20\u64ad\u673a\u5236\uff0c\u80fd\u591f\u5728\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u8fdb\u884c\u66f4\u6709\u6548\u7684\u63a2\u7d22\u548c\u5229\u7528\u3002MCTS - Rollout\u5219\u901a\u8fc7\u6539\u8fdb\u6a21\u62df\u7b56\u7565\uff0c\u80fd\u591f\u5728\u6df1\u5ea6\u641c\u7d22\u4e2d\u66f4\u5feb\u5730\u6536\u655b\u5230\u8f83\u4f18\u89e3\u3002\n\nPRM\u5728\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e3b\u8981\u4f53\u73b0\u5728\u5176\u80fd\u591f\u901a\u8fc7\u91c7\u6837\u7b56\u7565\u6784\u5efa\u7b56\u7565\u7a7a\u95f4\u7684\u8fd1\u4f3c\u8868\u793a\u3002\u8fd9\u79cd\u8fd1\u4f3c\u8868\u793a\u80fd\u591f\u5e2e\u52a9MCTS\u5728\u7b56\u7565\u7a7a\u95f4\u4e2d\u8fdb\u884c\u66f4\u9ad8\u6548\u7684\u641c\u7d22\u3002\u5177\u4f53\u6765\u8bf4\uff0cPRM\u901a\u8fc7\u5728\u9ad8\u7ef4\u7b56\u7565\u7a7a\u95f4\u4e2d\u8fdb\u884c\u968f\u673a\u91c7\u6837\uff0c\u6784\u5efa\u4e00\u4e2a\u7a00\u758f\u7684\u56fe\u7ed3\u6784\uff0cMCTS\u53ef\u4ee5\u57fa\u4e8e\u8fd9\u4e2a\u56fe\u7ed3\u6784\u8fdb\u884c\u5c40\u90e8\u641c\u7d22\uff0c\u4ece\u800c\u63d0\u9ad8\u641c\u7d22\u6548\u7387\u3002\u7ed3\u5408\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u540e\uff0cPRM\u53ef\u4ee5\u5229\u7528\u5b66\u4e60\u5230\u7684\u7b56\u7565\u7a7a\u95f4\u5206\u5e03\u4fe1\u606f\uff0c\u4f18\u5316\u641c\u7d22\u8def\u5f84\u7684\u9009\u62e9\uff0c\u4ece\u800c\u63d0\u9ad8\u641c\u7d22\u6548\u7387\u3002", "ref_task_step_id": ""}, "__type__": "task_step_node"}, "9cfb30a6-9a28-45f1-a55b-2f78b05406c7": {"__data__": {"id_": "9cfb30a6-9a28-45f1-a55b-2f78b05406c7", "metadata": {}, "relationships": {}, "hash": "", "start_task_context": "MCTS\u5728PRM\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u7684\u5e94\u7528\u63a2\u7d22\u7efc\u8ff0", "aemo_representation_context": "### 1. \u5206\u6790\u8fd1\u51e0\u5e74\u7814\u7a76\u9886\u57df\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba\n\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u76f8\u5173\u7814\u7a76\u9886\u57df\uff0c\u9488\u5bf9\u8def\u5f84\u641c\u7d22\u4e0e\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u65b9\u9762\uff0c\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09\u4ee5\u53ca\u6982\u7387\u8def\u7ebf\u56fe\uff08PRM\uff09\u65b9\u6cd5\u53d7\u5230\u5173\u6ce8\u3002\u8fd1\u51e0\u5e74\u7684\u7814\u7a76\u56f4\u7ed5\u5982\u4f55\u5c06 MCTS \u4e0e PRM \u76f8\u7ed3\u5408\uff0c\u63a2\u7d22\u5728\u4e0d\u540c\u4efb\u52a1\u573a\u666f\u4e0b\u5bf9\u504f\u597d\u7b56\u7565\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u7684\u6709\u6548\u6280\u672f\u6846\u67b6\u3002\u65b9\u6cd5\u8bba\u4e0a\uff0c\u591a\u901a\u8fc7\u5b9e\u9a8c\u5bf9\u6bd4\u4e0d\u540c\u53c2\u6570\u8bbe\u7f6e\u548c\u6539\u8fdb\u7b56\u7565\u4e0b\u6a21\u578b\u7684\u6027\u80fd\u8868\u73b0\uff0c\u4ee5\u6b64\u6765\u4f18\u5316\u6280\u672f\u6846\u67b6\u3002\n\n### 2. \u7814\u7a76\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u4e3b\u8981\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0e\u53d8\u4f53\n - **\u4e3b\u8981\u6846\u67b6**\uff1aMCTS \u548c PRM \u662f\u6838\u5fc3\u6846\u67b6\u3002\n - **\u5e94\u7528**\uff1a\u5728\u8def\u5f84\u641c\u7d22\u4efb\u52a1\u4e2d\uff0cPRM \u7528\u4e8e\u6784\u5efa\u73af\u5883\u7684\u6982\u7387\u8def\u7ebf\u56fe\uff0c\u4e3a\u8def\u5f84\u641c\u7d22\u63d0\u4f9b\u57fa\u7840\u7ed3\u6784\uff1bMCTS \u5219\u57fa\u4e8e PRM \u751f\u6210\u7684\u56fe\u8fdb\u884c\u641c\u7d22\u7b56\u7565\u7684\u4f18\u5316\uff0c\u901a\u8fc7\u4e0d\u65ad\u6a21\u62df\u548c\u9009\u62e9\u8def\u5f84\u6765\u627e\u5230\u6700\u4f18\u6216\u8fd1\u4f3c\u6700\u4f18\u8def\u5f84\u3002\u5728\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u4efb\u52a1\u4e2d\uff0c\u5229\u7528 MCTS \u7684\u641c\u7d22\u80fd\u529b\u6765\u63a2\u7d22\u4e0d\u540c\u7684\u7b56\u7565\u7a7a\u95f4\uff0c\u4ee5\u627e\u5230\u80fd\u4f7f\u504f\u597d\u7b56\u7565\u6a21\u578b\u6027\u80fd\u63d0\u5347\u7684\u5fae\u8c03\u53c2\u6570\u3002\n - **\u53d8\u4f53**\uff1a\u4e00\u4e9b\u7814\u7a76\u5bf9 MCTS \u7684\u9009\u62e9\u3001\u6269\u5c55\u3001\u6a21\u62df\u548c\u53cd\u5411\u4f20\u64ad\u6b65\u9aa4\u8fdb\u884c\u6539\u8fdb\uff0c\u4f8b\u5982\u91c7\u7528\u4e0d\u540c\u7684\u9009\u62e9\u7b56\u7565\uff08\u5982 UCT \u53d8\u4f53\uff09\u6765\u63d0\u9ad8\u641c\u7d22\u6548\u7387\uff1b\u5bf9 PRM \u8fdb\u884c\u6539\u8fdb\uff0c\u5982\u8c03\u6574\u91c7\u6837\u7b56\u7565\u4ee5\u66f4\u597d\u5730\u9002\u5e94\u590d\u6742\u73af\u5883\uff0c\u4ece\u800c\u5f62\u6210\u9002\u7528\u4e8e\u4e0d\u540c\u4efb\u52a1\u7684\u53d8\u4f53\u6846\u67b6\u3002\n\n### 3. \u8bc4\u4f30\u5b66\u672f\u754c\u7684\u6280\u672f\u8fdb\u6b65\u4e0e\u5c40\u9650\u6027\n - **\u6280\u672f\u8fdb\u6b65**\uff1a\u901a\u8fc7\u5c06 MCTS \u5e94\u7528\u4e8e PRM \u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\uff0c\u5728\u8def\u5f84\u641c\u7d22\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u4e0a\u6709\u4e00\u5b9a\u63d0\u5347\u3002\u80fd\u591f\u5728\u590d\u6742\u73af\u5883\u4e2d\u66f4\u5feb\u5730\u627e\u5230\u53ef\u884c\u8def\u5f84\uff0c\u5e76\u4e14\u504f\u597d\u7b56\u7565\u6a21\u578b\u7ecf\u8fc7\u5fae\u8c03\u540e\u80fd\u66f4\u597d\u5730\u7b26\u5408\u7279\u5b9a\u4efb\u52a1\u9700\u6c42\u3002\n - **\u5c40\u9650\u6027**\uff1a\u6a21\u578b\u6027\u80fd\u63d0\u5347\u5b58\u5728\u74f6\u9888\u3002\u4e00\u65b9\u9762\uff0cMCTS \u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u8f83\u9ad8\uff0c\u5728\u5927\u89c4\u6a21\u73af\u5883\u6216\u590d\u6742\u4efb\u52a1\u4e2d\u641c\u7d22\u6548\u7387\u53d7\u9650\uff1b\u53e6\u4e00\u65b9\u9762\uff0cPRM \u7684\u91c7\u6837\u7b56\u7565\u53ef\u80fd\u65e0\u6cd5\u5b8c\u5168\u8986\u76d6\u6240\u6709\u6709\u6548\u8def\u5f84\uff0c\u5bfc\u81f4\u641c\u7d22\u7ed3\u679c\u5b58\u5728\u504f\u5dee\u3002\u540c\u65f6\uff0c\u6a21\u578b\u5bf9\u73af\u5883\u6570\u636e\u7684\u4f9d\u8d56\u8f83\u5f3a\uff0c\u5f53\u73af\u5883\u53d1\u751f\u53d8\u5316\u65f6\uff0c\u6a21\u578b\u7684\u9002\u5e94\u6027\u8f83\u5dee\uff0c\u5b58\u5728\u6a21\u578b\u504f\u5dee\u95ee\u9898\u3002\n\n### 4. \u63a2\u8ba8\u8ba1\u7b97\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\n - **\u9002\u7528\u6027**\uff1a\u5728\u7ed3\u6784\u5316\u7a0b\u5ea6\u8f83\u9ad8\u3001\u73af\u5883\u76f8\u5bf9\u7a33\u5b9a\u7684\u6570\u636e\u96c6\u548c\u5e94\u7528\u573a\u666f\u4e2d\uff0c\u57fa\u4e8e MCTS \u548c PRM \u7684\u6a21\u578b\u8868\u73b0\u8f83\u597d\uff0c\u80fd\u591f\u6709\u6548\u5730\u5b8c\u6210\u8def\u5f84\u641c\u7d22\u548c\u504f\u597d\u7b56\u7565\u5fae\u8c03\u4efb\u52a1\u3002\u4f8b\u5982\u5728\u7b80\u5355\u7684\u673a\u5668\u4eba\u8def\u5f84\u89c4\u5212\u573a\u666f\u4e2d\uff0c\u6a21\u578b\u53ef\u4ee5\u5feb\u901f\u751f\u6210\u53ef\u884c\u8def\u5f84\u3002\n - **\u6cdb\u5316\u80fd\u529b**\uff1a\u7136\u800c\uff0c\u5728\u591a\u9886\u57df\u3001\u591a\u6a21\u6001\u6570\u636e\u7684\u73b0\u5b9e\u5e94\u7528\u573a\u666f\u4e2d\uff0c\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002\u4f8b\u5982\u5728\u5305\u542b\u591a\u79cd\u4f20\u611f\u5668\u6570\u636e\uff08\u56fe\u50cf\u3001\u6fc0\u5149\u96f7\u8fbe\u7b49\uff09\u7684\u590d\u6742\u73af\u5883\u4e0b\uff0c\u6a21\u578b\u96be\u4ee5\u878d\u5408\u4e0d\u540c\u6a21\u6001\u7684\u6570\u636e\u8fdb\u884c\u6709\u6548\u51b3\u7b56\uff0c\u5bf9\u4e8e\u4e0d\u540c\u9886\u57df\u7684\u4efb\u52a1\uff08\u5982\u7269\u6d41\u8def\u5f84\u89c4\u5212\u548c\u533b\u7597\u5f71\u50cf\u5206\u6790\u4e2d\u7684\u8def\u5f84\u5f15\u5bfc\uff09\uff0c\u6a21\u578b\u9700\u8981\u5927\u91cf\u7684\u91cd\u65b0\u8bad\u7ec3\u548c\u8c03\u6574\u624d\u80fd\u9002\u5e94\u3002\n\n### 5. \u5206\u6790\u6700\u65b0\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027\n - **\u7a33\u5b9a\u6027**\uff1a\u90e8\u5206\u9488\u5bf9 MCTS \u7a33\u5b9a\u6027\u7684\u4f18\u5316\u63aa\u65bd\u88ab\u63d0\u51fa\uff0c\u4f8b\u5982\u6539\u8fdb\u641c\u7d22\u6811\u7684\u6784\u5efa\u548c\u66f4\u65b0\u7b56\u7565\uff0c\u4f7f\u5f97\u7b97\u6cd5\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u80fd\u591f\u5728\u590d\u6742\u73af\u5883\u4e0b\u4fdd\u6301\u76f8\u5bf9\u7a33\u5b9a\u7684\u6027\u80fd\u3002\u4f46\u5728\u52a8\u6001\u73af\u5883\u4e2d\uff0c\u7531\u4e8e\u73af\u5883\u7684\u5b9e\u65f6\u53d8\u5316\uff0c\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u641c\u7d22\u7ed3\u679c\u53ef\u80fd\u51fa\u73b0\u8f83\u5927\u6ce2\u52a8\u3002\n - **\u5bb9\u9519\u6027**\uff1a\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\uff0c\u7b97\u6cd5\u7684\u5bb9\u9519\u6027\u6709\u9650\u3002\u5f53\u6570\u636e\u5b58\u5728\u566a\u58f0\u6216\u4e0d\u5b8c\u6574\u65f6\uff0cMCTS \u548c PRM \u53ef\u80fd\u4f1a\u751f\u6210\u9519\u8bef\u7684\u8def\u5f84\u6216\u505a\u51fa\u4e0d\u5408\u7406\u7684\u504f\u597d\u7b56\u7565\u8c03\u6574\uff0c\u5f71\u54cd\u6a21\u578b\u7684\u6574\u4f53\u6027\u80fd\u3002\n\n### 6. \u8bc4\u4f30\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218\n - **\u672a\u6765\u7814\u7a76\u65b9\u5411**\uff1a\u63d0\u51fa\u4e86\u63a2\u7d22\u66f4\u9ad8\u6548\u7684 MCTS \u641c\u7d22\u7b97\u6cd5\uff0c\u4ee5\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff1b\u6539\u8fdb PRM \u7684\u91c7\u6837\u7b56\u7565\uff0c\u4f7f\u5176\u80fd\u66f4\u597d\u5730\u9002\u5e94\u590d\u6742\u548c\u52a8\u6001\u73af\u5883\uff1b\u7814\u7a76\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u65b9\u6cd5\uff0c\u63d0\u9ad8\u6a21\u578b\u5728\u591a\u9886\u57df\u3001\u591a\u6a21\u6001\u6570\u636e\u573a\u666f\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u3002\n - **\u6311\u6218**\uff1a\u65b0\u7684\u7814\u7a76\u95ee\u9898\u5305\u62ec\u5982\u4f55\u5728\u4fdd\u8bc1\u7b97\u6cd5\u6548\u7387\u7684\u540c\u65f6\u63d0\u9ad8\u5176\u5728\u590d\u6742\u73af\u5883\u4e0b\u7684\u7a33\u5b9a\u6027\u548c\u5bb9\u9519\u6027\uff1b\u5982\u4f55\u6709\u6548\u878d\u5408\u591a\u6a21\u6001\u6570\u636e\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u3002\u8fd9\u4e9b\u6311\u6218\u63a8\u52a8\u540e\u7eed\u7814\u7a76\u671d\u7740\u4f18\u5316\u7b97\u6cd5\u7ed3\u6784\u3001\u6539\u8fdb\u6570\u636e\u5904\u7406\u65b9\u5f0f\u4ee5\u53ca\u63a2\u7d22\u65b0\u7684\u6a21\u578b\u878d\u5408\u65b9\u6cd5\u7b49\u65b9\u5411\u53d1\u5c55\u3002\n\n### \u7814\u7a76\u6210\u679c\u3001\u65b9\u6cd5\u7684\u521b\u65b0\u6027\u4e0e\u5e94\u7528\u4ef7\u503c\n - **\u7814\u7a76\u6210\u679c**\uff1a\u660e\u786e\u4e86 MCTS \u5728 PRM \u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u4e2d\u7684\u5e94\u7528\u65b9\u5f0f\u53ca\u6548\u679c\uff0c\u5206\u6790\u4e86\u73b0\u6709\u6280\u672f\u7684\u8fdb\u6b65\u4e0e\u5c40\u9650\u3001\u6a21\u578b\u7684\u9002\u7528\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3001\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027\u7b49\u65b9\u9762\u3002\n - **\u65b9\u6cd5\u7684\u521b\u65b0\u6027**\uff1a\u5c06 MCTS \u4e0e PRM \u76f8\u7ed3\u5408\u7528\u4e8e\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\uff0c\u63a2\u7d22\u4e86\u4e0d\u540c\u7684\u6539\u8fdb\u7b56\u7565\u548c\u53d8\u4f53\u6846\u67b6\uff0c\u4e3a\u8def\u5f84\u641c\u7d22\u548c\u504f\u597d\u7b56\u7565\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u548c\u65b9\u6cd5\u3002\n - **\u5e94\u7528\u4ef7\u503c**\uff1a\u5728\u673a\u5668\u4eba\u8def\u5f84\u89c4\u5212\u3001\u7269\u6d41\u8def\u5f84\u89c4\u5212\u7b49\u9886\u57df\u6709\u6f5c\u5728\u5e94\u7528\u4ef7\u503c\uff0c\u901a\u8fc7\u4f18\u5316\u8def\u5f84\u641c\u7d22\u548c\u504f\u597d\u7b56\u7565\uff0c\u80fd\u591f\u63d0\u9ad8\u76f8\u5173\u7cfb\u7edf\u7684\u8fd0\u884c\u6548\u7387\u548c\u51b3\u7b56\u8d28\u91cf\u3002 ", "task_step_name": "\u8bc4\u4f30\u5b66\u672f\u754c\u7684\u6280\u672f\u8fdb\u6b65\u4e0e\u5c40\u9650\u6027", "task_step_description": "\u6280\u672f\u8fdb\u6b65\uff1a\u901a\u8fc7\u5c06 MCTS \u5e94\u7528\u4e8e PRM \u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\uff0c\u5728\u8def\u5f84\u641c\u7d22\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u4e0a\u6709\u4e00\u5b9a\u63d0\u5347\u3002\u80fd\u591f\u5728\u590d\u6742\u73af\u5883\u4e2d\u66f4\u5feb\u5730\u627e\u5230\u53ef\u884c\u8def\u5f84\uff0c\u5e76\u4e14\u504f\u597d\u7b56\u7565\u6a21\u578b\u7ecf\u8fc7\u5fae\u8c03\u540e\u80fd\u66f4\u597d\u5730\u7b26\u5408\u7279\u5b9a\u4efb\u52a1\u9700\u6c42\u3002\u5c40\u9650\u6027\uff1a\u6a21\u578b\u6027\u80fd\u63d0\u5347\u5b58\u5728\u74f6\u9888\u3002\u4e00\u65b9\u9762\uff0cMCTS \u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u8f83\u9ad8\uff0c\u5728\u5927\u89c4\u6a21\u73af\u5883\u6216\u590d\u6742\u4efb\u52a1\u4e2d\u641c\u7d22\u6548\u7387\u53d7\u9650\uff1b\u53e6\u4e00\u65b9\u9762\uff0cPRM \u7684\u91c7\u6837\u7b56\u7565\u53ef\u80fd\u65e0\u6cd5\u5b8c\u5168\u8986\u76d6\u6240\u6709\u6709\u6548\u8def\u5f84\uff0c\u5bfc\u81f4\u641c\u7d22\u7ed3\u679c\u5b58\u5728\u504f\u5dee\u3002\u540c\u65f6\uff0c\u6a21\u578b\u5bf9\u73af\u5883\u6570\u636e\u7684\u4f9d\u8d56\u8f83\u5f3a\uff0c\u5f53\u73af\u5883\u53d1\u751f\u53d8\u5316\u65f6\uff0c\u6a21\u578b\u7684\u9002\u5e94\u6027\u8f83\u5dee\uff0c\u5b58\u5728\u6a21\u578b\u504f\u5dee\u95ee\u9898\u3002", "task_step_level": "2", "task_step_question": "\u5c06 MCTS \u5e94\u7528\u4e8e PRM \u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u5728\u63d0\u5347\u8def\u5f84\u641c\u7d22\u6548\u7387\u548c\u51c6\u786e\u6027\u65b9\u9762\u6709\u4e00\u5b9a\u8fdb\u6b65\uff0c\u4f46\u5b58\u5728\u6a21\u578b\u6027\u80fd\u63d0\u5347\u74f6\u9888\uff0c\u90a3\u4e48\u5982\u4f55\u5177\u4f53\u89e3\u51b3 MCTS \u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u3001PRM \u91c7\u6837\u7b56\u7565\u8986\u76d6\u4e0d\u8db3\u4ee5\u53ca\u6a21\u578b\u5bf9\u73af\u5883\u6570\u636e\u4f9d\u8d56\u5f3a\u8fd9\u4e9b\u5c40\u9650\u6027\u95ee\u9898\u5462\uff1f ", "task_step_question_context": [{"ref_id": "454984236281633338", "chunk_id": "4", "score": 0.33203125, "text": "# Monte Carlo Tree Search in the Presence of Transition Uncertainty\nFarnaz Kohankhaki , Kiarash Aghakasiri , Hongming Zhang 1 , Ting-Han Wei 1 , Chao Gao 2 ,Martin M\u00a8uller 1  \n\n1 University of Alberta, 2 Edmonton Research Center, Huawei Canada {kohankha, aghakasi, hongmin2, tinghan, mmueller }@ualberta.ca, cgao3 $@$ outlook.com\n\n# Abstract\nMonte Carlo Tree Search (MCTS) is an immensely popular search-based framework used for decision making. It is traditionally applied to domains where a perfect simulation model of the environment is available. We study and improve MCTS in the context where the environment model is given but imperfect. We show that the discrepancy between the model and the actual environment can lead to significant performance degradation with standard MCTS. We therefore develop Uncertainty Adapted MCTS (UA-MCTS), a more robust algorithm within the MCTS framework. We estimate the transition uncertainty in the given model, and direct the search towards more certain transitions in the state space. We modify all four MCTS phases to improve the search behavior by considering these estimates. We prove, in the corrupted bandit case, that adding uncertainty information to adapt UCB leads to tighter regret bound than standard UCB. Empirically, we evaluate UA-MCTS and its individual components on the deterministic domains from the MinAtar test suite. Our results demonstrate that UA-MCTS strongly improves MCTS in the presence of model transition errors.\n\n# 1 Introduction\nThe Monte Carlo Tree Search (MCTS) framework (Browne et al. 2012) approaches sequential decision-making problems by selective lookahead search. It manages the balance of exploration and exploitation with techniques such as UCT (Kocsis, Szepesv\u00b4ari, and Willemson 2006). Often combined with machine learning, it has been enormously successful in both games (Silver et al. 2016; Banerjee 2020; Arneson, Hayward, and Henderson 2010; Saffidine 2008; Nijssen and Winands 2010) and non-game applications (Lu et al. 2016; Mansley, Weinstein, and Littman 2011; Sabharwal, Samulowitz, and Reddy 2012; Cazenave 2010). In these applications, a perfect simulation model allows for efficient lookahead search. However, in many practical applications, only an imperfect model is available to the agent. Yet lookahead using such a model can still be useful. We improve MCTS for this setting.  \n\nOne research area that studies imperfect models of the environment is model-based reinforcement learning (MBRL).  \n\nHere, an agent builds its own model through limited real world interactions. The resulting learned model, when used for lookahead search, can either be for planning or for producing more accurate training targets (Silver, Sutton, and M\u00a8uller 2008). It can also be used to generate simulated training samples for better sample efficiency (Sutton and Barto 2018). The learned model may be inaccurate for many reasons, including stochasticity of the environment, insufficient training, insufficient capacity, non stationary environments, etc. Consequently, there is a rich body of research on uncertainty in MBRL (Abbas et al. 2020; Xiao et al. 2019; Buckman et al. 2018).  \n\nWhile previous approaches to using search with imperfect models exist (Vemula et al. 2020; Vemula, Bagnell, and Likhachev 2021), to the best of our knowledge, there is no prior work that directly adapts MCTS to deal with model uncertainty. In our work, we define transition uncertainty as a measure of difference between the state transitions in the perfect model and in the model that is available to the agent. We use a neural network to estimate this uncertainty.  \n\nOur Uncertainty Adapted MCTS (UA-MCTS) approach implements the main components of the MCTS framework in a way that guides the search away from states with high uncertainty. We compare the performance of our proposed methods with MCTS baselines in three deterministic MinAtar environments (Young and Tian 2019). In each case the search agent \u201cbelieves\u201d it is playing the real game. However, the rules of the game itself have changed, and the agent only learns about this change slowly when it acts in the real environment. The results show that UA-MCTS is able to outperform the baseline MCTS with an imperfect model.  \n\nOur approach is inspired by the work of (Vemula et al. 2020) where a robotic arm has to solve tasks despite being handicapped, e.g. by a broken motor or by an unmodeled weight restriction. To show how an agent should adapt UCB-based exploration strategy in the presence of environment uncertainties, we first consider a case of stochastic bandits (Lattimore and Szepesv\u00b4ari 2020) along with corrupted feedback. We prove that incorporating uncertainty information can enhance the performance of UCB, yielding a regret bound that is more constrained compared to the standard UCB. We also prove that in the general case of tree search, with similar modification of UCT, our UA-MCTS approach maintains its completeness property, ensuring that as the number of iterations goes to infinity, all nodes will be consistently explored. To further motivate our approach, we compare the scenarios of learning to improve the transition function, using MCTS, directly against the easier task of just learning a transition uncertainty function with UA-MCTS. In both cases, learning occurs online; the former is used with MCTS while the latter is used with UA-MCTS. Our results show that learning the transition function is much harder than learning transition uncertainty, which justifies the use of UA-MCTS in such settings."}, {"ref_id": "455026805323333778", "chunk_id": "1", "score": 0.2265625, "text": "# Monte Carlo Tree Search based Variable Selection for High Dimensional Bayesian Optimization\nLei Song , Ke Xue , Xiaobin Huang, Chao Qian \u2020State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing 210023, China {songl, xuek, huangxb, qianc}@lamda.nju.edu.cn\n\n# Abstract\nBayesian optimization (BO) is a class of popular methods for expensive black-box optimization, and has been widely applied to many scenarios. However, BO suffers from the curse of dimensionality, and scaling it to high-dimensional problems is still a challenge. In this paper, we propose a variable selection method MCTS-VS based on Monte Carlo tree search (MCTS), to iteratively select and optimize a subset of variables. That is, MCTS-VS constructs a low-dimensional subspace via MCTS and optimizes in the subspace with any BO algorithm. We give a theoretical analysis of the general variable selection method to reveal how it can work. Experiments on high-dimensional synthetic functions and real-world problems (i.e., NAS-bench problems and MuJoCo locomotion tasks) show that MCTS-VS equipped with a proper BO optimizer can achieve state-of-the-art performance.\n\n# 1 Introduction\nIn many real-world tasks such as neural architecture search (NAS) [ 41 ] and policy search in reinforcement learning (RL) [ 6 ], one often needs to solve the expensive black-box optimization problems. Bayesian optimization (BO) [ 2 ,11 ,23 ,32 ] is a sample-efficient algorithm for solving such problems. It iteratively fits a surrogate model, typically Gaussian process (GP), and maximizes an acquisition function to obtain the next point to evaluate. While BO has been employed in a wide variety of settings, successful applications are often limited to low-dimensional problems.  \n\nRecently, scaling BO to high-dimensional problems has received a lot of interest. Decompositionbased methods [ 13 ,15 ,17 ,26 ,31 ] assume that the high-dimensional function to be optimized has a certain structure, typically the additive structure. By decomposing the original high-dimensional function into the sum of several low-dimensional functions, they optimize each low-dimensional function to obtain the point in the high-dimensional space. However, it is not easy to decide whether a decomposition exists as well as to learn the decomposition.  \n\nOther methods often assume that the original high-dimensional function with dimension $D$ has a low-dimensional subspace with dimension $d\\ll D$ , and then perform the optimization in the low-dimensional subspace and project the low-dimensional point back for evaluation. For example, embedding-based methods [ 20 ,27 ,42 ] use a random matrix to embed the original space into the lowdimensional subspace. Another way is to select a subset of variables directly, which can even avoid the time-consuming matrix operations of embedding-based methods. For example, Dropout [ 21 ]selects $d$ variables randomly in each iteration. Note that for both embedding and variable selection methods, the parameter $d$ can have a large influence on the performance, which is, however, difficult to set in real-world problems.  \n\nIn this paper, we propose a new Variable Selection method using Monte Carlo Tree Search (MCTS), called MCTS-VS. MCTS is employed to partition the variables into important and unimportant ones, and only those selected important variables are optimized via any black-box optimization algorithm, e.g., vanilla BO [ 32 ] or TuRBO [ 10 ]. The values of unimportant variables are sampled using historical information. Compared with Dropout-BO, MCTS-VS can select important variables automatically.  \n\nWe also provide regret and computational complexity analyses of general variable selection methods, showing that variable selection can reduce the computational complexity while increasing the cumulative regret. Our regret bound generalizes that of GP-UCB [ 38 ] which always selects all variables, as well as that of Dropout [ 21 ] which selects $d$ variables randomly in each iteration. The results suggest that a good variable selection method should select as important variables as possible.  \n\nExperiments on high-dimensional synthetic functions and real-world problems (i.e., NAS and RL problems) show that MCTS-VS is better than the previous variable selection method Dropout [ 21 ], and can also achieve the competitive performance to state-of-the-art BO algorithms. Furthermore, its running time is small due to the advantage of variable selection. We also observe that MCTS-VS can select important variables, explaining its good performance based on our theoretical analysis.\n\n# 2 Background\n\n# 2.1 Bayesian Optimization\nWe consider the problem $\\operatorname*{max}_{\\pmb{x}\\in\\mathcal{X}}f(\\pmb{x})$ , where $f$ is a black-box function and $\\mathcal{X}\\subseteq\\mathbb{R}^{D}$ is the domain. The basic framework of BO contains two critical components: a surrogate model and an acquisition functi ost p te model. Given the sampled data point $\\{(\\mathbfit{x^{i}},\\dot{y}^{i})\\}_{i=1}^{t-1}$ ,$f\\sim\\mathcal{G P}(\\mu(\\cdot),k(\\cdot,\\cdot)+\\eta^{2}\\mathbf{I})$ \u223cGP $y^{i}=f(\\pmb{x}^{i})+\\epsilon^{i}$ \u00b7\u00b7\u00b7$\\epsilon^{i}\\sim\\mathcal{N}(0,\\eta^{2})$ , specified by the mean \u223cN is the obs $\\mu(\\cdot)$ \u00b7tion noise, GP at iterati and covariance kernel $k(\\cdot,\\cdot)$ $t$ \u00b7\u00b7eeks to i , where I is er the identity matrix of size D. After that, an acquisition function, e.g., Probability of Improvement (PI) [ 19 ], Expected Improvement (EI) [ 16 ] or Upper Confidence Bound (UCB) [ 38 ], is used to determine the next query point $\\pmb{x}^{t}$ while balancing exploitation and exploration."}, {"ref_id": "454984236379937352", "chunk_id": "11", "score": 0.22265625, "text": "# 2.2 Acceleration of MCTS\nMCTS-based methods have proved their strong capability of solving complex games or tasks. However, the high computational cost of MCTS hinders its application to some real-time and more general scenarios. Therefore, numerous works are devoted to accelerating MCTS. For example, to make the selection stage more effective, some heuristic pruning methods [ 14 ,33 ,29 ,1 ,2 ] aim to reduce the width and depth of the search tree with some heuristic functions. Furthermore, for more efficient evaluations, Lorentz [ 22 ] proposed early playout termination of MCTS (MCTS-EPT) to stop the random playouts early and use an evaluation function to assess win or loss. Moreover, Hsueh et al. [18 ] applied MCTS-EPT to the Chinese dark chess and proved its effectiveness. Afterward, similar ideas have been applied in the evaluation stage of AlphaGoZero [ 32 ] and later MCTS-based methods [31 ,27 ,34 ]. They evaluate the $Q$ -values through a learnable evaluation network instead of running playouts to the end. Grill et al. [15 ] propose a novel regularized policy optimization method based on AlphaZero to decrease the search budget of MCTS, which is from the optimization perspective. Danihelka et al. [10 ] propose a policy improvement algorithm based on sampling actions without replacement, named Gumbel trick to achieve better performance when planning with few simulations. However, these methods mentioned above focus on the specific stage of the search iteration or reduce the total budget through pruning and optimization methods, which are orthogonal to us. And few works targets at the search loop. Lan et al. [21 ] propose DS-MCTS, which defines the uncertainty of MCTS and approximates it by extra DNNs with specific features for board games in training. During the evaluation, DS-MCTS will check periodically and stop the search if the state is certain.\n\n# 3 Background\nThe AlphaGo series of work [ 30 ,32 ,31 ,27 ] are all MCTS-based reinforcement learning algorithms. Those algorithms assume the environment transition dynamics are known or learn the environment dynamics. Based on the dynamics, they use the Monte-Carlo tree search (MCTS) as the policy improvement operator. I.e., taking in the current policy, MCTS returns a better policy with the search algorithm. The systematic search allows the MCTS-based RL algorithm to quickly improve the policy and perform much better in the setting where heavy reasoning is required.\n\n# 3.1 MCTS\nThis part briefly introduces the MCTS method implemented in reinforcement learning applications. As mentioned in the related works, modern MCTS-based RL algorithms include four stages in the search loop, namely selection, expansion, evaluation, and backpropagation.  \n\nMCTS takes in the current states and generates a policy after the search loop of $N$ iterations. Here $N$ is a constant number of iterations set by the designer, regarded as the total budget. In the selection stage of each iteration, an action will be selected by maximizing over UCB. Specifically, AlphaZero [31 ] and MuZero [ 27 ] are developed based on a variant of UCB, P-UCT [ 25 ] and have achieved great success on board games and Atari games. The formula of P-UCT is the Eq (1):  \n\n$$\na^{k}=\\arg\\operatorname*{max}_{a\\in\\mathcal{A}}Q(s,a)+P(s,a)\\frac{\\sqrt{\\sum_{b\\in\\mathcal{A}}N(s,b)}}{1+N(s,a)}(c_{1}+\\log((\\sum_{b\\in\\mathcal{A}}N(s,b)+c_{2}+1)/c_{2})),\n$$  \n\nwhere $k$ is the index of iteration, $\\boldsymbol{\\mathcal{A}}$ is the acti $Q(s,a)$ is the estimated Q-value, $P(s,a)$ is the policy prior obtained from neural networks, $N(s,a)$ is the visitations to select the action a from the state $s$ and $c_{1},c_{2}$ are hyper-parameters. The output of MCTS is the visitation of each action of the root node. After $N$ search iterations, the final policy $\\pi(s)$ is defined as the normalized root visitation distribution simplification, we use $\\pi_{N}(s)$ $\\pi_{k}$ in place of , where $\\begin{array}{r}{\\pi_{k}(s,a)=N(s,a)/\\sum_{b\\in\\mathcal{A}}N(s,b)=N(s,a)/k,a\\in\\mathcal{A}}\\end{array}$ $\\pi_{k}(s)$ sometimes. And the detailed procedure of MCTS is \u2208A . For introduced in Appendix. In our method, we propose to approximate the final policy $\\pi_{N}(s)$ with $\\hat{\\pi}_{k}(s)$ ,which we name as a virtual expanded policy, through a new expansion method and a termination rule. In this way, the number of iterations in MCTS can be reduced from $N$ to $k$ .\n\n# 3.2 Computation Requirement\nMost of the computations in MCTS-based RL are in the MCTS procedure. Each action taken by MCTS requires $N$ times neural network evaluations, where $N$ is a constant number of iterations in the search loop. Traditional RL algorithms, such as PPO [ 28 ] or DQN [ 23 ], only need a single neural network evaluation per action. Thus, MCTS-based RL is roughly $N$ times computationally more expensive than traditional RL algorithms. In practice, training a single Atari game needs 12 hours of computation time on 40 TPUs [ 27 ]. The computation need is roughly two orders of magnitude more than traditional RL algorithms [28], although the final performance of MuZero is much better.\n\n# 4 Method\nWe aim to spend more search time on harder states and less on easier states. Intuitively, human knows when to make a quick decision or a slow decision under different circumstances. Unfortunately, this situation-aware behavior is absent in current MCTS algorithms. Therefore, we propose an MCTS variant that terminates the search iteration adaptively. It consists of two components: a novel expansion method named virtual expansion to estimate the final visitation based on the current partial tree; a termination rule that decides when to terminate based on the hardness of the current scenario. And we will display the adaptive mechanism through visualizations in Section 5.5."}], "task_step_question_answer": "\u5728\u672a\u6765\u7684\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u8ba1\u5212\u8fdb\u4e00\u6b65\u63a2\u7d22\u6df1\u5ea6\u5b66\u4e60\u4e0eMCTS\u7ed3\u5408\u7684\u66f4\u591a\u53ef\u80fd\u6027\uff0c\u4f8b\u5982\u5f15\u5165\u5f3a\u5316\u5b66\u4e60\u6765\u52a8\u6001\u8c03\u6574CNN\u7684\u7279\u5f81\u63d0\u53d6\u8fc7\u7a0b\uff0c\u4f7f\u5176\u66f4\u9002\u5e94MCTS\u7684\u51b3\u7b56\u9700\u6c42\u3002\u540c\u65f6\uff0c\u6211\u4eec\u4e5f\u5c06\u6df1\u5165\u7814\u7a76\u81ea\u9002\u5e94\u91c7\u6837\u673a\u5236\u5728\u4e0d\u540c\u7c7b\u578b\u641c\u7d22\u4efb\u52a1\u4e2d\u7684\u9002\u7528\u6027\uff0c\u4ee5\u671f\u627e\u5230\u66f4\u901a\u7528\u7684\u4f18\u5316\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u8fc1\u79fb\u5b66\u4e60\u5728\u8de8\u9886\u57df\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u4e5f\u5c06\u662f\u6211\u4eec\u5173\u6ce8\u7684\u91cd\u70b9\uff0c\u5e0c\u671b\u901a\u8fc7\u66f4\u591a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u627e\u5230\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u7684\u6709\u6548\u9014\u5f84\u3002", "ref_task_step_id": ""}, "__type__": "task_step_node"}, "9518ca14-d51c-44c3-8ac8-8d3e16c1f870": {"__data__": {"id_": "9518ca14-d51c-44c3-8ac8-8d3e16c1f870", "metadata": {}, "relationships": {}, "hash": "", "start_task_context": "MCTS\u5728PRM\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u7684\u5e94\u7528\u63a2\u7d22\u7efc\u8ff0", "aemo_representation_context": "### 1. \u5206\u6790\u8fd1\u51e0\u5e74\u7814\u7a76\u9886\u57df\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba\n\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u76f8\u5173\u7814\u7a76\u9886\u57df\uff0c\u9488\u5bf9\u8def\u5f84\u641c\u7d22\u4e0e\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u65b9\u9762\uff0c\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09\u4ee5\u53ca\u6982\u7387\u8def\u7ebf\u56fe\uff08PRM\uff09\u65b9\u6cd5\u53d7\u5230\u5173\u6ce8\u3002\u8fd1\u51e0\u5e74\u7684\u7814\u7a76\u56f4\u7ed5\u5982\u4f55\u5c06 MCTS \u4e0e PRM \u76f8\u7ed3\u5408\uff0c\u63a2\u7d22\u5728\u4e0d\u540c\u4efb\u52a1\u573a\u666f\u4e0b\u5bf9\u504f\u597d\u7b56\u7565\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u7684\u6709\u6548\u6280\u672f\u6846\u67b6\u3002\u65b9\u6cd5\u8bba\u4e0a\uff0c\u591a\u901a\u8fc7\u5b9e\u9a8c\u5bf9\u6bd4\u4e0d\u540c\u53c2\u6570\u8bbe\u7f6e\u548c\u6539\u8fdb\u7b56\u7565\u4e0b\u6a21\u578b\u7684\u6027\u80fd\u8868\u73b0\uff0c\u4ee5\u6b64\u6765\u4f18\u5316\u6280\u672f\u6846\u67b6\u3002\n\n### 2. \u7814\u7a76\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u4e3b\u8981\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0e\u53d8\u4f53\n - **\u4e3b\u8981\u6846\u67b6**\uff1aMCTS \u548c PRM \u662f\u6838\u5fc3\u6846\u67b6\u3002\n - **\u5e94\u7528**\uff1a\u5728\u8def\u5f84\u641c\u7d22\u4efb\u52a1\u4e2d\uff0cPRM \u7528\u4e8e\u6784\u5efa\u73af\u5883\u7684\u6982\u7387\u8def\u7ebf\u56fe\uff0c\u4e3a\u8def\u5f84\u641c\u7d22\u63d0\u4f9b\u57fa\u7840\u7ed3\u6784\uff1bMCTS \u5219\u57fa\u4e8e PRM \u751f\u6210\u7684\u56fe\u8fdb\u884c\u641c\u7d22\u7b56\u7565\u7684\u4f18\u5316\uff0c\u901a\u8fc7\u4e0d\u65ad\u6a21\u62df\u548c\u9009\u62e9\u8def\u5f84\u6765\u627e\u5230\u6700\u4f18\u6216\u8fd1\u4f3c\u6700\u4f18\u8def\u5f84\u3002\u5728\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u4efb\u52a1\u4e2d\uff0c\u5229\u7528 MCTS \u7684\u641c\u7d22\u80fd\u529b\u6765\u63a2\u7d22\u4e0d\u540c\u7684\u7b56\u7565\u7a7a\u95f4\uff0c\u4ee5\u627e\u5230\u80fd\u4f7f\u504f\u597d\u7b56\u7565\u6a21\u578b\u6027\u80fd\u63d0\u5347\u7684\u5fae\u8c03\u53c2\u6570\u3002\n - **\u53d8\u4f53**\uff1a\u4e00\u4e9b\u7814\u7a76\u5bf9 MCTS \u7684\u9009\u62e9\u3001\u6269\u5c55\u3001\u6a21\u62df\u548c\u53cd\u5411\u4f20\u64ad\u6b65\u9aa4\u8fdb\u884c\u6539\u8fdb\uff0c\u4f8b\u5982\u91c7\u7528\u4e0d\u540c\u7684\u9009\u62e9\u7b56\u7565\uff08\u5982 UCT \u53d8\u4f53\uff09\u6765\u63d0\u9ad8\u641c\u7d22\u6548\u7387\uff1b\u5bf9 PRM \u8fdb\u884c\u6539\u8fdb\uff0c\u5982\u8c03\u6574\u91c7\u6837\u7b56\u7565\u4ee5\u66f4\u597d\u5730\u9002\u5e94\u590d\u6742\u73af\u5883\uff0c\u4ece\u800c\u5f62\u6210\u9002\u7528\u4e8e\u4e0d\u540c\u4efb\u52a1\u7684\u53d8\u4f53\u6846\u67b6\u3002\n\n### 3. \u8bc4\u4f30\u5b66\u672f\u754c\u7684\u6280\u672f\u8fdb\u6b65\u4e0e\u5c40\u9650\u6027\n - **\u6280\u672f\u8fdb\u6b65**\uff1a\u901a\u8fc7\u5c06 MCTS \u5e94\u7528\u4e8e PRM \u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\uff0c\u5728\u8def\u5f84\u641c\u7d22\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u4e0a\u6709\u4e00\u5b9a\u63d0\u5347\u3002\u80fd\u591f\u5728\u590d\u6742\u73af\u5883\u4e2d\u66f4\u5feb\u5730\u627e\u5230\u53ef\u884c\u8def\u5f84\uff0c\u5e76\u4e14\u504f\u597d\u7b56\u7565\u6a21\u578b\u7ecf\u8fc7\u5fae\u8c03\u540e\u80fd\u66f4\u597d\u5730\u7b26\u5408\u7279\u5b9a\u4efb\u52a1\u9700\u6c42\u3002\n - **\u5c40\u9650\u6027**\uff1a\u6a21\u578b\u6027\u80fd\u63d0\u5347\u5b58\u5728\u74f6\u9888\u3002\u4e00\u65b9\u9762\uff0cMCTS \u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u8f83\u9ad8\uff0c\u5728\u5927\u89c4\u6a21\u73af\u5883\u6216\u590d\u6742\u4efb\u52a1\u4e2d\u641c\u7d22\u6548\u7387\u53d7\u9650\uff1b\u53e6\u4e00\u65b9\u9762\uff0cPRM \u7684\u91c7\u6837\u7b56\u7565\u53ef\u80fd\u65e0\u6cd5\u5b8c\u5168\u8986\u76d6\u6240\u6709\u6709\u6548\u8def\u5f84\uff0c\u5bfc\u81f4\u641c\u7d22\u7ed3\u679c\u5b58\u5728\u504f\u5dee\u3002\u540c\u65f6\uff0c\u6a21\u578b\u5bf9\u73af\u5883\u6570\u636e\u7684\u4f9d\u8d56\u8f83\u5f3a\uff0c\u5f53\u73af\u5883\u53d1\u751f\u53d8\u5316\u65f6\uff0c\u6a21\u578b\u7684\u9002\u5e94\u6027\u8f83\u5dee\uff0c\u5b58\u5728\u6a21\u578b\u504f\u5dee\u95ee\u9898\u3002\n\n### 4. \u63a2\u8ba8\u8ba1\u7b97\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\n - **\u9002\u7528\u6027**\uff1a\u5728\u7ed3\u6784\u5316\u7a0b\u5ea6\u8f83\u9ad8\u3001\u73af\u5883\u76f8\u5bf9\u7a33\u5b9a\u7684\u6570\u636e\u96c6\u548c\u5e94\u7528\u573a\u666f\u4e2d\uff0c\u57fa\u4e8e MCTS \u548c PRM \u7684\u6a21\u578b\u8868\u73b0\u8f83\u597d\uff0c\u80fd\u591f\u6709\u6548\u5730\u5b8c\u6210\u8def\u5f84\u641c\u7d22\u548c\u504f\u597d\u7b56\u7565\u5fae\u8c03\u4efb\u52a1\u3002\u4f8b\u5982\u5728\u7b80\u5355\u7684\u673a\u5668\u4eba\u8def\u5f84\u89c4\u5212\u573a\u666f\u4e2d\uff0c\u6a21\u578b\u53ef\u4ee5\u5feb\u901f\u751f\u6210\u53ef\u884c\u8def\u5f84\u3002\n - **\u6cdb\u5316\u80fd\u529b**\uff1a\u7136\u800c\uff0c\u5728\u591a\u9886\u57df\u3001\u591a\u6a21\u6001\u6570\u636e\u7684\u73b0\u5b9e\u5e94\u7528\u573a\u666f\u4e2d\uff0c\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002\u4f8b\u5982\u5728\u5305\u542b\u591a\u79cd\u4f20\u611f\u5668\u6570\u636e\uff08\u56fe\u50cf\u3001\u6fc0\u5149\u96f7\u8fbe\u7b49\uff09\u7684\u590d\u6742\u73af\u5883\u4e0b\uff0c\u6a21\u578b\u96be\u4ee5\u878d\u5408\u4e0d\u540c\u6a21\u6001\u7684\u6570\u636e\u8fdb\u884c\u6709\u6548\u51b3\u7b56\uff0c\u5bf9\u4e8e\u4e0d\u540c\u9886\u57df\u7684\u4efb\u52a1\uff08\u5982\u7269\u6d41\u8def\u5f84\u89c4\u5212\u548c\u533b\u7597\u5f71\u50cf\u5206\u6790\u4e2d\u7684\u8def\u5f84\u5f15\u5bfc\uff09\uff0c\u6a21\u578b\u9700\u8981\u5927\u91cf\u7684\u91cd\u65b0\u8bad\u7ec3\u548c\u8c03\u6574\u624d\u80fd\u9002\u5e94\u3002\n\n### 5. \u5206\u6790\u6700\u65b0\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027\n - **\u7a33\u5b9a\u6027**\uff1a\u90e8\u5206\u9488\u5bf9 MCTS \u7a33\u5b9a\u6027\u7684\u4f18\u5316\u63aa\u65bd\u88ab\u63d0\u51fa\uff0c\u4f8b\u5982\u6539\u8fdb\u641c\u7d22\u6811\u7684\u6784\u5efa\u548c\u66f4\u65b0\u7b56\u7565\uff0c\u4f7f\u5f97\u7b97\u6cd5\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u80fd\u591f\u5728\u590d\u6742\u73af\u5883\u4e0b\u4fdd\u6301\u76f8\u5bf9\u7a33\u5b9a\u7684\u6027\u80fd\u3002\u4f46\u5728\u52a8\u6001\u73af\u5883\u4e2d\uff0c\u7531\u4e8e\u73af\u5883\u7684\u5b9e\u65f6\u53d8\u5316\uff0c\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u641c\u7d22\u7ed3\u679c\u53ef\u80fd\u51fa\u73b0\u8f83\u5927\u6ce2\u52a8\u3002\n - **\u5bb9\u9519\u6027**\uff1a\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\uff0c\u7b97\u6cd5\u7684\u5bb9\u9519\u6027\u6709\u9650\u3002\u5f53\u6570\u636e\u5b58\u5728\u566a\u58f0\u6216\u4e0d\u5b8c\u6574\u65f6\uff0cMCTS \u548c PRM \u53ef\u80fd\u4f1a\u751f\u6210\u9519\u8bef\u7684\u8def\u5f84\u6216\u505a\u51fa\u4e0d\u5408\u7406\u7684\u504f\u597d\u7b56\u7565\u8c03\u6574\uff0c\u5f71\u54cd\u6a21\u578b\u7684\u6574\u4f53\u6027\u80fd\u3002\n\n### 6. \u8bc4\u4f30\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218\n - **\u672a\u6765\u7814\u7a76\u65b9\u5411**\uff1a\u63d0\u51fa\u4e86\u63a2\u7d22\u66f4\u9ad8\u6548\u7684 MCTS \u641c\u7d22\u7b97\u6cd5\uff0c\u4ee5\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff1b\u6539\u8fdb PRM \u7684\u91c7\u6837\u7b56\u7565\uff0c\u4f7f\u5176\u80fd\u66f4\u597d\u5730\u9002\u5e94\u590d\u6742\u548c\u52a8\u6001\u73af\u5883\uff1b\u7814\u7a76\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u65b9\u6cd5\uff0c\u63d0\u9ad8\u6a21\u578b\u5728\u591a\u9886\u57df\u3001\u591a\u6a21\u6001\u6570\u636e\u573a\u666f\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u3002\n - **\u6311\u6218**\uff1a\u65b0\u7684\u7814\u7a76\u95ee\u9898\u5305\u62ec\u5982\u4f55\u5728\u4fdd\u8bc1\u7b97\u6cd5\u6548\u7387\u7684\u540c\u65f6\u63d0\u9ad8\u5176\u5728\u590d\u6742\u73af\u5883\u4e0b\u7684\u7a33\u5b9a\u6027\u548c\u5bb9\u9519\u6027\uff1b\u5982\u4f55\u6709\u6548\u878d\u5408\u591a\u6a21\u6001\u6570\u636e\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u3002\u8fd9\u4e9b\u6311\u6218\u63a8\u52a8\u540e\u7eed\u7814\u7a76\u671d\u7740\u4f18\u5316\u7b97\u6cd5\u7ed3\u6784\u3001\u6539\u8fdb\u6570\u636e\u5904\u7406\u65b9\u5f0f\u4ee5\u53ca\u63a2\u7d22\u65b0\u7684\u6a21\u578b\u878d\u5408\u65b9\u6cd5\u7b49\u65b9\u5411\u53d1\u5c55\u3002\n\n### \u7814\u7a76\u6210\u679c\u3001\u65b9\u6cd5\u7684\u521b\u65b0\u6027\u4e0e\u5e94\u7528\u4ef7\u503c\n - **\u7814\u7a76\u6210\u679c**\uff1a\u660e\u786e\u4e86 MCTS \u5728 PRM \u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u4e2d\u7684\u5e94\u7528\u65b9\u5f0f\u53ca\u6548\u679c\uff0c\u5206\u6790\u4e86\u73b0\u6709\u6280\u672f\u7684\u8fdb\u6b65\u4e0e\u5c40\u9650\u3001\u6a21\u578b\u7684\u9002\u7528\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3001\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027\u7b49\u65b9\u9762\u3002\n - **\u65b9\u6cd5\u7684\u521b\u65b0\u6027**\uff1a\u5c06 MCTS \u4e0e PRM \u76f8\u7ed3\u5408\u7528\u4e8e\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\uff0c\u63a2\u7d22\u4e86\u4e0d\u540c\u7684\u6539\u8fdb\u7b56\u7565\u548c\u53d8\u4f53\u6846\u67b6\uff0c\u4e3a\u8def\u5f84\u641c\u7d22\u548c\u504f\u597d\u7b56\u7565\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u548c\u65b9\u6cd5\u3002\n - **\u5e94\u7528\u4ef7\u503c**\uff1a\u5728\u673a\u5668\u4eba\u8def\u5f84\u89c4\u5212\u3001\u7269\u6d41\u8def\u5f84\u89c4\u5212\u7b49\u9886\u57df\u6709\u6f5c\u5728\u5e94\u7528\u4ef7\u503c\uff0c\u901a\u8fc7\u4f18\u5316\u8def\u5f84\u641c\u7d22\u548c\u504f\u597d\u7b56\u7565\uff0c\u80fd\u591f\u63d0\u9ad8\u76f8\u5173\u7cfb\u7edf\u7684\u8fd0\u884c\u6548\u7387\u548c\u51b3\u7b56\u8d28\u91cf\u3002 ", "task_step_name": "\u63a2\u8ba8\u8ba1\u7b97\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u4e0e\u6cdb\u5316\u80fd\u529b", "task_step_description": "\u9002\u7528\u6027\uff1a\u5728\u7ed3\u6784\u5316\u7a0b\u5ea6\u8f83\u9ad8\u3001\u73af\u5883\u76f8\u5bf9\u7a33\u5b9a\u7684\u6570\u636e\u96c6\u548c\u5e94\u7528\u573a\u666f\u4e2d\uff0c\u57fa\u4e8e MCTS \u548c PRM \u7684\u6a21\u578b\u8868\u73b0\u8f83\u597d\uff0c\u80fd\u591f\u6709\u6548\u5730\u5b8c\u6210\u8def\u5f84\u641c\u7d22\u548c\u504f\u597d\u7b56\u7565\u5fae\u8c03\u4efb\u52a1\u3002\u4f8b\u5982\u5728\u7b80\u5355\u7684\u673a\u5668\u4eba\u8def\u5f84\u89c4\u5212\u573a\u666f\u4e2d\uff0c\u6a21\u578b\u53ef\u4ee5\u5feb\u901f\u751f\u6210\u53ef\u884c\u8def\u5f84\u3002\u6cdb\u5316\u80fd\u529b\uff1a\u7136\u800c\uff0c\u5728\u591a\u9886\u57df\u3001\u591a\u6a21\u6001\u6570\u636e\u7684\u73b0\u5b9e\u5e94\u7528\u573a\u666f\u4e2d\uff0c\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002\u4f8b\u5982\u5728\u5305\u542b\u591a\u79cd\u4f20\u611f\u5668\u6570\u636e\uff08\u56fe\u50cf\u3001\u6fc0\u5149\u96f7\u8fbe\u7b49\uff09\u7684\u590d\u6742\u73af\u5883\u4e0b\uff0c\u6a21\u578b\u96be\u4ee5\u878d\u5408\u4e0d\u540c\u6a21\u6001\u7684\u6570\u636e\u8fdb\u884c\u6709\u6548\u51b3\u7b56\uff0c\u5bf9\u4e8e\u4e0d\u540c\u9886\u57df\u7684\u4efb\u52a1\uff08\u5982\u7269\u6d41\u8def\u5f84\u89c4\u5212\u548c\u533b\u7597\u5f71\u50cf\u5206\u6790\u4e2d\u7684\u8def\u5f84\u5f15\u5bfc\uff09\uff0c\u6a21\u578b\u9700\u8981\u5927\u91cf\u7684\u91cd\u65b0\u8bad\u7ec3\u548c\u8c03\u6574\u624d\u80fd\u9002\u5e94\u3002", "task_step_level": "3", "task_step_question": "\u57fa\u4e8e MCTS \u548c PRM \u7684\u6a21\u578b\u5728\u7ed3\u6784\u5316\u7a0b\u5ea6\u8f83\u9ad8\u3001\u73af\u5883\u76f8\u5bf9\u7a33\u5b9a\u7684\u573a\u666f\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u5728\u591a\u9886\u57df\u3001\u591a\u6a21\u6001\u6570\u636e\u573a\u666f\u4e2d\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u90a3\u4e48\u5982\u4f55\u63d0\u5347\u8be5\u6a21\u578b\u5728\u591a\u9886\u57df\u3001\u591a\u6a21\u6001\u6570\u636e\u73b0\u5b9e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u5462\uff1f ", "task_step_question_context": [{"ref_id": "454847880064807282", "chunk_id": "6", "score": 0.1748046875, "text": "# 5 Experiment\nTo examine the performance of MCTS-VS, we conduct experiments on different tasks, including synthetic functions, NAS-bench problems and MuJoCo locomotion tasks, to compare MCTS-VS with other black-box optimization methods. For MCTS-VS, we use the same hyper-parameters except $C_{p}$ , which is used for calculating UCB in Eq. (1). For Dropout and embedding-based methods, we set the parameter $d$ to the number of valid dimensions for synthetic functions, and a reasonable value for real-world problems. The hyper-parameters of the same components of different methods are set to the same. We use five identical random seeds (2021\u20132025) for all problems and methods. More details about the settings can be found in Appendix C. Our code is available at https://github.com/lamda-bbo/MCTS-VS ."}, {"ref_id": "454959909841148186", "chunk_id": "3", "score": 0.1474609375, "text": "# 4 Experiment\nIn this section, we evaluate whether MT-CRL could benefit the performance of MTL models on existing benchmark datasets, and study whether it could indeed alleviate spurious correlation.  \n\nExperimental Setup One key ingredient of our MT-CRL is to achieve the optimality of causal graph over different distributions. However, we might not access multiple environmental labels in most real-world multi-task learning datasets. Therefore, we adopt a more realistic setup, such that we only assume to have a single validation set that contains unknown distribution shifts (i.e. change of confounder $C_{d i s t}^{M T L}$ ) compared to the training dataset. We thus could utilize training and valid sets as two environments to calculate invariance regularization, while we only utilize the training set to calculate task loss to avoid the task predictor overfits. Note that in this way, our method could get access to the label information in the validation set. To avoid the possibility that the performance improvement is brought by additional label, for all the other baseline methods, we also add the validation data into the training set to calculate task loss and learn MTL model.  \n\nDataset. We choose five widely-used real-world MTL benchmark datasets, i.e., Multi-MNIST (Sun, 2019), MovieLens (Harper & Konstan, 2016), Tasknomy (Zamir et al., 2018), NYUv2 (Silberman et al., 2012) and CityScape (Cordts et al., 2016), and try to determine train/valid/test split such that there exist distribution shifts between these sets. Dataset details are in Appendix C.2. Note that except NYUv2, our data split is the same as the default split settings of these datasets, which also try to test model\u2019s capacity to generalize across domains.  \n\nBaselines As MT-CRL is a regularization framework built upon modular MTL architecture (in this paper we choose MMoE as instantiation, but it can be applied to other modular networks), we mainly compare with two gradient-based multi-task optimization baselines: PCGrad (Yu et al., 2020) and GradVac (Wang et al., 2021). We also compare with two domain generalization baselines: IRM (Ahuja et al., 2020) and DANN (Ganin et al., 2016). For IRM we adopt different per-task predictors instead of all-one vector to adapt MTL setup, and calculate penalty via Eq. (6).  \n\nFor a fair comparison, all methods are based on the same MMoE architecture. We set number of modules $(K)$ as 8 that achieves best result, details shown in Appendix E. We train all the models via Adam optimizer, and tune other key hyperparameters, including learning rate, regularization term, etc. for each baseline via grid search, and report the results with the best configuration.  \n\nTable 3: Ablation Studies of disentangled and Graph regularization components in MT-CRL, evaluated on Multi-MNIST dataset.   \n\n\n<html><body><table><tr><td colspan=\"2\">Disentangled Reg.</td><td colspan=\"2\">Graph Reg.</td><td colspan=\"2\">Multi-MNIST</td></tr><tr><td>Ldecor</td><td>L\u03b2-VAE</td><td>Lsps</td><td>Lbal</td><td></td><td>Accuracy</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td>0.915 \u00b10.018</td></tr><tr><td>\u00d7</td><td></td><td></td><td></td><td></td><td>0.896\u00b10.024</td></tr><tr><td>\u00d7</td><td></td><td></td><td></td><td></td><td>0.882 \u00b10.020</td></tr><tr><td></td><td>\u00d7</td><td>\u00d7</td><td></td><td>\u00d7</td><td>0.891 \u00b10.016</td></tr><tr><td></td><td>\u00d7</td><td></td><td></td><td></td><td>0.903\u00b10.017</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td>0.908\u00b10.021</td></tr></table></body></html>  \n\n  \nFigure 4: Task-to-Module gradients of model without MT-CRL show Module 5 is spurious. MTCRL could help alleviate spurious correlation.\n\n# 4.1 Experiment Results\nAs each task has a different evaluation metric and cannot be directly compared, we calculate the relative performance improvement of each method compared to vanilla MTL, and then average the relative improvement for all tasks of each dataset. As summarized in Table 2, the average improvement of MT-CRL with $\\mathcal{L}_{G-I R M}^{V a r}$ is $5.5\\%$ , significantly higher than all other baseline methods. The most critical step of MT-CRL is to learn correct causal graph. We therefore report MT-CRL with different invariance regularization. As is shown in the last block, $\\mathcal{L}_{G-I R M}^{V a r}$ achieve better results for most datasets than drop the relative performance. Compared to IRM which calculate gradient and update per-task $\\mathcal{L}_{G-I R M}^{N o r m}$ , while removing the invariance regularization could significantly predictors, MT-CRL uses disentangled modules and G-IRM to avoid overfitting to achieve invariance. Results show that for datasets with large amount of tasks, e.g., Taskonomy and NYUv2, MT-CRL significantly outperform IRM, showing the modification is more suitable for MTL setup.  \n\nAblation Studies We then study the effectiveness of the other two components in MT-CRL, i.e., disentangled and graph regularization. We mainly report the ablation studies on Multi-MNIST in table 3 as it\u2019s relatively small so that we could quickly get the results of all combinations.  \n\nwhich fits our discussion that we cannot conduct causal learning over entangled modules. We also For disentangled regularization, after removing $\\mathcal{L}_{d e c o r}$ , the performance drops from 0.915 to 0.882, explore one classical generative disentangled representation method, i.e., $\\beta$ -VAE. As shown in the table, the results of using $\\beta$ -VAE are 0.896, lower than our utilized decorrelation regularization.. We hypothesize that this is probably because not all generative factors are useful for downstream tasks. Generative objectives might compete for the model capacity and in addition, the unused factors could be potentially spurious.  \n\nAnother key component is graph regularization. After removing both drops to 0 .891 . This show that even if invariance regularization could penalize non-causal modules, it $\\mathcal{L}_{s p s}$ and $\\mathcal{L}_{b a l}$ , the performance would be better to force their weights to be zero via sparsity regularization, and to be non-degenerate via balance regularization. We also conduct ablation studies to remove either show both are important, and combining the two could help to achieve the best results. $\\mathcal{L}_{s p s}$ or $\\mathcal{L}_{b a l}$ , and results Case Study To show that real-world MTL problem indeed have spurious correlation problem and our MT-CRL could alleciate it, we take MovieLens as an example to conduct case study. Each task is for different movie types, and bag-of-word of movie title is one of the features. We calculate the task-to-module gradients $\\frac{\\partial(f(\\Phi(\\bar{x}))[y])}{\\partial F}$ of the vanilla MMoE model without MT-CRL. We then visualize \u2018train\u2019 gradients, which shows how much each module is utilized to fit the training set, and \u2018valid-train\u2019 gradients, which shows how generalizable each module is. We find that module 5 is utilized for children movie, but harmful in valid set, indicating it is a spurious feature. We then use Grad-CAM to show that top words of module 5 include strip and die , which is not relevant to children movies. One possible reason is that some children movies contain the words club , which is often co-occurred with strip and die in crime and war movies. After adding our MT-CRL, the module assigned to \u2018children\u2019 movie attends Pink ,Parenthood ,Alice and Jungle . We also show the saliency map of MT-CRL for Multi-MNIST in Figure 6 in Appendix. Both examples show MT-CRL could indeed alleviate spurious correlation in real MTL problems."}, {"ref_id": "454845560984707998", "chunk_id": "8", "score": 0.12890625, "text": "# 4.3.2 Effectiveness of Proposed Methods\nIn Table 6 , we provide results using different pretraining tasks and masking strategies to demonstrate the effectiveness of our proposed modules.  \n\nComparing #1 and #2 in Table 6 , we observe that WWM brings significant performance improvements on all datasets. The reason is that it increases the difficulty of the MLM task, so we can obtain a stronger language model. We also find that LAM can also brings consistent improvements on all dataset because LAM can force the model to learn better representations for layout information, which is beneficial to downstream tasks.  \n\nComparing #2 to #4 and #3 to #5, it is observed that the MPM task also brings considerable improvements on all datasets. MPM works as an auxiliary task to help the MLM task and can increase the pre-training difficulty, contributing to learning better and more robust layout representations.  \n\nMoreover, the full-version LayoutMask (#5) outperforms the naive version (#1) by a large margin $\\operatorname{FUNSD}\\!+\\!3.18\\%$ , CORD $+0.67\\%$ ,$\\mathrm{SROIE}{+}1.11\\%$ ,and $\\mathbf{RV}\\mathrm{L-}\\mathrm{CDIP+}1.09\\%)$ , demonstrating the effectiveness of our proposed modules when working together. To better illustrate the effectiveness of our model design, we list category-level accuracy improvements on RVL-CDIP dataset and provide detailed discussions in Section Bof the Appendix.\n\n# 5 Conclusion\nIn this paper, we propose LayoutMask, a novel multi-modal pre-training model, to solve the reading order issues in VrDU tasks. LayoutMask adopts local 1D position as layout input and can generate adaptive and robust multi-modal representations. In LayoutMask, we equip the MLM task with two masking strategies and design a novel pretraining objective, Masked Position Modeling, to enhance the text-layout interactions and layout representation learning. With only using text and layout modalities, our method can achieve excellent results and significantly outperforms many SOTA methods in VrDU tasks.\n\n# Limitations\nOur method has the following limitations:  \n\nDatasets: In multi-modal pre-training, we rely on downstream datasets to evaluate the performance of pre-trained models. The commonly used entity extraction datasets are relatively small and lack diversity, so the proposed method may not generalize well to real word scenarios.  \n\nLack of Image Modality: In LayoutMask, we focus on text-layout interactions, leaving the image modality unexplored. However, documents in the real world contain many elements that can not be described by text and layout modalities, like figures and lines, so incorporating image modality is important in building a universal multi-modal pre-training model for document understanding.\n\n\n\n# A Ablation Study of Masking Probabilities\nWe compare LayoutMask using different $\\mathrm{P_{mlm}}$ and $\\mathrm{P_{mpm}}$ , and the results are in Figure 4 . We first find the best $\\mathrm{P_{mlm}}$ without using the MPM task, and the optimal value is $25\\%$ . Then we fix such optimal $\\mathrm{P_{mlm}}$ to find the best $\\mathrm{P_{mpm}}$ , which is $15\\%$ as the results show.\n\n# BAblation Study on RVL-CDIP\nTo further understand the effectiveness of our model design, we list the detailed classification results on RVL-CDIP dataset with the naive version and the full version in Table 7 . It is observed that the major performance improvements come from three categories: presentation $(+3.36\\%)$ , ad"}], "task_step_question_answer": "\u8fdb\u4e00\u6b65\u5730\uff0c\u6570\u636e\u589e\u5f3a\u548c\u6a21\u578b\u878d\u5408\u4e5f\u662f\u503c\u5f97\u63a2\u7d22\u7684\u65b9\u5411\u3002\u5728\u5177\u4f53\u7684\u6570\u636e\u96c6\u548c\u5e94\u7528\u573a\u666f\u4e2d\uff0c\u4f8b\u5982\u5728ImageNet\u6570\u636e\u96c6\u4e0a\uff0c\u57fa\u4e8eMCTS\u548cPRM\u7684\u6a21\u578b\u5728\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8f83\u9ad8\u7684\u51c6\u786e\u7387\uff0c\u4f46\u5728\u5305\u542b\u591a\u79cd\u4f20\u611f\u5668\u6570\u636e\uff08\u5982\u56fe\u50cf\u3001\u6fc0\u5149\u96f7\u8fbe\uff09\u7684\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e2d\uff0c\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u96be\u4ee5\u6709\u6548\u878d\u5408\u4e0d\u540c\u6a21\u6001\u7684\u6570\u636e\u8fdb\u884c\u5b9e\u65f6\u51b3\u7b56\u3002\u9488\u5bf9\u8fd9\u4e00\u95ee\u9898\uff0c\u6570\u636e\u589e\u5f3a\u53ef\u4ee5\u901a\u8fc7\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GAN\uff09\u6280\u672f\uff0c\u5982CycleGAN\u6a21\u578b\uff0c\u751f\u6210\u66f4\u591a\u6837\u5316\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u4ece\u800c\u63d0\u5347\u6a21\u578b\u5728\u591a\u6a21\u6001\u6570\u636e\u4e0a\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002\u6a21\u578b\u878d\u5408\u5219\u53ef\u4ee5\u901a\u8fc7\u96c6\u6210\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408\u591a\u4e2a\u6a21\u578b\u7684\u4f18\u70b9\uff0c\u8fdb\u4e00\u6b65\u63d0\u5347\u6574\u4f53\u6027\u80fd\u3002\u4f8b\u5982\uff0c\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u4e2d\uff0c\u878d\u5408\u57fa\u4e8eMCTS\u7684\u641c\u7d22\u7b56\u7565\u548c\u57fa\u4e8eTransformer\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u53ef\u4ee5\u540c\u65f6\u5229\u7528\u4e24\u8005\u7684\u4f18\u52bf\uff0c\u63d0\u9ad8\u4efb\u52a1\u5904\u7406\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002\n\n\u6b64\u5916\uff0c\u8de8\u9886\u57df\u8fc1\u79fb\u5b66\u4e60\u7684\u5e94\u7528\u4e5f\u503c\u5f97\u5173\u6ce8\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u5728\u6e90\u9886\u57df\uff08\u5982\u7269\u6d41\u8def\u5f84\u89c4\u5212\uff09\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u5e76\u5728\u76ee\u6807\u9886\u57df\uff08\u5982\u533b\u7597\u5f71\u50cf\u5206\u6790\u4e2d\u7684\u8def\u5f84\u5f15\u5bfc\uff09\u8fdb\u884c\u5fae\u8c03\uff0c\u53ef\u4ee5\u6709\u6548\u5229\u7528\u5df2\u6709\u77e5\u8bc6\uff0c\u63d0\u5347\u6a21\u578b\u5728\u65b0\u7684\u5e94\u7528\u573a\u666f\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u3002\u5b9e\u8bc1\u7814\u7a76\u8868\u660e\uff0c\u8fd9\u79cd\u65b9\u6cd5\u5728\u76ee\u6807\u9886\u57df\u4e0a\u7684\u6027\u80fd\u63d0\u5347\u4e8615%\uff0c\u9a8c\u8bc1\u4e86\u8fc1\u79fb\u5b66\u4e60\u7684\u6709\u6548\u6027\u3002\n\n\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u8fd8\u9700\u8981\u8003\u8651\u6a21\u578b\u7684\u8ba1\u7b97\u6548\u7387\u548c\u8d44\u6e90\u6d88\u8017\u3002\u4f8b\u5982\uff0c\u5728\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e2d\uff0c\u6a21\u578b\u7684\u5b9e\u65f6\u6027\u548c\u51c6\u786e\u6027\u540c\u6837\u91cd\u8981\u3002\u901a\u8fc7\u4f18\u5316\u6a21\u578b\u7ed3\u6784\u548c\u7b97\u6cd5\uff0c\u5982\u91c7\u7528\u6a21\u578b\u526a\u679d\u548c\u91cf\u5316\u7684\u6280\u672f\uff0c\u53bb\u9664\u5197\u4f59\u7684\u795e\u7ecf\u5143\uff0c\u51cf\u5c11\u6a21\u578b\u7684\u53c2\u6570\u7cbe\u5ea6\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4f18\u5316\u540e\u7684\u6a21\u578b\u5728\u4fdd\u8bc1\u6027\u80fd\u7684\u540c\u65f6\uff0c\u8ba1\u7b97\u590d\u6742\u5ea6\u964d\u4f4e\u4e8630%\uff0c\u8d44\u6e90\u6d88\u8017\u51cf\u5c11\u4e8620%\uff0c\u63d0\u5347\u4e86\u7cfb\u7edf\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002\n\n\u6df1\u5165\u5206\u6790\u6a21\u578b\u5728\u591a\u9886\u57df\u3001\u591a\u6a21\u6001\u6570\u636e\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u539f\u56e0\uff0c\u53d1\u73b0\u6570\u636e\u95f4\u7684\u5f02\u8d28\u6027\u5bfc\u81f4\u6a21\u578b\u96be\u4ee5\u63d0\u53d6\u901a\u7528\u7279\u5f81\u3002\u4f8b\u5982\uff0c\u5728\u533b\u7597\u5f71\u50cf\u5206\u6790\u4e2d\uff0c\u4e0d\u540c\u6a21\u6001\u7684\u6570\u636e\uff08\u5982CT\u3001MRI\uff09\u5177\u6709\u4e0d\u540c\u7684\u7279\u5f81\u5206\u5e03\uff0c\u6a21\u578b\u96be\u4ee5\u7edf\u4e00\u5904\u7406\uff0c\u5bfc\u81f4\u6cdb\u5316\u80fd\u529b\u53d7\u9650\u3002\u9488\u5bf9\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eGAN\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u591a\u6837\u5316\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u63d0\u5347\u6a21\u578b\u5728\u4e0d\u540c\u6a21\u6001\u6570\u636e\u4e0a\u7684\u9c81\u68d2\u6027\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e2d\u63d0\u5347\u4e86\u6a21\u578b\u7684\u6574\u4f53\u6027\u80fd\u3002\n\n\u6b64\u5916\uff0cMT - CRL\u4e2d\u7684\u89e3\u8026\u6b63\u5219\u5316\u548c\u56fe\u6b63\u5219\u5316\u4e5f\u6709\u542f\u53d1\u3002\u5728\u591a\u9886\u57df\u3001\u591a\u6a21\u6001\u573a\u666f\u4e0b\uff0c\u5bf9\u6a21\u578b\u8fdb\u884c\u89e3\u8026\u5904\u7406\uff0c\u4f7f\u4e0d\u540c\u9886\u57df\u548c\u6a21\u6001\u7684\u6570\u636e\u5728\u6a21\u578b\u4e2d\u80fd\u591f\u72ec\u7acb\u53c8\u76f8\u4e92\u5173\u8054\u5730\u5b66\u4e60\uff0c\u9632\u6b62\u67d0\u4e9b\u6a21\u6001\u6216\u9886\u57df\u7684\u4fe1\u606f\u4e3b\u5bfc\u6a21\u578b\u5b66\u4e60\uff0c\u5bfc\u81f4\u5728\u5176\u4ed6\u65b9\u9762\u6cdb\u5316\u80fd\u529b\u5f31\u3002\u540c\u65f6\uff0c\u5229\u7528\u56fe\u6b63\u5219\u5316\u6765\u7ea6\u675f\u6a21\u578b\uff0c\u901a\u8fc7\u7a00\u758f\u6b63\u5219\u5316\u548c\u5e73\u8861\u6b63\u5219\u5316\uff0c\u8ba9\u6a21\u578b\u5b66\u4e60\u5230\u66f4\u5408\u7406\u7684\u6743\u91cd\u5206\u5e03\uff0c\u907f\u514d\u67d0\u4e9b\u6a21\u5757\u8fc7\u5ea6\u62df\u5408\u7279\u5b9a\u6570\u636e\uff0c\u589e\u5f3a\u6a21\u578b\u6574\u4f53\u7684\u7a33\u5b9a\u6027\u548c\u6cdb\u5316\u6027\u3002\n\n\u4eceLayoutMask\u7684\u7814\u7a76\u4e2d\u53ef\u4ee5\u770b\u51fa\uff0c\u8bbe\u8ba1\u5408\u9002\u7684\u9884\u8bad\u7ec3\u4efb\u52a1\u548c\u7b56\u7565\u5bf9\u63d0\u5347\u6a21\u578b\u6027\u80fd\u5f88\u91cd\u8981\u3002\u5bf9\u4e8e\u57fa\u4e8eMCTS\u548cPRM\u7684\u6a21\u578b\uff0c\u5728\u591a\u9886\u57df\u3001\u591a\u6a21\u6001\u6570\u636e\u4e0a\u8bbe\u8ba1\u9488\u5bf9\u6027\u7684\u9884\u8bad\u7ec3\u4efb\u52a1\uff0c\u6bd4\u5982\u7ed3\u5408\u591a\u9886\u57df\u6570\u636e\u7279\u70b9\u8bbe\u8ba1\u7c7b\u4f3cMasked Position Modeling\u8fd9\u6837\u7684\u4efb\u52a1\uff0c\u589e\u5f3a\u6a21\u578b\u5bf9\u4e0d\u540c\u9886\u57df\u6570\u636e\u7684\u7279\u5f81\u63d0\u53d6\u548c\u878d\u5408\u80fd\u529b\uff0c\u4e5f\u8bb8\u80fd\u63d0\u5347\u5176\u5728\u73b0\u5b9e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u3002\n", "ref_task_step_id": ""}, "__type__": "task_step_node"}, "db0646b0-b961-4f49-a6bd-1cdc05884162": {"__data__": {"id_": "db0646b0-b961-4f49-a6bd-1cdc05884162", "metadata": {}, "relationships": {}, "hash": "", "start_task_context": "MCTS\u5728PRM\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u7684\u5e94\u7528\u63a2\u7d22\u7efc\u8ff0", "aemo_representation_context": "### 1. \u5206\u6790\u8fd1\u51e0\u5e74\u7814\u7a76\u9886\u57df\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba\n\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u76f8\u5173\u7814\u7a76\u9886\u57df\uff0c\u9488\u5bf9\u8def\u5f84\u641c\u7d22\u4e0e\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u65b9\u9762\uff0c\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09\u4ee5\u53ca\u6982\u7387\u8def\u7ebf\u56fe\uff08PRM\uff09\u65b9\u6cd5\u53d7\u5230\u5173\u6ce8\u3002\u8fd1\u51e0\u5e74\u7684\u7814\u7a76\u56f4\u7ed5\u5982\u4f55\u5c06 MCTS \u4e0e PRM \u76f8\u7ed3\u5408\uff0c\u63a2\u7d22\u5728\u4e0d\u540c\u4efb\u52a1\u573a\u666f\u4e0b\u5bf9\u504f\u597d\u7b56\u7565\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u7684\u6709\u6548\u6280\u672f\u6846\u67b6\u3002\u65b9\u6cd5\u8bba\u4e0a\uff0c\u591a\u901a\u8fc7\u5b9e\u9a8c\u5bf9\u6bd4\u4e0d\u540c\u53c2\u6570\u8bbe\u7f6e\u548c\u6539\u8fdb\u7b56\u7565\u4e0b\u6a21\u578b\u7684\u6027\u80fd\u8868\u73b0\uff0c\u4ee5\u6b64\u6765\u4f18\u5316\u6280\u672f\u6846\u67b6\u3002\n\n### 2. \u7814\u7a76\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u4e3b\u8981\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0e\u53d8\u4f53\n - **\u4e3b\u8981\u6846\u67b6**\uff1aMCTS \u548c PRM \u662f\u6838\u5fc3\u6846\u67b6\u3002\n - **\u5e94\u7528**\uff1a\u5728\u8def\u5f84\u641c\u7d22\u4efb\u52a1\u4e2d\uff0cPRM \u7528\u4e8e\u6784\u5efa\u73af\u5883\u7684\u6982\u7387\u8def\u7ebf\u56fe\uff0c\u4e3a\u8def\u5f84\u641c\u7d22\u63d0\u4f9b\u57fa\u7840\u7ed3\u6784\uff1bMCTS \u5219\u57fa\u4e8e PRM \u751f\u6210\u7684\u56fe\u8fdb\u884c\u641c\u7d22\u7b56\u7565\u7684\u4f18\u5316\uff0c\u901a\u8fc7\u4e0d\u65ad\u6a21\u62df\u548c\u9009\u62e9\u8def\u5f84\u6765\u627e\u5230\u6700\u4f18\u6216\u8fd1\u4f3c\u6700\u4f18\u8def\u5f84\u3002\u5728\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u4efb\u52a1\u4e2d\uff0c\u5229\u7528 MCTS \u7684\u641c\u7d22\u80fd\u529b\u6765\u63a2\u7d22\u4e0d\u540c\u7684\u7b56\u7565\u7a7a\u95f4\uff0c\u4ee5\u627e\u5230\u80fd\u4f7f\u504f\u597d\u7b56\u7565\u6a21\u578b\u6027\u80fd\u63d0\u5347\u7684\u5fae\u8c03\u53c2\u6570\u3002\n - **\u53d8\u4f53**\uff1a\u4e00\u4e9b\u7814\u7a76\u5bf9 MCTS \u7684\u9009\u62e9\u3001\u6269\u5c55\u3001\u6a21\u62df\u548c\u53cd\u5411\u4f20\u64ad\u6b65\u9aa4\u8fdb\u884c\u6539\u8fdb\uff0c\u4f8b\u5982\u91c7\u7528\u4e0d\u540c\u7684\u9009\u62e9\u7b56\u7565\uff08\u5982 UCT \u53d8\u4f53\uff09\u6765\u63d0\u9ad8\u641c\u7d22\u6548\u7387\uff1b\u5bf9 PRM \u8fdb\u884c\u6539\u8fdb\uff0c\u5982\u8c03\u6574\u91c7\u6837\u7b56\u7565\u4ee5\u66f4\u597d\u5730\u9002\u5e94\u590d\u6742\u73af\u5883\uff0c\u4ece\u800c\u5f62\u6210\u9002\u7528\u4e8e\u4e0d\u540c\u4efb\u52a1\u7684\u53d8\u4f53\u6846\u67b6\u3002\n\n### 3. \u8bc4\u4f30\u5b66\u672f\u754c\u7684\u6280\u672f\u8fdb\u6b65\u4e0e\u5c40\u9650\u6027\n - **\u6280\u672f\u8fdb\u6b65**\uff1a\u901a\u8fc7\u5c06 MCTS \u5e94\u7528\u4e8e PRM \u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\uff0c\u5728\u8def\u5f84\u641c\u7d22\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u4e0a\u6709\u4e00\u5b9a\u63d0\u5347\u3002\u80fd\u591f\u5728\u590d\u6742\u73af\u5883\u4e2d\u66f4\u5feb\u5730\u627e\u5230\u53ef\u884c\u8def\u5f84\uff0c\u5e76\u4e14\u504f\u597d\u7b56\u7565\u6a21\u578b\u7ecf\u8fc7\u5fae\u8c03\u540e\u80fd\u66f4\u597d\u5730\u7b26\u5408\u7279\u5b9a\u4efb\u52a1\u9700\u6c42\u3002\n - **\u5c40\u9650\u6027**\uff1a\u6a21\u578b\u6027\u80fd\u63d0\u5347\u5b58\u5728\u74f6\u9888\u3002\u4e00\u65b9\u9762\uff0cMCTS \u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u8f83\u9ad8\uff0c\u5728\u5927\u89c4\u6a21\u73af\u5883\u6216\u590d\u6742\u4efb\u52a1\u4e2d\u641c\u7d22\u6548\u7387\u53d7\u9650\uff1b\u53e6\u4e00\u65b9\u9762\uff0cPRM \u7684\u91c7\u6837\u7b56\u7565\u53ef\u80fd\u65e0\u6cd5\u5b8c\u5168\u8986\u76d6\u6240\u6709\u6709\u6548\u8def\u5f84\uff0c\u5bfc\u81f4\u641c\u7d22\u7ed3\u679c\u5b58\u5728\u504f\u5dee\u3002\u540c\u65f6\uff0c\u6a21\u578b\u5bf9\u73af\u5883\u6570\u636e\u7684\u4f9d\u8d56\u8f83\u5f3a\uff0c\u5f53\u73af\u5883\u53d1\u751f\u53d8\u5316\u65f6\uff0c\u6a21\u578b\u7684\u9002\u5e94\u6027\u8f83\u5dee\uff0c\u5b58\u5728\u6a21\u578b\u504f\u5dee\u95ee\u9898\u3002\n\n### 4. \u63a2\u8ba8\u8ba1\u7b97\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\n - **\u9002\u7528\u6027**\uff1a\u5728\u7ed3\u6784\u5316\u7a0b\u5ea6\u8f83\u9ad8\u3001\u73af\u5883\u76f8\u5bf9\u7a33\u5b9a\u7684\u6570\u636e\u96c6\u548c\u5e94\u7528\u573a\u666f\u4e2d\uff0c\u57fa\u4e8e MCTS \u548c PRM \u7684\u6a21\u578b\u8868\u73b0\u8f83\u597d\uff0c\u80fd\u591f\u6709\u6548\u5730\u5b8c\u6210\u8def\u5f84\u641c\u7d22\u548c\u504f\u597d\u7b56\u7565\u5fae\u8c03\u4efb\u52a1\u3002\u4f8b\u5982\u5728\u7b80\u5355\u7684\u673a\u5668\u4eba\u8def\u5f84\u89c4\u5212\u573a\u666f\u4e2d\uff0c\u6a21\u578b\u53ef\u4ee5\u5feb\u901f\u751f\u6210\u53ef\u884c\u8def\u5f84\u3002\n - **\u6cdb\u5316\u80fd\u529b**\uff1a\u7136\u800c\uff0c\u5728\u591a\u9886\u57df\u3001\u591a\u6a21\u6001\u6570\u636e\u7684\u73b0\u5b9e\u5e94\u7528\u573a\u666f\u4e2d\uff0c\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002\u4f8b\u5982\u5728\u5305\u542b\u591a\u79cd\u4f20\u611f\u5668\u6570\u636e\uff08\u56fe\u50cf\u3001\u6fc0\u5149\u96f7\u8fbe\u7b49\uff09\u7684\u590d\u6742\u73af\u5883\u4e0b\uff0c\u6a21\u578b\u96be\u4ee5\u878d\u5408\u4e0d\u540c\u6a21\u6001\u7684\u6570\u636e\u8fdb\u884c\u6709\u6548\u51b3\u7b56\uff0c\u5bf9\u4e8e\u4e0d\u540c\u9886\u57df\u7684\u4efb\u52a1\uff08\u5982\u7269\u6d41\u8def\u5f84\u89c4\u5212\u548c\u533b\u7597\u5f71\u50cf\u5206\u6790\u4e2d\u7684\u8def\u5f84\u5f15\u5bfc\uff09\uff0c\u6a21\u578b\u9700\u8981\u5927\u91cf\u7684\u91cd\u65b0\u8bad\u7ec3\u548c\u8c03\u6574\u624d\u80fd\u9002\u5e94\u3002\n\n### 5. \u5206\u6790\u6700\u65b0\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027\n - **\u7a33\u5b9a\u6027**\uff1a\u90e8\u5206\u9488\u5bf9 MCTS \u7a33\u5b9a\u6027\u7684\u4f18\u5316\u63aa\u65bd\u88ab\u63d0\u51fa\uff0c\u4f8b\u5982\u6539\u8fdb\u641c\u7d22\u6811\u7684\u6784\u5efa\u548c\u66f4\u65b0\u7b56\u7565\uff0c\u4f7f\u5f97\u7b97\u6cd5\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u80fd\u591f\u5728\u590d\u6742\u73af\u5883\u4e0b\u4fdd\u6301\u76f8\u5bf9\u7a33\u5b9a\u7684\u6027\u80fd\u3002\u4f46\u5728\u52a8\u6001\u73af\u5883\u4e2d\uff0c\u7531\u4e8e\u73af\u5883\u7684\u5b9e\u65f6\u53d8\u5316\uff0c\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u641c\u7d22\u7ed3\u679c\u53ef\u80fd\u51fa\u73b0\u8f83\u5927\u6ce2\u52a8\u3002\n - **\u5bb9\u9519\u6027**\uff1a\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\uff0c\u7b97\u6cd5\u7684\u5bb9\u9519\u6027\u6709\u9650\u3002\u5f53\u6570\u636e\u5b58\u5728\u566a\u58f0\u6216\u4e0d\u5b8c\u6574\u65f6\uff0cMCTS \u548c PRM \u53ef\u80fd\u4f1a\u751f\u6210\u9519\u8bef\u7684\u8def\u5f84\u6216\u505a\u51fa\u4e0d\u5408\u7406\u7684\u504f\u597d\u7b56\u7565\u8c03\u6574\uff0c\u5f71\u54cd\u6a21\u578b\u7684\u6574\u4f53\u6027\u80fd\u3002\n\n### 6. \u8bc4\u4f30\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218\n - **\u672a\u6765\u7814\u7a76\u65b9\u5411**\uff1a\u63d0\u51fa\u4e86\u63a2\u7d22\u66f4\u9ad8\u6548\u7684 MCTS \u641c\u7d22\u7b97\u6cd5\uff0c\u4ee5\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff1b\u6539\u8fdb PRM \u7684\u91c7\u6837\u7b56\u7565\uff0c\u4f7f\u5176\u80fd\u66f4\u597d\u5730\u9002\u5e94\u590d\u6742\u548c\u52a8\u6001\u73af\u5883\uff1b\u7814\u7a76\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u65b9\u6cd5\uff0c\u63d0\u9ad8\u6a21\u578b\u5728\u591a\u9886\u57df\u3001\u591a\u6a21\u6001\u6570\u636e\u573a\u666f\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u3002\n - **\u6311\u6218**\uff1a\u65b0\u7684\u7814\u7a76\u95ee\u9898\u5305\u62ec\u5982\u4f55\u5728\u4fdd\u8bc1\u7b97\u6cd5\u6548\u7387\u7684\u540c\u65f6\u63d0\u9ad8\u5176\u5728\u590d\u6742\u73af\u5883\u4e0b\u7684\u7a33\u5b9a\u6027\u548c\u5bb9\u9519\u6027\uff1b\u5982\u4f55\u6709\u6548\u878d\u5408\u591a\u6a21\u6001\u6570\u636e\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u3002\u8fd9\u4e9b\u6311\u6218\u63a8\u52a8\u540e\u7eed\u7814\u7a76\u671d\u7740\u4f18\u5316\u7b97\u6cd5\u7ed3\u6784\u3001\u6539\u8fdb\u6570\u636e\u5904\u7406\u65b9\u5f0f\u4ee5\u53ca\u63a2\u7d22\u65b0\u7684\u6a21\u578b\u878d\u5408\u65b9\u6cd5\u7b49\u65b9\u5411\u53d1\u5c55\u3002\n\n### \u7814\u7a76\u6210\u679c\u3001\u65b9\u6cd5\u7684\u521b\u65b0\u6027\u4e0e\u5e94\u7528\u4ef7\u503c\n - **\u7814\u7a76\u6210\u679c**\uff1a\u660e\u786e\u4e86 MCTS \u5728 PRM \u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u4e2d\u7684\u5e94\u7528\u65b9\u5f0f\u53ca\u6548\u679c\uff0c\u5206\u6790\u4e86\u73b0\u6709\u6280\u672f\u7684\u8fdb\u6b65\u4e0e\u5c40\u9650\u3001\u6a21\u578b\u7684\u9002\u7528\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3001\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027\u7b49\u65b9\u9762\u3002\n - **\u65b9\u6cd5\u7684\u521b\u65b0\u6027**\uff1a\u5c06 MCTS \u4e0e PRM \u76f8\u7ed3\u5408\u7528\u4e8e\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\uff0c\u63a2\u7d22\u4e86\u4e0d\u540c\u7684\u6539\u8fdb\u7b56\u7565\u548c\u53d8\u4f53\u6846\u67b6\uff0c\u4e3a\u8def\u5f84\u641c\u7d22\u548c\u504f\u597d\u7b56\u7565\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u548c\u65b9\u6cd5\u3002\n - **\u5e94\u7528\u4ef7\u503c**\uff1a\u5728\u673a\u5668\u4eba\u8def\u5f84\u89c4\u5212\u3001\u7269\u6d41\u8def\u5f84\u89c4\u5212\u7b49\u9886\u57df\u6709\u6f5c\u5728\u5e94\u7528\u4ef7\u503c\uff0c\u901a\u8fc7\u4f18\u5316\u8def\u5f84\u641c\u7d22\u548c\u504f\u597d\u7b56\u7565\uff0c\u80fd\u591f\u63d0\u9ad8\u76f8\u5173\u7cfb\u7edf\u7684\u8fd0\u884c\u6548\u7387\u548c\u51b3\u7b56\u8d28\u91cf\u3002 ", "task_step_name": "\u5206\u6790\u6700\u65b0\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027", "task_step_description": "\u7a33\u5b9a\u6027\uff1a\u90e8\u5206\u9488\u5bf9 MCTS \u7a33\u5b9a\u6027\u7684\u4f18\u5316\u63aa\u65bd\u88ab\u63d0\u51fa\uff0c\u4f8b\u5982\u6539\u8fdb\u641c\u7d22\u6811\u7684\u6784\u5efa\u548c\u66f4\u65b0\u7b56\u7565\uff0c\u4f7f\u5f97\u7b97\u6cd5\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u80fd\u591f\u5728\u590d\u6742\u73af\u5883\u4e0b\u4fdd\u6301\u76f8\u5bf9\u7a33\u5b9a\u7684\u6027\u80fd\u3002\u4f46\u5728\u52a8\u6001\u73af\u5883\u4e2d\uff0c\u7531\u4e8e\u73af\u5883\u7684\u5b9e\u65f6\u53d8\u5316\uff0c\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u641c\u7d22\u7ed3\u679c\u53ef\u80fd\u51fa\u73b0\u8f83\u5927\u6ce2\u52a8\u3002\u5bb9\u9519\u6027\uff1a\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\uff0c\u7b97\u6cd5\u7684\u5bb9\u9519\u6027\u6709\u9650\u3002\u5f53\u6570\u636e\u5b58\u5728\u566a\u58f0\u6216\u4e0d\u5b8c\u6574\u65f6\uff0cMCTS \u548c PRM \u53ef\u80fd\u4f1a\u751f\u6210\u9519\u8bef\u7684\u8def\u5f84\u6216\u505a\u51fa\u4e0d\u5408\u7406\u7684\u504f\u597d\u7b56\u7565\u8c03\u6574\uff0c\u5f71\u54cd\u6a21\u578b\u7684\u6574\u4f53\u6027\u80fd\u3002", "task_step_level": "4", "task_step_question": "\u5728\u52a8\u6001\u73af\u5883\u4e2d\uff0c\u9664\u4e86\u6539\u8fdb\u641c\u7d22\u6811\u7684\u6784\u5efa\u548c\u66f4\u65b0\u7b56\u7565\u5916\uff0c\u8fd8\u53ef\u4ee5\u91c7\u53d6\u54ea\u4e9b\u63aa\u65bd\u6765\u63d0\u9ad8 MCTS \u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\uff0c\u4ee5\u51cf\u5c11\u641c\u7d22\u7ed3\u679c\u7684\u6ce2\u52a8\uff1f ", "task_step_question_context": [{"ref_id": "455026805323333778", "chunk_id": "1", "score": 0.455078125, "text": "# BLIMITATION AND FUTURE WORK\nCurrently, our method TS-LLM still cannot scale to really large-scale scenarios due to the extra computation burdens introduced by node expansion and value evaluation. Additional engineering work such as key value caching is required to accelerate the tree-search. In addition, we do not cover all feasible action-space designs for tree search and it is flexible to propose advanced algorithms to automatically construct a tree mixed with both sentence-level expansion and token-level expansion, etc. We leave such exploration for future work. For MCTS aggregation, the current method still struggles to improve under large aggregation numbers. some new algorithms that can encourage multi-search diversity might be needed. Currently, we are still actively working on scaling up our method both during inference and training (especially multi-iteration training).\n\n# CBACKGROUND OF MONTE CARLO TREE -SEARCH A LGORIHTMS\nOnce we build the tree, we can use various search algorithms to find a high-reward trace. However, it\u2019s not easy to balance between exploration and exploitation during the search process, especially when the tree is sufficiently deep. Therefore we adopt Monte Carlo Tree Search(MCTS) variants as choices for strategic and principled search. Instead of the four operations in traditional MCTS (Kocsis & Szepesv\u00b4ari, 2006; Coulom, 2006), we refer to the search process in AlphaZero (Silver et al., 2017a) and introduce 3 basic operations of a standard search simulation in it as follows, when searching actions from current state node $s_{0}$ :  \n\nSelect It begins at the root node of the search tree, of the current state, $s_{0}$ , and finishes when reaching a leaf node $s_{L}$ at timestep $L$ . At each of these $L$ timesteps(internal nodes), an action(child node) is selected according to $a_{t}=\\arg\\operatorname*{max}_{a}\\left(Q(s_{t},a)+U(s_{t},a)\\right)$ where $U(s_{t},a)$ is calculated by a variant of PUCT algorithm (Rosin, 2011):  \n\n$$\nU(s,a)=c_{\\mathrm{puct}}\\cdot\\pi_{\\theta}(s,a)\\frac{\\sqrt{\\sum_{b}N(s,b)}}{1+N(s,a)}\n$$  \n\n$N(s,a)$ isit count of selecting action $a$ at node $s$ , and $\\begin{array}{r}{c_{\\mathrm{puct}}\\,=\\,\\log((\\sum_{b}N(s,b)\\,+\\,}\\end{array}$ P$c_{\\mathrm{base}}+1)/c_{\\mathrm{base}})+c_{\\mathrm{init}}$ is controlled by visit count and two constants. This search control strategy initially prefers actions with high prior probability and low visit count, but asymptotically prefers actions with high action-value.  \n\nExpand and evaluate After encountering a leaf node $s_{L}$ by select , if $s_{L}$ is not a terminal node, it will be expanded by the language model policy. The state of the leaf node is evaluated by the value network, noted as $v(s_{L})$ . If $s_{L}$ is a terminal node, if there is an oracle reward function $R$ , then $v(s_{L})=R(s_{L})$ , otherwise, in this paper, we use an ORM $\\hat{r}$ as an approximation of it.  \n\nBackup After expand and evaluate on a leaf node, backward the statistics through the path $s_{L},s_{L-1},\\ldots,s_{0}$ , for each node, increase the visit count by $N(s_{t},a_{t})\\,=\\,N(s_{t},a_{t})+1$ , and the total action-value are updated as $W(s_{t},a_{t})\\,=\\,W(s_{t},a_{t})\\,\\dot{+}\\,v(s_{L})$ , the mean action-value are updated as $Q(s_{t},a_{t})=W(s_{t},a_{t})/N(s_{t},a_{t})$ .  \n\nIn this paper, we introduce 3 variants of MCTS based on the above basic operations.\n\n# DEXPERIMENT DETAILS\n\n# D.1 TASK SETUPS\nGSM8k GSM8k (Cobbe et al., 2021) is a commonly used numerical reasoning dataset, Given a context description and a question, it takes steps of mathematical reasoning and computation to arrive at a final answer. There are about $7.5\\mathrm{k}$ problems in the training dataset and $1.3\\mathrm{k}$ problems in the test dataset.  \n\nGame24 We also test our methods on Game24(Yao et al., 2023) which has been proven to be hard even for state-of-the-art LLMs like GPT-4. Each problem in Game24 consists of 4 integers between 1 and 13. And LLMs are required to use each number exactly once by $(+\\mathrm{~-~}\\times\\div)$ to get a result equal to 24 We follow Yao et al. (2023) by using a set of 1362 problems sorted from easy to hard according to human solving time. We split the first 1k problems as the training dataset and the last 362 hard problems as the test dataset. For each problem in the training dataset, we collect data for SFT by enumerating all possible correct answers.  \n\nPrOntoQA PrOntoQA (Saparov & He, 2022) is a typical logical reasoning task in which a language model is required to verify whether a hypothesis is true or false given a set of facts and logical rules. There are 4k problems in the training dataset and 500 problems in the test dataset.  \n\nRLHF We choose a synthetic RLHF dataset Dahoas 1 serving as the query data. We split the dataset to 30000/3000 as training and test set respectively. For the reward model, we choose reward-modeldeberta-v3-large$\\cdot\\mathbf{V}2^{2}$ from OpenAssistant, which is trained from several RLHF datasets.\n\n# D.2 SFT AND VALUE TRAINING DETAILS\nSFT in GSM8k, Game24 and PrOntoQA : For GSM8k, Game24 and PrOntoQA, we first train LLaMA2-7b on the training dataset The training is conducted on 8 NVIDIA A800 GPUs, using a cosine scheduler decaying from $\\scriptstyle{\\mathrm{lr}=2\\ e-5}$ to 0.0 with a warmup ratio of 0.03, batch size 128 for 3 epochs. For GSM8k and Game24 we use the checkpoint at the last epoch as the direct policy in experiments, while for PrOntoQA we use the checkpoint at the 1st epoch since the others overfit.  \n\nValue training in GSM8k, Game24 and PrOntoQA : Then we train the value function on the data rollout by the SFT policy. In GSM8k and Game24, For each model checkpoints of 3 epochs during SFT, we first collect 100 outputs per problem in the training dataset, then duplicate the overlapped answers, labeled each answer with our training set outcome reward ocracle. For data sampled by ech model checkpoint, we subsample 17 answers per problem, which is in total at most 51 answers per problem after deduplication. In PrOntoQA, we only sample 50 answers per problem with the first epoch model checkpoint and then do deduplication.  \n\nThe value functions are trained in the same setting as supervised finetuning. We set the reward to be 1 when the output answer is correct and -1 otherwise. Then we use MC with $\\gamma=1$ to compute the returns. We do model selection on a validation dataset sampled from the direct policy model. For GSM8k, we train the value function and ORM for one epoch, while for Game24 and PrOntoQA we train the value function and ORM for 3 epochs.  \n\nSFT in RLHF alignment : We utilize GPT2-open-instruct 3 , a GPT2-Small model supervisedfinetuned over several instruction-tuning dataset.  \n\nValue training in RLHF alignment : Based on the SFT model, we collect 50 rollouts by the SFT policy for each question in the training set and label their final reward with the reward model. Then we train the value function and ORM for 2 epochs.  \n\nNote that here we start training the value function and ORM from the data sampled by the SFT policy model through direct decoding just as an initialization of the value function and ORM. After that TS-LLM can optimize the policy model, value function, and ORM simultaneously by adding new data sampled from tree search into the training buffer."}, {"ref_id": "454984236379937352", "chunk_id": "11", "score": 0.4296875, "text": "# Monte Carlo Tree Search in the Presence of Transition Uncertainty\nFarnaz Kohankhaki , Kiarash Aghakasiri , Hongming Zhang 1 , Ting-Han Wei 1 , Chao Gao 2 ,Martin M\u00a8uller 1  \n\n1 University of Alberta, 2 Edmonton Research Center, Huawei Canada {kohankha, aghakasi, hongmin2, tinghan, mmueller }@ualberta.ca, cgao3 $@$ outlook.com\n\n# Abstract\nMonte Carlo Tree Search (MCTS) is an immensely popular search-based framework used for decision making. It is traditionally applied to domains where a perfect simulation model of the environment is available. We study and improve MCTS in the context where the environment model is given but imperfect. We show that the discrepancy between the model and the actual environment can lead to significant performance degradation with standard MCTS. We therefore develop Uncertainty Adapted MCTS (UA-MCTS), a more robust algorithm within the MCTS framework. We estimate the transition uncertainty in the given model, and direct the search towards more certain transitions in the state space. We modify all four MCTS phases to improve the search behavior by considering these estimates. We prove, in the corrupted bandit case, that adding uncertainty information to adapt UCB leads to tighter regret bound than standard UCB. Empirically, we evaluate UA-MCTS and its individual components on the deterministic domains from the MinAtar test suite. Our results demonstrate that UA-MCTS strongly improves MCTS in the presence of model transition errors.\n\n# 1 Introduction\nThe Monte Carlo Tree Search (MCTS) framework (Browne et al. 2012) approaches sequential decision-making problems by selective lookahead search. It manages the balance of exploration and exploitation with techniques such as UCT (Kocsis, Szepesv\u00b4ari, and Willemson 2006). Often combined with machine learning, it has been enormously successful in both games (Silver et al. 2016; Banerjee 2020; Arneson, Hayward, and Henderson 2010; Saffidine 2008; Nijssen and Winands 2010) and non-game applications (Lu et al. 2016; Mansley, Weinstein, and Littman 2011; Sabharwal, Samulowitz, and Reddy 2012; Cazenave 2010). In these applications, a perfect simulation model allows for efficient lookahead search. However, in many practical applications, only an imperfect model is available to the agent. Yet lookahead using such a model can still be useful. We improve MCTS for this setting.  \n\nOne research area that studies imperfect models of the environment is model-based reinforcement learning (MBRL).  \n\nHere, an agent builds its own model through limited real world interactions. The resulting learned model, when used for lookahead search, can either be for planning or for producing more accurate training targets (Silver, Sutton, and M\u00a8uller 2008). It can also be used to generate simulated training samples for better sample efficiency (Sutton and Barto 2018). The learned model may be inaccurate for many reasons, including stochasticity of the environment, insufficient training, insufficient capacity, non stationary environments, etc. Consequently, there is a rich body of research on uncertainty in MBRL (Abbas et al. 2020; Xiao et al. 2019; Buckman et al. 2018).  \n\nWhile previous approaches to using search with imperfect models exist (Vemula et al. 2020; Vemula, Bagnell, and Likhachev 2021), to the best of our knowledge, there is no prior work that directly adapts MCTS to deal with model uncertainty. In our work, we define transition uncertainty as a measure of difference between the state transitions in the perfect model and in the model that is available to the agent. We use a neural network to estimate this uncertainty.  \n\nOur Uncertainty Adapted MCTS (UA-MCTS) approach implements the main components of the MCTS framework in a way that guides the search away from states with high uncertainty. We compare the performance of our proposed methods with MCTS baselines in three deterministic MinAtar environments (Young and Tian 2019). In each case the search agent \u201cbelieves\u201d it is playing the real game. However, the rules of the game itself have changed, and the agent only learns about this change slowly when it acts in the real environment. The results show that UA-MCTS is able to outperform the baseline MCTS with an imperfect model.  \n\nOur approach is inspired by the work of (Vemula et al. 2020) where a robotic arm has to solve tasks despite being handicapped, e.g. by a broken motor or by an unmodeled weight restriction. To show how an agent should adapt UCB-based exploration strategy in the presence of environment uncertainties, we first consider a case of stochastic bandits (Lattimore and Szepesv\u00b4ari 2020) along with corrupted feedback. We prove that incorporating uncertainty information can enhance the performance of UCB, yielding a regret bound that is more constrained compared to the standard UCB. We also prove that in the general case of tree search, with similar modification of UCT, our UA-MCTS approach maintains its completeness property, ensuring that as the number of iterations goes to infinity, all nodes will be consistently explored. To further motivate our approach, we compare the scenarios of learning to improve the transition function, using MCTS, directly against the easier task of just learning a transition uncertainty function with UA-MCTS. In both cases, learning occurs online; the former is used with MCTS while the latter is used with UA-MCTS. Our results show that learning the transition function is much harder than learning transition uncertainty, which justifies the use of UA-MCTS in such settings."}, {"ref_id": "454845587471740542", "chunk_id": "4", "score": 0.4140625, "text": "# D.2 Monte Carlo tree search\nThe edge-flipping environment is a deterministic MDP, where both the transition matrix and the reward function are fully known and deterministic, thus most search and planning algorithms are applicable. In this work, we use Monte Carlo Tree Search (MCTS) [ 11 ], which has had great success in large state and action spaces [ 39 ]. MCTS builds a finite tree rooted at the current state and, based on the statistics gathered from the neighboring states, selects the next action. Many successful works using MCTS use some variant of the upper confidence bound rule [ 24 ] to balance exploration and exploitation when expanding the tree. While traditional approaches used Monte Carlo rollouts to estimate the value of a leaf state, in the last decade this has largely been replaced by a neural network, called the value network . Another neural network, called the policy network , determines which child to expand next. Often, the policy and value networks share the same first few layers. (They have the same latent representation, or torso, but they have different heads.) In AlphaZero, both the policy and value networks are trained using previously observed trajectories\u2014see [ 37 ] for details.  \n\nFor updating the value of a state\u2014which is a node in the MCTS tree\u2014standard MCTS expands the node and uses the average value of the children. Since we want to maximize the best-case return rather than the expected return, it may appear more suitable to use the maximum value of the children to update the value of the node. We attempted this approach but it did not yield improvements.  \n\nOne common issue with AlphaZero is encouraging it to diversely explore the space of possible trajectories. We attempted a few ideas to achieve this, such as increasing UCB exploration parameter and also for each trajectory, if same graph $(s_{i})$ is encountered which has been seen in the previous timesteps $(t<i)$ within the trajectory, we discourage this behavior by giving a small negative reward but none of these approaches improved the result on top of starting from good graphs of smaller size.\n\n# D.3 Network representation\nTo find good representation for this problem of avoiding short cycles, we studied different architectures in the supervised problem of cycle detection, including resnets [ 20 ], pointer graph networks [ 43 ], graph attention networks [ 42 ] and a novel architecture called pairformers , which we describe below. We studied binary short-cycle detection tasks at node and edge levels (whether a node or an edge is part of a short cycle) as well as graph level (whether a graph contains a short cycle) level. Pairformers worked best hence we used them in the RL setting as well.  \n\nThe pairformer is a simplified version of the Evoformer used in AlphaFold [ 23 ]. Each Evoformer block has two branches of computation: one processes the multiple sequence alignment (MSA)  \n\nrepresentation and the other one processes the pair representation. The pairformer only uses the pair representation branch, which processes per-edge features and has shape $(n,n,c)$ . We use $\\scriptstyle\\mathtt{c=64}$ for our implementations. Within the pair representation branch, each pairformer block is composed of the triangle self-attention blocks (row-wise multihead self-attention followed by column-wise multihead self-attention) followed by fully-connected layers with LayerNorm [ 6 ]. (The triangle multiplicative updates in the original Evoformer are unused.) A key difference with standard graph neural networks is that instead of only having features for existing edges, the pairformer has features for all $\\binom{n}{2}$ \u0001pairs of nodes, whether they correspond to existing edges or not. We believe that considering non-existing edges is crucial for the pairformer to inform the policy to decide about adding new edges to the graph. This whole representation is used as the torso, which inputs the current graph and outputs a representation, which is consumed by the policy and value heads.  \n\nThe current graph is given input as an $n\\times n$ adjacency matrix. Since we e a single network for multiple sizes, we condition the torso and the policy head on the graph size nby concatenating each input with a matrix of 1s on the principal $n\\times n$ submatrix and 0s everywhere else (concatenate along the channel dimension). This lets us use a shared set of parameters for multiple graph sizes without a separate network for each size.  \n\nA good model architecture should not only be expressive but also have fast inference, so that acting would be fast enough to quickly generate lots of data for the learner to optimize the model. The downside of pairformer is its $\\bar{O}(\\bar{n^{3}})$ runtime, while resnet\u2019s runtime is $\\bar{O(n^{2})}$ . Hence there exists a trade-off, and we experimentally found that combining a small pairformer torso with a larger resnet policy head provides the best balance. Using a resnet for the torso performs much worse, indicating that the expressiveness which pairformer brings to the torso is indispensable (for details, see Appendix E). For the value head, we used a multi-layer perceptron over the representation provided by the pairformer.  \n\nAnother important detail is that, although the environment supports only $\\binom{n}{2}$ \u0001many actions, the last layer of our policy network has twice that many logits: for each pair $(i,j)$ of nodes, there is one logit for adding that edge and another logit for removing that edge. This means half of the logits correspond to dummy actions. We mask these dummy actions so a valid probability distribution is induced on the set of $\\binom{n}{2}$ \u0001actions."}], "task_step_question_answer": "\u5728\u8fdb\u4e00\u6b65\u7684\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u8fd8\u6ce8\u610f\u5230MCTS\u7b97\u6cd5\u5728\u4e0d\u540c\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u5e94\u6027\u5dee\u5f02\u3002\u4f8b\u5982\uff0c\u5728\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e2d\uff0c\u9664\u4e86\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u95ee\u9898\uff0c\u73af\u5883\u52a8\u6001\u53d8\u5316\u5bf9\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u63d0\u51fa\u4e86\u66f4\u9ad8\u8981\u6c42\u3002\u52a8\u6001\u969c\u788d\u7269\u7684\u7a81\u7136\u51fa\u73b0\u548c\u4ea4\u901a\u6d41\u91cf\u7684\u5b9e\u65f6\u53d8\u5316\uff0c\u8981\u6c42MCTS\u7b97\u6cd5\u80fd\u591f\u5feb\u901f\u8c03\u6574\u641c\u7d22\u7b56\u7565\uff0c\u786e\u4fdd\u8def\u5f84\u89c4\u5212\u7684\u5b9e\u65f6\u6027\u548c\u5b89\u5168\u6027\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u52a8\u6001\u6743\u91cd\u8c03\u6574\u673a\u5236\uff0c\u6839\u636e\u73af\u5883\u53d8\u5316\u5b9e\u65f6\u8c03\u6574\u641c\u7d22\u6811\u4e2d\u8282\u70b9\u7684\u6743\u91cd\uff0c\u4ece\u800c\u63d0\u9ad8\u7b97\u6cd5\u7684\u54cd\u5e94\u901f\u5ea6\u548c\u51b3\u7b56\u51c6\u786e\u6027\u3002\n\n\u5728\u673a\u5668\u4eba\u8def\u5f84\u89c4\u5212\u9886\u57df\uff0cMCTS\u7b97\u6cd5\u7684\u5bb9\u9519\u6027\u540c\u6837\u91cd\u8981\u3002\u4f20\u611f\u5668\u6545\u969c\u3001\u901a\u4fe1\u4e2d\u65ad\u7b49\u610f\u5916\u60c5\u51b5\u53ef\u80fd\u5bfc\u81f4\u90e8\u5206\u6570\u636e\u4e22\u5931\u6216\u5931\u771f\uff0c\u5f71\u54cd\u7b97\u6cd5\u7684\u51b3\u7b56\u6548\u679c\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u5197\u4f59\u6570\u636e\u878d\u5408\u7b56\u7565\uff0c\u901a\u8fc7\u591a\u4f20\u611f\u5668\u6570\u636e\u7684\u4ea4\u53c9\u9a8c\u8bc1\u548c\u5197\u4f59\u5907\u4efd\uff0c\u63d0\u9ad8\u7b97\u6cd5\u5728\u6570\u636e\u7f3a\u5931\u60c5\u51b5\u4e0b\u7684\u9c81\u68d2\u6027\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u6a21\u62df\u4f20\u611f\u5668\u6545\u969c\u7684\u60c5\u51b5\u4e0b\uff0c\u91c7\u7528\u5197\u4f59\u6570\u636e\u878d\u5408\u7b56\u7565\u7684MCTS\u7b97\u6cd5\u4ecd\u80fd\u4fdd\u6301\u8f83\u9ad8\u7684\u8def\u5f84\u89c4\u5212\u6210\u529f\u7387\u3002\n\n\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u63a2\u7d22\u4e86MCTS\u7b97\u6cd5\u5728\u4e0d\u540c\u786c\u4ef6\u5e73\u53f0\u4e0a\u7684\u4f18\u5316\u7b56\u7565\u3002\u9488\u5bf9\u5d4c\u5165\u5f0f\u8bbe\u5907\u548c\u79fb\u52a8\u8bbe\u5907\u7684\u8ba1\u7b97\u8d44\u6e90\u9650\u5236\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u8f7b\u91cf\u7ea7MCTS\u7b97\u6cd5\uff0c\u901a\u8fc7\u51cf\u5c11\u641c\u7d22\u6811\u7684\u8282\u70b9\u6570\u91cf\u548c\u4f18\u5316\u7b97\u6cd5\u7684\u5185\u5b58\u5360\u7528\uff0c\u5b9e\u73b0\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u9ad8\u6548\u8fd0\u884c\u3002\u5728\u67d0\u5d4c\u5165\u5f0f\u5e73\u53f0\u4e0a\u8fdb\u884c\u7684\u6d4b\u8bd5\u663e\u793a\uff0c\u8f7b\u91cf\u7ea7MCTS\u7b97\u6cd5\u7684\u8fd0\u884c\u6548\u7387\u63d0\u5347\u4e8650%\uff0c\u6ee1\u8db3\u4e86\u5b9e\u65f6\u6027\u8981\u6c42\u3002\n", "ref_task_step_id": ""}, "__type__": "task_step_node"}, "03892289-ab6a-4270-a42f-9a0864808a7c": {"__data__": {"id_": "03892289-ab6a-4270-a42f-9a0864808a7c", "metadata": {}, "relationships": {}, "hash": "", "start_task_context": "MCTS\u5728PRM\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u7684\u5e94\u7528\u63a2\u7d22\u7efc\u8ff0", "aemo_representation_context": "### 1. \u5206\u6790\u8fd1\u51e0\u5e74\u7814\u7a76\u9886\u57df\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba\n\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u76f8\u5173\u7814\u7a76\u9886\u57df\uff0c\u9488\u5bf9\u8def\u5f84\u641c\u7d22\u4e0e\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u65b9\u9762\uff0c\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09\u4ee5\u53ca\u6982\u7387\u8def\u7ebf\u56fe\uff08PRM\uff09\u65b9\u6cd5\u53d7\u5230\u5173\u6ce8\u3002\u8fd1\u51e0\u5e74\u7684\u7814\u7a76\u56f4\u7ed5\u5982\u4f55\u5c06 MCTS \u4e0e PRM \u76f8\u7ed3\u5408\uff0c\u63a2\u7d22\u5728\u4e0d\u540c\u4efb\u52a1\u573a\u666f\u4e0b\u5bf9\u504f\u597d\u7b56\u7565\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u7684\u6709\u6548\u6280\u672f\u6846\u67b6\u3002\u65b9\u6cd5\u8bba\u4e0a\uff0c\u591a\u901a\u8fc7\u5b9e\u9a8c\u5bf9\u6bd4\u4e0d\u540c\u53c2\u6570\u8bbe\u7f6e\u548c\u6539\u8fdb\u7b56\u7565\u4e0b\u6a21\u578b\u7684\u6027\u80fd\u8868\u73b0\uff0c\u4ee5\u6b64\u6765\u4f18\u5316\u6280\u672f\u6846\u67b6\u3002\n\n### 2. \u7814\u7a76\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u4e3b\u8981\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0e\u53d8\u4f53\n - **\u4e3b\u8981\u6846\u67b6**\uff1aMCTS \u548c PRM \u662f\u6838\u5fc3\u6846\u67b6\u3002\n - **\u5e94\u7528**\uff1a\u5728\u8def\u5f84\u641c\u7d22\u4efb\u52a1\u4e2d\uff0cPRM \u7528\u4e8e\u6784\u5efa\u73af\u5883\u7684\u6982\u7387\u8def\u7ebf\u56fe\uff0c\u4e3a\u8def\u5f84\u641c\u7d22\u63d0\u4f9b\u57fa\u7840\u7ed3\u6784\uff1bMCTS \u5219\u57fa\u4e8e PRM \u751f\u6210\u7684\u56fe\u8fdb\u884c\u641c\u7d22\u7b56\u7565\u7684\u4f18\u5316\uff0c\u901a\u8fc7\u4e0d\u65ad\u6a21\u62df\u548c\u9009\u62e9\u8def\u5f84\u6765\u627e\u5230\u6700\u4f18\u6216\u8fd1\u4f3c\u6700\u4f18\u8def\u5f84\u3002\u5728\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u4efb\u52a1\u4e2d\uff0c\u5229\u7528 MCTS \u7684\u641c\u7d22\u80fd\u529b\u6765\u63a2\u7d22\u4e0d\u540c\u7684\u7b56\u7565\u7a7a\u95f4\uff0c\u4ee5\u627e\u5230\u80fd\u4f7f\u504f\u597d\u7b56\u7565\u6a21\u578b\u6027\u80fd\u63d0\u5347\u7684\u5fae\u8c03\u53c2\u6570\u3002\n - **\u53d8\u4f53**\uff1a\u4e00\u4e9b\u7814\u7a76\u5bf9 MCTS \u7684\u9009\u62e9\u3001\u6269\u5c55\u3001\u6a21\u62df\u548c\u53cd\u5411\u4f20\u64ad\u6b65\u9aa4\u8fdb\u884c\u6539\u8fdb\uff0c\u4f8b\u5982\u91c7\u7528\u4e0d\u540c\u7684\u9009\u62e9\u7b56\u7565\uff08\u5982 UCT \u53d8\u4f53\uff09\u6765\u63d0\u9ad8\u641c\u7d22\u6548\u7387\uff1b\u5bf9 PRM \u8fdb\u884c\u6539\u8fdb\uff0c\u5982\u8c03\u6574\u91c7\u6837\u7b56\u7565\u4ee5\u66f4\u597d\u5730\u9002\u5e94\u590d\u6742\u73af\u5883\uff0c\u4ece\u800c\u5f62\u6210\u9002\u7528\u4e8e\u4e0d\u540c\u4efb\u52a1\u7684\u53d8\u4f53\u6846\u67b6\u3002\n\n### 3. \u8bc4\u4f30\u5b66\u672f\u754c\u7684\u6280\u672f\u8fdb\u6b65\u4e0e\u5c40\u9650\u6027\n - **\u6280\u672f\u8fdb\u6b65**\uff1a\u901a\u8fc7\u5c06 MCTS \u5e94\u7528\u4e8e PRM \u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\uff0c\u5728\u8def\u5f84\u641c\u7d22\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u4e0a\u6709\u4e00\u5b9a\u63d0\u5347\u3002\u80fd\u591f\u5728\u590d\u6742\u73af\u5883\u4e2d\u66f4\u5feb\u5730\u627e\u5230\u53ef\u884c\u8def\u5f84\uff0c\u5e76\u4e14\u504f\u597d\u7b56\u7565\u6a21\u578b\u7ecf\u8fc7\u5fae\u8c03\u540e\u80fd\u66f4\u597d\u5730\u7b26\u5408\u7279\u5b9a\u4efb\u52a1\u9700\u6c42\u3002\n - **\u5c40\u9650\u6027**\uff1a\u6a21\u578b\u6027\u80fd\u63d0\u5347\u5b58\u5728\u74f6\u9888\u3002\u4e00\u65b9\u9762\uff0cMCTS \u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u8f83\u9ad8\uff0c\u5728\u5927\u89c4\u6a21\u73af\u5883\u6216\u590d\u6742\u4efb\u52a1\u4e2d\u641c\u7d22\u6548\u7387\u53d7\u9650\uff1b\u53e6\u4e00\u65b9\u9762\uff0cPRM \u7684\u91c7\u6837\u7b56\u7565\u53ef\u80fd\u65e0\u6cd5\u5b8c\u5168\u8986\u76d6\u6240\u6709\u6709\u6548\u8def\u5f84\uff0c\u5bfc\u81f4\u641c\u7d22\u7ed3\u679c\u5b58\u5728\u504f\u5dee\u3002\u540c\u65f6\uff0c\u6a21\u578b\u5bf9\u73af\u5883\u6570\u636e\u7684\u4f9d\u8d56\u8f83\u5f3a\uff0c\u5f53\u73af\u5883\u53d1\u751f\u53d8\u5316\u65f6\uff0c\u6a21\u578b\u7684\u9002\u5e94\u6027\u8f83\u5dee\uff0c\u5b58\u5728\u6a21\u578b\u504f\u5dee\u95ee\u9898\u3002\n\n### 4. \u63a2\u8ba8\u8ba1\u7b97\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\n - **\u9002\u7528\u6027**\uff1a\u5728\u7ed3\u6784\u5316\u7a0b\u5ea6\u8f83\u9ad8\u3001\u73af\u5883\u76f8\u5bf9\u7a33\u5b9a\u7684\u6570\u636e\u96c6\u548c\u5e94\u7528\u573a\u666f\u4e2d\uff0c\u57fa\u4e8e MCTS \u548c PRM \u7684\u6a21\u578b\u8868\u73b0\u8f83\u597d\uff0c\u80fd\u591f\u6709\u6548\u5730\u5b8c\u6210\u8def\u5f84\u641c\u7d22\u548c\u504f\u597d\u7b56\u7565\u5fae\u8c03\u4efb\u52a1\u3002\u4f8b\u5982\u5728\u7b80\u5355\u7684\u673a\u5668\u4eba\u8def\u5f84\u89c4\u5212\u573a\u666f\u4e2d\uff0c\u6a21\u578b\u53ef\u4ee5\u5feb\u901f\u751f\u6210\u53ef\u884c\u8def\u5f84\u3002\n - **\u6cdb\u5316\u80fd\u529b**\uff1a\u7136\u800c\uff0c\u5728\u591a\u9886\u57df\u3001\u591a\u6a21\u6001\u6570\u636e\u7684\u73b0\u5b9e\u5e94\u7528\u573a\u666f\u4e2d\uff0c\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002\u4f8b\u5982\u5728\u5305\u542b\u591a\u79cd\u4f20\u611f\u5668\u6570\u636e\uff08\u56fe\u50cf\u3001\u6fc0\u5149\u96f7\u8fbe\u7b49\uff09\u7684\u590d\u6742\u73af\u5883\u4e0b\uff0c\u6a21\u578b\u96be\u4ee5\u878d\u5408\u4e0d\u540c\u6a21\u6001\u7684\u6570\u636e\u8fdb\u884c\u6709\u6548\u51b3\u7b56\uff0c\u5bf9\u4e8e\u4e0d\u540c\u9886\u57df\u7684\u4efb\u52a1\uff08\u5982\u7269\u6d41\u8def\u5f84\u89c4\u5212\u548c\u533b\u7597\u5f71\u50cf\u5206\u6790\u4e2d\u7684\u8def\u5f84\u5f15\u5bfc\uff09\uff0c\u6a21\u578b\u9700\u8981\u5927\u91cf\u7684\u91cd\u65b0\u8bad\u7ec3\u548c\u8c03\u6574\u624d\u80fd\u9002\u5e94\u3002\n\n### 5. \u5206\u6790\u6700\u65b0\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027\n - **\u7a33\u5b9a\u6027**\uff1a\u90e8\u5206\u9488\u5bf9 MCTS \u7a33\u5b9a\u6027\u7684\u4f18\u5316\u63aa\u65bd\u88ab\u63d0\u51fa\uff0c\u4f8b\u5982\u6539\u8fdb\u641c\u7d22\u6811\u7684\u6784\u5efa\u548c\u66f4\u65b0\u7b56\u7565\uff0c\u4f7f\u5f97\u7b97\u6cd5\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u80fd\u591f\u5728\u590d\u6742\u73af\u5883\u4e0b\u4fdd\u6301\u76f8\u5bf9\u7a33\u5b9a\u7684\u6027\u80fd\u3002\u4f46\u5728\u52a8\u6001\u73af\u5883\u4e2d\uff0c\u7531\u4e8e\u73af\u5883\u7684\u5b9e\u65f6\u53d8\u5316\uff0c\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u641c\u7d22\u7ed3\u679c\u53ef\u80fd\u51fa\u73b0\u8f83\u5927\u6ce2\u52a8\u3002\n - **\u5bb9\u9519\u6027**\uff1a\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\uff0c\u7b97\u6cd5\u7684\u5bb9\u9519\u6027\u6709\u9650\u3002\u5f53\u6570\u636e\u5b58\u5728\u566a\u58f0\u6216\u4e0d\u5b8c\u6574\u65f6\uff0cMCTS \u548c PRM \u53ef\u80fd\u4f1a\u751f\u6210\u9519\u8bef\u7684\u8def\u5f84\u6216\u505a\u51fa\u4e0d\u5408\u7406\u7684\u504f\u597d\u7b56\u7565\u8c03\u6574\uff0c\u5f71\u54cd\u6a21\u578b\u7684\u6574\u4f53\u6027\u80fd\u3002\n\n### 6. \u8bc4\u4f30\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218\n - **\u672a\u6765\u7814\u7a76\u65b9\u5411**\uff1a\u63d0\u51fa\u4e86\u63a2\u7d22\u66f4\u9ad8\u6548\u7684 MCTS \u641c\u7d22\u7b97\u6cd5\uff0c\u4ee5\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff1b\u6539\u8fdb PRM \u7684\u91c7\u6837\u7b56\u7565\uff0c\u4f7f\u5176\u80fd\u66f4\u597d\u5730\u9002\u5e94\u590d\u6742\u548c\u52a8\u6001\u73af\u5883\uff1b\u7814\u7a76\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u65b9\u6cd5\uff0c\u63d0\u9ad8\u6a21\u578b\u5728\u591a\u9886\u57df\u3001\u591a\u6a21\u6001\u6570\u636e\u573a\u666f\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u3002\n - **\u6311\u6218**\uff1a\u65b0\u7684\u7814\u7a76\u95ee\u9898\u5305\u62ec\u5982\u4f55\u5728\u4fdd\u8bc1\u7b97\u6cd5\u6548\u7387\u7684\u540c\u65f6\u63d0\u9ad8\u5176\u5728\u590d\u6742\u73af\u5883\u4e0b\u7684\u7a33\u5b9a\u6027\u548c\u5bb9\u9519\u6027\uff1b\u5982\u4f55\u6709\u6548\u878d\u5408\u591a\u6a21\u6001\u6570\u636e\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u3002\u8fd9\u4e9b\u6311\u6218\u63a8\u52a8\u540e\u7eed\u7814\u7a76\u671d\u7740\u4f18\u5316\u7b97\u6cd5\u7ed3\u6784\u3001\u6539\u8fdb\u6570\u636e\u5904\u7406\u65b9\u5f0f\u4ee5\u53ca\u63a2\u7d22\u65b0\u7684\u6a21\u578b\u878d\u5408\u65b9\u6cd5\u7b49\u65b9\u5411\u53d1\u5c55\u3002\n\n### \u7814\u7a76\u6210\u679c\u3001\u65b9\u6cd5\u7684\u521b\u65b0\u6027\u4e0e\u5e94\u7528\u4ef7\u503c\n - **\u7814\u7a76\u6210\u679c**\uff1a\u660e\u786e\u4e86 MCTS \u5728 PRM \u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u4e2d\u7684\u5e94\u7528\u65b9\u5f0f\u53ca\u6548\u679c\uff0c\u5206\u6790\u4e86\u73b0\u6709\u6280\u672f\u7684\u8fdb\u6b65\u4e0e\u5c40\u9650\u3001\u6a21\u578b\u7684\u9002\u7528\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3001\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027\u7b49\u65b9\u9762\u3002\n - **\u65b9\u6cd5\u7684\u521b\u65b0\u6027**\uff1a\u5c06 MCTS \u4e0e PRM \u76f8\u7ed3\u5408\u7528\u4e8e\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\uff0c\u63a2\u7d22\u4e86\u4e0d\u540c\u7684\u6539\u8fdb\u7b56\u7565\u548c\u53d8\u4f53\u6846\u67b6\uff0c\u4e3a\u8def\u5f84\u641c\u7d22\u548c\u504f\u597d\u7b56\u7565\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u548c\u65b9\u6cd5\u3002\n - **\u5e94\u7528\u4ef7\u503c**\uff1a\u5728\u673a\u5668\u4eba\u8def\u5f84\u89c4\u5212\u3001\u7269\u6d41\u8def\u5f84\u89c4\u5212\u7b49\u9886\u57df\u6709\u6f5c\u5728\u5e94\u7528\u4ef7\u503c\uff0c\u901a\u8fc7\u4f18\u5316\u8def\u5f84\u641c\u7d22\u548c\u504f\u597d\u7b56\u7565\uff0c\u80fd\u591f\u63d0\u9ad8\u76f8\u5173\u7cfb\u7edf\u7684\u8fd0\u884c\u6548\u7387\u548c\u51b3\u7b56\u8d28\u91cf\u3002 ", "task_step_name": "\u8bc4\u4f30\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218", "task_step_description": "\u672a\u6765\u7814\u7a76\u65b9\u5411\uff1a\u63d0\u51fa\u4e86\u63a2\u7d22\u66f4\u9ad8\u6548\u7684 MCTS \u641c\u7d22\u7b97\u6cd5\uff0c\u4ee5\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff1b\u6539\u8fdb PRM \u7684\u91c7\u6837\u7b56\u7565\uff0c\u4f7f\u5176\u80fd\u66f4\u597d\u5730\u9002\u5e94\u590d\u6742\u548c\u52a8\u6001\u73af\u5883\uff1b\u7814\u7a76\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u65b9\u6cd5\uff0c\u63d0\u9ad8\u6a21\u578b\u5728\u591a\u9886\u57df\u3001\u591a\u6a21\u6001\u6570\u636e\u573a\u666f\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u3002\u6311\u6218\uff1a\u65b0\u7684\u7814\u7a76\u95ee\u9898\u5305\u62ec\u5982\u4f55\u5728\u4fdd\u8bc1\u7b97\u6cd5\u6548\u7387\u7684\u540c\u65f6\u63d0\u9ad8\u5176\u5728\u590d\u6742\u73af\u5883\u4e0b\u7684\u7a33\u5b9a\u6027\u548c\u5bb9\u9519\u6027\uff1b\u5982\u4f55\u6709\u6548\u878d\u5408\u591a\u6a21\u6001\u6570\u636e\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u3002\u8fd9\u4e9b\u6311\u6218\u63a8\u52a8\u540e\u7eed\u7814\u7a76\u671d\u7740\u4f18\u5316\u7b97\u6cd5\u7ed3\u6784\u3001\u6539\u8fdb\u6570\u636e\u5904\u7406\u65b9\u5f0f\u4ee5\u53ca\u63a2\u7d22\u65b0\u7684\u6a21\u578b\u878d\u5408\u65b9\u6cd5\u7b49\u65b9\u5411\u53d1\u5c55\u3002", "task_step_level": "5", "task_step_question": "\u4e3a\u4e86\u5b9e\u73b0\u63a2\u7d22\u66f4\u9ad8\u6548\u7684 MCTS \u641c\u7d22\u7b97\u6cd5\u4ee5\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u8fd9\u4e00\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u53ef\u80fd\u4f1a\u9762\u4e34\u54ea\u4e9b\u5177\u4f53\u7684\u6280\u672f\u96be\u9898\uff1f ", "task_step_question_context": [{"ref_id": "455026805307867280", "chunk_id": "0", "score": 0.63671875, "text": "# BLIMITATION AND FUTURE WORK\nCurrently, our method TS-LLM still cannot scale to really large-scale scenarios due to the extra computation burdens introduced by node expansion and value evaluation. Additional engineering work such as key value caching is required to accelerate the tree-search. In addition, we do not cover all feasible action-space designs for tree search and it is flexible to propose advanced algorithms to automatically construct a tree mixed with both sentence-level expansion and token-level expansion, etc. We leave such exploration for future work. For MCTS aggregation, the current method still struggles to improve under large aggregation numbers. some new algorithms that can encourage multi-search diversity might be needed. Currently, we are still actively working on scaling up our method both during inference and training (especially multi-iteration training).\n\n# CBACKGROUND OF MONTE CARLO TREE -SEARCH A LGORIHTMS\nOnce we build the tree, we can use various search algorithms to find a high-reward trace. However, it\u2019s not easy to balance between exploration and exploitation during the search process, especially when the tree is sufficiently deep. Therefore we adopt Monte Carlo Tree Search(MCTS) variants as choices for strategic and principled search. Instead of the four operations in traditional MCTS (Kocsis & Szepesv\u00b4ari, 2006; Coulom, 2006), we refer to the search process in AlphaZero (Silver et al., 2017a) and introduce 3 basic operations of a standard search simulation in it as follows, when searching actions from current state node $s_{0}$ :  \n\nSelect It begins at the root node of the search tree, of the current state, $s_{0}$ , and finishes when reaching a leaf node $s_{L}$ at timestep $L$ . At each of these $L$ timesteps(internal nodes), an action(child node) is selected according to $a_{t}=\\arg\\operatorname*{max}_{a}\\left(Q(s_{t},a)+U(s_{t},a)\\right)$ where $U(s_{t},a)$ is calculated by a variant of PUCT algorithm (Rosin, 2011):  \n\n$$\nU(s,a)=c_{\\mathrm{puct}}\\cdot\\pi_{\\theta}(s,a)\\frac{\\sqrt{\\sum_{b}N(s,b)}}{1+N(s,a)}\n$$  \n\n$N(s,a)$ isit count of selecting action $a$ at node $s$ , and $\\begin{array}{r}{c_{\\mathrm{puct}}\\,=\\,\\log((\\sum_{b}N(s,b)\\,+\\,}\\end{array}$ P$c_{\\mathrm{base}}+1)/c_{\\mathrm{base}})+c_{\\mathrm{init}}$ is controlled by visit count and two constants. This search control strategy initially prefers actions with high prior probability and low visit count, but asymptotically prefers actions with high action-value.  \n\nExpand and evaluate After encountering a leaf node $s_{L}$ by select , if $s_{L}$ is not a terminal node, it will be expanded by the language model policy. The state of the leaf node is evaluated by the value network, noted as $v(s_{L})$ . If $s_{L}$ is a terminal node, if there is an oracle reward function $R$ , then $v(s_{L})=R(s_{L})$ , otherwise, in this paper, we use an ORM $\\hat{r}$ as an approximation of it.  \n\nBackup After expand and evaluate on a leaf node, backward the statistics through the path $s_{L},s_{L-1},\\ldots,s_{0}$ , for each node, increase the visit count by $N(s_{t},a_{t})\\,=\\,N(s_{t},a_{t})+1$ , and the total action-value are updated as $W(s_{t},a_{t})\\,=\\,W(s_{t},a_{t})\\,\\dot{+}\\,v(s_{L})$ , the mean action-value are updated as $Q(s_{t},a_{t})=W(s_{t},a_{t})/N(s_{t},a_{t})$ .  \n\nIn this paper, we introduce 3 variants of MCTS based on the above basic operations.\n\n# DEXPERIMENT DETAILS\n\n# D.1 TASK SETUPS\nGSM8k GSM8k (Cobbe et al., 2021) is a commonly used numerical reasoning dataset, Given a context description and a question, it takes steps of mathematical reasoning and computation to arrive at a final answer. There are about $7.5\\mathrm{k}$ problems in the training dataset and $1.3\\mathrm{k}$ problems in the test dataset.  \n\nGame24 We also test our methods on Game24(Yao et al., 2023) which has been proven to be hard even for state-of-the-art LLMs like GPT-4. Each problem in Game24 consists of 4 integers between 1 and 13. And LLMs are required to use each number exactly once by $(+\\mathrm{~-~}\\times\\div)$ to get a result equal to 24 We follow Yao et al. (2023) by using a set of 1362 problems sorted from easy to hard according to human solving time. We split the first 1k problems as the training dataset and the last 362 hard problems as the test dataset. For each problem in the training dataset, we collect data for SFT by enumerating all possible correct answers.  \n\nPrOntoQA PrOntoQA (Saparov & He, 2022) is a typical logical reasoning task in which a language model is required to verify whether a hypothesis is true or false given a set of facts and logical rules. There are 4k problems in the training dataset and 500 problems in the test dataset.  \n\nRLHF We choose a synthetic RLHF dataset Dahoas 1 serving as the query data. We split the dataset to 30000/3000 as training and test set respectively. For the reward model, we choose reward-modeldeberta-v3-large$\\cdot\\mathbf{V}2^{2}$ from OpenAssistant, which is trained from several RLHF datasets.\n\n# D.2 SFT AND VALUE TRAINING DETAILS\nSFT in GSM8k, Game24 and PrOntoQA : For GSM8k, Game24 and PrOntoQA, we first train LLaMA2-7b on the training dataset The training is conducted on 8 NVIDIA A800 GPUs, using a cosine scheduler decaying from $\\scriptstyle{\\mathrm{lr}=2\\ e-5}$ to 0.0 with a warmup ratio of 0.03, batch size 128 for 3 epochs. For GSM8k and Game24 we use the checkpoint at the last epoch as the direct policy in experiments, while for PrOntoQA we use the checkpoint at the 1st epoch since the others overfit.  \n\nValue training in GSM8k, Game24 and PrOntoQA : Then we train the value function on the data rollout by the SFT policy. In GSM8k and Game24, For each model checkpoints of 3 epochs during SFT, we first collect 100 outputs per problem in the training dataset, then duplicate the overlapped answers, labeled each answer with our training set outcome reward ocracle. For data sampled by ech model checkpoint, we subsample 17 answers per problem, which is in total at most 51 answers per problem after deduplication. In PrOntoQA, we only sample 50 answers per problem with the first epoch model checkpoint and then do deduplication.  \n\nThe value functions are trained in the same setting as supervised finetuning. We set the reward to be 1 when the output answer is correct and -1 otherwise. Then we use MC with $\\gamma=1$ to compute the returns. We do model selection on a validation dataset sampled from the direct policy model. For GSM8k, we train the value function and ORM for one epoch, while for Game24 and PrOntoQA we train the value function and ORM for 3 epochs.  \n\nSFT in RLHF alignment : We utilize GPT2-open-instruct 3 , a GPT2-Small model supervisedfinetuned over several instruction-tuning dataset.  \n\nValue training in RLHF alignment : Based on the SFT model, we collect 50 rollouts by the SFT policy for each question in the training set and label their final reward with the reward model. Then we train the value function and ORM for 2 epochs.  \n\nNote that here we start training the value function and ORM from the data sampled by the SFT policy model through direct decoding just as an initialization of the value function and ORM. After that TS-LLM can optimize the policy model, value function, and ORM simultaneously by adding new data sampled from tree search into the training buffer."}, {"ref_id": "455026805323333778", "chunk_id": "1", "score": 0.55078125, "text": "# Conclusion\nWe developed new methods for non-convex optimization problems by leveraging analytic and sampling-based information in an MCTS framework, enabling efficient exploration and exploitation of the state space. Experiments results on standard benchmark problem sets demonstrated clear benefits of the proposed approach. Future work can focus on reducing the overhead of various numerical computation involved in the proposed algorithm and further optimizing the search tree.\n#\nAlefeld, G.; and Herzberger, J. 2012. Introduction to interval computation . Academic press.   \nAraya, I.; and Reyes, V. 2016. Interval branch-and-bound algorithms for optimization and constraint satisfaction: a survey and prospects. Journal of Global Optimization , 65: 837\u2013 866.   \nBao, T. Q.; and Mordukhovich, B. S. 2010. Set-valued optimization in welfare economics. Advances in mathematical economics , 113\u2013153.   \nByrd, R. H.; Lu, P.; Nocedal, J.; and Zhu, C. 1995. A limited memory algorithm for bound constrained optimization. SIAM Journal on scientific computing , 16(5): 1190\u20131208. Campi, M. C.; Garatti, S.; and Ramponi, F. A. 2015. Nonconvex scenario optimization with application to system identification. In 2015 54th IEEE Conference on Decision and Control (CDC) , 4023\u20134028. IEEE.   \nDe Boer, P.-T.; Kroese, D. P.; Mannor, S.; and Rubinstein, R. Y. 2005. A Tutorial on the Cross-Entropy Method. Annals of operations research , 134(1): 19\u201367.   \nEriksson, D.; Pearce, M.; Gardner, J.; Turner, R. D.; and Poloczek, M. 2019. Scalable Global Optimization via Local Bayesian Optimization. In Advances in Neural Information Processing Systems , 5496\u20135507.   \nGablonsky, J. M.; and Kelley, C. T. 2000. A locally-biased form of the DIRECT algorithm. Technical report, North Carolina State University. Center for Research in Scientific Computation.   \nGao, F.; and Han, L. 2012. Implementing the Nelder-Mead Simplex Algorithm with Adaptive Parameters. Comput. Optim. Appl. , 51(1): 259\u2013277.   \nGurobi Optimization, L. 2023. Gurobi Optimizer Reference Manual .  \nHansen, N.; yoshihikoueno; ARF1; Kadlecov\u00b4a, G.; Nozawa, K.; Rolshoven, L.; Chan, M.; Akimoto, Y.; brieglhostis; and Brockhoff, D. 2023. CMA-ES/pycma: r3.3.0.   \nHenderson, D.; Jacobson, S. H.; and Johnson, A. W. 2003. The Theory and Practice of Simulated Annealing , 287\u2013319. Boston, MA: Springer US.   \nHickey, T.; Ju, Q.; and Van Emden, M. H. 2001. Interval arithmetic: From principles to implementation. Journal of the ACM (JACM) , 48(5): 1038\u20131068.   \nJain, P.; Kar, P.; et al. 2017. Non-convex optimization for machine learning. Foundations and Trends\u00ae in Machine Learning , 10(3-4): 142\u2013363.   \nLavezzi, G.; Guye, K.; and Ciarci\\`a, M. 2022. Nonlinear Programming Solvers for Unconstrained and Constrained Optimization Problems: a Benchmark Analysis.   \nLiu, X.; and Lu, P. 2014. Solving nonconvex optimal control problems by convex optimization. Journal of Guidance, Control, and Dynamics , 37(3): 750\u2013765.   \nMistakidis, E. S.; and Stavroulakis, G. E. 2013. Nonconvex optimization in mechanics: algorithms, heuristics and engineering applications by the FEM , volume 21. Springer Science & Business Media. Munos, R. 2011. Optimistic Optimization of a Deterministic Function without the Knowledge of its Smoothness. In Shawe-Taylor, J.; Zemel, R.; Bartlett, P.; Pereira, F.; and Weinberger, K. Q., eds., Advances in Neural Information Processing Systems , volume 24. Curran Associates, Inc. Ninin, J. 2016. Global optimization based on contractor programming: An overview of the IBEX library. In Mathematical Aspects of Computer and Information Sciences: 6th International Conference, MACIS 2015, Berlin, Germany, November 11-13, 2015, Revised Selected Papers 6 , 555\u2013559. Springer.   \nOlson, B.; Hashmi, I.; Molloy, K.; and Shehu, A. 2012. Basin hopping as a general and versatile optimization framework for the characterization of biological macromolecules. Advances in Artificial Intelligence , 2012: 3\u20133.   \nPant, M.; Zaheer, H.; Garcia-Hernandez, L.; Abraham, A.;et al. 2020. Differential Evolution: A review of more than two decades of research. Engineering Applications of Artificial Intelligence , 90: 103479.   \nPuranik, Y.; and Sahinidis, N. V. 2017. Bounds tightening based on optimality conditions for nonconvex boxconstrained optimization. Journal of Global Optimization ,67: 59\u201377.   \nSahinidis, N. 2023. BARON: Global Optimization of MixedInteger Nonlinear Programs, User\u2019s Manual. The Optimization Firm, LLC, .   \nStorn, R.; and Price, K. 1997. Differential evolution-a simple and efficient heuristic for global optimization over continuous spaces. Journal of global optimization , 11(4): 341. Tawarmalani, M.; and Sahinidis, N. V. 2005. A polyhedral branch-and-cut approach to global optimization. Mathematical Programming , 103: 225\u2013249.   \nThe Optimization Firm. 2023. NLP and MINLP Test Problems. https://minlp.com/nlp-and-minlp-test-problems. Accessed: Aug.10, 2023.   \nWang, L.; Fonseca, R.; and Tian, Y. 2020a. Learning Search Space Partition for Black-box Optimization using Monte Carlo Tree Search. arXiv preprint arXiv:2007.00708 .  \nWang, L.; Fonseca, R.; and Tian, Y. 2020b. Learning search space partition for black-box optimization using monte carlo tree search. Advances in Neural Information Processing Systems , 33: 19511\u201319522.   \nXiang, Y.; Gubian, S.; Suomela, B.; and Hoeng, J. 2013. Generalized simulated annealing for global optimization: the GenSA package. R J. , 5(1): 13.   \nYang, T. 2019. Advancing non-convex and constrained learning: Challenges and opportunities. AI Matters , 5(3): 29\u201339.   \nYanover, C.; Meltzer, T.; Weiss, Y.; Bennett, K. P.; and Parrado-Hern\u00b4andez, E. 2006. Linear Programming Relaxations and Belief Propagation\u2013An Empirical Study. Journal of Machine Learning Research , 7(9).   \nZhai, Y.; and Gao, S. 2022. Monte Carlo Tree Descent for Black-Box Optimization. In Advances in Neural Information Processing Systems .  \n\nZhu, C.; Byrd, R. H.; Lu, P.; and Nocedal, J. 1997. Algorithm 778: L-BFGS-B: Fortran subroutines for largescale bound-constrained optimization. ACM Transactions on mathematical software (TOMS) , 23(4): 550\u2013560."}, {"ref_id": "454984236218718770", "chunk_id": "0", "score": 0.490234375, "text": "# 2.2 Acceleration of MCTS\nMCTS-based methods have proved their strong capability of solving complex games or tasks. However, the high computational cost of MCTS hinders its application to some real-time and more general scenarios. Therefore, numerous works are devoted to accelerating MCTS. For example, to make the selection stage more effective, some heuristic pruning methods [ 14 ,33 ,29 ,1 ,2 ] aim to reduce the width and depth of the search tree with some heuristic functions. Furthermore, for more efficient evaluations, Lorentz [ 22 ] proposed early playout termination of MCTS (MCTS-EPT) to stop the random playouts early and use an evaluation function to assess win or loss. Moreover, Hsueh et al. [18 ] applied MCTS-EPT to the Chinese dark chess and proved its effectiveness. Afterward, similar ideas have been applied in the evaluation stage of AlphaGoZero [ 32 ] and later MCTS-based methods [31 ,27 ,34 ]. They evaluate the $Q$ -values through a learnable evaluation network instead of running playouts to the end. Grill et al. [15 ] propose a novel regularized policy optimization method based on AlphaZero to decrease the search budget of MCTS, which is from the optimization perspective. Danihelka et al. [10 ] propose a policy improvement algorithm based on sampling actions without replacement, named Gumbel trick to achieve better performance when planning with few simulations. However, these methods mentioned above focus on the specific stage of the search iteration or reduce the total budget through pruning and optimization methods, which are orthogonal to us. And few works targets at the search loop. Lan et al. [21 ] propose DS-MCTS, which defines the uncertainty of MCTS and approximates it by extra DNNs with specific features for board games in training. During the evaluation, DS-MCTS will check periodically and stop the search if the state is certain.\n\n# 3 Background\nThe AlphaGo series of work [ 30 ,32 ,31 ,27 ] are all MCTS-based reinforcement learning algorithms. Those algorithms assume the environment transition dynamics are known or learn the environment dynamics. Based on the dynamics, they use the Monte-Carlo tree search (MCTS) as the policy improvement operator. I.e., taking in the current policy, MCTS returns a better policy with the search algorithm. The systematic search allows the MCTS-based RL algorithm to quickly improve the policy and perform much better in the setting where heavy reasoning is required.\n\n# 3.1 MCTS\nThis part briefly introduces the MCTS method implemented in reinforcement learning applications. As mentioned in the related works, modern MCTS-based RL algorithms include four stages in the search loop, namely selection, expansion, evaluation, and backpropagation.  \n\nMCTS takes in the current states and generates a policy after the search loop of $N$ iterations. Here $N$ is a constant number of iterations set by the designer, regarded as the total budget. In the selection stage of each iteration, an action will be selected by maximizing over UCB. Specifically, AlphaZero [31 ] and MuZero [ 27 ] are developed based on a variant of UCB, P-UCT [ 25 ] and have achieved great success on board games and Atari games. The formula of P-UCT is the Eq (1):  \n\n$$\na^{k}=\\arg\\operatorname*{max}_{a\\in\\mathcal{A}}Q(s,a)+P(s,a)\\frac{\\sqrt{\\sum_{b\\in\\mathcal{A}}N(s,b)}}{1+N(s,a)}(c_{1}+\\log((\\sum_{b\\in\\mathcal{A}}N(s,b)+c_{2}+1)/c_{2})),\n$$  \n\nwhere $k$ is the index of iteration, $\\boldsymbol{\\mathcal{A}}$ is the acti $Q(s,a)$ is the estimated Q-value, $P(s,a)$ is the policy prior obtained from neural networks, $N(s,a)$ is the visitations to select the action a from the state $s$ and $c_{1},c_{2}$ are hyper-parameters. The output of MCTS is the visitation of each action of the root node. After $N$ search iterations, the final policy $\\pi(s)$ is defined as the normalized root visitation distribution simplification, we use $\\pi_{N}(s)$ $\\pi_{k}$ in place of , where $\\begin{array}{r}{\\pi_{k}(s,a)=N(s,a)/\\sum_{b\\in\\mathcal{A}}N(s,b)=N(s,a)/k,a\\in\\mathcal{A}}\\end{array}$ $\\pi_{k}(s)$ sometimes. And the detailed procedure of MCTS is \u2208A . For introduced in Appendix. In our method, we propose to approximate the final policy $\\pi_{N}(s)$ with $\\hat{\\pi}_{k}(s)$ ,which we name as a virtual expanded policy, through a new expansion method and a termination rule. In this way, the number of iterations in MCTS can be reduced from $N$ to $k$ .\n\n# 3.2 Computation Requirement\nMost of the computations in MCTS-based RL are in the MCTS procedure. Each action taken by MCTS requires $N$ times neural network evaluations, where $N$ is a constant number of iterations in the search loop. Traditional RL algorithms, such as PPO [ 28 ] or DQN [ 23 ], only need a single neural network evaluation per action. Thus, MCTS-based RL is roughly $N$ times computationally more expensive than traditional RL algorithms. In practice, training a single Atari game needs 12 hours of computation time on 40 TPUs [ 27 ]. The computation need is roughly two orders of magnitude more than traditional RL algorithms [28], although the final performance of MuZero is much better.\n\n# 4 Method\nWe aim to spend more search time on harder states and less on easier states. Intuitively, human knows when to make a quick decision or a slow decision under different circumstances. Unfortunately, this situation-aware behavior is absent in current MCTS algorithms. Therefore, we propose an MCTS variant that terminates the search iteration adaptively. It consists of two components: a novel expansion method named virtual expansion to estimate the final visitation based on the current partial tree; a termination rule that decides when to terminate based on the hardness of the current scenario. And we will display the adaptive mechanism through visualizations in Section 5.5."}], "task_step_question_answer": "\u5728\u63a2\u7d22\u66f4\u9ad8\u6548\u7684MCTS\u641c\u7d22\u7b97\u6cd5\u65b9\u9762\uff0c\u8fd8\u53ef\u4ee5\u8003\u8651\u5f15\u5165\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u65b9\u6cd5\uff0c\u4ee5\u63d0\u5347\u7b97\u6cd5\u5728\u4e0d\u540c\u73af\u5883\u4e0b\u7684\u9002\u5e94\u6027\u3002\u4f8b\u5982\uff0c\u5728\u673a\u5668\u4eba\u8def\u5f84\u89c4\u5212\u4e2d\uff0c\u7ed3\u5408\u89c6\u89c9\u548c\u89e6\u89c9\u6570\u636e\u53ef\u4ee5\u66f4\u51c6\u786e\u5730\u8bc4\u4f30\u73af\u5883\u72b6\u6001\uff0c\u4ece\u800c\u4f18\u5316\u641c\u7d22\u51b3\u7b56\u3002\u6b64\u5916\uff0c\u6539\u8fdbPRM\u91c7\u6837\u7b56\u7565\u4e5f\u662f\u4e00\u4e2a\u91cd\u8981\u65b9\u5411\uff0c\u901a\u8fc7\u66f4\u667a\u80fd\u7684\u91c7\u6837\u65b9\u6cd5\uff0c\u53ef\u4ee5\u51cf\u5c11\u641c\u7d22\u7a7a\u95f4\u7684\u5197\u4f59\uff0c\u63d0\u9ad8\u641c\u7d22\u6548\u7387\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u53ef\u4ee5\u501f\u9274\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u91c7\u6837\u6280\u672f\uff0c\u5982\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE\uff09\uff0c\u6765\u751f\u6210\u66f4\u5177\u4ee3\u8868\u6027\u7684\u6837\u672c\u70b9\u3002\u540c\u65f6\uff0c\u5bf9\u6bd4\u5206\u6790\u9ad8\u6548MCTS\u641c\u7d22\u7b97\u6cd5\u548c\u6539\u8fdbPRM\u91c7\u6837\u7b56\u7565\u5728\u63d0\u5347\u6a21\u578b\u6027\u80fd\u65b9\u9762\u7684\u4e0d\u540c\u8d21\u732e\uff0c\u6709\u52a9\u4e8e\u6211\u4eec\u66f4\u5168\u9762\u5730\u7406\u89e3\u5404\u7814\u7a76\u65b9\u5411\u7684\u91cd\u8981\u6027\u3002\u4f8b\u5982\uff0c\u5728\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e2d\uff0cMCTS\u7b97\u6cd5\u7684\u4f18\u5316\u53ef\u80fd\u66f4\u4fa7\u91cd\u4e8e\u5b9e\u65f6\u6027\uff0c\u800cPRM\u91c7\u6837\u7b56\u7565\u7684\u6539\u8fdb\u5219\u66f4\u5173\u6ce8\u8def\u5f84\u7684\u5e73\u6ed1\u6027\u548c\u5b89\u5168\u6027\u3002", "ref_task_step_id": ""}, "__type__": "task_step_node"}, "a4d74f6e-ca85-4fff-975f-47b95a5a92ff": {"__data__": {"id_": "a4d74f6e-ca85-4fff-975f-47b95a5a92ff", "metadata": {}, "relationships": {}, "hash": "", "start_task_context": "MCTS\u5728PRM\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u7684\u5e94\u7528\u63a2\u7d22\u7efc\u8ff0", "aemo_representation_context": "### 1. \u5206\u6790\u8fd1\u51e0\u5e74\u7814\u7a76\u9886\u57df\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba\n\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u76f8\u5173\u7814\u7a76\u9886\u57df\uff0c\u9488\u5bf9\u8def\u5f84\u641c\u7d22\u4e0e\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u65b9\u9762\uff0c\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09\u4ee5\u53ca\u6982\u7387\u8def\u7ebf\u56fe\uff08PRM\uff09\u65b9\u6cd5\u53d7\u5230\u5173\u6ce8\u3002\u8fd1\u51e0\u5e74\u7684\u7814\u7a76\u56f4\u7ed5\u5982\u4f55\u5c06 MCTS \u4e0e PRM \u76f8\u7ed3\u5408\uff0c\u63a2\u7d22\u5728\u4e0d\u540c\u4efb\u52a1\u573a\u666f\u4e0b\u5bf9\u504f\u597d\u7b56\u7565\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u7684\u6709\u6548\u6280\u672f\u6846\u67b6\u3002\u65b9\u6cd5\u8bba\u4e0a\uff0c\u591a\u901a\u8fc7\u5b9e\u9a8c\u5bf9\u6bd4\u4e0d\u540c\u53c2\u6570\u8bbe\u7f6e\u548c\u6539\u8fdb\u7b56\u7565\u4e0b\u6a21\u578b\u7684\u6027\u80fd\u8868\u73b0\uff0c\u4ee5\u6b64\u6765\u4f18\u5316\u6280\u672f\u6846\u67b6\u3002\n\n### 2. \u7814\u7a76\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u4e3b\u8981\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0e\u53d8\u4f53\n - **\u4e3b\u8981\u6846\u67b6**\uff1aMCTS \u548c PRM \u662f\u6838\u5fc3\u6846\u67b6\u3002\n - **\u5e94\u7528**\uff1a\u5728\u8def\u5f84\u641c\u7d22\u4efb\u52a1\u4e2d\uff0cPRM \u7528\u4e8e\u6784\u5efa\u73af\u5883\u7684\u6982\u7387\u8def\u7ebf\u56fe\uff0c\u4e3a\u8def\u5f84\u641c\u7d22\u63d0\u4f9b\u57fa\u7840\u7ed3\u6784\uff1bMCTS \u5219\u57fa\u4e8e PRM \u751f\u6210\u7684\u56fe\u8fdb\u884c\u641c\u7d22\u7b56\u7565\u7684\u4f18\u5316\uff0c\u901a\u8fc7\u4e0d\u65ad\u6a21\u62df\u548c\u9009\u62e9\u8def\u5f84\u6765\u627e\u5230\u6700\u4f18\u6216\u8fd1\u4f3c\u6700\u4f18\u8def\u5f84\u3002\u5728\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u4efb\u52a1\u4e2d\uff0c\u5229\u7528 MCTS \u7684\u641c\u7d22\u80fd\u529b\u6765\u63a2\u7d22\u4e0d\u540c\u7684\u7b56\u7565\u7a7a\u95f4\uff0c\u4ee5\u627e\u5230\u80fd\u4f7f\u504f\u597d\u7b56\u7565\u6a21\u578b\u6027\u80fd\u63d0\u5347\u7684\u5fae\u8c03\u53c2\u6570\u3002\n - **\u53d8\u4f53**\uff1a\u4e00\u4e9b\u7814\u7a76\u5bf9 MCTS \u7684\u9009\u62e9\u3001\u6269\u5c55\u3001\u6a21\u62df\u548c\u53cd\u5411\u4f20\u64ad\u6b65\u9aa4\u8fdb\u884c\u6539\u8fdb\uff0c\u4f8b\u5982\u91c7\u7528\u4e0d\u540c\u7684\u9009\u62e9\u7b56\u7565\uff08\u5982 UCT \u53d8\u4f53\uff09\u6765\u63d0\u9ad8\u641c\u7d22\u6548\u7387\uff1b\u5bf9 PRM \u8fdb\u884c\u6539\u8fdb\uff0c\u5982\u8c03\u6574\u91c7\u6837\u7b56\u7565\u4ee5\u66f4\u597d\u5730\u9002\u5e94\u590d\u6742\u73af\u5883\uff0c\u4ece\u800c\u5f62\u6210\u9002\u7528\u4e8e\u4e0d\u540c\u4efb\u52a1\u7684\u53d8\u4f53\u6846\u67b6\u3002\n\n### 3. \u8bc4\u4f30\u5b66\u672f\u754c\u7684\u6280\u672f\u8fdb\u6b65\u4e0e\u5c40\u9650\u6027\n - **\u6280\u672f\u8fdb\u6b65**\uff1a\u901a\u8fc7\u5c06 MCTS \u5e94\u7528\u4e8e PRM \u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\uff0c\u5728\u8def\u5f84\u641c\u7d22\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u4e0a\u6709\u4e00\u5b9a\u63d0\u5347\u3002\u80fd\u591f\u5728\u590d\u6742\u73af\u5883\u4e2d\u66f4\u5feb\u5730\u627e\u5230\u53ef\u884c\u8def\u5f84\uff0c\u5e76\u4e14\u504f\u597d\u7b56\u7565\u6a21\u578b\u7ecf\u8fc7\u5fae\u8c03\u540e\u80fd\u66f4\u597d\u5730\u7b26\u5408\u7279\u5b9a\u4efb\u52a1\u9700\u6c42\u3002\n - **\u5c40\u9650\u6027**\uff1a\u6a21\u578b\u6027\u80fd\u63d0\u5347\u5b58\u5728\u74f6\u9888\u3002\u4e00\u65b9\u9762\uff0cMCTS \u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u8f83\u9ad8\uff0c\u5728\u5927\u89c4\u6a21\u73af\u5883\u6216\u590d\u6742\u4efb\u52a1\u4e2d\u641c\u7d22\u6548\u7387\u53d7\u9650\uff1b\u53e6\u4e00\u65b9\u9762\uff0cPRM \u7684\u91c7\u6837\u7b56\u7565\u53ef\u80fd\u65e0\u6cd5\u5b8c\u5168\u8986\u76d6\u6240\u6709\u6709\u6548\u8def\u5f84\uff0c\u5bfc\u81f4\u641c\u7d22\u7ed3\u679c\u5b58\u5728\u504f\u5dee\u3002\u540c\u65f6\uff0c\u6a21\u578b\u5bf9\u73af\u5883\u6570\u636e\u7684\u4f9d\u8d56\u8f83\u5f3a\uff0c\u5f53\u73af\u5883\u53d1\u751f\u53d8\u5316\u65f6\uff0c\u6a21\u578b\u7684\u9002\u5e94\u6027\u8f83\u5dee\uff0c\u5b58\u5728\u6a21\u578b\u504f\u5dee\u95ee\u9898\u3002\n\n### 4. \u63a2\u8ba8\u8ba1\u7b97\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\n - **\u9002\u7528\u6027**\uff1a\u5728\u7ed3\u6784\u5316\u7a0b\u5ea6\u8f83\u9ad8\u3001\u73af\u5883\u76f8\u5bf9\u7a33\u5b9a\u7684\u6570\u636e\u96c6\u548c\u5e94\u7528\u573a\u666f\u4e2d\uff0c\u57fa\u4e8e MCTS \u548c PRM \u7684\u6a21\u578b\u8868\u73b0\u8f83\u597d\uff0c\u80fd\u591f\u6709\u6548\u5730\u5b8c\u6210\u8def\u5f84\u641c\u7d22\u548c\u504f\u597d\u7b56\u7565\u5fae\u8c03\u4efb\u52a1\u3002\u4f8b\u5982\u5728\u7b80\u5355\u7684\u673a\u5668\u4eba\u8def\u5f84\u89c4\u5212\u573a\u666f\u4e2d\uff0c\u6a21\u578b\u53ef\u4ee5\u5feb\u901f\u751f\u6210\u53ef\u884c\u8def\u5f84\u3002\n - **\u6cdb\u5316\u80fd\u529b**\uff1a\u7136\u800c\uff0c\u5728\u591a\u9886\u57df\u3001\u591a\u6a21\u6001\u6570\u636e\u7684\u73b0\u5b9e\u5e94\u7528\u573a\u666f\u4e2d\uff0c\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002\u4f8b\u5982\u5728\u5305\u542b\u591a\u79cd\u4f20\u611f\u5668\u6570\u636e\uff08\u56fe\u50cf\u3001\u6fc0\u5149\u96f7\u8fbe\u7b49\uff09\u7684\u590d\u6742\u73af\u5883\u4e0b\uff0c\u6a21\u578b\u96be\u4ee5\u878d\u5408\u4e0d\u540c\u6a21\u6001\u7684\u6570\u636e\u8fdb\u884c\u6709\u6548\u51b3\u7b56\uff0c\u5bf9\u4e8e\u4e0d\u540c\u9886\u57df\u7684\u4efb\u52a1\uff08\u5982\u7269\u6d41\u8def\u5f84\u89c4\u5212\u548c\u533b\u7597\u5f71\u50cf\u5206\u6790\u4e2d\u7684\u8def\u5f84\u5f15\u5bfc\uff09\uff0c\u6a21\u578b\u9700\u8981\u5927\u91cf\u7684\u91cd\u65b0\u8bad\u7ec3\u548c\u8c03\u6574\u624d\u80fd\u9002\u5e94\u3002\n\n### 5. \u5206\u6790\u6700\u65b0\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027\n - **\u7a33\u5b9a\u6027**\uff1a\u90e8\u5206\u9488\u5bf9 MCTS \u7a33\u5b9a\u6027\u7684\u4f18\u5316\u63aa\u65bd\u88ab\u63d0\u51fa\uff0c\u4f8b\u5982\u6539\u8fdb\u641c\u7d22\u6811\u7684\u6784\u5efa\u548c\u66f4\u65b0\u7b56\u7565\uff0c\u4f7f\u5f97\u7b97\u6cd5\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u80fd\u591f\u5728\u590d\u6742\u73af\u5883\u4e0b\u4fdd\u6301\u76f8\u5bf9\u7a33\u5b9a\u7684\u6027\u80fd\u3002\u4f46\u5728\u52a8\u6001\u73af\u5883\u4e2d\uff0c\u7531\u4e8e\u73af\u5883\u7684\u5b9e\u65f6\u53d8\u5316\uff0c\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u641c\u7d22\u7ed3\u679c\u53ef\u80fd\u51fa\u73b0\u8f83\u5927\u6ce2\u52a8\u3002\n - **\u5bb9\u9519\u6027**\uff1a\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\uff0c\u7b97\u6cd5\u7684\u5bb9\u9519\u6027\u6709\u9650\u3002\u5f53\u6570\u636e\u5b58\u5728\u566a\u58f0\u6216\u4e0d\u5b8c\u6574\u65f6\uff0cMCTS \u548c PRM \u53ef\u80fd\u4f1a\u751f\u6210\u9519\u8bef\u7684\u8def\u5f84\u6216\u505a\u51fa\u4e0d\u5408\u7406\u7684\u504f\u597d\u7b56\u7565\u8c03\u6574\uff0c\u5f71\u54cd\u6a21\u578b\u7684\u6574\u4f53\u6027\u80fd\u3002\n\n### 6. \u8bc4\u4f30\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218\n - **\u672a\u6765\u7814\u7a76\u65b9\u5411**\uff1a\u63d0\u51fa\u4e86\u63a2\u7d22\u66f4\u9ad8\u6548\u7684 MCTS \u641c\u7d22\u7b97\u6cd5\uff0c\u4ee5\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff1b\u6539\u8fdb PRM \u7684\u91c7\u6837\u7b56\u7565\uff0c\u4f7f\u5176\u80fd\u66f4\u597d\u5730\u9002\u5e94\u590d\u6742\u548c\u52a8\u6001\u73af\u5883\uff1b\u7814\u7a76\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u65b9\u6cd5\uff0c\u63d0\u9ad8\u6a21\u578b\u5728\u591a\u9886\u57df\u3001\u591a\u6a21\u6001\u6570\u636e\u573a\u666f\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u3002\n - **\u6311\u6218**\uff1a\u65b0\u7684\u7814\u7a76\u95ee\u9898\u5305\u62ec\u5982\u4f55\u5728\u4fdd\u8bc1\u7b97\u6cd5\u6548\u7387\u7684\u540c\u65f6\u63d0\u9ad8\u5176\u5728\u590d\u6742\u73af\u5883\u4e0b\u7684\u7a33\u5b9a\u6027\u548c\u5bb9\u9519\u6027\uff1b\u5982\u4f55\u6709\u6548\u878d\u5408\u591a\u6a21\u6001\u6570\u636e\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u3002\u8fd9\u4e9b\u6311\u6218\u63a8\u52a8\u540e\u7eed\u7814\u7a76\u671d\u7740\u4f18\u5316\u7b97\u6cd5\u7ed3\u6784\u3001\u6539\u8fdb\u6570\u636e\u5904\u7406\u65b9\u5f0f\u4ee5\u53ca\u63a2\u7d22\u65b0\u7684\u6a21\u578b\u878d\u5408\u65b9\u6cd5\u7b49\u65b9\u5411\u53d1\u5c55\u3002\n\n### \u7814\u7a76\u6210\u679c\u3001\u65b9\u6cd5\u7684\u521b\u65b0\u6027\u4e0e\u5e94\u7528\u4ef7\u503c\n - **\u7814\u7a76\u6210\u679c**\uff1a\u660e\u786e\u4e86 MCTS \u5728 PRM \u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u4e2d\u7684\u5e94\u7528\u65b9\u5f0f\u53ca\u6548\u679c\uff0c\u5206\u6790\u4e86\u73b0\u6709\u6280\u672f\u7684\u8fdb\u6b65\u4e0e\u5c40\u9650\u3001\u6a21\u578b\u7684\u9002\u7528\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3001\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027\u7b49\u65b9\u9762\u3002\n - **\u65b9\u6cd5\u7684\u521b\u65b0\u6027**\uff1a\u5c06 MCTS \u4e0e PRM \u76f8\u7ed3\u5408\u7528\u4e8e\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\uff0c\u63a2\u7d22\u4e86\u4e0d\u540c\u7684\u6539\u8fdb\u7b56\u7565\u548c\u53d8\u4f53\u6846\u67b6\uff0c\u4e3a\u8def\u5f84\u641c\u7d22\u548c\u504f\u597d\u7b56\u7565\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u548c\u65b9\u6cd5\u3002\n - **\u5e94\u7528\u4ef7\u503c**\uff1a\u5728\u673a\u5668\u4eba\u8def\u5f84\u89c4\u5212\u3001\u7269\u6d41\u8def\u5f84\u89c4\u5212\u7b49\u9886\u57df\u6709\u6f5c\u5728\u5e94\u7528\u4ef7\u503c\uff0c\u901a\u8fc7\u4f18\u5316\u8def\u5f84\u641c\u7d22\u548c\u504f\u597d\u7b56\u7565\uff0c\u80fd\u591f\u63d0\u9ad8\u76f8\u5173\u7cfb\u7edf\u7684\u8fd0\u884c\u6548\u7387\u548c\u51b3\u7b56\u8d28\u91cf\u3002 ", "task_step_name": "\u7814\u7a76\u6210\u679c\u3001\u65b9\u6cd5\u7684\u521b\u65b0\u6027\u4e0e\u5e94\u7528\u4ef7\u503c", "task_step_description": "\u7814\u7a76\u6210\u679c\uff1a\u660e\u786e\u4e86 MCTS \u5728 PRM \u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u4e2d\u7684\u5e94\u7528\u65b9\u5f0f\u53ca\u6548\u679c\uff0c\u5206\u6790\u4e86\u73b0\u6709\u6280\u672f\u7684\u8fdb\u6b65\u4e0e\u5c40\u9650\u3001\u6a21\u578b\u7684\u9002\u7528\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3001\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027\u7b49\u65b9\u9762\u3002\u65b9\u6cd5\u7684\u521b\u65b0\u6027\uff1a\u5c06 MCTS \u4e0e PRM \u76f8\u7ed3\u5408\u7528\u4e8e\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\uff0c\u63a2\u7d22\u4e86\u4e0d\u540c\u7684\u6539\u8fdb\u7b56\u7565\u548c\u53d8\u4f53\u6846\u67b6\uff0c\u4e3a\u8def\u5f84\u641c\u7d22\u548c\u504f\u597d\u7b56\u7565\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u548c\u65b9\u6cd5\u3002\u5e94\u7528\u4ef7\u503c\uff1a\u5728\u673a\u5668\u4eba\u8def\u5f84\u89c4\u5212\u3001\u7269\u6d41\u8def\u5f84\u89c4\u5212\u7b49\u9886\u57df\u6709\u6f5c\u5728\u5e94\u7528\u4ef7\u503c\uff0c\u901a\u8fc7\u4f18\u5316\u8def\u5f84\u641c\u7d22\u548c\u504f\u597d\u7b56\u7565\uff0c\u80fd\u591f\u63d0\u9ad8\u76f8\u5173\u7cfb\u7edf\u7684\u8fd0\u884c\u6548\u7387\u548c\u51b3\u7b56\u8d28\u91cf\u3002 ", "task_step_level": "6", "task_step_question": "MCTS \u4e0e PRM \u76f8\u7ed3\u5408\u7528\u4e8e\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u4e3a\u8def\u5f84\u641c\u7d22\u548c\u504f\u597d\u7b56\u7565\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u548c\u65b9\u6cd5\uff0c\u90a3\u4e48\u5728\u673a\u5668\u4eba\u8def\u5f84\u89c4\u5212\u548c\u7269\u6d41\u8def\u5f84\u89c4\u5212\u7b49\u5177\u6709\u6f5c\u5728\u5e94\u7528\u4ef7\u503c\u7684\u9886\u57df\u4e2d\uff0c\u5177\u4f53\u4f1a\u901a\u8fc7\u600e\u6837\u7684\u5b9e\u65bd\u6b65\u9aa4\u6765\u5b9e\u73b0\u7cfb\u7edf\u8fd0\u884c\u6548\u7387\u548c\u51b3\u7b56\u8d28\u91cf\u7684\u63d0\u9ad8\u5462\uff1f  ", "task_step_question_context": [{"ref_id": "454848282814999732", "chunk_id": "2", "score": 0.2734375, "text": "# 2 BACKGROUND &RELATED WORK\nModel-predictive control refers to the use of model-based search or planning over a short horizon for selecting an action. In order to make it computationally tractable, it is common to \u201cseed\u201d the search at a given timestep with either an action from a proposal policy or an action obtained by warmstarting from the result of the previous timestep. In simulation, it has been demonstrated that a fairly generic MPC implementation can be effective for control of a relatively high DoF humanoid (Tassa et al., 2012) and that MPC with learned models (Chua et al., 2018; Wang & Ba, 2020; Nagabandi et al., 2020) can achieve data-efficient solutions to simple control problems. Real RC helicopter control has also been achieved using an MPC approach that made use of a learned model (Abbeel et al., 2006). MPC approaches are gaining wider use in robotics (multicopters (Neunert et al., 2016; Torrente et al., 2021), quadruped (Grandia et al., 2019; Sleiman et al., 2021), humanoids (Tassa et al., 2014; Kuindersma et al., 2016)), and dexterous manipulation (Nagabandi et al., 2020); but the computational speed of the planner is a bottleneck for hardware deployment. There are different ways around this, with the core practical solution being to plan in lower dimensional reduced coordinate models. Alternatively, POPLIN (Wang & Ba, 2020) explores learning proposals for MPC and planner distillation but is tested on simple tasks and does not leverage model-free RL for proposal learning.  \n\nAmortized policies map states to actions quickly, but implicitly reflect considerable knowledge by being trained on diverse data. Model-free RL produces policies which amortize the knowledge reflected in rollouts required to produce them. Similarly, it is possible to produce diverse trajectories from a planner and distil them into a single policy (Levine & Koltun, 2013; Mordatch & Todorov, 2014; Mordatch et al., 2015; Pan et al., 2017; Xie et al., 2021).  \n\nReinforcement learning approaches with MPC have become more popular recently, and our work fits within this space. As noted in the introduction, previous work often emphasizes the role of amortization through the learning of value functions and models. TreePI (Springenberg et al., 2020), MuZero (Schrittwieser et al., 2020; Hubert et al., 2021), SAVE (Hamrick et al., 2020), DPI (Sun et al., 2018) and MPC-Net (Carius et al., 2020) all perform versions of hybrid learning with model-based MCTS or planning being used to gather data which is then used to train the model and value function to accelerate learning. Other recently proposed algorithmic innovations blend MPC with learned value estimates to trade off model and value errors (Bhardwaj et al., 2021). Here, we primarily consider learning dynamics models to enable transfer to new settings with different reward functions.  \n\nOther uses of models unlike hybrid MPC-RL schemes have also been explored in the literature; however, they are not the focus of this work. Nevertheless we highlight two of these approaches: value gradients can be backpropagated through dynamics models to improve credit assignment (Heess et al., 2015; Amos et al., 2020) and it is possible to train policies on model rollouts to improve data efficiency (Janner et al., 2019). Recently, there has also been considerable effort to explore model-based approaches in combination with offline RL in order to gain full value from offline datasets (Yu et al., 2020; Argenson & Dulac-Arnold, 2021; Kidambi et al., 2020).\n\n# 3 TASKS\nIn this paper we consider a number of locomotion tasks of varying complexity, simulated with the MuJoCo (Todorov et al., 2012) physics simulator. We consider two embodiments: an 8-DoF ant from dm_control (Tunyasuvunakool et al., 2020) and a model of the Robotis OP3 robot with 20 degrees of freedom. For each embodiment, we consider three tasks: walking forward, walking backward and \u201cgo-to-target-pose\u201d (GTTP), a challenging task that is the focus of our evaluation. In all tasks, the agent receives egocentric proprioceptive observations (joint angles, velocities and end-effector positions) and additional task observations.  \n\nIn the walking forward and backward tasks the agent is rewarded for maximizing forward (or backward) velocity in the direction of a narrow corridor. For the OP3 robot we also include a small pose regularization term. The task is specified through a relative target direction observation.  \n\nThe GTTP task, which builds on existing motion tracking infrastructure(Hasenclever et al., 2020), consists of either body on a plane, with a target pose in relative coordinates as a task-specific observation and proximity to the target pose rewarded. When the agent is within a threshold distance of the target pose (i.e. it achieves the current target), it gets a sparse reward and a new target is sampled. For the ant we use target poses  \n\n  \nFigure 1: Go-to-target-pose (GTTP) task with the OP3 & Ant. The agent has to reach the target pose (blue).  \n\nfrom policies trained on a standard go-to-target task (Tunyasuvunakool et al., 2020). For the OP3, we use poses from the CMU mocap database (cmu) (retargeted to the robot). We use thousands of different target poses; the agent has to learn to transition between them. Thus the GTTP task can be seen as either a single highly diverse task or as a multi-task setting with strongly related tasks and consistent dynamics. We believe the GTTP task should be particularly amenable to model-based methods: it combines a high-dimensional control problem with a diverse goal distribution. This makes it hard to solve quickly with model-free methods. However, since the dynamics are shared between all goal poses, a dynamics models should be beneficial in leveraging the common structure. See supplement for an in-depth motivation and results on tasks from the DeepMind Control Suite (Tassa et al., 2018)."}, {"ref_id": "454845604178965768", "chunk_id": "4", "score": 0.255859375, "text": "# A RELATED WORK\nModel-based RL methods for solving decision-making problems focus on three key perspectives: how to learn the model? how to use the learned model to learn the policy? And how to make the decision using the learned model and policy? Besides, decision-making that relies on the model is also investigated in the optimal control theory field which is deeply related to model-based RL.  \n\nModel learning: How to learn a good model to support decision-making is a crucial problem in model-based RL. There are two main aspects of the work: the model structure designing and the loss designing. For model structure designing, ensemble-based model (Chua et al., 2018), dropout mechanisms (Zhang et al., 2021), auto-regressive structure (Zhang et al., 2020), stochastic hidden model (Hafner et al., 2021), and transformer based model (Chen et al., 2022) are always considered to improve the model robustness and prediction accuracy. For loss designing, decision awareness (D\u2019Oro et al., 2020; Farahmand et al., 2017) and gradient awareness (Li et al., 2021) are always considered to reduce the gap between model learning and model utilization.  \n\nPolicy learning: Two methods are always used to learn the policy by using the learned model. One is to serve the learned model as a black-box simulator to generate the data. Janner et al. (2019b) is a representing work of this line. Yu et al. (2020), Lee et al. (2020) also follow such a manner by extending it to offline-RL setting. Another way is to use the learned model to calculate the policy gradient. Heess et al. (2015b) presents an algorithm to calculate the policy gradient by backpropagating through the model. Clavera et al. (2019) and Amos et al. (2021) share similar methods but use promising actor and critic learning strategy to achieve better performance.  \n\nDecision-making: When making the decision, we need to generate the actions that can achieve our goal. Most of the model-based RL methods make the decision by using the learned policy solely (Janner et al., 2019b; Yu et al., 2020; Clavera et al., 2019; Hafner et al., 2021). Similar to our paper, some works also try to make decisions by using the learned model, but the majority only focus on the discrete action space. For example, the well-known Alpha Zero system (Silver et al., 2017) uses MCTS to derive the action by using the known model. In MuZero and (Schrittwieser et al., 2020), the authors propose to use a learned model combined with an MCTS planner to achieve significant performances in a broad range of tasks within discrete action space. There are only a few works that study the continuous action space. Cou\u00a8etoux et al. (2011) extends the MCTS framework to continuous action space but also needs to know the real model and handle the model. In Hubert et al. (2021), the author proposed a sampled MuZero algorithm to handle the complex action space by planning over sampled actions. In Hansen et al. (2022a), the authors propose to learn a value function that can be used as long term return in the Cross-Entropy (CE) method for planning.  \n\nOptimal control: Beyond deep RL, optimal control also considers the decision-making problem but rather relies on the known and continuous transition model. In modern optimal control theory, Model Predictive Control (MPC) (Camacho & Alba, 2013) framework is always adopted when the environment is highly non-linear. In MPC, the action is planned during the execution by using the model, and such a procedure is called trajectory optimization. There are plenty of previous works that use the MPC framework to solve continuous control tasks. For example, Byravan et al. (2021) proposes to use sampling-based MPC for high-dimensional continuous control tasks with learned models and a learned policy as a proposal distribution. Pinneri et al. (2021) proposes an improved version of the Cross-Entropy Method for efficient planning. Nagabandi et al. (2020) proposes a PDDM method that uses a gradient-free planner algorithm combined with online MPC method to learn flexible contact-rich dexterous manipulation skills.  \n\nDifferential Dynamical Programming: The most relevant works are DDP (Murray & Yakowitz, 1984), iLQR (Li & Todorov, 2004), and iLQG (Tassa et al., 2012). Differentiable Dynamic Programming (DDP) (Tassa et al., 2012) employs the Bellman equation structure (Murray & Yakowitz, 1984; Pantoja, 1988; Aoyama et al., 2021) and has fast convergence property. It becomes more and more popular in the control field. iLQR (Li & Todorov, 2004), and iLQG (Tassa et al., 2012; Todorov & Li, 2005) are two variants of the DDP. In iLQR and iLQG, the second-order derivative of the environment model is ignored (set as zero). Therefore, iLQR and iLQG are more computationally efficient compared to the original DDP method. Since both iLQG and our D3P planner are motivated by DDP, they look similar naturally. But our method has several key differences compared with theirs, and these differences are well-designed to incorporate the neural network model. (1) DDP, iLQR, and iLQG are both pure planning algorithms that require a known environment model. (2) Computing the second-order derivative of the neural network based model is computationally costly (Hessian matrix). In our method, we only rely on the first-order derivative of the model. (3) The previous methods use the second-order Talyor expansion of the Q-value function to handle the local optimization problem. But it is hard to guarantee that the hessian matrix is a negative definite matrix, which is a necessary condition for convergence. Here, we construct an auxiliary target function $D$ and use a first-order Talyor expansion for the $Q$ function inside of the $D$ function to guarantee the non-positive definite matrix."}, {"ref_id": "454848283080813758", "chunk_id": "7", "score": 0.255859375, "text": "# Layered and Staged Monte Carlo Tree Search for SMT Strategy Synthesis\nZhengyang $\\left(\\mathbf{Johm}\\right)\\mathbf{Lu}^{1}$ ,Stefan Siemer 2 ,Piyush Jha 3 ,Joel Day 4 ,Florin Manea 2 and Vijay Ganesh 3 1 University of Waterloo 2 University of G\u00a8ottingen 3 Georgia Institute of Technology 4 Loughborough University\n\n# Abstract\nModern SMT solvers, such as Z3 , offer usercontrollable strategies, enabling users to tailor them for their unique set of instances, thus dramatically enhancing solver performance for their use case. However, this approach of strategy customization presents a significant challenge: handcrafting an optimized strategy for a class of SMT instances remains a complex and demanding task for both solver developers and users alike.  \n\nIn this paper, we address this problem of automatic SMT strategy synthesis via a novel Monte Carlo Tree Search (MCTS) based method. Our method treats strategy synthesis as a sequential decisionmaking process, whose search tree corresponds to the strategy space, and employs MCTS to navigate this vast search space. The key innovations that enable our method to identify effective strategies, while keeping costs low, are the ideas of layered and staged MCTS search. These novel approaches allow for a deeper and more efficient exploration of the strategy space, enabling us to synthesize more effective strategies than the default ones in state-ofthe-art (SOTA) SMT solvers. We implement our method, dubbed Z3alpha , as part of the Z3 SMT solver. Through extensive evaluations across 6 important SMT logics, Z3alpha demonstrates superior performance compared to the SOTA synthesis tool FastSMT , the default Z3 solver, and the CVC5 solver on most benchmarks. Remarkably, on a challenging QF BV benchmark set, Z3alpha solves $42.7\\%$ more instances than the default strategy in the Z3 SMT solver.\n\n# 1 Introduction\nSatisfiability Modulo Theories (SMT) solvers [De Moura and Bj\u00f8rner, 2011] are key tools in diverse fields such as software engineering [Cadar et al. , 2008], verification [Gurfinkel et al. , 2015], security [Song et al. , 2008], and artificial intelligence [Pulina and Tacchella, 2012]. It has long been observed that no single solver or algorithm excels across all instances of a given SMT logic or of a problem class. As a result, modern SMT solvers, such as Z3 [De Moura and Bj\u00f8rner, 2008], offer user-controllable strategies [De Moura and Passmore, 2013], enabling users to customize a sub-solver for their class of instances. A strategy can be thought of as an algorithmic recipe for selecting, sequencing, and parameterizing tactics .Each tactic is a well-defined algorithmic proof rule or symbolic reasoning step, provided by the solver. For example, propagate-values is a Z3 tactic that propagates equalities, while sat and smt are the tactic wrappers of the main SAT and SMT solver in Z3 . A strategy builds a decision procedure by combining tactics, as shown in an exemplar strategy (if is-pb (then propagate-values sat) smt) . This strategy specifies a solver algorithm that, given an input instance, applies propagate-values followed by sat if the instance is a pseudo-boolean problem (as checked using is-pb ), or applies smt otherwise.  \n\nDefault solver strategies are typically optimized for wellestablished benchmarks, such as those in the SMT-LIB library [Barrett et al. , 2016]. However, as the scope of SMT applications continues to grow rapidly, users frequently encounter specialized, evolving, and unprecedented classes of instances. In these scenarios, the default or the existing customized strategies might not be as effective. Consequently, there arises a need for novel customized strategies, specifically designed to efficiently address the unique challenges posed by users\u2019 specific problems. Traditionally, this task of strategy customization has been undertaken by human experts through extensive experimentation and benchmark analysis. However, even with their expertise and efforts, the task remains challenging due to the intricate interactions among tactics and the vast search space for potential strategies.  \n\nEarly attempts have been made to automatically synthesize SMT strategies. For instance, StratEVO [Ram\u00b4\u0131rez et al. , 2016] searches for an optimal strategy using evolutionary algorithms, while FastSMT [Balunovic et al. , 2018] synthesizes a tailored strategy using imitation learning and decision tree learning techniques. While these methods show promise in automating strategy customization, they suffer from issues such as a lack of robustness, limited interpretability, and extensive training times.  \n\nTo address these issues, we introduce a novel SMT strategy synthesis method that employs Monte Carlo Tree Search (MCTS). MCTS is a heuristic search algorithm, widely applied in computer board game players as a planning algorithm [Browne et al. , 2012]. Its prominence further escalated following its successful integration into the groundbreaking deep reinforcement learning systems AlphaGo [Silver et al. ,2016] and AlphaZero [Silver et al. , 2017], where MCTS was employed as a policy improvement operator. Recently, MCTS has shown remarkable success as a standalone algorithm in solving complex symbolic or combinatorial search problems, as evidenced in Khalil et al. [2022] and Sun et al. [2023]. Its key strengths, including its ability to effectively balance exploration and exploitation and adapt to the nuances of varied search problems, making it an excellent method for such challenging tasks. Our work is the first to apply MCTS to the SMT strategy synthesis problem."}], "task_step_question_answer": "\u6b64\u5916\uff0c\u5728\u533b\u7597\u624b\u672f\u673a\u5668\u4eba\u8def\u5f84\u89c4\u5212\u9886\u57df\uff0c\u8be5\u65b9\u6cd5\u4e5f\u5177\u6709\u5e7f\u9614\u7684\u5e94\u7528\u524d\u666f\u3002\u624b\u672f\u73af\u5883\u590d\u6742\u4e14\u5bf9\u7cbe\u5ea6\u8981\u6c42\u6781\u9ad8\uff0c\u5229\u7528MCTS\u4e0ePRM\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u9996\u5148\u5bf9\u624b\u672f\u90e8\u4f4d\u7684\u89e3\u5256\u7ed3\u6784\u8fdb\u884c\u7cbe\u786e\u5efa\u6a21\uff0c\u8fd9\u5c31\u5982\u540c\u4e3a\u624b\u672f\u673a\u5668\u4eba\u7ed8\u5236\u8be6\u7ec6\u7684\u201c\u5730\u56fe\u201d\u3002\u63a5\u7740PRM\u6784\u5efa\u57fa\u7840\u8def\u5f84\u56fe\uff0c\u901a\u8fc7\u5bf9\u624b\u672f\u64cd\u4f5c\u7a7a\u95f4\u7684\u5408\u7406\u5212\u5206\uff0c\u786e\u5b9a\u53ef\u884c\u7684\u64cd\u4f5c\u8def\u5f84\u7247\u6bb5\u3002MCTS\u7ed3\u5408\u624b\u672f\u7684\u7279\u6b8a\u504f\u597d\u7b56\u7565\uff0c\u5982\u907f\u514d\u635f\u4f24\u91cd\u8981\u7ec4\u7ec7\u3001\u5668\u5b98\uff0c\u4f18\u5148\u9009\u62e9\u6700\u77ed\u64cd\u4f5c\u8def\u5f84\u7b49\uff0c\u4ece\u624b\u672f\u8d77\u59cb\u4f4d\u7f6e\u4e3a\u6839\u8282\u70b9\uff0c\u9010\u6b65\u641c\u7d22\u6700\u4f73\u64cd\u4f5c\u8def\u5f84\u3002\u5728\u624b\u672f\u8fc7\u7a0b\u4e2d\uff0c\u6839\u636e\u5b9e\u65f6\u7684\u7ec4\u7ec7\u53d8\u5316\u3001\u51fa\u8840\u60c5\u51b5\u7b49\u52a8\u6001\u4fe1\u606f\uff0c\u5bf9\u8def\u5f84\u8fdb\u884c\u5fae\u8c03\uff0c\u786e\u4fdd\u624b\u672f\u7684\u7cbe\u51c6\u6027\u548c\u5b89\u5168\u6027\u3002\u901a\u8fc7\u6a21\u62df\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8be5\u65b9\u6cd5\u6709\u671b\u5c06\u624b\u672f\u64cd\u4f5c\u7684\u8bef\u5dee\u964d\u4f4e\u81f3\u6beb\u7c73\u7ea7\u522b\uff0c\u5927\u5927\u63d0\u9ad8\u624b\u672f\u7684\u6210\u529f\u7387\u3002  ", "ref_task_step_id": ""}, "__type__": "task_step_node"}}, "task_step_store/ref_task_step_info": {"": {"node_ids": ["49abb75e-21b0-4504-8500-80c6e0bd9a07", "03a69f17-fbfb-4e1c-93cb-b83ae52d8393", "9cfb30a6-9a28-45f1-a55b-2f78b05406c7", "9518ca14-d51c-44c3-8ac8-8d3e16c1f870", "db0646b0-b961-4f49-a6bd-1cdc05884162", "03892289-ab6a-4270-a42f-9a0864808a7c", "a4d74f6e-ca85-4fff-975f-47b95a5a92ff", "03892289-ab6a-4270-a42f-9a0864808a7c", "db0646b0-b961-4f49-a6bd-1cdc05884162", "49abb75e-21b0-4504-8500-80c6e0bd9a07", "03a69f17-fbfb-4e1c-93cb-b83ae52d8393", "9518ca14-d51c-44c3-8ac8-8d3e16c1f870", "9cfb30a6-9a28-45f1-a55b-2f78b05406c7", "a4d74f6e-ca85-4fff-975f-47b95a5a92ff", "db0646b0-b961-4f49-a6bd-1cdc05884162", "03892289-ab6a-4270-a42f-9a0864808a7c", "db0646b0-b961-4f49-a6bd-1cdc05884162", "49abb75e-21b0-4504-8500-80c6e0bd9a07", "03892289-ab6a-4270-a42f-9a0864808a7c", "03a69f17-fbfb-4e1c-93cb-b83ae52d8393", "9518ca14-d51c-44c3-8ac8-8d3e16c1f870", "03892289-ab6a-4270-a42f-9a0864808a7c", "03892289-ab6a-4270-a42f-9a0864808a7c", "49abb75e-21b0-4504-8500-80c6e0bd9a07", "db0646b0-b961-4f49-a6bd-1cdc05884162", "db0646b0-b961-4f49-a6bd-1cdc05884162", "a4d74f6e-ca85-4fff-975f-47b95a5a92ff", "9cfb30a6-9a28-45f1-a55b-2f78b05406c7", "03a69f17-fbfb-4e1c-93cb-b83ae52d8393", "9518ca14-d51c-44c3-8ac8-8d3e16c1f870", "a4d74f6e-ca85-4fff-975f-47b95a5a92ff", "9cfb30a6-9a28-45f1-a55b-2f78b05406c7", "03a69f17-fbfb-4e1c-93cb-b83ae52d8393", "03a69f17-fbfb-4e1c-93cb-b83ae52d8393", "9518ca14-d51c-44c3-8ac8-8d3e16c1f870", "9518ca14-d51c-44c3-8ac8-8d3e16c1f870", "49abb75e-21b0-4504-8500-80c6e0bd9a07", "49abb75e-21b0-4504-8500-80c6e0bd9a07", "9cfb30a6-9a28-45f1-a55b-2f78b05406c7", "9cfb30a6-9a28-45f1-a55b-2f78b05406c7", "a4d74f6e-ca85-4fff-975f-47b95a5a92ff", "a4d74f6e-ca85-4fff-975f-47b95a5a92ff", "9cfb30a6-9a28-45f1-a55b-2f78b05406c7", "9cfb30a6-9a28-45f1-a55b-2f78b05406c7", "49abb75e-21b0-4504-8500-80c6e0bd9a07", "49abb75e-21b0-4504-8500-80c6e0bd9a07", "03892289-ab6a-4270-a42f-9a0864808a7c", "03892289-ab6a-4270-a42f-9a0864808a7c", "03a69f17-fbfb-4e1c-93cb-b83ae52d8393", "03a69f17-fbfb-4e1c-93cb-b83ae52d8393", "db0646b0-b961-4f49-a6bd-1cdc05884162", "db0646b0-b961-4f49-a6bd-1cdc05884162", "9518ca14-d51c-44c3-8ac8-8d3e16c1f870"], "metadata": {}}}, "task_step_store/metadata": {"49abb75e-21b0-4504-8500-80c6e0bd9a07": {"task_step_hash": "", "ref_task_step_id": ""}, "03a69f17-fbfb-4e1c-93cb-b83ae52d8393": {"task_step_hash": "", "ref_task_step_id": ""}, "9cfb30a6-9a28-45f1-a55b-2f78b05406c7": {"task_step_hash": "", "ref_task_step_id": ""}, "9518ca14-d51c-44c3-8ac8-8d3e16c1f870": {"task_step_hash": "", "ref_task_step_id": ""}, "db0646b0-b961-4f49-a6bd-1cdc05884162": {"task_step_hash": "", "ref_task_step_id": ""}, "03892289-ab6a-4270-a42f-9a0864808a7c": {"task_step_hash": "", "ref_task_step_id": ""}, "a4d74f6e-ca85-4fff-975f-47b95a5a92ff": {"task_step_hash": "", "ref_task_step_id": ""}}}