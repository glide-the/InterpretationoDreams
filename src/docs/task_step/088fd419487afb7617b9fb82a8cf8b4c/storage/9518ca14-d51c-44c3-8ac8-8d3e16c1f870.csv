角色,内容,分镜
9518ca14-d51c-44c3-8ac8-8d3e16c1f870,探讨计算模型在不同数据集与应用场景下的适用性与泛化能力,3
9518ca14-d51c-44c3-8ac8-8d3e16c1f870,基于 MCTS 和 PRM 的模型在结构化程度较高、环境相对稳定的场景中表现良好，在多领域、多模态数据场景中泛化能力不足，那么如何提升该模型在多领域、多模态数据现实应用场景下的泛化能力呢？ ,3
9518ca14-d51c-44c3-8ac8-8d3e16c1f870,"ref_ids: 454847880064807282, chunk_ids: 6, Score: 0.1748, Text: # 5 Experiment
To examine the performance of MCTS-VS, we conduct experiments on different tasks, including synthetic functions, NAS-bench problems and MuJoCo locomotion tasks, to compare MCTS-VS with other black-box optimization methods. For MCTS-VS, we use the same hyper-parameters except $C_{p}$ , which is used for calculating UCB in Eq. (1). For Dropout and embedding-based methods, we set the parameter $d$ to the number of valid dimensions for synthetic functions, and a reasonable value for real-world problems. The hyper-parameters of the same components of different methods are set to the same. We use five identical random seeds (2021–2025) for all problems and methods. More details about the settings can be found in Appendix C. Our code is available at https://github.com/lamda-bbo/MCTS-VS .",3
9518ca14-d51c-44c3-8ac8-8d3e16c1f870,"ref_ids: 454959909841148186, chunk_ids: 3, Score: 0.1475, Text: # 4 Experiment
In this section, we evaluate whether MT-CRL could benefit the performance of MTL models on existing benchmark datasets, and study whether it could indeed alleviate spurious correlation.  

Experimental Setup One key ingredient of our MT-CRL is to achieve the optimality of causal graph over different distributions. However, we might not access multiple environmental labels in most real-world multi-task learning datasets. Therefore, we adopt a more realistic setup, such that we only assume to have a single validation set that contains unknown distribution shifts (i.e. change of confounder $C_{d i s t}^{M T L}$ ) compared to the training dataset. We thus could utilize training and valid sets as two environments to calculate invariance regularization, while we only utilize the training set to calculate task loss to avoid the task predictor overfits. Note that in this way, our method could get access to the label information in the validation set. To avoid the possibility that the performance improvement is brought by additional label, for all the other baseline methods, we also add the validation data into the training set to calculate task loss and learn MTL model.  

Dataset. We choose five widely-used real-world MTL benchmark datasets, i.e., Multi-MNIST (Sun, 2019), MovieLens (Harper & Konstan, 2016), Tasknomy (Zamir et al., 2018), NYUv2 (Silberman et al., 2012) and CityScape (Cordts et al., 2016), and try to determine train/valid/test split such that there exist distribution shifts between these sets. Dataset details are in Appendix C.2. Note that except NYUv2, our data split is the same as the default split settings of these datasets, which also try to test model’s capacity to generalize across domains.  

Baselines As MT-CRL is a regularization framework built upon modular MTL architecture (in this paper we choose MMoE as instantiation, but it can be applied to other modular networks), we mainly compare with two gradient-based multi-task optimization baselines: PCGrad (Yu et al., 2020) and GradVac (Wang et al., 2021). We also compare with two domain generalization baselines: IRM (Ahuja et al., 2020) and DANN (Ganin et al., 2016). For IRM we adopt different per-task predictors instead of all-one vector to adapt MTL setup, and calculate penalty via Eq. (6).  

For a fair comparison, all methods are based on the same MMoE architecture. We set number of modules $(K)$ as 8 that achieves best result, details shown in Appendix E. We train all the models via Adam optimizer, and tune other key hyperparameters, including learning rate, regularization term, etc. for each baseline via grid search, and report the results with the best configuration.  

Table 3: Ablation Studies of disentangled and Graph regularization components in MT-CRL, evaluated on Multi-MNIST dataset.   


<html><body><table><tr><td colspan=""2"">Disentangled Reg.</td><td colspan=""2"">Graph Reg.</td><td colspan=""2"">Multi-MNIST</td></tr><tr><td>Ldecor</td><td>Lβ-VAE</td><td>Lsps</td><td>Lbal</td><td></td><td>Accuracy</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td>0.915 ±0.018</td></tr><tr><td>×</td><td></td><td></td><td></td><td></td><td>0.896±0.024</td></tr><tr><td>×</td><td></td><td></td><td></td><td></td><td>0.882 ±0.020</td></tr><tr><td></td><td>×</td><td>×</td><td></td><td>×</td><td>0.891 ±0.016</td></tr><tr><td></td><td>×</td><td></td><td></td><td></td><td>0.903±0.017</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td>0.908±0.021</td></tr></table></body></html>  

  
Figure 4: Task-to-Module gradients of model without MT-CRL show Module 5 is spurious. MTCRL could help alleviate spurious correlation.

# 4.1 Experiment Results
As each task has a different evaluation metric and cannot be directly compared, we calculate the relative performance improvement of each method compared to vanilla MTL, and then average the relative improvement for all tasks of each dataset. As summarized in Table 2, the average improvement of MT-CRL with $\\mathcal{L}_{G-I R M}^{V a r}$ is $5.5\\%$ , significantly higher than all other baseline methods. The most critical step of MT-CRL is to learn correct causal graph. We therefore report MT-CRL with different invariance regularization. As is shown in the last block, $\\mathcal{L}_{G-I R M}^{V a r}$ achieve better results for most datasets than drop the relative performance. Compared to IRM which calculate gradient and update per-task $\\mathcal{L}_{G-I R M}^{N o r m}$ , while removing the invariance regularization could significantly predictors, MT-CRL uses disentangled modules and G-IRM to avoid overfitting to achieve invariance. Results show that for datasets with large amount of tasks, e.g., Taskonomy and NYUv2, MT-CRL significantly outperform IRM, showing the modification is more suitable for MTL setup.  

Ablation Studies We then study the effectiveness of the other two components in MT-CRL, i.e., disentangled and graph regularization. We mainly report the ablation studies on Multi-MNIST in table 3 as it’s relatively small so that we could quickly get the results of all combinations.  

which fits our discussion that we cannot conduct causal learning over entangled modules. We also For disentangled regularization, after removing $\\mathcal{L}_{d e c o r}$ , the performance drops from 0.915 to 0.882, explore one classical generative disentangled representation method, i.e., $\\beta$ -VAE. As shown in the table, the results of using $\\beta$ -VAE are 0.896, lower than our utilized decorrelation regularization.. We hypothesize that this is probably because not all generative factors are useful for downstream tasks. Generative objectives might compete for the model capacity and in addition, the unused factors could be potentially spurious.  

Another key component is graph regularization. After removing both drops to 0 .891 . This show that even if invariance regularization could penalize non-causal modules, it $\\mathcal{L}_{s p s}$ and $\\mathcal{L}_{b a l}$ , the performance would be better to force their weights to be zero via sparsity regularization, and to be non-degenerate via balance regularization. We also conduct ablation studies to remove either show both are important, and combining the two could help to achieve the best results. $\\mathcal{L}_{s p s}$ or $\\mathcal{L}_{b a l}$ , and results Case Study To show that real-world MTL problem indeed have spurious correlation problem and our MT-CRL could alleciate it, we take MovieLens as an example to conduct case study. Each task is for different movie types, and bag-of-word of movie title is one of the features. We calculate the task-to-module gradients $\\frac{\\partial(f(\\Phi(\\bar{x}))[y])}{\\partial F}$ of the vanilla MMoE model without MT-CRL. We then visualize ‘train’ gradients, which shows how much each module is utilized to fit the training set, and ‘valid-train’ gradients, which shows how generalizable each module is. We find that module 5 is utilized for children movie, but harmful in valid set, indicating it is a spurious feature. We then use Grad-CAM to show that top words of module 5 include strip and die , which is not relevant to children movies. One possible reason is that some children movies contain the words club , which is often co-occurred with strip and die in crime and war movies. After adding our MT-CRL, the module assigned to ‘children’ movie attends Pink ,Parenthood ,Alice and Jungle . We also show the saliency map of MT-CRL for Multi-MNIST in Figure 6 in Appendix. Both examples show MT-CRL could indeed alleviate spurious correlation in real MTL problems.",3
9518ca14-d51c-44c3-8ac8-8d3e16c1f870,"ref_ids: 454845560984707998, chunk_ids: 8, Score: 0.1289, Text: # 4.3.2 Effectiveness of Proposed Methods
In Table 6 , we provide results using different pretraining tasks and masking strategies to demonstrate the effectiveness of our proposed modules.  

Comparing #1 and #2 in Table 6 , we observe that WWM brings significant performance improvements on all datasets. The reason is that it increases the difficulty of the MLM task, so we can obtain a stronger language model. We also find that LAM can also brings consistent improvements on all dataset because LAM can force the model to learn better representations for layout information, which is beneficial to downstream tasks.  

Comparing #2 to #4 and #3 to #5, it is observed that the MPM task also brings considerable improvements on all datasets. MPM works as an auxiliary task to help the MLM task and can increase the pre-training difficulty, contributing to learning better and more robust layout representations.  

Moreover, the full-version LayoutMask (#5) outperforms the naive version (#1) by a large margin $\\operatorname{FUNSD}\\!+\\!3.18\\%$ , CORD $+0.67\\%$ ,$\\mathrm{SROIE}{+}1.11\\%$ ,and $\\mathbf{RV}\\mathrm{L-}\\mathrm{CDIP+}1.09\\%)$ , demonstrating the effectiveness of our proposed modules when working together. To better illustrate the effectiveness of our model design, we list category-level accuracy improvements on RVL-CDIP dataset and provide detailed discussions in Section Bof the Appendix.

# 5 Conclusion
In this paper, we propose LayoutMask, a novel multi-modal pre-training model, to solve the reading order issues in VrDU tasks. LayoutMask adopts local 1D position as layout input and can generate adaptive and robust multi-modal representations. In LayoutMask, we equip the MLM task with two masking strategies and design a novel pretraining objective, Masked Position Modeling, to enhance the text-layout interactions and layout representation learning. With only using text and layout modalities, our method can achieve excellent results and significantly outperforms many SOTA methods in VrDU tasks.

# Limitations
Our method has the following limitations:  

Datasets: In multi-modal pre-training, we rely on downstream datasets to evaluate the performance of pre-trained models. The commonly used entity extraction datasets are relatively small and lack diversity, so the proposed method may not generalize well to real word scenarios.  

Lack of Image Modality: In LayoutMask, we focus on text-layout interactions, leaving the image modality unexplored. However, documents in the real world contain many elements that can not be described by text and layout modalities, like figures and lines, so incorporating image modality is important in building a universal multi-modal pre-training model for document understanding.



# A Ablation Study of Masking Probabilities
We compare LayoutMask using different $\\mathrm{P_{mlm}}$ and $\\mathrm{P_{mpm}}$ , and the results are in Figure 4 . We first find the best $\\mathrm{P_{mlm}}$ without using the MPM task, and the optimal value is $25\\%$ . Then we fix such optimal $\\mathrm{P_{mlm}}$ to find the best $\\mathrm{P_{mpm}}$ , which is $15\\%$ as the results show.

# BAblation Study on RVL-CDIP
To further understand the effectiveness of our model design, we list the detailed classification results on RVL-CDIP dataset with the naive version and the full version in Table 7 . It is observed that the major performance improvements come from three categories: presentation $(+3.36\\%)$ , ad",3
