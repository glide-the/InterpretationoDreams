{"task_step_store/data": {"276fc94b-2aa5-4712-ab27-c1e4f31af69f": {"__data__": {"id_": "276fc94b-2aa5-4712-ab27-c1e4f31af69f", "metadata": {}, "relationships": {}, "hash": "", "start_task_context": "MCTS\u5728PRM\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u7684\u5e94\u7528\u63a2\u7d22\u7efc\u8ff0", "aemo_representation_context": "### Step 1: \u5206\u6790\u8fd1\u51e0\u5e74\u7814\u7a76\u9886\u57df\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba\n\n**\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09**\uff1a\n- **\u57fa\u672c\u539f\u7406**\uff1aMCTS\u662f\u4e00\u79cd\u7528\u4e8e\u51b3\u7b56\u8fc7\u7a0b\u7684\u542f\u53d1\u5f0f\u641c\u7d22\u7b97\u6cd5\uff0c\u5e7f\u6cdb\u5e94\u7528\u4e8e\u6e38\u620fAI\u3001\u8def\u5f84\u89c4\u5212\u7b49\u9886\u57df\u3002\u5176\u6838\u5fc3\u601d\u60f3\u662f\u901a\u8fc7\u6a21\u62df\u591a\u6b21\u968f\u673a\u6e38\u620f\u6765\u8bc4\u4f30\u4e0d\u540c\u51b3\u7b56\u7684\u4f18\u52a3\u3002\n- **\u6280\u672f\u6846\u67b6**\uff1a\u4e3b\u8981\u5305\u62ec\u9009\u62e9\uff08Selection\uff09\u3001\u6269\u5c55\uff08Expansion\uff09\u3001\u6a21\u62df\uff08Simulation\uff09\u548c\u56de\u6eaf\uff08Backpropagation\uff09\u56db\u4e2a\u6b65\u9aa4\u3002\n\n**\u504f\u597d\u7b56\u7565\u6a21\u578b\uff08PRM\uff09**\uff1a\n- **\u57fa\u672c\u539f\u7406**\uff1aPRM\u65e8\u5728\u901a\u8fc7\u5b66\u4e60\u7528\u6237\u7684\u504f\u597d\u6765\u4f18\u5316\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5e38\u7528\u4e8e\u63a8\u8350\u7cfb\u7edf\u3001\u4e2a\u6027\u5316\u670d\u52a1\u7b49\u3002\n- **\u6280\u672f\u6846\u67b6**\uff1a\u901a\u5e38\u5305\u62ec\u504f\u597d\u5efa\u6a21\u3001\u7b56\u7565\u4f18\u5316\u548c\u6a21\u578b\u5fae\u8c03\u7b49\u73af\u8282\u3002\n\n**\u65b9\u6cd5\u8bba**\uff1a\n- **\u96c6\u6210\u5b66\u4e60**\uff1a\u5c06MCTS\u4e0ePRM\u7ed3\u5408\uff0c\u5229\u7528MCTS\u7684\u641c\u7d22\u80fd\u529b\u4f18\u5316PRM\u7684\u51b3\u7b56\u8fc7\u7a0b\u3002\n- **\u5f3a\u5316\u5b66\u4e60**\uff1a\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u673a\u5236\u4e0d\u65ad\u8c03\u6574PRM\u7684\u53c2\u6570\uff0c\u4ee5\u9002\u5e94\u52a8\u6001\u73af\u5883\u3002\n\n### Step 2: \u7814\u7a76\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u4e3b\u8981\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0e\u53d8\u4f53\n\n**\u5e94\u7528\u573a\u666f**\uff1a\n- **\u6e38\u620fAI**\uff1aMCTS\u4e0ePRM\u7ed3\u5408\u7528\u4e8e\u6e38\u620f\u4e2d\u7684\u667a\u80fd\u51b3\u7b56\uff0c\u5982AlphaGo\u3002\n- **\u8def\u5f84\u89c4\u5212**\uff1a\u5728\u673a\u5668\u4eba\u5bfc\u822a\u548c\u81ea\u52a8\u9a7e\u9a76\u4e2d\uff0c\u5229\u7528MCTS\u4f18\u5316\u8def\u5f84\u9009\u62e9\u3002\n- **\u63a8\u8350\u7cfb\u7edf**\uff1a\u901a\u8fc7PRM\u5b66\u4e60\u7528\u6237\u504f\u597d\uff0c\u7ed3\u5408MCTS\u8fdb\u884c\u66f4\u7cbe\u51c6\u7684\u63a8\u8350\u3002\n\n**\u53d8\u4f53**\uff1a\n- **\u6df1\u5ea6MCTS**\uff1a\u7ed3\u5408\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff0c\u63d0\u5347MCTS\u7684\u51b3\u7b56\u80fd\u529b\u3002\n- **\u591a\u6a21\u6001PRM**\uff1a\u878d\u5408\u591a\u79cd\u6570\u636e\u6e90\uff08\u5982\u6587\u672c\u3001\u56fe\u50cf\uff09\uff0c\u589e\u5f3aPRM\u7684\u6cdb\u5316\u80fd\u529b\u3002\n\n### Step 3: \u8bc4\u4f30\u5b66\u672f\u754c\u7684\u6280\u672f\u8fdb\u6b65\u4e0e\u5c40\u9650\u6027\n\n**\u6280\u672f\u8fdb\u6b65**\uff1a\n- **\u6027\u80fd\u63d0\u5347**\uff1aMCTS\u4e0ePRM\u7ed3\u5408\u663e\u8457\u63d0\u5347\u4e86\u51b3\u7b56\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002\n- **\u6cdb\u5316\u80fd\u529b**\uff1a\u591a\u6a21\u6001PRM\u5728\u5904\u7406\u591a\u6837\u5316\u6570\u636e\u65f6\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002\n\n**\u5c40\u9650\u6027**\uff1a\n- **\u8ba1\u7b97\u590d\u6742\u5ea6**\uff1aMCTS\u5728\u5927\u89c4\u6a21\u95ee\u9898\u4e2d\u8ba1\u7b97\u91cf\u5927\uff0c\u6548\u7387\u6709\u5f85\u63d0\u5347\u3002\n- **\u6570\u636e\u4f9d\u8d56**\uff1aPRM\u7684\u6027\u80fd\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u9ad8\u8d28\u91cf\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u6570\u636e\u504f\u5dee\u95ee\u9898\u4f9d\u7136\u5b58\u5728\u3002\n\n### Step 4: \u63a2\u8ba8\u8ba1\u7b97\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\n\n**\u9002\u7528\u6027**\uff1a\n- **\u591a\u9886\u57df\u5e94\u7528**\uff1aMCTS+PRM\u6846\u67b6\u5728\u6e38\u620f\u3001\u8def\u5f84\u89c4\u5212\u3001\u63a8\u8350\u7cfb\u7edf\u7b49\u591a\u4e2a\u9886\u57df\u8868\u73b0\u51fa\u826f\u597d\u7684\u9002\u7528\u6027\u3002\n- **\u591a\u6a21\u6001\u6570\u636e**\uff1a\u591a\u6a21\u6001PRM\u80fd\u591f\u5904\u7406\u591a\u79cd\u7c7b\u578b\u7684\u6570\u636e\uff0c\u9002\u7528\u4e8e\u590d\u6742\u5e94\u7528\u573a\u666f\u3002\n\n**\u6cdb\u5316\u80fd\u529b**\uff1a\n- **\u8de8\u9886\u57df\u6cdb\u5316**\uff1a\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\uff0cMCTS+PRM\u6846\u67b6\u5728\u4e0d\u540c\u9886\u57df\u95f4\u5c55\u73b0\u51fa\u8f83\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002\n- **\u52a8\u6001\u73af\u5883\u9002\u5e94**\uff1a\u5f3a\u5316\u5b66\u4e60\u673a\u5236\u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u9002\u5e94\u52a8\u6001\u53d8\u5316\u7684\u73af\u5883\u3002\n\n### Step 5: \u5206\u6790\u6700\u65b0\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027\n\n**\u7a33\u5b9a\u6027**\uff1a\n- **\u7b97\u6cd5\u4f18\u5316**\uff1a\u901a\u8fc7\u6539\u8fdbMCTS\u7684\u9009\u62e9\u7b56\u7565\u548cPRM\u7684\u53c2\u6570\u8c03\u6574\u673a\u5236\uff0c\u63d0\u5347\u4e86\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u3002\n- **\u9c81\u68d2\u6027\u589e\u5f3a**\uff1a\u5f15\u5165\u5bf9\u6297\u8bad\u7ec3\u548c\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\uff0c\u63d0\u9ad8\u6a21\u578b\u5728\u590d\u6742\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u6027\u3002\n\n**\u5bb9\u9519\u6027**\uff1a\n- **\u9519\u8bef\u5904\u7406**\uff1a\u5728\u6a21\u62df\u548c\u56de\u6eaf\u9636\u6bb5\u5f15\u5165\u5bb9\u9519\u673a\u5236\uff0c\u51cf\u5c11\u9519\u8bef\u51b3\u7b56\u7684\u5f71\u54cd\u3002\n- **\u5927\u89c4\u6a21\u6570\u636e\u9002\u5e94\u6027**\uff1a\u4f18\u5316\u7b97\u6cd5\u7ed3\u6784\uff0c\u63d0\u5347\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\u7684\u5904\u7406\u80fd\u529b\u3002\n\n### Step 6: \u8bc4\u4f30\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218\n\n**\u672a\u6765\u7814\u7a76\u65b9\u5411**\uff1a\n- **\u9ad8\u6548\u8ba1\u7b97**\uff1a\u7814\u7a76\u66f4\u9ad8\u6548\u7684MCTS\u7b97\u6cd5\uff0c\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002\n- **\u6570\u636e\u589e\u5f3a**\uff1a\u63a2\u7d22\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u51cf\u5c11PRM\u5bf9\u9ad8\u8d28\u91cf\u6570\u636e\u7684\u4f9d\u8d56\u3002\n\n**\u6311\u6218**\uff1a\n- **\u8de8\u6a21\u6001\u878d\u5408**\uff1a\u5982\u4f55\u6709\u6548\u878d\u5408\u591a\u79cd\u6a21\u6001\u6570\u636e\uff0c\u63d0\u5347\u6a21\u578b\u7684\u7efc\u5408\u6027\u80fd\u3002\n- **\u52a8\u6001\u73af\u5883\u9002\u5e94**\uff1a\u5728\u52a8\u6001\u73af\u5883\u4e0b\uff0c\u5982\u4f55\u4fdd\u6301\u6a21\u578b\u7684\u7a33\u5b9a\u6027\u548c\u9ad8\u6548\u6027\u3002\n\n**\u63a8\u52a8\u540e\u7eed\u7814\u7a76**\uff1a\n- **\u65b0\u95ee\u9898\u63d0\u51fa**\uff1a\u5982\u591a\u6a21\u6001\u6570\u636e\u4e0b\u7684\u504f\u597d\u5efa\u6a21\u3001\u52a8\u6001\u73af\u5883\u4e0b\u7684\u51b3\u7b56\u4f18\u5316\u7b49\u3002\n- **\u7814\u7a76\u5207\u5165\u70b9**\uff1a\u4ece\u7b97\u6cd5\u4f18\u5316\u3001\u6570\u636e\u589e\u5f3a\u3001\u8de8\u6a21\u6001\u878d\u5408\u7b49\u591a\u4e2a\u89d2\u5ea6\u5207\u5165\uff0c\u63a8\u52a8\u6280\u672f\u8fdb\u6b65\u3002\n\n### \u603b\u7ed3\n\n\u8fd1\u5e74\u6765\uff0cMCTS\u5728PRM\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u7684\u5e94\u7528\u63a2\u7d22\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4e3b\u8981\u4f53\u73b0\u5728\u6280\u672f\u6846\u67b6\u7684\u5b8c\u5584\u3001\u5e94\u7528\u573a\u666f\u7684\u62d3\u5c55\u3001\u6027\u80fd\u7684\u63d0\u5347\u4ee5\u53ca\u7a33\u5b9a\u6027\u548c\u5bb9\u9519\u6027\u7684\u589e\u5f3a\u3002\u7136\u800c\uff0c\u4ecd\u5b58\u5728\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u3001\u6570\u636e\u4f9d\u8d56\u6027\u5f3a\u7b49\u5c40\u9650\u6027\u3002\u672a\u6765\u7684\u7814\u7a76\u5e94\u5173\u6ce8\u9ad8\u6548\u8ba1\u7b97\u3001\u6570\u636e\u589e\u5f3a\u548c\u8de8\u6a21\u6001\u878d\u5408\u7b49\u65b9\u5411\uff0c\u4ee5\u5e94\u5bf9\u52a8\u6001\u73af\u5883\u4e0b\u7684\u51b3\u7b56\u4f18\u5316\u7b49\u6311\u6218\uff0c\u63a8\u52a8\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002", "task_step_name": "\u5206\u6790\u8fd1\u51e0\u5e74\u7814\u7a76\u9886\u57df\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba", "task_step_description": "\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09\u662f\u4e00\u79cd\u7528\u4e8e\u51b3\u7b56\u8fc7\u7a0b\u7684\u542f\u53d1\u5f0f\u641c\u7d22\u7b97\u6cd5\uff0c\u5e7f\u6cdb\u5e94\u7528\u4e8e\u6e38\u620fAI\u3001\u8def\u5f84\u89c4\u5212\u7b49\u9886\u57df\u3002\u5176\u6838\u5fc3\u601d\u60f3\u662f\u901a\u8fc7\u6a21\u62df\u591a\u6b21\u968f\u673a\u6e38\u620f\u6765\u8bc4\u4f30\u4e0d\u540c\u51b3\u7b56\u7684\u4f18\u52a3\u3002", "task_step_level": "0", "task_step_question": "### \u95ee\u9898\n\n\u5728\u5206\u6790\u8fd1\u51e0\u5e74\u7814\u7a76\u9886\u57df\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba\u65f6\uff0c\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09\u4f5c\u4e3a\u4e00\u79cd\u542f\u53d1\u5f0f\u641c\u7d22\u7b97\u6cd5\uff0c\u5176\u6838\u5fc3\u601d\u60f3\u662f\u901a\u8fc7\u6a21\u62df\u591a\u6b21\u968f\u673a\u6e38\u620f\u6765\u8bc4\u4f30\u4e0d\u540c\u51b3\u7b56\u7684\u4f18\u52a3\u3002\u7136\u800c\uff0cMCTS\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u9762\u4e34\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u5927\u89c4\u6a21\u95ee\u9898\u4e2d\u3002\u57fa\u4e8e\u6b64\uff0c**\u5982\u4f55\u901a\u8fc7\u6539\u8fdbMCTS\u7684\u6280\u672f\u6846\u67b6\u6216\u5f15\u5165\u65b0\u7684\u65b9\u6cd5\u8bba\u6765\u964d\u4f4e\u5176\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u5176\u51b3\u7b56\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\uff1f**", "task_step_question_context": [{"ref_id": "455026805323333778", "chunk_id": "1", "score": 0.5390625, "text": "# Layered and Staged Monte Carlo Tree Search for SMT Strategy Synthesis\nZhengyang $\\left(\\mathbf{Johm}\\right)\\mathbf{Lu}^{1}$ ,Stefan Siemer 2 ,Piyush Jha 3 ,Joel Day 4 ,Florin Manea 2 and Vijay Ganesh 3 1 University of Waterloo 2 University of G\u00a8ottingen 3 Georgia Institute of Technology 4 Loughborough University\n\n# Abstract\nModern SMT solvers, such as Z3 , offer usercontrollable strategies, enabling users to tailor them for their unique set of instances, thus dramatically enhancing solver performance for their use case. However, this approach of strategy customization presents a significant challenge: handcrafting an optimized strategy for a class of SMT instances remains a complex and demanding task for both solver developers and users alike.  \n\nIn this paper, we address this problem of automatic SMT strategy synthesis via a novel Monte Carlo Tree Search (MCTS) based method. Our method treats strategy synthesis as a sequential decisionmaking process, whose search tree corresponds to the strategy space, and employs MCTS to navigate this vast search space. The key innovations that enable our method to identify effective strategies, while keeping costs low, are the ideas of layered and staged MCTS search. These novel approaches allow for a deeper and more efficient exploration of the strategy space, enabling us to synthesize more effective strategies than the default ones in state-ofthe-art (SOTA) SMT solvers. We implement our method, dubbed Z3alpha , as part of the Z3 SMT solver. Through extensive evaluations across 6 important SMT logics, Z3alpha demonstrates superior performance compared to the SOTA synthesis tool FastSMT , the default Z3 solver, and the CVC5 solver on most benchmarks. Remarkably, on a challenging QF BV benchmark set, Z3alpha solves $42.7\\%$ more instances than the default strategy in the Z3 SMT solver.\n\n# 1 Introduction\nSatisfiability Modulo Theories (SMT) solvers [De Moura and Bj\u00f8rner, 2011] are key tools in diverse fields such as software engineering [Cadar et al. , 2008], verification [Gurfinkel et al. , 2015], security [Song et al. , 2008], and artificial intelligence [Pulina and Tacchella, 2012]. It has long been observed that no single solver or algorithm excels across all instances of a given SMT logic or of a problem class. As a result, modern SMT solvers, such as Z3 [De Moura and Bj\u00f8rner, 2008], offer user-controllable strategies [De Moura and Passmore, 2013], enabling users to customize a sub-solver for their class of instances. A strategy can be thought of as an algorithmic recipe for selecting, sequencing, and parameterizing tactics .Each tactic is a well-defined algorithmic proof rule or symbolic reasoning step, provided by the solver. For example, propagate-values is a Z3 tactic that propagates equalities, while sat and smt are the tactic wrappers of the main SAT and SMT solver in Z3 . A strategy builds a decision procedure by combining tactics, as shown in an exemplar strategy (if is-pb (then propagate-values sat) smt) . This strategy specifies a solver algorithm that, given an input instance, applies propagate-values followed by sat if the instance is a pseudo-boolean problem (as checked using is-pb ), or applies smt otherwise.  \n\nDefault solver strategies are typically optimized for wellestablished benchmarks, such as those in the SMT-LIB library [Barrett et al. , 2016]. However, as the scope of SMT applications continues to grow rapidly, users frequently encounter specialized, evolving, and unprecedented classes of instances. In these scenarios, the default or the existing customized strategies might not be as effective. Consequently, there arises a need for novel customized strategies, specifically designed to efficiently address the unique challenges posed by users\u2019 specific problems. Traditionally, this task of strategy customization has been undertaken by human experts through extensive experimentation and benchmark analysis. However, even with their expertise and efforts, the task remains challenging due to the intricate interactions among tactics and the vast search space for potential strategies.  \n\nEarly attempts have been made to automatically synthesize SMT strategies. For instance, StratEVO [Ram\u00b4\u0131rez et al. , 2016] searches for an optimal strategy using evolutionary algorithms, while FastSMT [Balunovic et al. , 2018] synthesizes a tailored strategy using imitation learning and decision tree learning techniques. While these methods show promise in automating strategy customization, they suffer from issues such as a lack of robustness, limited interpretability, and extensive training times.  \n\nTo address these issues, we introduce a novel SMT strategy synthesis method that employs Monte Carlo Tree Search (MCTS). MCTS is a heuristic search algorithm, widely applied in computer board game players as a planning algorithm [Browne et al. , 2012]. Its prominence further escalated following its successful integration into the groundbreaking deep reinforcement learning systems AlphaGo [Silver et al. ,2016] and AlphaZero [Silver et al. , 2017], where MCTS was employed as a policy improvement operator. Recently, MCTS has shown remarkable success as a standalone algorithm in solving complex symbolic or combinatorial search problems, as evidenced in Khalil et al. [2022] and Sun et al. [2023]. Its key strengths, including its ability to effectively balance exploration and exploitation and adapt to the nuances of varied search problems, making it an excellent method for such challenging tasks. Our work is the first to apply MCTS to the SMT strategy synthesis problem."}, {"ref_id": "454845587396505206", "chunk_id": "0", "score": 0.53125, "text": "# BLIMITATION AND FUTURE WORK\nCurrently, our method TS-LLM still cannot scale to really large-scale scenarios due to the extra computation burdens introduced by node expansion and value evaluation. Additional engineering work such as key value caching is required to accelerate the tree-search. In addition, we do not cover all feasible action-space designs for tree search and it is flexible to propose advanced algorithms to automatically construct a tree mixed with both sentence-level expansion and token-level expansion, etc. We leave such exploration for future work. For MCTS aggregation, the current method still struggles to improve under large aggregation numbers. some new algorithms that can encourage multi-search diversity might be needed. Currently, we are still actively working on scaling up our method both during inference and training (especially multi-iteration training).\n\n# CBACKGROUND OF MONTE CARLO TREE -SEARCH A LGORIHTMS\nOnce we build the tree, we can use various search algorithms to find a high-reward trace. However, it\u2019s not easy to balance between exploration and exploitation during the search process, especially when the tree is sufficiently deep. Therefore we adopt Monte Carlo Tree Search(MCTS) variants as choices for strategic and principled search. Instead of the four operations in traditional MCTS (Kocsis & Szepesv\u00b4ari, 2006; Coulom, 2006), we refer to the search process in AlphaZero (Silver et al., 2017a) and introduce 3 basic operations of a standard search simulation in it as follows, when searching actions from current state node $s_{0}$ :  \n\nSelect It begins at the root node of the search tree, of the current state, $s_{0}$ , and finishes when reaching a leaf node $s_{L}$ at timestep $L$ . At each of these $L$ timesteps(internal nodes), an action(child node) is selected according to $a_{t}=\\arg\\operatorname*{max}_{a}\\left(Q(s_{t},a)+U(s_{t},a)\\right)$ where $U(s_{t},a)$ is calculated by a variant of PUCT algorithm (Rosin, 2011):  \n\n$$\nU(s,a)=c_{\\mathrm{puct}}\\cdot\\pi_{\\theta}(s,a)\\frac{\\sqrt{\\sum_{b}N(s,b)}}{1+N(s,a)}\n$$  \n\n$N(s,a)$ isit count of selecting action $a$ at node $s$ , and $\\begin{array}{r}{c_{\\mathrm{puct}}\\,=\\,\\log((\\sum_{b}N(s,b)\\,+\\,}\\end{array}$ P$c_{\\mathrm{base}}+1)/c_{\\mathrm{base}})+c_{\\mathrm{init}}$ is controlled by visit count and two constants. This search control strategy initially prefers actions with high prior probability and low visit count, but asymptotically prefers actions with high action-value.  \n\nExpand and evaluate After encountering a leaf node $s_{L}$ by select , if $s_{L}$ is not a terminal node, it will be expanded by the language model policy. The state of the leaf node is evaluated by the value network, noted as $v(s_{L})$ . If $s_{L}$ is a terminal node, if there is an oracle reward function $R$ , then $v(s_{L})=R(s_{L})$ , otherwise, in this paper, we use an ORM $\\hat{r}$ as an approximation of it.  \n\nBackup After expand and evaluate on a leaf node, backward the statistics through the path $s_{L},s_{L-1},\\ldots,s_{0}$ , for each node, increase the visit count by $N(s_{t},a_{t})\\,=\\,N(s_{t},a_{t})+1$ , and the total action-value are updated as $W(s_{t},a_{t})\\,=\\,W(s_{t},a_{t})\\,\\dot{+}\\,v(s_{L})$ , the mean action-value are updated as $Q(s_{t},a_{t})=W(s_{t},a_{t})/N(s_{t},a_{t})$ .  \n\nIn this paper, we introduce 3 variants of MCTS based on the above basic operations.\n\n# DEXPERIMENT DETAILS\n\n# D.1 TASK SETUPS\nGSM8k GSM8k (Cobbe et al., 2021) is a commonly used numerical reasoning dataset, Given a context description and a question, it takes steps of mathematical reasoning and computation to arrive at a final answer. There are about $7.5\\mathrm{k}$ problems in the training dataset and $1.3\\mathrm{k}$ problems in the test dataset.  \n\nGame24 We also test our methods on Game24(Yao et al., 2023) which has been proven to be hard even for state-of-the-art LLMs like GPT-4. Each problem in Game24 consists of 4 integers between 1 and 13. And LLMs are required to use each number exactly once by $(+\\mathrm{~-~}\\times\\div)$ to get a result equal to 24 We follow Yao et al. (2023) by using a set of 1362 problems sorted from easy to hard according to human solving time. We split the first 1k problems as the training dataset and the last 362 hard problems as the test dataset. For each problem in the training dataset, we collect data for SFT by enumerating all possible correct answers.  \n\nPrOntoQA PrOntoQA (Saparov & He, 2022) is a typical logical reasoning task in which a language model is required to verify whether a hypothesis is true or false given a set of facts and logical rules. There are 4k problems in the training dataset and 500 problems in the test dataset.  \n\nRLHF We choose a synthetic RLHF dataset Dahoas 1 serving as the query data. We split the dataset to 30000/3000 as training and test set respectively. For the reward model, we choose reward-modeldeberta-v3-large$\\cdot\\mathbf{V}2^{2}$ from OpenAssistant, which is trained from several RLHF datasets.\n\n# D.2 SFT AND VALUE TRAINING DETAILS\nSFT in GSM8k, Game24 and PrOntoQA : For GSM8k, Game24 and PrOntoQA, we first train LLaMA2-7b on the training dataset The training is conducted on 8 NVIDIA A800 GPUs, using a cosine scheduler decaying from $\\scriptstyle{\\mathrm{lr}=2\\ e-5}$ to 0.0 with a warmup ratio of 0.03, batch size 128 for 3 epochs. For GSM8k and Game24 we use the checkpoint at the last epoch as the direct policy in experiments, while for PrOntoQA we use the checkpoint at the 1st epoch since the others overfit.  \n\nValue training in GSM8k, Game24 and PrOntoQA : Then we train the value function on the data rollout by the SFT policy. In GSM8k and Game24, For each model checkpoints of 3 epochs during SFT, we first collect 100 outputs per problem in the training dataset, then duplicate the overlapped answers, labeled each answer with our training set outcome reward ocracle. For data sampled by ech model checkpoint, we subsample 17 answers per problem, which is in total at most 51 answers per problem after deduplication. In PrOntoQA, we only sample 50 answers per problem with the first epoch model checkpoint and then do deduplication.  \n\nThe value functions are trained in the same setting as supervised finetuning. We set the reward to be 1 when the output answer is correct and -1 otherwise. Then we use MC with $\\gamma=1$ to compute the returns. We do model selection on a validation dataset sampled from the direct policy model. For GSM8k, we train the value function and ORM for one epoch, while for Game24 and PrOntoQA we train the value function and ORM for 3 epochs.  \n\nSFT in RLHF alignment : We utilize GPT2-open-instruct 3 , a GPT2-Small model supervisedfinetuned over several instruction-tuning dataset.  \n\nValue training in RLHF alignment : Based on the SFT model, we collect 50 rollouts by the SFT policy for each question in the training set and label their final reward with the reward model. Then we train the value function and ORM for 2 epochs.  \n\nNote that here we start training the value function and ORM from the data sampled by the SFT policy model through direct decoding just as an initialization of the value function and ORM. After that TS-LLM can optimize the policy model, value function, and ORM simultaneously by adding new data sampled from tree search into the training buffer."}, {"ref_id": "454845580969520102", "chunk_id": "5", "score": 0.51953125, "text": "# Spending Thinking Time Wisely: Accelerating MCTS with Virtual Expansions\nWeirui $\\mathbf{Ye}^{*}$ Pieter Abbeel \u2020Yang Gao $\\ast\\ddag\\S$ \u2217Tsinghua University, \u2020UC Berkeley, \u00a7Shanghai Qi Zhi Institute\n\n# Abstract\nOne of the most important AI research questions is to trade off computation versus performance since \u201cperfect rationality\" exists in theory but is impossible to achieve in practice. Recently, Monte-Carlo tree search (MCTS) has attracted considerable attention due to the significant performance improvement in various challenging domains. However, the expensive time cost during search severely restricts its scope for applications. This paper proposes the Virtual MCTS (V-MCTS), a variant of MCTS that spends more search time on harder states and less search time on simpler states adaptively. We give theoretical bounds of the proposed method and evaluate the performance and computations on $9\\times9$ Go board games and Atari games. Experiments show that our method can achieve comparable performances to the original search algorithm while requiring less than $50\\%$ search time on average. We believe that this approach is a viable alternative for tasks under limited time and resources. The code is available at https://github.com/YeWR/V-MCTS.git .\n\n# 1 Introduction\nWhen artificial intelligence was first studied in the 1950s, researchers have sought to find the solution to the question \u201cHow to build an agent with perfect rationality\". The term \u201cperfect rationality\" [7 ,24 ,26 ] here refers to the decision made with infinite amounts of computations. However, one can only solve small-scale problems without considering the practical computation time since classical search algorithms usually exhibit exponential running time. Therefore, recent AI research would no longer seek to achieve \u201cperfect rationality\", but instead carefully trade-off computation versus the level of rationality. People have developed computational models like \u201cbounded optimality\" to model these settings [ 26 ]. The increasing level of rationality under the same computational budget has given us a lot of AI successes. Algorithms include the Monte-Carlo sampling algorithms, the variational inference algorithms, and using DNNs as universal function approximators [9, 8, 13, 30, 17].  \n\nRecently, MCTS-based RL algorithms have achieved much success, mainly on board games. The most notable achievement is that AlphaGo beats Hui Fan in 2015 [ 30 ]. It is the first time a computer program beat a human professional Go player. Afterward, AlphaGo beats two top-ranking human players, Lee Sedol in 2016 and Jie Ke in 2017, the latter of which ranked first worldwide at the time. Later, MCTS-based RL algorithms were further extended to other board games and Atari games [ 27 ]. EfficientZero [ 34 ] significantly improves the sample efficiency of MCTS-based RL algorithms, shedding light on its future applications in the real world like robotics and self-driving.  \n\nDespite the impressive performance of MCTS-based RL algorithms, they require massive amounts of computation to train and evaluate. For example, MuZero [ 27 ] used 1000 TPUs trained for 12 hours to learn the game of Go, and for a single Atari game, it needs 40 TPUs to train 12 hours. Compared to previous algorithms on the Atari games benchmark, it needs around two orders of magnitude more compute. This prohibitively large computational requirement has slowed down both the further development of MCTS-based RL algorithms as well as its practical use.  \n\nUnder the hood, MCTS-based RL algorithms imagine the futures when taking different future action sequences. However, this imaging process for the current method is not computationally efficient. For example, AlphaGo needs to look ahead 1600 game states to place a single stone. On the contrary, top human professional players can only think through around 100-200 game states per minute [ 30 ]. Apart from the inefficiency, the current MCTS algorithm deals with easy and challenging cases with the same computational budget. However, human knows to use their time when it is most needed.  \n\nIn this paper, we aim to design new algorithms that save the computational time of the MCTSbased RL methods. We make three key contributions : (1) We present Virtual MCTS, a variant of MCTS, to approximate the vanilla MCTS search policies with less computation. Moreover, unlike previous pruning-based methods that focus on the selection or evaluation stage in MCTS, our method improves the search loop. It terminates the search iterations earlier adaptively when current states are simpler; (2) Theoretically, we provide some error bounds of the proposed method. Furthermore, the visualization results indicate that Virtual MCTS has a better computation and performance trade-off than vanilla MCTS; (3) Empirically, our method can save more than $50\\%$ of search times on the challenging game Go $9\\times9$ and more than $60\\%$ on the visually complex Atari games while keeping comparable performances to those of vanilla MCTS.\n\n# 2 Related Work\n\n# 2.1 Reinforcement Learning with MCTS\nFor a long time, Computer Go has been regarded as a remarkably challenging game [ 3 ,6 ]. Researchers attempt to use Monte-Carlo techniques that evaluate the value of the node state through random playouts [ 4 ,11 ,12 ,30 ]. Afterward, UCT algorithms have generally been applied in Monte-Carlo tree search (MCTS) algorithms, which use UCB1 to select action at each node of the tree [ 20 ]. Recently, MCTS-based RL methods [ 30 ,32 ,31 ,27 ] have become increasingly popular and achieved super-human performances on board games because of their strong ability to search.  \n\nModern MCTS-based RL algorithms include four stages in the search loop : selection, expansion, evaluation, and backpropagation. The computation bottlenecks in vanilla MCTS come from the search loop, especially for the evaluation stage and the selection stage of each iteration. The selection stage is time-consuming when the search tree becomes wider and deeper. The evaluation stage is quite expensive because people attempt to evaluate the node value by random playouts to the end in previous researches. Due to the search loop, MCTS-based algorithms have multiple model inferences compared to other model-free RL methods like PPO [28] and SAC [16]."}], "task_step_question_answer": "276fc94b-2aa5-4712-ab27-c1e4f31af69f:\u300c\u9488\u5bf9MCTS\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u7684\u95ee\u9898\uff0c\u8fd1\u5e74\u6765\u7814\u7a76\u8005\u63d0\u51fa\u4e86\u591a\u79cd\u6539\u8fdb\u65b9\u6cd5\uff0c\u4ee5\u4e0b\u662f\u4e00\u4e9b\u5173\u952e\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba\uff1a\n\n1. **\u5206\u5c42\u4e0e\u5206\u9636\u6bb5MCTS\uff08Layered and Staged MCTS\uff09**  \n   \u5728SMT\u7b56\u7565\u5408\u6210\u4e2d\uff0c\u7814\u7a76\u8005\u63d0\u51fa\u4e86\u5206\u5c42\u4e0e\u5206\u9636\u6bb5MCTS\u7684\u65b9\u6cd5\u3002\u901a\u8fc7\u5c06\u641c\u7d22\u8fc7\u7a0b\u5206\u4e3a\u591a\u4e2a\u5c42\u6b21\u548c\u9636\u6bb5\uff0c\u80fd\u591f\u66f4\u9ad8\u6548\u5730\u63a2\u7d22\u7b56\u7565\u7a7a\u95f4\uff0c\u51cf\u5c11\u4e0d\u5fc5\u8981\u7684\u8ba1\u7b97\u5f00\u9500\u3002\u4f8b\u5982\uff0cZ3alpha\u65b9\u6cd5\u901a\u8fc7\u8fd9\u79cd\u6539\u8fdb\uff0c\u5728\u591a\u4e2aSMT\u903b\u8f91\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u5de5\u5177\u3002\n\n2. **\u865a\u62dfMCTS\uff08Virtual MCTS, V-MCTS\uff09**  \n   V-MCTS\u901a\u8fc7\u81ea\u9002\u5e94\u5730\u5206\u914d\u641c\u7d22\u65f6\u95f4\uff0c\u5c06\u66f4\u591a\u8d44\u6e90\u7528\u4e8e\u590d\u6742\u72b6\u6001\uff0c\u51cf\u5c11\u5bf9\u7b80\u5355\u72b6\u6001\u7684\u8ba1\u7b97\u3002\u8fd9\u79cd\u65b9\u6cd5\u57289x9\u56f4\u68cb\u548cAtari\u6e38\u620f\u4e2d\u5b9e\u73b0\u4e86\u4e0e\u539f\u59cbMCTS\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u540c\u65f6\u51cf\u5c11\u4e8650%\u4ee5\u4e0a\u7684\u641c\u7d22\u65f6\u95f4\u3002\u5176\u6838\u5fc3\u601d\u60f3\u662f\u63d0\u524d\u7ec8\u6b62\u5bf9\u7b80\u5355\u72b6\u6001\u7684\u641c\u7d22\u8fed\u4ee3\uff0c\u4ece\u800c\u4f18\u5316\u8ba1\u7b97\u6548\u7387\u3002\n\n3. **\u6811\u641c\u7d22\u4e0e\u8bed\u8a00\u6a21\u578b\u7ed3\u5408\uff08TS-LLM\uff09**  \n   \u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u4e2d\uff0c\u7814\u7a76\u8005\u5c06MCTS\u4e0e\u8bed\u8a00\u6a21\u578b\u7ed3\u5408\uff0c\u901a\u8fc7\u8282\u70b9\u6269\u5c55\u548c\u503c\u8bc4\u4f30\u4f18\u5316\u641c\u7d22\u8fc7\u7a0b\u3002\u5c3d\u7ba1\u5f53\u524d\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u573a\u666f\u4e2d\u4ecd\u9762\u4e34\u8ba1\u7b97\u8d1f\u62c5\uff0c\u4f46\u901a\u8fc7\u5f15\u5165\u952e\u503c\u7f13\u5b58\u7b49\u5de5\u7a0b\u4f18\u5316\uff0c\u672a\u6765\u6709\u671b\u8fdb\u4e00\u6b65\u63d0\u5347\u5176\u6269\u5c55\u6027\u3002\n\n4. **\u591a\u641c\u7d22\u591a\u6837\u6027\u589e\u5f3a\u7b97\u6cd5**  \n   \u9488\u5bf9MCTS\u5728\u5927\u89c4\u6a21\u805a\u5408\u4e2d\u7684\u6027\u80fd\u74f6\u9888\uff0c\u7814\u7a76\u8005\u63d0\u51fa\u5f15\u5165\u591a\u641c\u7d22\u591a\u6837\u6027\u589e\u5f3a\u7b97\u6cd5\u3002\u901a\u8fc7\u9f13\u52b1\u641c\u7d22\u8fc7\u7a0b\u4e2d\u7684\u591a\u6837\u6027\uff0c\u53ef\u4ee5\u66f4\u9ad8\u6548\u5730\u63a2\u7d22\u89e3\u7a7a\u95f4\uff0c\u4ece\u800c\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002\n\n5. **\u7406\u8bba\u5206\u6790\u4e0e\u8bef\u5dee\u754c**  \n   \u5728\u6539\u8fdbMCTS\u7684\u540c\u65f6\uff0c\u7814\u7a76\u8005\u4e5f\u6ce8\u91cd\u7406\u8bba\u5206\u6790\u3002\u4f8b\u5982\uff0cV-MCTS\u63d0\u4f9b\u4e86\u8bef\u5dee\u754c\uff0c\u786e\u4fdd\u5728\u51cf\u5c11\u8ba1\u7b97\u91cf\u7684\u540c\u65f6\uff0c\u6027\u80fd\u635f\u5931\u53ef\u63a7\u3002\u8fd9\u79cd\u7406\u8bba\u652f\u6301\u4e3a\u65b9\u6cd5\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u9760\u6027\u4fdd\u969c\u3002\n\n\u7efc\u4e0a\u6240\u8ff0\uff0c\u901a\u8fc7\u5206\u5c42\u4e0e\u5206\u9636\u6bb5\u641c\u7d22\u3001\u81ea\u9002\u5e94\u8d44\u6e90\u5206\u914d\u3001\u7ed3\u5408\u8bed\u8a00\u6a21\u578b\u3001\u589e\u5f3a\u641c\u7d22\u591a\u6837\u6027\u4ee5\u53ca\u7406\u8bba\u5206\u6790\uff0c\u53ef\u4ee5\u6709\u6548\u964d\u4f4eMCTS\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u5176\u51b3\u7b56\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002\u8fd9\u4e9b\u65b9\u6cd5\u4e3aMCTS\u5728\u5927\u89c4\u6a21\u95ee\u9898\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u3002\u300d", "ref_task_step_id": ""}, "__type__": "task_step_node"}, "3fc9a567-9ad0-439c-bd1f-3954fb72323f": {"__data__": {"id_": "3fc9a567-9ad0-439c-bd1f-3954fb72323f", "metadata": {}, "relationships": {}, "hash": "", "start_task_context": "MCTS\u5728PRM\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u7684\u5e94\u7528\u63a2\u7d22\u7efc\u8ff0", "aemo_representation_context": "### Step 1: \u5206\u6790\u8fd1\u51e0\u5e74\u7814\u7a76\u9886\u57df\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba\n\n**\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09**\uff1a\n- **\u57fa\u672c\u539f\u7406**\uff1aMCTS\u662f\u4e00\u79cd\u7528\u4e8e\u51b3\u7b56\u8fc7\u7a0b\u7684\u542f\u53d1\u5f0f\u641c\u7d22\u7b97\u6cd5\uff0c\u5e7f\u6cdb\u5e94\u7528\u4e8e\u6e38\u620fAI\u3001\u8def\u5f84\u89c4\u5212\u7b49\u9886\u57df\u3002\u5176\u6838\u5fc3\u601d\u60f3\u662f\u901a\u8fc7\u6a21\u62df\u591a\u6b21\u968f\u673a\u6e38\u620f\u6765\u8bc4\u4f30\u4e0d\u540c\u51b3\u7b56\u7684\u4f18\u52a3\u3002\n- **\u6280\u672f\u6846\u67b6**\uff1a\u4e3b\u8981\u5305\u62ec\u9009\u62e9\uff08Selection\uff09\u3001\u6269\u5c55\uff08Expansion\uff09\u3001\u6a21\u62df\uff08Simulation\uff09\u548c\u56de\u6eaf\uff08Backpropagation\uff09\u56db\u4e2a\u6b65\u9aa4\u3002\n\n**\u504f\u597d\u7b56\u7565\u6a21\u578b\uff08PRM\uff09**\uff1a\n- **\u57fa\u672c\u539f\u7406**\uff1aPRM\u65e8\u5728\u901a\u8fc7\u5b66\u4e60\u7528\u6237\u7684\u504f\u597d\u6765\u4f18\u5316\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5e38\u7528\u4e8e\u63a8\u8350\u7cfb\u7edf\u3001\u4e2a\u6027\u5316\u670d\u52a1\u7b49\u3002\n- **\u6280\u672f\u6846\u67b6**\uff1a\u901a\u5e38\u5305\u62ec\u504f\u597d\u5efa\u6a21\u3001\u7b56\u7565\u4f18\u5316\u548c\u6a21\u578b\u5fae\u8c03\u7b49\u73af\u8282\u3002\n\n**\u65b9\u6cd5\u8bba**\uff1a\n- **\u96c6\u6210\u5b66\u4e60**\uff1a\u5c06MCTS\u4e0ePRM\u7ed3\u5408\uff0c\u5229\u7528MCTS\u7684\u641c\u7d22\u80fd\u529b\u4f18\u5316PRM\u7684\u51b3\u7b56\u8fc7\u7a0b\u3002\n- **\u5f3a\u5316\u5b66\u4e60**\uff1a\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u673a\u5236\u4e0d\u65ad\u8c03\u6574PRM\u7684\u53c2\u6570\uff0c\u4ee5\u9002\u5e94\u52a8\u6001\u73af\u5883\u3002\n\n### Step 2: \u7814\u7a76\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u4e3b\u8981\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0e\u53d8\u4f53\n\n**\u5e94\u7528\u573a\u666f**\uff1a\n- **\u6e38\u620fAI**\uff1aMCTS\u4e0ePRM\u7ed3\u5408\u7528\u4e8e\u6e38\u620f\u4e2d\u7684\u667a\u80fd\u51b3\u7b56\uff0c\u5982AlphaGo\u3002\n- **\u8def\u5f84\u89c4\u5212**\uff1a\u5728\u673a\u5668\u4eba\u5bfc\u822a\u548c\u81ea\u52a8\u9a7e\u9a76\u4e2d\uff0c\u5229\u7528MCTS\u4f18\u5316\u8def\u5f84\u9009\u62e9\u3002\n- **\u63a8\u8350\u7cfb\u7edf**\uff1a\u901a\u8fc7PRM\u5b66\u4e60\u7528\u6237\u504f\u597d\uff0c\u7ed3\u5408MCTS\u8fdb\u884c\u66f4\u7cbe\u51c6\u7684\u63a8\u8350\u3002\n\n**\u53d8\u4f53**\uff1a\n- **\u6df1\u5ea6MCTS**\uff1a\u7ed3\u5408\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff0c\u63d0\u5347MCTS\u7684\u51b3\u7b56\u80fd\u529b\u3002\n- **\u591a\u6a21\u6001PRM**\uff1a\u878d\u5408\u591a\u79cd\u6570\u636e\u6e90\uff08\u5982\u6587\u672c\u3001\u56fe\u50cf\uff09\uff0c\u589e\u5f3aPRM\u7684\u6cdb\u5316\u80fd\u529b\u3002\n\n### Step 3: \u8bc4\u4f30\u5b66\u672f\u754c\u7684\u6280\u672f\u8fdb\u6b65\u4e0e\u5c40\u9650\u6027\n\n**\u6280\u672f\u8fdb\u6b65**\uff1a\n- **\u6027\u80fd\u63d0\u5347**\uff1aMCTS\u4e0ePRM\u7ed3\u5408\u663e\u8457\u63d0\u5347\u4e86\u51b3\u7b56\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002\n- **\u6cdb\u5316\u80fd\u529b**\uff1a\u591a\u6a21\u6001PRM\u5728\u5904\u7406\u591a\u6837\u5316\u6570\u636e\u65f6\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002\n\n**\u5c40\u9650\u6027**\uff1a\n- **\u8ba1\u7b97\u590d\u6742\u5ea6**\uff1aMCTS\u5728\u5927\u89c4\u6a21\u95ee\u9898\u4e2d\u8ba1\u7b97\u91cf\u5927\uff0c\u6548\u7387\u6709\u5f85\u63d0\u5347\u3002\n- **\u6570\u636e\u4f9d\u8d56**\uff1aPRM\u7684\u6027\u80fd\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u9ad8\u8d28\u91cf\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u6570\u636e\u504f\u5dee\u95ee\u9898\u4f9d\u7136\u5b58\u5728\u3002\n\n### Step 4: \u63a2\u8ba8\u8ba1\u7b97\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\n\n**\u9002\u7528\u6027**\uff1a\n- **\u591a\u9886\u57df\u5e94\u7528**\uff1aMCTS+PRM\u6846\u67b6\u5728\u6e38\u620f\u3001\u8def\u5f84\u89c4\u5212\u3001\u63a8\u8350\u7cfb\u7edf\u7b49\u591a\u4e2a\u9886\u57df\u8868\u73b0\u51fa\u826f\u597d\u7684\u9002\u7528\u6027\u3002\n- **\u591a\u6a21\u6001\u6570\u636e**\uff1a\u591a\u6a21\u6001PRM\u80fd\u591f\u5904\u7406\u591a\u79cd\u7c7b\u578b\u7684\u6570\u636e\uff0c\u9002\u7528\u4e8e\u590d\u6742\u5e94\u7528\u573a\u666f\u3002\n\n**\u6cdb\u5316\u80fd\u529b**\uff1a\n- **\u8de8\u9886\u57df\u6cdb\u5316**\uff1a\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\uff0cMCTS+PRM\u6846\u67b6\u5728\u4e0d\u540c\u9886\u57df\u95f4\u5c55\u73b0\u51fa\u8f83\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002\n- **\u52a8\u6001\u73af\u5883\u9002\u5e94**\uff1a\u5f3a\u5316\u5b66\u4e60\u673a\u5236\u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u9002\u5e94\u52a8\u6001\u53d8\u5316\u7684\u73af\u5883\u3002\n\n### Step 5: \u5206\u6790\u6700\u65b0\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027\n\n**\u7a33\u5b9a\u6027**\uff1a\n- **\u7b97\u6cd5\u4f18\u5316**\uff1a\u901a\u8fc7\u6539\u8fdbMCTS\u7684\u9009\u62e9\u7b56\u7565\u548cPRM\u7684\u53c2\u6570\u8c03\u6574\u673a\u5236\uff0c\u63d0\u5347\u4e86\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u3002\n- **\u9c81\u68d2\u6027\u589e\u5f3a**\uff1a\u5f15\u5165\u5bf9\u6297\u8bad\u7ec3\u548c\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\uff0c\u63d0\u9ad8\u6a21\u578b\u5728\u590d\u6742\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u6027\u3002\n\n**\u5bb9\u9519\u6027**\uff1a\n- **\u9519\u8bef\u5904\u7406**\uff1a\u5728\u6a21\u62df\u548c\u56de\u6eaf\u9636\u6bb5\u5f15\u5165\u5bb9\u9519\u673a\u5236\uff0c\u51cf\u5c11\u9519\u8bef\u51b3\u7b56\u7684\u5f71\u54cd\u3002\n- **\u5927\u89c4\u6a21\u6570\u636e\u9002\u5e94\u6027**\uff1a\u4f18\u5316\u7b97\u6cd5\u7ed3\u6784\uff0c\u63d0\u5347\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\u7684\u5904\u7406\u80fd\u529b\u3002\n\n### Step 6: \u8bc4\u4f30\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218\n\n**\u672a\u6765\u7814\u7a76\u65b9\u5411**\uff1a\n- **\u9ad8\u6548\u8ba1\u7b97**\uff1a\u7814\u7a76\u66f4\u9ad8\u6548\u7684MCTS\u7b97\u6cd5\uff0c\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002\n- **\u6570\u636e\u589e\u5f3a**\uff1a\u63a2\u7d22\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u51cf\u5c11PRM\u5bf9\u9ad8\u8d28\u91cf\u6570\u636e\u7684\u4f9d\u8d56\u3002\n\n**\u6311\u6218**\uff1a\n- **\u8de8\u6a21\u6001\u878d\u5408**\uff1a\u5982\u4f55\u6709\u6548\u878d\u5408\u591a\u79cd\u6a21\u6001\u6570\u636e\uff0c\u63d0\u5347\u6a21\u578b\u7684\u7efc\u5408\u6027\u80fd\u3002\n- **\u52a8\u6001\u73af\u5883\u9002\u5e94**\uff1a\u5728\u52a8\u6001\u73af\u5883\u4e0b\uff0c\u5982\u4f55\u4fdd\u6301\u6a21\u578b\u7684\u7a33\u5b9a\u6027\u548c\u9ad8\u6548\u6027\u3002\n\n**\u63a8\u52a8\u540e\u7eed\u7814\u7a76**\uff1a\n- **\u65b0\u95ee\u9898\u63d0\u51fa**\uff1a\u5982\u591a\u6a21\u6001\u6570\u636e\u4e0b\u7684\u504f\u597d\u5efa\u6a21\u3001\u52a8\u6001\u73af\u5883\u4e0b\u7684\u51b3\u7b56\u4f18\u5316\u7b49\u3002\n- **\u7814\u7a76\u5207\u5165\u70b9**\uff1a\u4ece\u7b97\u6cd5\u4f18\u5316\u3001\u6570\u636e\u589e\u5f3a\u3001\u8de8\u6a21\u6001\u878d\u5408\u7b49\u591a\u4e2a\u89d2\u5ea6\u5207\u5165\uff0c\u63a8\u52a8\u6280\u672f\u8fdb\u6b65\u3002\n\n### \u603b\u7ed3\n\n\u8fd1\u5e74\u6765\uff0cMCTS\u5728PRM\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u7684\u5e94\u7528\u63a2\u7d22\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4e3b\u8981\u4f53\u73b0\u5728\u6280\u672f\u6846\u67b6\u7684\u5b8c\u5584\u3001\u5e94\u7528\u573a\u666f\u7684\u62d3\u5c55\u3001\u6027\u80fd\u7684\u63d0\u5347\u4ee5\u53ca\u7a33\u5b9a\u6027\u548c\u5bb9\u9519\u6027\u7684\u589e\u5f3a\u3002\u7136\u800c\uff0c\u4ecd\u5b58\u5728\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u3001\u6570\u636e\u4f9d\u8d56\u6027\u5f3a\u7b49\u5c40\u9650\u6027\u3002\u672a\u6765\u7684\u7814\u7a76\u5e94\u5173\u6ce8\u9ad8\u6548\u8ba1\u7b97\u3001\u6570\u636e\u589e\u5f3a\u548c\u8de8\u6a21\u6001\u878d\u5408\u7b49\u65b9\u5411\uff0c\u4ee5\u5e94\u5bf9\u52a8\u6001\u73af\u5883\u4e0b\u7684\u51b3\u7b56\u4f18\u5316\u7b49\u6311\u6218\uff0c\u63a8\u52a8\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002", "task_step_name": "\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09", "task_step_description": "MCTS\u662f\u4e00\u79cd\u7528\u4e8e\u51b3\u7b56\u8fc7\u7a0b\u7684\u542f\u53d1\u5f0f\u641c\u7d22\u7b97\u6cd5\uff0c\u5e7f\u6cdb\u5e94\u7528\u4e8e\u6e38\u620fAI\u3001\u8def\u5f84\u89c4\u5212\u7b49\u9886\u57df\u3002\u5176\u6838\u5fc3\u601d\u60f3\u662f\u901a\u8fc7\u6a21\u62df\u591a\u6b21\u968f\u673a\u6e38\u620f\u6765\u8bc4\u4f30\u4e0d\u540c\u51b3\u7b56\u7684\u4f18\u52a3\u3002", "task_step_level": "0>1", "task_step_question": "### \u95ee\u9898\u63d0\u51fa\n\n\u5728\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09\u7684\u5e94\u7528\u4e2d\uff0c\u7279\u522b\u662f\u5728\u4e0e\u504f\u597d\u7b56\u7565\u6a21\u578b\uff08PRM\uff09\u7ed3\u5408\u7684\u573a\u666f\u4e0b\uff0c\u5982\u4f55\u6709\u6548\u964d\u4f4eMCTS\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u5176\u5728\u590d\u6742\u51b3\u7b56\u4efb\u52a1\u4e2d\u7684\u9ad8\u6548\u6027\u548c\u51c6\u786e\u6027\uff1f\u5177\u4f53\u6765\u8bf4\uff0c\u6709\u54ea\u4e9b\u7b97\u6cd5\u4f18\u5316\u6216\u6280\u672f\u6539\u8fdb\u53ef\u4ee5\u663e\u8457\u51cf\u5c11MCTS\u5728\u5927\u89c4\u6a21\u95ee\u9898\u4e2d\u7684\u8ba1\u7b97\u91cf\uff0c\u800c\u4e0d\u727a\u7272\u5176\u51b3\u7b56\u8d28\u91cf\uff1f\u6b64\u5916\uff0c\u8fd9\u4e9b\u4f18\u5316\u65b9\u6cd5\u5728\u4e0d\u540c\u5e94\u7528\u573a\u666f\uff08\u5982\u6e38\u620fAI\u3001\u8def\u5f84\u89c4\u5212\u3001\u63a8\u8350\u7cfb\u7edf\uff09\u4e2d\u7684\u9002\u7528\u6027\u548c\u6548\u679c\u5982\u4f55\uff1f", "task_step_question_context": [{"ref_id": "454845766462370026", "chunk_id": "6", "score": 0.63671875, "text": "# Monte Carlo Tree Search in the Presence of Transition Uncertainty\nFarnaz Kohankhaki , Kiarash Aghakasiri , Hongming Zhang 1 , Ting-Han Wei 1 , Chao Gao 2 ,Martin M\u00a8uller 1  \n\n1 University of Alberta, 2 Edmonton Research Center, Huawei Canada {kohankha, aghakasi, hongmin2, tinghan, mmueller }@ualberta.ca, cgao3 $@$ outlook.com\n\n# Abstract\nMonte Carlo Tree Search (MCTS) is an immensely popular search-based framework used for decision making. It is traditionally applied to domains where a perfect simulation model of the environment is available. We study and improve MCTS in the context where the environment model is given but imperfect. We show that the discrepancy between the model and the actual environment can lead to significant performance degradation with standard MCTS. We therefore develop Uncertainty Adapted MCTS (UA-MCTS), a more robust algorithm within the MCTS framework. We estimate the transition uncertainty in the given model, and direct the search towards more certain transitions in the state space. We modify all four MCTS phases to improve the search behavior by considering these estimates. We prove, in the corrupted bandit case, that adding uncertainty information to adapt UCB leads to tighter regret bound than standard UCB. Empirically, we evaluate UA-MCTS and its individual components on the deterministic domains from the MinAtar test suite. Our results demonstrate that UA-MCTS strongly improves MCTS in the presence of model transition errors.\n\n# 1 Introduction\nThe Monte Carlo Tree Search (MCTS) framework (Browne et al. 2012) approaches sequential decision-making problems by selective lookahead search. It manages the balance of exploration and exploitation with techniques such as UCT (Kocsis, Szepesv\u00b4ari, and Willemson 2006). Often combined with machine learning, it has been enormously successful in both games (Silver et al. 2016; Banerjee 2020; Arneson, Hayward, and Henderson 2010; Saffidine 2008; Nijssen and Winands 2010) and non-game applications (Lu et al. 2016; Mansley, Weinstein, and Littman 2011; Sabharwal, Samulowitz, and Reddy 2012; Cazenave 2010). In these applications, a perfect simulation model allows for efficient lookahead search. However, in many practical applications, only an imperfect model is available to the agent. Yet lookahead using such a model can still be useful. We improve MCTS for this setting.  \n\nOne research area that studies imperfect models of the environment is model-based reinforcement learning (MBRL).  \n\nHere, an agent builds its own model through limited real world interactions. The resulting learned model, when used for lookahead search, can either be for planning or for producing more accurate training targets (Silver, Sutton, and M\u00a8uller 2008). It can also be used to generate simulated training samples for better sample efficiency (Sutton and Barto 2018). The learned model may be inaccurate for many reasons, including stochasticity of the environment, insufficient training, insufficient capacity, non stationary environments, etc. Consequently, there is a rich body of research on uncertainty in MBRL (Abbas et al. 2020; Xiao et al. 2019; Buckman et al. 2018).  \n\nWhile previous approaches to using search with imperfect models exist (Vemula et al. 2020; Vemula, Bagnell, and Likhachev 2021), to the best of our knowledge, there is no prior work that directly adapts MCTS to deal with model uncertainty. In our work, we define transition uncertainty as a measure of difference between the state transitions in the perfect model and in the model that is available to the agent. We use a neural network to estimate this uncertainty.  \n\nOur Uncertainty Adapted MCTS (UA-MCTS) approach implements the main components of the MCTS framework in a way that guides the search away from states with high uncertainty. We compare the performance of our proposed methods with MCTS baselines in three deterministic MinAtar environments (Young and Tian 2019). In each case the search agent \u201cbelieves\u201d it is playing the real game. However, the rules of the game itself have changed, and the agent only learns about this change slowly when it acts in the real environment. The results show that UA-MCTS is able to outperform the baseline MCTS with an imperfect model.  \n\nOur approach is inspired by the work of (Vemula et al. 2020) where a robotic arm has to solve tasks despite being handicapped, e.g. by a broken motor or by an unmodeled weight restriction. To show how an agent should adapt UCB-based exploration strategy in the presence of environment uncertainties, we first consider a case of stochastic bandits (Lattimore and Szepesv\u00b4ari 2020) along with corrupted feedback. We prove that incorporating uncertainty information can enhance the performance of UCB, yielding a regret bound that is more constrained compared to the standard UCB. We also prove that in the general case of tree search, with similar modification of UCT, our UA-MCTS approach maintains its completeness property, ensuring that as the number of iterations goes to infinity, all nodes will be consistently explored. To further motivate our approach, we compare the scenarios of learning to improve the transition function, using MCTS, directly against the easier task of just learning a transition uncertainty function with UA-MCTS. In both cases, learning occurs online; the former is used with MCTS while the latter is used with UA-MCTS. Our results show that learning the transition function is much harder than learning transition uncertainty, which justifies the use of UA-MCTS in such settings."}, {"ref_id": "454846996555341700", "chunk_id": "2", "score": 0.56640625, "text": "# 2.2 Acceleration of MCTS\nMCTS-based methods have proved their strong capability of solving complex games or tasks. However, the high computational cost of MCTS hinders its application to some real-time and more general scenarios. Therefore, numerous works are devoted to accelerating MCTS. For example, to make the selection stage more effective, some heuristic pruning methods [ 14 ,33 ,29 ,1 ,2 ] aim to reduce the width and depth of the search tree with some heuristic functions. Furthermore, for more efficient evaluations, Lorentz [ 22 ] proposed early playout termination of MCTS (MCTS-EPT) to stop the random playouts early and use an evaluation function to assess win or loss. Moreover, Hsueh et al. [18 ] applied MCTS-EPT to the Chinese dark chess and proved its effectiveness. Afterward, similar ideas have been applied in the evaluation stage of AlphaGoZero [ 32 ] and later MCTS-based methods [31 ,27 ,34 ]. They evaluate the $Q$ -values through a learnable evaluation network instead of running playouts to the end. Grill et al. [15 ] propose a novel regularized policy optimization method based on AlphaZero to decrease the search budget of MCTS, which is from the optimization perspective. Danihelka et al. [10 ] propose a policy improvement algorithm based on sampling actions without replacement, named Gumbel trick to achieve better performance when planning with few simulations. However, these methods mentioned above focus on the specific stage of the search iteration or reduce the total budget through pruning and optimization methods, which are orthogonal to us. And few works targets at the search loop. Lan et al. [21 ] propose DS-MCTS, which defines the uncertainty of MCTS and approximates it by extra DNNs with specific features for board games in training. During the evaluation, DS-MCTS will check periodically and stop the search if the state is certain.\n\n# 3 Background\nThe AlphaGo series of work [ 30 ,32 ,31 ,27 ] are all MCTS-based reinforcement learning algorithms. Those algorithms assume the environment transition dynamics are known or learn the environment dynamics. Based on the dynamics, they use the Monte-Carlo tree search (MCTS) as the policy improvement operator. I.e., taking in the current policy, MCTS returns a better policy with the search algorithm. The systematic search allows the MCTS-based RL algorithm to quickly improve the policy and perform much better in the setting where heavy reasoning is required.\n\n# 3.1 MCTS\nThis part briefly introduces the MCTS method implemented in reinforcement learning applications. As mentioned in the related works, modern MCTS-based RL algorithms include four stages in the search loop, namely selection, expansion, evaluation, and backpropagation.  \n\nMCTS takes in the current states and generates a policy after the search loop of $N$ iterations. Here $N$ is a constant number of iterations set by the designer, regarded as the total budget. In the selection stage of each iteration, an action will be selected by maximizing over UCB. Specifically, AlphaZero [31 ] and MuZero [ 27 ] are developed based on a variant of UCB, P-UCT [ 25 ] and have achieved great success on board games and Atari games. The formula of P-UCT is the Eq (1):  \n\n$$\na^{k}=\\arg\\operatorname*{max}_{a\\in\\mathcal{A}}Q(s,a)+P(s,a)\\frac{\\sqrt{\\sum_{b\\in\\mathcal{A}}N(s,b)}}{1+N(s,a)}(c_{1}+\\log((\\sum_{b\\in\\mathcal{A}}N(s,b)+c_{2}+1)/c_{2})),\n$$  \n\nwhere $k$ is the index of iteration, $\\boldsymbol{\\mathcal{A}}$ is the acti $Q(s,a)$ is the estimated Q-value, $P(s,a)$ is the policy prior obtained from neural networks, $N(s,a)$ is the visitations to select the action a from the state $s$ and $c_{1},c_{2}$ are hyper-parameters. The output of MCTS is the visitation of each action of the root node. After $N$ search iterations, the final policy $\\pi(s)$ is defined as the normalized root visitation distribution simplification, we use $\\pi_{N}(s)$ $\\pi_{k}$ in place of , where $\\begin{array}{r}{\\pi_{k}(s,a)=N(s,a)/\\sum_{b\\in\\mathcal{A}}N(s,b)=N(s,a)/k,a\\in\\mathcal{A}}\\end{array}$ $\\pi_{k}(s)$ sometimes. And the detailed procedure of MCTS is \u2208A . For introduced in Appendix. In our method, we propose to approximate the final policy $\\pi_{N}(s)$ with $\\hat{\\pi}_{k}(s)$ ,which we name as a virtual expanded policy, through a new expansion method and a termination rule. In this way, the number of iterations in MCTS can be reduced from $N$ to $k$ .\n\n# 3.2 Computation Requirement\nMost of the computations in MCTS-based RL are in the MCTS procedure. Each action taken by MCTS requires $N$ times neural network evaluations, where $N$ is a constant number of iterations in the search loop. Traditional RL algorithms, such as PPO [ 28 ] or DQN [ 23 ], only need a single neural network evaluation per action. Thus, MCTS-based RL is roughly $N$ times computationally more expensive than traditional RL algorithms. In practice, training a single Atari game needs 12 hours of computation time on 40 TPUs [ 27 ]. The computation need is roughly two orders of magnitude more than traditional RL algorithms [28], although the final performance of MuZero is much better.\n\n# 4 Method\nWe aim to spend more search time on harder states and less on easier states. Intuitively, human knows when to make a quick decision or a slow decision under different circumstances. Unfortunately, this situation-aware behavior is absent in current MCTS algorithms. Therefore, we propose an MCTS variant that terminates the search iteration adaptively. It consists of two components: a novel expansion method named virtual expansion to estimate the final visitation based on the current partial tree; a termination rule that decides when to terminate based on the hardness of the current scenario. And we will display the adaptive mechanism through visualizations in Section 5.5."}, {"ref_id": "455026805307867280", "chunk_id": "0", "score": 0.546875, "text": "# Spending Thinking Time Wisely: Accelerating MCTS with Virtual Expansions\nWeirui $\\mathbf{Ye}^{*}$ Pieter Abbeel \u2020Yang Gao $\\ast\\ddag\\S$ \u2217Tsinghua University, \u2020UC Berkeley, \u00a7Shanghai Qi Zhi Institute\n\n# Abstract\nOne of the most important AI research questions is to trade off computation versus performance since \u201cperfect rationality\" exists in theory but is impossible to achieve in practice. Recently, Monte-Carlo tree search (MCTS) has attracted considerable attention due to the significant performance improvement in various challenging domains. However, the expensive time cost during search severely restricts its scope for applications. This paper proposes the Virtual MCTS (V-MCTS), a variant of MCTS that spends more search time on harder states and less search time on simpler states adaptively. We give theoretical bounds of the proposed method and evaluate the performance and computations on $9\\times9$ Go board games and Atari games. Experiments show that our method can achieve comparable performances to the original search algorithm while requiring less than $50\\%$ search time on average. We believe that this approach is a viable alternative for tasks under limited time and resources. The code is available at https://github.com/YeWR/V-MCTS.git .\n\n# 1 Introduction\nWhen artificial intelligence was first studied in the 1950s, researchers have sought to find the solution to the question \u201cHow to build an agent with perfect rationality\". The term \u201cperfect rationality\" [7 ,24 ,26 ] here refers to the decision made with infinite amounts of computations. However, one can only solve small-scale problems without considering the practical computation time since classical search algorithms usually exhibit exponential running time. Therefore, recent AI research would no longer seek to achieve \u201cperfect rationality\", but instead carefully trade-off computation versus the level of rationality. People have developed computational models like \u201cbounded optimality\" to model these settings [ 26 ]. The increasing level of rationality under the same computational budget has given us a lot of AI successes. Algorithms include the Monte-Carlo sampling algorithms, the variational inference algorithms, and using DNNs as universal function approximators [9, 8, 13, 30, 17].  \n\nRecently, MCTS-based RL algorithms have achieved much success, mainly on board games. The most notable achievement is that AlphaGo beats Hui Fan in 2015 [ 30 ]. It is the first time a computer program beat a human professional Go player. Afterward, AlphaGo beats two top-ranking human players, Lee Sedol in 2016 and Jie Ke in 2017, the latter of which ranked first worldwide at the time. Later, MCTS-based RL algorithms were further extended to other board games and Atari games [ 27 ]. EfficientZero [ 34 ] significantly improves the sample efficiency of MCTS-based RL algorithms, shedding light on its future applications in the real world like robotics and self-driving.  \n\nDespite the impressive performance of MCTS-based RL algorithms, they require massive amounts of computation to train and evaluate. For example, MuZero [ 27 ] used 1000 TPUs trained for 12 hours to learn the game of Go, and for a single Atari game, it needs 40 TPUs to train 12 hours. Compared to previous algorithms on the Atari games benchmark, it needs around two orders of magnitude more compute. This prohibitively large computational requirement has slowed down both the further development of MCTS-based RL algorithms as well as its practical use.  \n\nUnder the hood, MCTS-based RL algorithms imagine the futures when taking different future action sequences. However, this imaging process for the current method is not computationally efficient. For example, AlphaGo needs to look ahead 1600 game states to place a single stone. On the contrary, top human professional players can only think through around 100-200 game states per minute [ 30 ]. Apart from the inefficiency, the current MCTS algorithm deals with easy and challenging cases with the same computational budget. However, human knows to use their time when it is most needed.  \n\nIn this paper, we aim to design new algorithms that save the computational time of the MCTSbased RL methods. We make three key contributions : (1) We present Virtual MCTS, a variant of MCTS, to approximate the vanilla MCTS search policies with less computation. Moreover, unlike previous pruning-based methods that focus on the selection or evaluation stage in MCTS, our method improves the search loop. It terminates the search iterations earlier adaptively when current states are simpler; (2) Theoretically, we provide some error bounds of the proposed method. Furthermore, the visualization results indicate that Virtual MCTS has a better computation and performance trade-off than vanilla MCTS; (3) Empirically, our method can save more than $50\\%$ of search times on the challenging game Go $9\\times9$ and more than $60\\%$ on the visually complex Atari games while keeping comparable performances to those of vanilla MCTS.\n\n# 2 Related Work\n\n# 2.1 Reinforcement Learning with MCTS\nFor a long time, Computer Go has been regarded as a remarkably challenging game [ 3 ,6 ]. Researchers attempt to use Monte-Carlo techniques that evaluate the value of the node state through random playouts [ 4 ,11 ,12 ,30 ]. Afterward, UCT algorithms have generally been applied in Monte-Carlo tree search (MCTS) algorithms, which use UCB1 to select action at each node of the tree [ 20 ]. Recently, MCTS-based RL methods [ 30 ,32 ,31 ,27 ] have become increasingly popular and achieved super-human performances on board games because of their strong ability to search.  \n\nModern MCTS-based RL algorithms include four stages in the search loop : selection, expansion, evaluation, and backpropagation. The computation bottlenecks in vanilla MCTS come from the search loop, especially for the evaluation stage and the selection stage of each iteration. The selection stage is time-consuming when the search tree becomes wider and deeper. The evaluation stage is quite expensive because people attempt to evaluate the node value by random playouts to the end in previous researches. Due to the search loop, MCTS-based algorithms have multiple model inferences compared to other model-free RL methods like PPO [28] and SAC [16]."}], "task_step_question_answer": "\u5728MCTS\u7684\u4f18\u5316\u8fc7\u7a0b\u4e2d\uff0c\u7406\u8bba\u652f\u6301\u662f\u4e0d\u53ef\u6216\u7f3a\u7684\u3002\u4f8b\u5982\uff0cV-MCTS\u63d0\u4f9b\u4e86\u8bef\u5dee\u754c\uff0c\u786e\u4fdd\u5728\u51cf\u5c11\u8ba1\u7b97\u91cf\u7684\u540c\u65f6\uff0c\u6027\u80fd\u635f\u5931\u53ef\u63a7\u3002\u8fd9\u79cd\u7406\u8bba\u652f\u6301\u4e3a\u65b9\u6cd5\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u9760\u6027\u4fdd\u969c\u3002\u6b64\u5916\uff0c\u542f\u53d1\u5f0f\u526a\u679d\u7684\u6536\u655b\u6027\u5206\u6790\u4e5f\u4e3a\u5176\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u7406\u8bba\u4f9d\u636e\u3002\u901a\u8fc7\u7ed3\u5408\u8fd9\u4e9b\u7406\u8bba\u5206\u6790\uff0c\u53ef\u4ee5\u8fdb\u4e00\u6b65\u4f18\u5316MCTS\u7684\u6027\u80fd\uff0c\u5e76\u786e\u4fdd\u5176\u5728\u4e0d\u540c\u5e94\u7528\u573a\u666f\u4e2d\u7684\u7a33\u5b9a\u6027\u548c\u9ad8\u6548\u6027\u3002", "ref_task_step_id": ""}, "__type__": "task_step_node"}, "372a76b7-8c9e-42aa-b783-cf530fb1852d": {"__data__": {"id_": "372a76b7-8c9e-42aa-b783-cf530fb1852d", "metadata": {}, "relationships": {}, "hash": "", "start_task_context": "MCTS\u5728PRM\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u7684\u5e94\u7528\u63a2\u7d22\u7efc\u8ff0", "aemo_representation_context": "### Step 1: \u5206\u6790\u8fd1\u51e0\u5e74\u7814\u7a76\u9886\u57df\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba\n\n**\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09**\uff1a\n- **\u57fa\u672c\u539f\u7406**\uff1aMCTS\u662f\u4e00\u79cd\u7528\u4e8e\u51b3\u7b56\u8fc7\u7a0b\u7684\u542f\u53d1\u5f0f\u641c\u7d22\u7b97\u6cd5\uff0c\u5e7f\u6cdb\u5e94\u7528\u4e8e\u6e38\u620fAI\u3001\u8def\u5f84\u89c4\u5212\u7b49\u9886\u57df\u3002\u5176\u6838\u5fc3\u601d\u60f3\u662f\u901a\u8fc7\u6a21\u62df\u591a\u6b21\u968f\u673a\u6e38\u620f\u6765\u8bc4\u4f30\u4e0d\u540c\u51b3\u7b56\u7684\u4f18\u52a3\u3002\n- **\u6280\u672f\u6846\u67b6**\uff1a\u4e3b\u8981\u5305\u62ec\u9009\u62e9\uff08Selection\uff09\u3001\u6269\u5c55\uff08Expansion\uff09\u3001\u6a21\u62df\uff08Simulation\uff09\u548c\u56de\u6eaf\uff08Backpropagation\uff09\u56db\u4e2a\u6b65\u9aa4\u3002\n\n**\u504f\u597d\u7b56\u7565\u6a21\u578b\uff08PRM\uff09**\uff1a\n- **\u57fa\u672c\u539f\u7406**\uff1aPRM\u65e8\u5728\u901a\u8fc7\u5b66\u4e60\u7528\u6237\u7684\u504f\u597d\u6765\u4f18\u5316\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5e38\u7528\u4e8e\u63a8\u8350\u7cfb\u7edf\u3001\u4e2a\u6027\u5316\u670d\u52a1\u7b49\u3002\n- **\u6280\u672f\u6846\u67b6**\uff1a\u901a\u5e38\u5305\u62ec\u504f\u597d\u5efa\u6a21\u3001\u7b56\u7565\u4f18\u5316\u548c\u6a21\u578b\u5fae\u8c03\u7b49\u73af\u8282\u3002\n\n**\u65b9\u6cd5\u8bba**\uff1a\n- **\u96c6\u6210\u5b66\u4e60**\uff1a\u5c06MCTS\u4e0ePRM\u7ed3\u5408\uff0c\u5229\u7528MCTS\u7684\u641c\u7d22\u80fd\u529b\u4f18\u5316PRM\u7684\u51b3\u7b56\u8fc7\u7a0b\u3002\n- **\u5f3a\u5316\u5b66\u4e60**\uff1a\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u673a\u5236\u4e0d\u65ad\u8c03\u6574PRM\u7684\u53c2\u6570\uff0c\u4ee5\u9002\u5e94\u52a8\u6001\u73af\u5883\u3002\n\n### Step 2: \u7814\u7a76\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u4e3b\u8981\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0e\u53d8\u4f53\n\n**\u5e94\u7528\u573a\u666f**\uff1a\n- **\u6e38\u620fAI**\uff1aMCTS\u4e0ePRM\u7ed3\u5408\u7528\u4e8e\u6e38\u620f\u4e2d\u7684\u667a\u80fd\u51b3\u7b56\uff0c\u5982AlphaGo\u3002\n- **\u8def\u5f84\u89c4\u5212**\uff1a\u5728\u673a\u5668\u4eba\u5bfc\u822a\u548c\u81ea\u52a8\u9a7e\u9a76\u4e2d\uff0c\u5229\u7528MCTS\u4f18\u5316\u8def\u5f84\u9009\u62e9\u3002\n- **\u63a8\u8350\u7cfb\u7edf**\uff1a\u901a\u8fc7PRM\u5b66\u4e60\u7528\u6237\u504f\u597d\uff0c\u7ed3\u5408MCTS\u8fdb\u884c\u66f4\u7cbe\u51c6\u7684\u63a8\u8350\u3002\n\n**\u53d8\u4f53**\uff1a\n- **\u6df1\u5ea6MCTS**\uff1a\u7ed3\u5408\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff0c\u63d0\u5347MCTS\u7684\u51b3\u7b56\u80fd\u529b\u3002\n- **\u591a\u6a21\u6001PRM**\uff1a\u878d\u5408\u591a\u79cd\u6570\u636e\u6e90\uff08\u5982\u6587\u672c\u3001\u56fe\u50cf\uff09\uff0c\u589e\u5f3aPRM\u7684\u6cdb\u5316\u80fd\u529b\u3002\n\n### Step 3: \u8bc4\u4f30\u5b66\u672f\u754c\u7684\u6280\u672f\u8fdb\u6b65\u4e0e\u5c40\u9650\u6027\n\n**\u6280\u672f\u8fdb\u6b65**\uff1a\n- **\u6027\u80fd\u63d0\u5347**\uff1aMCTS\u4e0ePRM\u7ed3\u5408\u663e\u8457\u63d0\u5347\u4e86\u51b3\u7b56\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002\n- **\u6cdb\u5316\u80fd\u529b**\uff1a\u591a\u6a21\u6001PRM\u5728\u5904\u7406\u591a\u6837\u5316\u6570\u636e\u65f6\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002\n\n**\u5c40\u9650\u6027**\uff1a\n- **\u8ba1\u7b97\u590d\u6742\u5ea6**\uff1aMCTS\u5728\u5927\u89c4\u6a21\u95ee\u9898\u4e2d\u8ba1\u7b97\u91cf\u5927\uff0c\u6548\u7387\u6709\u5f85\u63d0\u5347\u3002\n- **\u6570\u636e\u4f9d\u8d56**\uff1aPRM\u7684\u6027\u80fd\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u9ad8\u8d28\u91cf\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u6570\u636e\u504f\u5dee\u95ee\u9898\u4f9d\u7136\u5b58\u5728\u3002\n\n### Step 4: \u63a2\u8ba8\u8ba1\u7b97\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\n\n**\u9002\u7528\u6027**\uff1a\n- **\u591a\u9886\u57df\u5e94\u7528**\uff1aMCTS+PRM\u6846\u67b6\u5728\u6e38\u620f\u3001\u8def\u5f84\u89c4\u5212\u3001\u63a8\u8350\u7cfb\u7edf\u7b49\u591a\u4e2a\u9886\u57df\u8868\u73b0\u51fa\u826f\u597d\u7684\u9002\u7528\u6027\u3002\n- **\u591a\u6a21\u6001\u6570\u636e**\uff1a\u591a\u6a21\u6001PRM\u80fd\u591f\u5904\u7406\u591a\u79cd\u7c7b\u578b\u7684\u6570\u636e\uff0c\u9002\u7528\u4e8e\u590d\u6742\u5e94\u7528\u573a\u666f\u3002\n\n**\u6cdb\u5316\u80fd\u529b**\uff1a\n- **\u8de8\u9886\u57df\u6cdb\u5316**\uff1a\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\uff0cMCTS+PRM\u6846\u67b6\u5728\u4e0d\u540c\u9886\u57df\u95f4\u5c55\u73b0\u51fa\u8f83\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002\n- **\u52a8\u6001\u73af\u5883\u9002\u5e94**\uff1a\u5f3a\u5316\u5b66\u4e60\u673a\u5236\u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u9002\u5e94\u52a8\u6001\u53d8\u5316\u7684\u73af\u5883\u3002\n\n### Step 5: \u5206\u6790\u6700\u65b0\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027\n\n**\u7a33\u5b9a\u6027**\uff1a\n- **\u7b97\u6cd5\u4f18\u5316**\uff1a\u901a\u8fc7\u6539\u8fdbMCTS\u7684\u9009\u62e9\u7b56\u7565\u548cPRM\u7684\u53c2\u6570\u8c03\u6574\u673a\u5236\uff0c\u63d0\u5347\u4e86\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u3002\n- **\u9c81\u68d2\u6027\u589e\u5f3a**\uff1a\u5f15\u5165\u5bf9\u6297\u8bad\u7ec3\u548c\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\uff0c\u63d0\u9ad8\u6a21\u578b\u5728\u590d\u6742\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u6027\u3002\n\n**\u5bb9\u9519\u6027**\uff1a\n- **\u9519\u8bef\u5904\u7406**\uff1a\u5728\u6a21\u62df\u548c\u56de\u6eaf\u9636\u6bb5\u5f15\u5165\u5bb9\u9519\u673a\u5236\uff0c\u51cf\u5c11\u9519\u8bef\u51b3\u7b56\u7684\u5f71\u54cd\u3002\n- **\u5927\u89c4\u6a21\u6570\u636e\u9002\u5e94\u6027**\uff1a\u4f18\u5316\u7b97\u6cd5\u7ed3\u6784\uff0c\u63d0\u5347\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\u7684\u5904\u7406\u80fd\u529b\u3002\n\n### Step 6: \u8bc4\u4f30\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218\n\n**\u672a\u6765\u7814\u7a76\u65b9\u5411**\uff1a\n- **\u9ad8\u6548\u8ba1\u7b97**\uff1a\u7814\u7a76\u66f4\u9ad8\u6548\u7684MCTS\u7b97\u6cd5\uff0c\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002\n- **\u6570\u636e\u589e\u5f3a**\uff1a\u63a2\u7d22\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u51cf\u5c11PRM\u5bf9\u9ad8\u8d28\u91cf\u6570\u636e\u7684\u4f9d\u8d56\u3002\n\n**\u6311\u6218**\uff1a\n- **\u8de8\u6a21\u6001\u878d\u5408**\uff1a\u5982\u4f55\u6709\u6548\u878d\u5408\u591a\u79cd\u6a21\u6001\u6570\u636e\uff0c\u63d0\u5347\u6a21\u578b\u7684\u7efc\u5408\u6027\u80fd\u3002\n- **\u52a8\u6001\u73af\u5883\u9002\u5e94**\uff1a\u5728\u52a8\u6001\u73af\u5883\u4e0b\uff0c\u5982\u4f55\u4fdd\u6301\u6a21\u578b\u7684\u7a33\u5b9a\u6027\u548c\u9ad8\u6548\u6027\u3002\n\n**\u63a8\u52a8\u540e\u7eed\u7814\u7a76**\uff1a\n- **\u65b0\u95ee\u9898\u63d0\u51fa**\uff1a\u5982\u591a\u6a21\u6001\u6570\u636e\u4e0b\u7684\u504f\u597d\u5efa\u6a21\u3001\u52a8\u6001\u73af\u5883\u4e0b\u7684\u51b3\u7b56\u4f18\u5316\u7b49\u3002\n- **\u7814\u7a76\u5207\u5165\u70b9**\uff1a\u4ece\u7b97\u6cd5\u4f18\u5316\u3001\u6570\u636e\u589e\u5f3a\u3001\u8de8\u6a21\u6001\u878d\u5408\u7b49\u591a\u4e2a\u89d2\u5ea6\u5207\u5165\uff0c\u63a8\u52a8\u6280\u672f\u8fdb\u6b65\u3002\n\n### \u603b\u7ed3\n\n\u8fd1\u5e74\u6765\uff0cMCTS\u5728PRM\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u7684\u5e94\u7528\u63a2\u7d22\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4e3b\u8981\u4f53\u73b0\u5728\u6280\u672f\u6846\u67b6\u7684\u5b8c\u5584\u3001\u5e94\u7528\u573a\u666f\u7684\u62d3\u5c55\u3001\u6027\u80fd\u7684\u63d0\u5347\u4ee5\u53ca\u7a33\u5b9a\u6027\u548c\u5bb9\u9519\u6027\u7684\u589e\u5f3a\u3002\u7136\u800c\uff0c\u4ecd\u5b58\u5728\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u3001\u6570\u636e\u4f9d\u8d56\u6027\u5f3a\u7b49\u5c40\u9650\u6027\u3002\u672a\u6765\u7684\u7814\u7a76\u5e94\u5173\u6ce8\u9ad8\u6548\u8ba1\u7b97\u3001\u6570\u636e\u589e\u5f3a\u548c\u8de8\u6a21\u6001\u878d\u5408\u7b49\u65b9\u5411\uff0c\u4ee5\u5e94\u5bf9\u52a8\u6001\u73af\u5883\u4e0b\u7684\u51b3\u7b56\u4f18\u5316\u7b49\u6311\u6218\uff0c\u63a8\u52a8\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002", "task_step_name": "\u504f\u597d\u7b56\u7565\u6a21\u578b\uff08PRM\uff09", "task_step_description": "PRM\u65e8\u5728\u901a\u8fc7\u5b66\u4e60\u7528\u6237\u7684\u504f\u597d\u6765\u4f18\u5316\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5e38\u7528\u4e8e\u63a8\u8350\u7cfb\u7edf\u3001\u4e2a\u6027\u5316\u670d\u52a1\u7b49\u3002", "task_step_level": "0>2", "task_step_question": "### \u95ee\u9898\u63d0\u51fa\n\n\u5728MCTS\u4e0ePRM\u7ed3\u5408\u7684\u6846\u67b6\u4e2d\uff0c\u5982\u4f55\u6709\u6548\u5229\u7528MCTS\u7684\u641c\u7d22\u80fd\u529b\u6765\u4f18\u5316PRM\u7684\u504f\u597d\u5efa\u6a21\u8fc7\u7a0b\uff0c\u7279\u522b\u662f\u5728\u63a8\u8350\u7cfb\u7edf\u548c\u4e2a\u6027\u5316\u670d\u52a1\u4e2d\uff0c\u5982\u4f55\u786e\u4fdd\u6a21\u578b\u80fd\u591f\u51c6\u786e\u6355\u6349\u7528\u6237\u7684\u52a8\u6001\u504f\u597d\u5e76\u505a\u51fa\u5b9e\u65f6\u51b3\u7b56\uff1f\n\n### \u95ee\u9898\u80cc\u666f\n\nPRM\u901a\u8fc7\u5b66\u4e60\u7528\u6237\u7684\u504f\u597d\u6765\u4f18\u5316\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5e38\u7528\u4e8e\u63a8\u8350\u7cfb\u7edf\u548c\u4e2a\u6027\u5316\u670d\u52a1\u3002\u7136\u800c\uff0c\u7528\u6237\u7684\u504f\u597d\u5f80\u5f80\u662f\u52a8\u6001\u53d8\u5316\u7684\uff0c\u4f20\u7edf\u7684PRM\u6a21\u578b\u53ef\u80fd\u96be\u4ee5\u5b9e\u65f6\u6355\u6349\u8fd9\u4e9b\u53d8\u5316\u3002MCTS\u4f5c\u4e3a\u4e00\u79cd\u542f\u53d1\u5f0f\u641c\u7d22\u7b97\u6cd5\uff0c\u5177\u6709\u5f3a\u5927\u7684\u51b3\u7b56\u4f18\u5316\u80fd\u529b\uff0c\u4f46\u5176\u5728\u5927\u89c4\u6a21\u95ee\u9898\u4e2d\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u8f83\u9ad8\u3002\u56e0\u6b64\uff0c\u5982\u4f55\u5c06MCTS\u4e0ePRM\u6709\u6548\u7ed3\u5408\uff0c\u65e2\u5229\u7528MCTS\u7684\u641c\u7d22\u80fd\u529b\u4f18\u5316PRM\u7684\u51b3\u7b56\u8fc7\u7a0b\uff0c\u53c8\u786e\u4fdd\u6a21\u578b\u80fd\u591f\u5b9e\u65f6\u6355\u6349\u7528\u6237\u7684\u52a8\u6001\u504f\u597d\uff0c\u662f\u4e00\u4e2a\u503c\u5f97\u63a2\u8ba8\u7684\u95ee\u9898\u3002\n\n### \u95ee\u9898\u7ec6\u5316\n\n1. **\u52a8\u6001\u504f\u597d\u6355\u6349**\uff1a\u5728\u63a8\u8350\u7cfb\u7edf\u548c\u4e2a\u6027\u5316\u670d\u52a1\u4e2d\uff0c\u7528\u6237\u7684\u504f\u597d\u53ef\u80fd\u4f1a\u968f\u65f6\u95f4\u3001\u60c5\u5883\u7b49\u56e0\u7d20\u53d1\u751f\u53d8\u5316\u3002\u5982\u4f55\u8bbe\u8ba1MCTS\u4e0ePRM\u7ed3\u5408\u7684\u6846\u67b6\uff0c\u4f7f\u5176\u80fd\u591f\u5b9e\u65f6\u6355\u6349\u8fd9\u4e9b\u52a8\u6001\u53d8\u5316\uff1f\n   \n2. **\u8ba1\u7b97\u6548\u7387\u4f18\u5316**\uff1aMCTS\u5728\u5927\u89c4\u6a21\u95ee\u9898\u4e2d\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u8f83\u9ad8\uff0c\u5982\u4f55\u4f18\u5316MCTS\u7684\u641c\u7d22\u7b56\u7565\uff0c\u4f7f\u5176\u5728PRM\u7684\u504f\u597d\u5efa\u6a21\u8fc7\u7a0b\u4e2d\u4fdd\u6301\u9ad8\u6548\uff1f\n\n3. **\u6a21\u578b\u6cdb\u5316\u80fd\u529b**\uff1a\u5982\u4f55\u786e\u4fddMCTS\u4e0ePRM\u7ed3\u5408\u7684\u6a21\u578b\u5728\u4e0d\u540c\u5e94\u7528\u573a\u666f\u4e0b\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u591a\u6a21\u6001\u6570\u636e\u65f6\uff1f\n\n4. **\u5b9e\u65f6\u51b3\u7b56\u652f\u6301**\uff1a\u5728\u63a8\u8350\u7cfb\u7edf\u548c\u4e2a\u6027\u5316\u670d\u52a1\u4e2d\uff0c\u5982\u4f55\u786e\u4fddMCTS\u4e0ePRM\u7ed3\u5408\u7684\u6a21\u578b\u80fd\u591f\u5feb\u901f\u505a\u51fa\u51b3\u7b56\uff0c\u6ee1\u8db3\u5b9e\u65f6\u6027\u8981\u6c42\uff1f\n\n### \u95ee\u9898\u610f\u4e49\n\n\u901a\u8fc7\u89e3\u51b3\u4e0a\u8ff0\u95ee\u9898\uff0c\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63d0\u5347MCTS\u4e0ePRM\u7ed3\u5408\u6846\u67b6\u5728\u63a8\u8350\u7cfb\u7edf\u548c\u4e2a\u6027\u5316\u670d\u52a1\u4e2d\u7684\u5e94\u7528\u6548\u679c\uff0c\u4f7f\u5176\u80fd\u591f\u66f4\u51c6\u786e\u5730\u6355\u6349\u7528\u6237\u7684\u52a8\u6001\u504f\u597d\uff0c\u5e76\u505a\u51fa\u5b9e\u65f6\u3001\u9ad8\u6548\u7684\u51b3\u7b56\u3002\u8fd9\u4e0d\u4ec5\u6709\u52a9\u4e8e\u63d0\u5347\u7528\u6237\u4f53\u9a8c\uff0c\u8fd8\u80fd\u63a8\u52a8\u76f8\u5173\u9886\u57df\u7684\u6280\u672f\u8fdb\u6b65\u3002", "task_step_question_context": [{"ref_id": "454848282814999732", "chunk_id": "2", "score": 0.3359375, "text": "# 2 RELATED WORK\nThe full version of the related work is in Appendix A, we briefly introduce several highly related works here. In general, model-based RL for solving decision-making problems can be divided into three perspectives: model learning, policy learning, and decision-making. Moreover, optimal control theory also concerns the decision-making problem and is deeply related to model-based RL.  \n\nModel learning: How to learn a good model to support decision-making is crucial in model-based RL. There are two main aspects of the work: the model structure designing (Chua et al., 2018; Zhang  \n\net al., 2021; 2020; Hafner et al., 2021; Chen et al., 2022) and the loss designing (D\u2019Oro et al., 2020;   \nFarahmand et al., 2017; Li et al., 2021).  \n\nPolicy learning: Two methods are always used to learn the policy by using the learned model. One is to serve the learned model as a black-box simulator to generate the data (Janner et al., 2019b; Yu et al., 2020; Lee et al., 2020). Another way is to use the learned model to calculate the policy gradient (Heess et al., 2015b; Clavera et al., 2019; Amos et al., 2021).  \n\nDecision-making: When making the decision, we need to generate the actions that can achieve our goal. Many of the model-based RL methods make the decision by using the learned policy solely (Hafner et al., 2021). Similar to our paper, some works also try to make decisions by using the learned model, but the majority only focus on the discrete action space. The well-known MCTS method achieves a lot of success. For example, the well-known Alpha Zero (Silver et al., 2017), MuZero (Schrittwieser et al., 2020). There are only a few works that study the continuous action space, such as the Continuous UCT (Cou\u00a8etoux et al., 2011), the sampled MuZero (Hubert et al., 2021), the TreePI (Springenberg et al., 2020), and the TD-MPC (Hansen et al., 2022a).  \n\nOptimal control theory: Beyond deep RL, optimal control also considers the decision-making problem but rather relies on the known and continuous transition model. In modern optimal control, Model Predictive Control (MPC) (Camacho & Alba, 2013) framework is always adopted when the environment is highly non-linear. In MPC, the action is planned during the execution by using the model, and such a procedure is called trajectory optimization. Plenty of previous works (Byravan et al., 2021; Chua et al., 2018; Pinneri et al., 2021; Nagabandi et al., 2020) use MPC framework to solve the continuous control tasks, but most of them are based on zero-order or sample-based method to do the planning. The most relevant works are DDP (Murray & Yakowitz, 1984), iLQR (Li & Todorov, 2004), and iLQG (Todorov & Li, 2005; Tassa et al., 2012). We discuss the detailed differences between our method and these methods in Appendix A.  \n\nSince our planning algorithm relies on the learned model and learned policy, we build our algorithm based on these works on model learning and policy learning . Our POMP algorithm tries to solve a more challenging task compared to the related work on decision-making : efficiently optimize the trajectory in continuous action space when the environment model is unknown. Different from our works, the MPC with DDP as trajectory optimizer from optimal control theory requires the known environment model, and also requires the hessian matrix for online optimization from scratch.\n\n# 3 PRELIMINARIES\nReinforcement Le onsider discrete-time Marko Decision Process (M $\\mathcal{M}$ the tuple ($(\\mathcal{X},\\mathcal{A},f,r,\\gamma)$ XA $\\mathcal{X}$ state space, A is the action space, $f\\,:\\,x_{t+1}\\,=$   \n$f(x_{t},a_{t})$ is the transition model, $r:\\mathcal{X}\\times\\mathcal{A}\\to\\mathbb{R}$ X \u00d7 A \u2192 is the reward function, $\\gamma$ is the discount factor. $t$ $\\begin{array}{r}{R_{t}=\\sum_{t^{\\prime}=t}^{\\infty}\\gamma^{t^{\\prime}-t}r_{t^{\\prime}}}\\end{array}$ , and Reinforcement Learn  \n$\\begin{array}{r}{\\operatorname*{max}_{\\theta}J(\\theta)=\\operatorname*{max}_{\\theta}\\mathbb{E}_{\\pi_{\\theta}}R_{t}=\\operatorname*{max}_{\\theta}\\mathbb{E}_{\\pi_{\\theta}}\\Big[\\sum_{t^{\\prime}=t}^{\\infty}\\gamma^{t^{\\prime}-t}r(x_{t^{\\prime}},a_{t^{\\prime}})\\Big].}\\end{array}$ ing (RL) aims to find a policy $\\pi_{\\theta}:\\mathcal{X}\\times\\mathcal{A}\\rightarrow\\mathbb{R}^{+}$ X \u00d7 A \u2192 h P that can maximize the expected return .$J$ . where  \n\n$\\begin{array}{r}{\\operatorname*{max}_{a_{t}}\\mathbb{E}\\Big[r(x_{t},a_{t}|x_{t}\\,=\\,x)+\\gamma V^{*}(x_{t+1})\\Big]}\\end{array}$ value function obeys an important identity known as the Bellman optimality equation Bellman Equation. hWe define the optimal value function i . The idea behind this equation is that if we know the $V^{*}(x)=\\operatorname*{max}\\mathbb{E}[R_{t}|x_{t}=x]$ |. The optimal $V^{*}(x)\\;=\\;$ $r(x_{t},a_{t})$ for any $a_{t}$ and next step value function $V^{*}(x_{t+1})$ for any $s_{t+1}$ , we can recursively select the action $a_{t}$ which m $r(x_{t},a_{t}|x_{t}=x)+\\gamma V^{*}(x_{t+1})$ milarly, we can denote the optimal action-value function $Q^{*}(x,a)\\;=\\;\\operatorname*{max}\\mathbb{E}[R_{t}|x_{t}\\;=\\;x,a_{t}\\;=\\;a]$ |], and it obeys a similar Bellman optimility equation $\\begin{array}{r}{Q^{*}(x,a)=\\operatorname*{max}_{a_{t+1}}\\mathbb{E}\\Big[r(x_{t},a_{t}|x_{t}=x,a_{t}=a)+\\gamma Q^{*}(x_{t+1},a_{t+1})\\Big].}\\end{array}$ .  \n\nModel-based RL. Model-based RL method distinguishes itself from model-free counterparts by using the data to learn a transition model. Following Janner et al. (2019a) and Clavera et al. (2019), we use parametric neural networks to approximate the transition function, reward function, policy function and $\\mathrm{^Q}$ -value function with the following objective function to be optimized  $J_{f}(\\psi)\\,=\\,\\mathbb{E}\\big[\\log f(x_{t+1}|x_{t},a_{t})\\big]$ '', $J_{r}(\\omega)\\,=\\,\\mathbb{E}\\big[\\log r(r_{t}|x_{t},a_{t})\\big]$ '', $\\begin{array}{r}{\\bar{J_{\\pi}}(\\theta)\\,=\\,\\mathbb{E}\\bigl[\\sum_{t=0}^{H-1}\\gamma^{t}r(\\bar{x}_{t},a_{t})\\,+\\,}\\end{array}$ ' P$\\gamma^{H}Q(x_{H},a_{H})]$ 'and $J_{Q}\\,=\\,\\mathbb{E}\\bigl[\\|Q(x_{t},a_{t})-(r+\\tilde{Q}(x_{t+1},a_{t+1}))\\|_{2}\\bigr]$ '\u2225\u2212\u2225', respectively. In ${\\cal J}_{\\pi}(\\theta)$ , we truncate the trajectory in horizon Hto avoid long time model rollout.  \n\nNotations. For one-dimensional state and action case, we denote the partial differentiation of function by using its output with subscripts, e.g. ,$\\begin{array}{r}{r_{x}\\ \\triangleq\\ \\frac{\\partial r(x,a)}{\\partial x},\\ r_{a}\\ \\triangleq\\ \\frac{\\bigtriangleup r(x,a)}{\\partial a},\\ f_{x}\\ \\triangleq\\ \\frac{\\partial f(x,a)}{\\partial x}}\\end{array}$ ,$f_{a}\\triangleq{\\frac{\\partial f(x,a)}{\\partial a}}$ ,$\\begin{array}{r}{Q_{x}\\triangleq\\frac{\\partial Q(x,a)}{\\partial x}}\\end{array}$ and $\\begin{array}{r}{Q_{a}\\triangleq\\frac{\\partial Q(x,a)}{\\partial a}}\\end{array}$ . See Appendix E for the multi-dimension case."}, {"ref_id": "454845581169535994", "chunk_id": "7", "score": 0.306640625, "text": "# 4 Theoretical Analysis\nAlthough it is difficult to analyze the regret of MCTS-VS directly, we can theoretically analyze the influence of general variable selection by adopting the acquisition function GP-UCB. The considered general variable selection framework is as follows: after selecting a subset of variables at each iteration, the corresponding observation data (i.e., the data points sampled-so-far where only the selected variables are used) is used to build a GP model, and the next data point is sampled by maximizing GP-UCB. We use $\\mathbb{M}_{t}$ to denote the sampled variable index subset at iteration $t$ , and let $\\left|\\mathbb{M}_{t}\\right|=d_{t}$ .  \n\nRegret Analysis. Let $x^{*}$ denote an optimal solution. We analyze the cumulative regret $R_{T}\\,=$ $\\begin{array}{r}{\\sum_{t=1}^{\\bar{T}}(f(\\pmb{x}^{*})-f(\\pmb{x}^{t}))}\\end{array}$ the selected points by iteration \u2212, i.e., the Tum of the gap between the opti . To derive an upper bound on $R_{T}$ m and the function values of , we pessimistically assume that the worst function value, i.e., $\\begin{array}{r}{\\operatorname*{min}_{\\pmb{x}_{[D]\\setminus\\mathbb{M}_{t}}}f([\\pmb{x}_{\\mathbb{M}_{t}},\\pmb{x}_{[D]\\setminus\\mathbb{M}_{t}}])}\\end{array}$ , given ${\\pmb x}_{\\mathbb{M}_{t}}$ is returned in evaluation. As in [ Lipschitz assumption. 21 ,38 ], we assume that $\\mathcal{X}\\subset[0,r]^{D}$ is convex and compact, and $f$ satisfies the following Assumption 4.1. The function $f$ is a GP sample path. For some $a,b>0$ , given $L>0$ , the partial derivatives of $f$ satisfy that $\\forall i\\in[D]$ ,$\\exists\\alpha_{i}\\geq0$ ,  \n\n$$\nP\\left(\\operatorname*{sup}_{x\\in\\mathcal{X}}\\left|\\partial f/\\partial x_{i}\\right|<\\alpha_{i}L\\right)\\geq1-a e^{-\\left(L/b\\right)^{2}}.\n$$  \n\nBased on Assumption 4.1, we define $\\alpha_{i}^{*}$ to be the minimum value of $\\alpha_{i}$ such that Eq. (3) holds, which characterizes the importance of the $i$ -th variable $x_{i}$ . The larger $\\alpha_{i}^{*}$ , the greater influence of $x_{i}$ on the function $f$ . Let $\\alpha_{\\mathrm{max}}=\\operatorname*{max}_{i\\in[D]}\\alpha_{i}^{*}$ .  \n\nTheorem 4.2 gives an upper bound on the cumulative regret $R_{T}$ with high probability for general variable selection methods. The proof is inspired by that of GP-UCB without variable selection [ 38 ]$\\forall i:\\alpha_{i}^{*}\\leq1$ and provided in Appendix B.1. If we select all variables each time (i.e., \u2264(4) becomes $R_{T}\\leq\\sqrt{C_{1}T\\beta_{T}^{*}\\gamma_{T}}+2$ p, which is consistent with [ $\\forall t:\\mathbb{M}_{t}=[D])$ and assume 38 ]. Note that \u2200$\\forall t:|\\mathbb{M}_{t}|\\,=\\,d_{t}\\,=\\,D$ ||in this case, which implies that $\\beta_{t}$ increases with $t$ , leading to $\\beta_{T}^{*}=\\beta_{T}$ . We can see that usi variable selection will $R_{T}$ by $\\begin{array}{r}{2\\sum_{t=1}^{T}\\sum_{i\\in[D]\\backslash\\mathbb{M}_{t}}\\alpha_{i}^{*}L r}\\end{array}$ P,variables unselected, the larger related to the importance (i.e., $R_{T}$ $\\alpha_{i}^{*}$ . Meanwhile, the term ) of unselected variables at each iteration. The more important $\\sqrt{C_{1}T\\beta_{T}^{*}\\gamma_{T}}$ pwill decrease as \u2208$\\beta_{T}^{*}$ \\relies on the number $d_{t}$ of selected variables positively. Ideally, if the unselected variables at each iteration are always unrelated (i.e., $\\alpha_{i}^{*}\\!=\\!0$ ), the regret bound will be better than that of using all variables [38].  \n\n$b\\sqrt{\\log(4D a/\\delta)}$ Theorem 4.2. p$\\forall\\delta\\ \\in\\ (0,1)$ , where $r$ is the upper bound on each variable, and , let $\\beta_{t}\\ =\\ 2\\log(4\\pi_{t}/\\delta)\\,+\\,2d_{t}\\log(d_{t}t^{2}b r\\sqrt{\\log(4D a/\\delta)})$ {$\\{\\pi_{t}\\}_{t\\ge1}$ }\u2265satisfies $\\textstyle\\sum_{t\\geq1}\\pi_{t}^{-1}=1$ and $L\\;=$ and $\\pi_{t}>0$ . Let $\\beta_{T}^{*}=\\operatorname*{max}_{1\\leq i\\leq T}\\beta_{t}$ . At iteration $T$ , the cumulative regret  \n\n$$\nR_{T}\\leq\\sqrt{C_{1}T\\beta_{T}^{*}\\gamma_{T}}+2\\alpha_{\\operatorname*{max}}+2\\sum_{t=1}^{T}\\sum_{\\substack{i\\in[D]\\backslash\\mathbb{M}_{t}}}\\alpha_{i}^{*}L r\n$$  \n\nholds with probability least $1\\!-\\!\\delta$ , where $C_{1}$ is a constant, $\\begin{array}{r}{\\gamma_{T}\\!=\\!\\operatorname*{max}_{|\\mathcal{D}|=\\!T}I(\\pmb{y}_{\\mathcal{D}},\\pmb{f}_{\\mathcal{D}}),}\\end{array}$ $I(\\cdot,\\cdot)$ is the information gain, and $\\scriptstyle y_{\\mathcal{D}}$ Dand $f_{\\mathcal{D}}$ Dare the noisy and true observations of a set Dof points, respectively.  \n\nBy selecting been proved [21] that the cumulative regret of Dropout satisfies $d$ variables randomly at each iteration and assuming that $r=1$ and $\\forall i:\\alpha_{i}^{*}\\leq1$ \u2264, it has  \n\n$$\nR_{T}\\leq\\sqrt{C_{1}T\\beta_{T}\\gamma_{T}}+2+2T L(D-d).\n$$  \n\nIn this case, we have $d_{t}=|\\mathbb{M}_{t}|=d$ ,$r=1$ and $\\forall i:\\alpha_{i}^{*}\\leq1$ \u2264. Thus, Eq. (4) becomes  \n\n$$\nR_{T}\\leq\\sqrt{C_{1}T\\beta_{T}^{*}\\gamma_{T}}+2+2T L(D-d).\n$$  \n\nNote that $\\beta_{T}^{*}=\\beta_{T}$ here, as $\\beta_{t}$ increases with $t$ given $d_{t}=d$ . This implies that our bound Eq. (4) for general variable selection is a generalization of Eq. (5) for Dropout [ 21 ]. In [ 33 ], a regret bound analysis has also been performed for variable selection, by optimizing over $d$ fixed important variables and using a common parameter $\\alpha$ to characterize the importance of all the other $D-d$ variables.  \n\nComputational Complexity Analysis. The computational complexity of one iteration of BO depends on three critical components: fitting a GP surrogate model, maximizing an acquisition function and evaluating a sampled point. If using the squared exponential kernel, the computational complexity of fitting a GP model at iteration $t$ is $\\bar{O(t^{3}\\!+\\!t^{2}d_{t})}$ . Maximizing an acquisition function is related to the optimization algorithm. If we use the Quasi-Newton method to optimize GP-UCB, the computational complexity is $\\bar{\\mathcal{O}}(m(t^{2}+t d_{t}+d_{t}^{2}))$ [28 ], where $m$ denotes the Quasi-Newton\u2019s running rounds. The cost of evaluating a sampled point is fixed. Thus, by selecting only a subset of variables, instead of all variables, to optimize, the computational complexity can be decreased significantly. The detailed analysis is provided in Appendix B.2.  \n\nInsight. The above regret and computational complexity analyses have shown that variable selection can reduce the computational complexity while increasing the regret. Given the number $d_{t}$ of variables to be selected, a good variable selection method should select as important variables as possible, i.e., variables with as large $\\alpha_{i}^{*}$ as possible, which may help to design and evaluate variable selection methods. The experiments in Section 5.1 will show that MCTS-VS can select a good subset of variables while maintaining a small computational complexity."}, {"ref_id": "454847097820262972", "chunk_id": "6", "score": 0.298828125, "text": "# 4.1 User Modeling\nWe firstly encode the state $s_{t}$ , which contains all the conversational information of the prior $t\\!-\\!1$ turns. The current state includes six components: $s_{t}\\,=\\,\\{\\dot{u},\\dot{\\mathcal{P}}_{u}^{(t)},\\mathcal{P}_{r e j}^{(t)},\\mathcal{V}_{r e j}^{(t)},\\mathcal{P}_{c a n d}^{(t)},\\mathcal{V}_{c a n d}^{(t)}\\}$ . Previous methods [ 8,13 ,15 ] for MCR only extract the user\u2019s interest from the current state, ignoring the complements of historical interactions to the current user\u2019s preference. To this end, we construct a current graph and a global graph to jointly learn user representations. Moreover, we develop an iterative multi-interest extractor to obtain multiple interests of the user, which will be discussed in subsection 5.1.\n\n# 4.2 Consultation\nOnce the system finishes the user modeling step, it will move to the consultation step, with the purpose to decide whether to ask attribute instances or to recommend items. To make the next action more profitable and recommend successfully with the fewer turns, we employ a reinforcement learning (RL) method based on the extracted multiple interests of the user to learn the policy. The action space includes all candidate items and candidate attribute instances. However, in the real world, the number of items and attribute instances is very large, which severely limits the efficiency of CRS. To improve the efficiency, we sample $K_{v}$ items and $K_{p}$ attribute instances as action space $\\mathcal{A}_{t}$ . We develop a novel dueling Q-network [ 34 ] to calculate the Q-value of each action in $\\mathcal{A}_{t}$ . If CRS decides to ask a question, our method will select $K_{a}$ attribute instances in ${\\mathcal{A}}_{t}$ with the same attribute type to generate attribute type-based multiple choice questions . The user can choose zero (the option \"Others\" as shown in conversation (b) of Figure 1), one, or more attribute instances with the given attribute type. If CRS decides to recommend items, the system will select $K$ items in ${\\mathcal{A}}_{t}$ to recommend. We will discuss the details of sampling strategies and policy learning in subsection 5.2.\n\n# 4.3 Transition\nWhen the user responds to the action of agent, the transition step will be triggered. This step will transition the current state to the next state $s_{t+1}$ . If the user responds to the question, attribute instance sets that the user accepts and rejects in this turn can be defined as $\\mathcal{P}_{c u r\\_a c c}^{(t)}$ and $\\mathcal{P}_{c u r\\_r e j}^{(t)}$ respectively. Some components are updated by $\\mathcal{P}_{c a n d}^{(t+1)}=\\mathcal{P}_{c a n d}^{(t)}-\\overset{-}{\\mathcal{P}}_{c u r\\_r e j}^{(t)}-\\mathcal{P}_{c u r\\_a c c}^{(t)},\\mathcal{P}_{r e j}^{(t+1)}=\\mathcal{P}_{r e j}^{(t)}\\cup\\mathcal{P}_{c u r\\_r e j}^{(t)}$ and $\\mathcal{P}_{u}^{(t+1)}\\;=\\;\\mathcal{P}_{u}^{(t)}\\;\\cup\\;\\mathcal{P}_{c u r\\_a c c}^{(t)}$ . When the user is recommended items, if the set $\\mathcal{V}_{r e c}^{(t)}$ of recommended items are all rejected, the next state can be updated by $\\mathcal{V}_{r e j}^{(t+1)}=\\mathcal{V}_{r e j}^{(t)}\\cup\\mathcal{V}_{r e c}^{(t)}$ . Otherwise, this conversation session ends. Finally, we need to update the candidate item set $\\mathcal{V}_{c a n d}^{(t+1)}$ based on the user\u2019s feedback. Previous works [ 8,15 ]update candidate items based the intersection set strategy, that is, only the items satisfying all the accepted attribute instances in $\\mathcal{P}_{u}^{(t+1)}$ remain, which obviously deviates from the scenario. In fact, the user might not prefer the combination of all attribute instances, but rather part of them. To this end, we propose the attribute instance-based union set strategy to update $\\bar{\\mathcal{V}}_{c a n d}^{(t+\\bar{1})}$ as follows:  \n\n  \nFigure 2: The overview of Multi-Interest Policy Learning (MIPL).  \n\n$$\n\\begin{array}{r}{\\mathcal{V}_{c a n d}^{(t+1)}=\\{v\\vert v\\in\\mathcal{V}_{p_{0}}-\\mathcal{V}_{r e j}^{(t+1)}\\ \\mathrm{~and~}\\,\\mathcal{P}_{v}\\cap\\mathcal{P}_{u}^{(t+1)}\\neq\\varnothing}\\\\ {\\qquad\\qquad\\qquad\\qquad\\qquad\\mathrm{~and~}\\,\\mathcal{P}_{v}\\cap\\mathcal{P}_{r e j}^{(t+1)}=\\varnothing\\}}\\end{array}\n$$  \n\nwhere $\\mathcal{V}_{p_{0}}$ is the item set in which all items are associated to attribute instance $\\scriptstyle{\\mathcal{P}}0$ which initializes the conversation session. In this way, we can get the next state, which will be updated as $s_{t+1}~=$ $\\{\\Bar{u_{*}}^{\\prime}\\mathcal{P}_{u}^{(t+1)},\\mathcal{P}_{r e j}^{(\\Bar{t}+1)},\\mathcal{V}_{r e j}^{(t+1)},\\mathcal{P}_{c a n d}^{(t+1)},\\mathcal{V}_{c a n d}^{(t+1)}\\}$ .\n\n# 4.4 Reward\nIn this work, five kinds of rewards are defined following [ 8,15 ],   \nnamely, (1) $r_{r e c\\_s u c}$ , a strongly positive reward when the recommen  \ndation succeeds, (2) $r_{r e c\\_f a i l}$ , a strongly negative reward when the $r_{a s k\\_s u c}$ , a slightly positive reward when   \nthe user accepts an asked attribute instance, $(4)\\,r_{a s k\\_f a i l}$ , a negative $r_{q u i t}$ , a   \nstrongly negative reward if the session reaches the maximum number   \nof turns. In addition, since our method asks multiple choice ques  \ntions, we design the reward from the user\u2019s feedback on a question   \nin the form of sum as $\\begin{array}{r}{r_{t}=\\sum_{\\mathcal{P}_{c u r_{-}a c c}^{(t)}}r_{a s k_{-}s u c}+\\sum_{\\mathcal{P}_{c u r_{-}r e j}^{(t)}}r_{a s k_{-}r e j}.}\\end{array}$\n\n# 5MULTI-INTEREST POLICY LEARNING\nIn this section, we detail the design of Multi-Interest Policy Learning (MIPL) module. As shown in Figure 2, to obtain more comprehensive user representations, we establish a current graph to capture user current preferences, and a global graph to capture long-term preferences. Based on the learned node representations of the two graphs, we propose an iterative multi-interest extractor to model user\u2019s preferences for different combinations of attribute instances. Moreover, we design a new dueling Q-network [ 34 ] to decide the next action based on the extracted multiple interests."}], "task_step_question_answer": "\u5728\u4e2a\u6027\u5316\u670d\u52a1\u4e2d\uff0c\u901a\u8fc7\u4f18\u5316\u8ba1\u7b97\u6548\u7387\u548c\u589e\u5f3a\u6a21\u578b\u6cdb\u5316\u80fd\u529b\uff0c\u6a21\u578b\u53ef\u4ee5\u66f4\u9ad8\u6548\u5730\u5904\u7406\u591a\u6a21\u6001\u6570\u636e\uff0c\u63d0\u4f9b\u66f4\u4e2a\u6027\u5316\u7684\u670d\u52a1\u4f53\u9a8c\u3002\u4f8b\u5982\uff0c\u5728\u89c6\u9891\u63a8\u8350\u7cfb\u7edf\u4e2d\uff0c\u901a\u8fc7\u7ed3\u5408\u7528\u6237\u7684\u5386\u53f2\u89c2\u770b\u8bb0\u5f55\u548c\u5b9e\u65f6\u53cd\u9988\uff0c\u6a21\u578b\u53ef\u4ee5\u52a8\u6001\u8c03\u6574\u63a8\u8350\u7b56\u7565\uff0c\u786e\u4fdd\u63a8\u8350\u5185\u5bb9\u4e0e\u7528\u6237\u5f53\u524d\u5174\u8da3\u9ad8\u5ea6\u5339\u914d\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u5f15\u5165\u8fc1\u79fb\u5b66\u4e60\uff0c\u6a21\u578b\u53ef\u4ee5\u5728\u4e0d\u540c\u5e94\u7528\u573a\u666f\u4e2d\u5feb\u901f\u9002\u5e94\uff0c\u63d0\u5347\u5176\u6cdb\u5316\u80fd\u529b\u3002\u5728\u5b9e\u65f6\u51b3\u7b56\u652f\u6301\u65b9\u9762\uff0c\u4f7f\u7528\u9ad8\u6548\u7684\u786c\u4ef6\u52a0\u901f\u548c\u5e76\u884c\u8ba1\u7b97\u6280\u672f\uff0c\u6a21\u578b\u53ef\u4ee5\u5728\u6beb\u79d2\u7ea7\u65f6\u95f4\u5185\u751f\u6210\u63a8\u8350\u7ed3\u679c\uff0c\u6ee1\u8db3\u5b9e\u65f6\u6027\u8981\u6c42\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u4f18\u5316\u65b9\u6cd5\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u4ecd\u9700\u8003\u8651\u6570\u636e\u9690\u79c1\u548c\u6a21\u578b\u89e3\u91ca\u6027\u95ee\u9898\uff0c\u4ee5\u786e\u4fdd\u7528\u6237\u4fe1\u4efb\u548c\u5408\u89c4\u6027\u3002", "ref_task_step_id": ""}, "__type__": "task_step_node"}, "d6916989-efb3-47e8-978b-9f9e079e1eaf": {"__data__": {"id_": "d6916989-efb3-47e8-978b-9f9e079e1eaf", "metadata": {}, "relationships": {}, "hash": "", "start_task_context": "MCTS\u5728PRM\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u7684\u5e94\u7528\u63a2\u7d22\u7efc\u8ff0", "aemo_representation_context": "### Step 1: \u5206\u6790\u8fd1\u51e0\u5e74\u7814\u7a76\u9886\u57df\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba\n\n**\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09**\uff1a\n- **\u57fa\u672c\u539f\u7406**\uff1aMCTS\u662f\u4e00\u79cd\u7528\u4e8e\u51b3\u7b56\u8fc7\u7a0b\u7684\u542f\u53d1\u5f0f\u641c\u7d22\u7b97\u6cd5\uff0c\u5e7f\u6cdb\u5e94\u7528\u4e8e\u6e38\u620fAI\u3001\u8def\u5f84\u89c4\u5212\u7b49\u9886\u57df\u3002\u5176\u6838\u5fc3\u601d\u60f3\u662f\u901a\u8fc7\u6a21\u62df\u591a\u6b21\u968f\u673a\u6e38\u620f\u6765\u8bc4\u4f30\u4e0d\u540c\u51b3\u7b56\u7684\u4f18\u52a3\u3002\n- **\u6280\u672f\u6846\u67b6**\uff1a\u4e3b\u8981\u5305\u62ec\u9009\u62e9\uff08Selection\uff09\u3001\u6269\u5c55\uff08Expansion\uff09\u3001\u6a21\u62df\uff08Simulation\uff09\u548c\u56de\u6eaf\uff08Backpropagation\uff09\u56db\u4e2a\u6b65\u9aa4\u3002\n\n**\u504f\u597d\u7b56\u7565\u6a21\u578b\uff08PRM\uff09**\uff1a\n- **\u57fa\u672c\u539f\u7406**\uff1aPRM\u65e8\u5728\u901a\u8fc7\u5b66\u4e60\u7528\u6237\u7684\u504f\u597d\u6765\u4f18\u5316\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5e38\u7528\u4e8e\u63a8\u8350\u7cfb\u7edf\u3001\u4e2a\u6027\u5316\u670d\u52a1\u7b49\u3002\n- **\u6280\u672f\u6846\u67b6**\uff1a\u901a\u5e38\u5305\u62ec\u504f\u597d\u5efa\u6a21\u3001\u7b56\u7565\u4f18\u5316\u548c\u6a21\u578b\u5fae\u8c03\u7b49\u73af\u8282\u3002\n\n**\u65b9\u6cd5\u8bba**\uff1a\n- **\u96c6\u6210\u5b66\u4e60**\uff1a\u5c06MCTS\u4e0ePRM\u7ed3\u5408\uff0c\u5229\u7528MCTS\u7684\u641c\u7d22\u80fd\u529b\u4f18\u5316PRM\u7684\u51b3\u7b56\u8fc7\u7a0b\u3002\n- **\u5f3a\u5316\u5b66\u4e60**\uff1a\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u673a\u5236\u4e0d\u65ad\u8c03\u6574PRM\u7684\u53c2\u6570\uff0c\u4ee5\u9002\u5e94\u52a8\u6001\u73af\u5883\u3002\n\n### Step 2: \u7814\u7a76\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u4e3b\u8981\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0e\u53d8\u4f53\n\n**\u5e94\u7528\u573a\u666f**\uff1a\n- **\u6e38\u620fAI**\uff1aMCTS\u4e0ePRM\u7ed3\u5408\u7528\u4e8e\u6e38\u620f\u4e2d\u7684\u667a\u80fd\u51b3\u7b56\uff0c\u5982AlphaGo\u3002\n- **\u8def\u5f84\u89c4\u5212**\uff1a\u5728\u673a\u5668\u4eba\u5bfc\u822a\u548c\u81ea\u52a8\u9a7e\u9a76\u4e2d\uff0c\u5229\u7528MCTS\u4f18\u5316\u8def\u5f84\u9009\u62e9\u3002\n- **\u63a8\u8350\u7cfb\u7edf**\uff1a\u901a\u8fc7PRM\u5b66\u4e60\u7528\u6237\u504f\u597d\uff0c\u7ed3\u5408MCTS\u8fdb\u884c\u66f4\u7cbe\u51c6\u7684\u63a8\u8350\u3002\n\n**\u53d8\u4f53**\uff1a\n- **\u6df1\u5ea6MCTS**\uff1a\u7ed3\u5408\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff0c\u63d0\u5347MCTS\u7684\u51b3\u7b56\u80fd\u529b\u3002\n- **\u591a\u6a21\u6001PRM**\uff1a\u878d\u5408\u591a\u79cd\u6570\u636e\u6e90\uff08\u5982\u6587\u672c\u3001\u56fe\u50cf\uff09\uff0c\u589e\u5f3aPRM\u7684\u6cdb\u5316\u80fd\u529b\u3002\n\n### Step 3: \u8bc4\u4f30\u5b66\u672f\u754c\u7684\u6280\u672f\u8fdb\u6b65\u4e0e\u5c40\u9650\u6027\n\n**\u6280\u672f\u8fdb\u6b65**\uff1a\n- **\u6027\u80fd\u63d0\u5347**\uff1aMCTS\u4e0ePRM\u7ed3\u5408\u663e\u8457\u63d0\u5347\u4e86\u51b3\u7b56\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002\n- **\u6cdb\u5316\u80fd\u529b**\uff1a\u591a\u6a21\u6001PRM\u5728\u5904\u7406\u591a\u6837\u5316\u6570\u636e\u65f6\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002\n\n**\u5c40\u9650\u6027**\uff1a\n- **\u8ba1\u7b97\u590d\u6742\u5ea6**\uff1aMCTS\u5728\u5927\u89c4\u6a21\u95ee\u9898\u4e2d\u8ba1\u7b97\u91cf\u5927\uff0c\u6548\u7387\u6709\u5f85\u63d0\u5347\u3002\n- **\u6570\u636e\u4f9d\u8d56**\uff1aPRM\u7684\u6027\u80fd\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u9ad8\u8d28\u91cf\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u6570\u636e\u504f\u5dee\u95ee\u9898\u4f9d\u7136\u5b58\u5728\u3002\n\n### Step 4: \u63a2\u8ba8\u8ba1\u7b97\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\n\n**\u9002\u7528\u6027**\uff1a\n- **\u591a\u9886\u57df\u5e94\u7528**\uff1aMCTS+PRM\u6846\u67b6\u5728\u6e38\u620f\u3001\u8def\u5f84\u89c4\u5212\u3001\u63a8\u8350\u7cfb\u7edf\u7b49\u591a\u4e2a\u9886\u57df\u8868\u73b0\u51fa\u826f\u597d\u7684\u9002\u7528\u6027\u3002\n- **\u591a\u6a21\u6001\u6570\u636e**\uff1a\u591a\u6a21\u6001PRM\u80fd\u591f\u5904\u7406\u591a\u79cd\u7c7b\u578b\u7684\u6570\u636e\uff0c\u9002\u7528\u4e8e\u590d\u6742\u5e94\u7528\u573a\u666f\u3002\n\n**\u6cdb\u5316\u80fd\u529b**\uff1a\n- **\u8de8\u9886\u57df\u6cdb\u5316**\uff1a\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\uff0cMCTS+PRM\u6846\u67b6\u5728\u4e0d\u540c\u9886\u57df\u95f4\u5c55\u73b0\u51fa\u8f83\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002\n- **\u52a8\u6001\u73af\u5883\u9002\u5e94**\uff1a\u5f3a\u5316\u5b66\u4e60\u673a\u5236\u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u9002\u5e94\u52a8\u6001\u53d8\u5316\u7684\u73af\u5883\u3002\n\n### Step 5: \u5206\u6790\u6700\u65b0\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027\n\n**\u7a33\u5b9a\u6027**\uff1a\n- **\u7b97\u6cd5\u4f18\u5316**\uff1a\u901a\u8fc7\u6539\u8fdbMCTS\u7684\u9009\u62e9\u7b56\u7565\u548cPRM\u7684\u53c2\u6570\u8c03\u6574\u673a\u5236\uff0c\u63d0\u5347\u4e86\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u3002\n- **\u9c81\u68d2\u6027\u589e\u5f3a**\uff1a\u5f15\u5165\u5bf9\u6297\u8bad\u7ec3\u548c\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\uff0c\u63d0\u9ad8\u6a21\u578b\u5728\u590d\u6742\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u6027\u3002\n\n**\u5bb9\u9519\u6027**\uff1a\n- **\u9519\u8bef\u5904\u7406**\uff1a\u5728\u6a21\u62df\u548c\u56de\u6eaf\u9636\u6bb5\u5f15\u5165\u5bb9\u9519\u673a\u5236\uff0c\u51cf\u5c11\u9519\u8bef\u51b3\u7b56\u7684\u5f71\u54cd\u3002\n- **\u5927\u89c4\u6a21\u6570\u636e\u9002\u5e94\u6027**\uff1a\u4f18\u5316\u7b97\u6cd5\u7ed3\u6784\uff0c\u63d0\u5347\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\u7684\u5904\u7406\u80fd\u529b\u3002\n\n### Step 6: \u8bc4\u4f30\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218\n\n**\u672a\u6765\u7814\u7a76\u65b9\u5411**\uff1a\n- **\u9ad8\u6548\u8ba1\u7b97**\uff1a\u7814\u7a76\u66f4\u9ad8\u6548\u7684MCTS\u7b97\u6cd5\uff0c\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002\n- **\u6570\u636e\u589e\u5f3a**\uff1a\u63a2\u7d22\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u51cf\u5c11PRM\u5bf9\u9ad8\u8d28\u91cf\u6570\u636e\u7684\u4f9d\u8d56\u3002\n\n**\u6311\u6218**\uff1a\n- **\u8de8\u6a21\u6001\u878d\u5408**\uff1a\u5982\u4f55\u6709\u6548\u878d\u5408\u591a\u79cd\u6a21\u6001\u6570\u636e\uff0c\u63d0\u5347\u6a21\u578b\u7684\u7efc\u5408\u6027\u80fd\u3002\n- **\u52a8\u6001\u73af\u5883\u9002\u5e94**\uff1a\u5728\u52a8\u6001\u73af\u5883\u4e0b\uff0c\u5982\u4f55\u4fdd\u6301\u6a21\u578b\u7684\u7a33\u5b9a\u6027\u548c\u9ad8\u6548\u6027\u3002\n\n**\u63a8\u52a8\u540e\u7eed\u7814\u7a76**\uff1a\n- **\u65b0\u95ee\u9898\u63d0\u51fa**\uff1a\u5982\u591a\u6a21\u6001\u6570\u636e\u4e0b\u7684\u504f\u597d\u5efa\u6a21\u3001\u52a8\u6001\u73af\u5883\u4e0b\u7684\u51b3\u7b56\u4f18\u5316\u7b49\u3002\n- **\u7814\u7a76\u5207\u5165\u70b9**\uff1a\u4ece\u7b97\u6cd5\u4f18\u5316\u3001\u6570\u636e\u589e\u5f3a\u3001\u8de8\u6a21\u6001\u878d\u5408\u7b49\u591a\u4e2a\u89d2\u5ea6\u5207\u5165\uff0c\u63a8\u52a8\u6280\u672f\u8fdb\u6b65\u3002\n\n### \u603b\u7ed3\n\n\u8fd1\u5e74\u6765\uff0cMCTS\u5728PRM\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u7684\u5e94\u7528\u63a2\u7d22\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4e3b\u8981\u4f53\u73b0\u5728\u6280\u672f\u6846\u67b6\u7684\u5b8c\u5584\u3001\u5e94\u7528\u573a\u666f\u7684\u62d3\u5c55\u3001\u6027\u80fd\u7684\u63d0\u5347\u4ee5\u53ca\u7a33\u5b9a\u6027\u548c\u5bb9\u9519\u6027\u7684\u589e\u5f3a\u3002\u7136\u800c\uff0c\u4ecd\u5b58\u5728\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u3001\u6570\u636e\u4f9d\u8d56\u6027\u5f3a\u7b49\u5c40\u9650\u6027\u3002\u672a\u6765\u7684\u7814\u7a76\u5e94\u5173\u6ce8\u9ad8\u6548\u8ba1\u7b97\u3001\u6570\u636e\u589e\u5f3a\u548c\u8de8\u6a21\u6001\u878d\u5408\u7b49\u65b9\u5411\uff0c\u4ee5\u5e94\u5bf9\u52a8\u6001\u73af\u5883\u4e0b\u7684\u51b3\u7b56\u4f18\u5316\u7b49\u6311\u6218\uff0c\u63a8\u52a8\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002", "task_step_name": "\u7814\u7a76\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u4e3b\u8981\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0e\u53d8\u4f53", "task_step_description": "MCTS\u4e0ePRM\u7ed3\u5408\u7528\u4e8e\u6e38\u620f\u4e2d\u7684\u667a\u80fd\u51b3\u7b56\uff0c\u5982AlphaGo\u3002", "task_step_level": "1", "task_step_question": "### \u95ee\u9898\u63d0\u51fa\n\n\u5728\u201c\u7814\u7a76\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u4e3b\u8981\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0e\u53d8\u4f53\u201d\u8fd9\u4e00\u6b65\u9aa4\u4e2d\uff0cMCTS\u4e0ePRM\u7ed3\u5408\u7528\u4e8e\u6e38\u620f\u4e2d\u7684\u667a\u80fd\u51b3\u7b56\uff0c\u5982AlphaGo\u3002\u57fa\u4e8e\u8fd9\u4e00\u80cc\u666f\uff0c\u63d0\u51fa\u4ee5\u4e0b\u95ee\u9898\uff1a\n\n**\u95ee\u9898**\uff1a\u5728\u6e38\u620fAI\u9886\u57df\uff0cMCTS\u4e0ePRM\u7ed3\u5408\u7684\u5177\u4f53\u5b9e\u73b0\u65b9\u5f0f\u6709\u54ea\u4e9b\uff1f\u8fd9\u4e9b\u5b9e\u73b0\u65b9\u5f0f\u5728AlphaGo\u7b49\u5b9e\u9645\u5e94\u7528\u4e2d\u5982\u4f55\u4f18\u5316\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5e76\u63d0\u5347\u6e38\u620fAI\u7684\u667a\u80fd\u6c34\u5e73\uff1f\n\n**\u95ee\u9898\u89e3\u6790**\uff1a\n1. **\u5177\u4f53\u5b9e\u73b0\u65b9\u5f0f**\uff1a\u63a2\u8ba8MCTS\u4e0ePRM\u5728\u6e38\u620fAI\u4e2d\u7684\u7ed3\u5408\u65b9\u5f0f\uff0c\u4f8b\u5982\u5982\u4f55\u5229\u7528MCTS\u7684\u641c\u7d22\u80fd\u529b\u4f18\u5316PRM\u7684\u51b3\u7b56\u7b56\u7565\uff0c\u6216\u8005\u5982\u4f55\u901a\u8fc7PRM\u7684\u504f\u597d\u5efa\u6a21\u6307\u5bfcMCTS\u7684\u641c\u7d22\u65b9\u5411\u3002\n2. **\u4f18\u5316\u51b3\u7b56\u8fc7\u7a0b**\uff1a\u5206\u6790\u8fd9\u4e9b\u7ed3\u5408\u65b9\u5f0f\u5728\u5b9e\u9645\u5e94\u7528\uff08\u5982AlphaGo\uff09\u4e2d\u5982\u4f55\u63d0\u5347\u51b3\u7b56\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u4f8b\u5982\u901a\u8fc7\u51cf\u5c11\u641c\u7d22\u7a7a\u95f4\u3001\u63d0\u9ad8\u6a21\u62df\u6548\u7387\u6216\u589e\u5f3a\u7b56\u7565\u7684\u9002\u5e94\u6027\u3002\n3. **\u63d0\u5347\u667a\u80fd\u6c34\u5e73**\uff1a\u8bc4\u4f30\u8fd9\u4e9b\u7ed3\u5408\u65b9\u5f0f\u5bf9\u6e38\u620fAI\u667a\u80fd\u6c34\u5e73\u7684\u63d0\u5347\u6548\u679c\uff0c\u4f8b\u5982\u5728\u590d\u6742\u6e38\u620f\u73af\u5883\u4e2d\u7684\u8868\u73b0\u3001\u5bf9\u52a8\u6001\u53d8\u5316\u7684\u9002\u5e94\u80fd\u529b\u4ee5\u53ca\u5bf9\u591a\u6837\u5316\u7b56\u7565\u7684\u751f\u6210\u80fd\u529b\u3002\n\n\u901a\u8fc7\u56de\u7b54\u8fd9\u4e9b\u95ee\u9898\uff0c\u53ef\u4ee5\u66f4\u6df1\u5165\u5730\u7406\u89e3MCTS\u4e0ePRM\u5728\u6e38\u620fAI\u4e2d\u7684\u5e94\u7528\u4ef7\u503c\uff0c\u5e76\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u53c2\u8003\u3002", "task_step_question_context": [{"ref_id": "455026805323333778", "chunk_id": "1", "score": 0.5859375, "text": "# 2.2 Acceleration of MCTS\nMCTS-based methods have proved their strong capability of solving complex games or tasks. However, the high computational cost of MCTS hinders its application to some real-time and more general scenarios. Therefore, numerous works are devoted to accelerating MCTS. For example, to make the selection stage more effective, some heuristic pruning methods [ 14 ,33 ,29 ,1 ,2 ] aim to reduce the width and depth of the search tree with some heuristic functions. Furthermore, for more efficient evaluations, Lorentz [ 22 ] proposed early playout termination of MCTS (MCTS-EPT) to stop the random playouts early and use an evaluation function to assess win or loss. Moreover, Hsueh et al. [18 ] applied MCTS-EPT to the Chinese dark chess and proved its effectiveness. Afterward, similar ideas have been applied in the evaluation stage of AlphaGoZero [ 32 ] and later MCTS-based methods [31 ,27 ,34 ]. They evaluate the $Q$ -values through a learnable evaluation network instead of running playouts to the end. Grill et al. [15 ] propose a novel regularized policy optimization method based on AlphaZero to decrease the search budget of MCTS, which is from the optimization perspective. Danihelka et al. [10 ] propose a policy improvement algorithm based on sampling actions without replacement, named Gumbel trick to achieve better performance when planning with few simulations. However, these methods mentioned above focus on the specific stage of the search iteration or reduce the total budget through pruning and optimization methods, which are orthogonal to us. And few works targets at the search loop. Lan et al. [21 ] propose DS-MCTS, which defines the uncertainty of MCTS and approximates it by extra DNNs with specific features for board games in training. During the evaluation, DS-MCTS will check periodically and stop the search if the state is certain.\n\n# 3 Background\nThe AlphaGo series of work [ 30 ,32 ,31 ,27 ] are all MCTS-based reinforcement learning algorithms. Those algorithms assume the environment transition dynamics are known or learn the environment dynamics. Based on the dynamics, they use the Monte-Carlo tree search (MCTS) as the policy improvement operator. I.e., taking in the current policy, MCTS returns a better policy with the search algorithm. The systematic search allows the MCTS-based RL algorithm to quickly improve the policy and perform much better in the setting where heavy reasoning is required.\n\n# 3.1 MCTS\nThis part briefly introduces the MCTS method implemented in reinforcement learning applications. As mentioned in the related works, modern MCTS-based RL algorithms include four stages in the search loop, namely selection, expansion, evaluation, and backpropagation.  \n\nMCTS takes in the current states and generates a policy after the search loop of $N$ iterations. Here $N$ is a constant number of iterations set by the designer, regarded as the total budget. In the selection stage of each iteration, an action will be selected by maximizing over UCB. Specifically, AlphaZero [31 ] and MuZero [ 27 ] are developed based on a variant of UCB, P-UCT [ 25 ] and have achieved great success on board games and Atari games. The formula of P-UCT is the Eq (1):  \n\n$$\na^{k}=\\arg\\operatorname*{max}_{a\\in\\mathcal{A}}Q(s,a)+P(s,a)\\frac{\\sqrt{\\sum_{b\\in\\mathcal{A}}N(s,b)}}{1+N(s,a)}(c_{1}+\\log((\\sum_{b\\in\\mathcal{A}}N(s,b)+c_{2}+1)/c_{2})),\n$$  \n\nwhere $k$ is the index of iteration, $\\boldsymbol{\\mathcal{A}}$ is the acti $Q(s,a)$ is the estimated Q-value, $P(s,a)$ is the policy prior obtained from neural networks, $N(s,a)$ is the visitations to select the action a from the state $s$ and $c_{1},c_{2}$ are hyper-parameters. The output of MCTS is the visitation of each action of the root node. After $N$ search iterations, the final policy $\\pi(s)$ is defined as the normalized root visitation distribution simplification, we use $\\pi_{N}(s)$ $\\pi_{k}$ in place of , where $\\begin{array}{r}{\\pi_{k}(s,a)=N(s,a)/\\sum_{b\\in\\mathcal{A}}N(s,b)=N(s,a)/k,a\\in\\mathcal{A}}\\end{array}$ $\\pi_{k}(s)$ sometimes. And the detailed procedure of MCTS is \u2208A . For introduced in Appendix. In our method, we propose to approximate the final policy $\\pi_{N}(s)$ with $\\hat{\\pi}_{k}(s)$ ,which we name as a virtual expanded policy, through a new expansion method and a termination rule. In this way, the number of iterations in MCTS can be reduced from $N$ to $k$ .\n\n# 3.2 Computation Requirement\nMost of the computations in MCTS-based RL are in the MCTS procedure. Each action taken by MCTS requires $N$ times neural network evaluations, where $N$ is a constant number of iterations in the search loop. Traditional RL algorithms, such as PPO [ 28 ] or DQN [ 23 ], only need a single neural network evaluation per action. Thus, MCTS-based RL is roughly $N$ times computationally more expensive than traditional RL algorithms. In practice, training a single Atari game needs 12 hours of computation time on 40 TPUs [ 27 ]. The computation need is roughly two orders of magnitude more than traditional RL algorithms [28], although the final performance of MuZero is much better.\n\n# 4 Method\nWe aim to spend more search time on harder states and less on easier states. Intuitively, human knows when to make a quick decision or a slow decision under different circumstances. Unfortunately, this situation-aware behavior is absent in current MCTS algorithms. Therefore, we propose an MCTS variant that terminates the search iteration adaptively. It consists of two components: a novel expansion method named virtual expansion to estimate the final visitation based on the current partial tree; a termination rule that decides when to terminate based on the hardness of the current scenario. And we will display the adaptive mechanism through visualizations in Section 5.5."}, {"ref_id": "455026805307867280", "chunk_id": "0", "score": 0.51171875, "text": "# Related Works\nTwo streams of research are particularly relevant to our work: learnable (lifelong) MAPF methods and utilizing MCTS for multi-agent systems and MAPF in particular. Next, we review both of these domains.  \n\nLearnable (L)MAPF Solvers Among the recent works dedicated to MAPF, one of the first ones that were specifically dedicated to creating a learning-based MAPF solver was (Sartoretti et al. 2019). A combination of reinforcement learning and learning from expert demonstrations was used to create a learnable policy called Primal, tailored to solve conventional MAPF problems. Later in (Damani et al. 2021), an enhanced version of this solver, Primal2, was introduced. The latter was equipped with special corridor reasoning techniques, aiming at avoiding the deadlocks in narrow corridors, and it supported lifelong MAPF setting (therefore, we choose Primal2 as one of the baselines we compare our method to). Among the other learnable MAPF solvers that use reinforcement learning to obtain a decision-making policy, one can name (Riviere et al. 2020; Wang et al. 2020). The learnable methods introduced in (Li et al. 2020; Ma, Luo, and Ma 2021; Li et al. 2022) add communication capabilities to the agents, i.e., allow the agents to communicate to resolve deadlocks and avoid congestion. In this work, we compare with one of the most recent communication-based methods, i.e., SCRIMP (Wang et al. 2023). However, it is worth noting that our method does not rely on agent communication.  \n\nMCTS for MAPF Initially, Monte Carlo Tree Search (MCTS) algorithms demonstrated their effectiveness in competitive games with complete information, such as chess or Go (Silver et al. 2017). More recent versions of MCTS utilize deep neural networks to approximate the values of game states instead of relying solely on simulations. These approaches have also shown promising results in singleagent scenarios, where agents can learn a model of the environment and play Atari games (Schrittwieser et al. 2020; Ye et al. 2021). Besides gaming, MCTS methods have found applications in other domains, such as matrix multiplication optimization (Fawzi et al. 2022) and theorem proving using the Hyper Tree approach (Lample et al. 2022). Additionally, MCTS techniques have demonstrated applicability in robotics (Best et al. 2019; Dam et al. 2022).  \n\nDespite the growing interest in utilizing MCTS for multiagent tasks, there have been limited applications of MCTS for MAPF. In their work (Zerbel and Yliniemi 2019), the authors propose a multi-agent MCTS for Anonymous MAPF in a grid-world environment. Their environment has a dense reward signal (the agent who reached any goal on the map received a reward and ended the episode), and there are no obstacles, making collision avoidance easier. The authors build a separate tree for each agent using a classical algorithm. They then jointly apply the best actions (forming a plan) from the trees in the simulator to receive true scores of the solution and update the trees on that difference. This approach performs well even with a large number of agents.  \n\nA recent paper (Skrynnik et al. 2021) proposed a more sophisticated approach for multi-agent planning that combines RL and MCTS. The authors suggested a two-part scheme that includes a goal achievement module and a conflict resolution module. The latter was trained using MCTS. The construction of the search tree for each of the agents was also performed independently, and actions for other agents were selected using the currently trained policy. This work used MCTS only during training to train the conflict resolution policy."}, {"ref_id": "454847012122763908", "chunk_id": "1", "score": 0.478515625, "text": "# Spending Thinking Time Wisely: Accelerating MCTS with Virtual Expansions\nWeirui $\\mathbf{Ye}^{*}$ Pieter Abbeel \u2020Yang Gao $\\ast\\ddag\\S$ \u2217Tsinghua University, \u2020UC Berkeley, \u00a7Shanghai Qi Zhi Institute\n\n# Abstract\nOne of the most important AI research questions is to trade off computation versus performance since \u201cperfect rationality\" exists in theory but is impossible to achieve in practice. Recently, Monte-Carlo tree search (MCTS) has attracted considerable attention due to the significant performance improvement in various challenging domains. However, the expensive time cost during search severely restricts its scope for applications. This paper proposes the Virtual MCTS (V-MCTS), a variant of MCTS that spends more search time on harder states and less search time on simpler states adaptively. We give theoretical bounds of the proposed method and evaluate the performance and computations on $9\\times9$ Go board games and Atari games. Experiments show that our method can achieve comparable performances to the original search algorithm while requiring less than $50\\%$ search time on average. We believe that this approach is a viable alternative for tasks under limited time and resources. The code is available at https://github.com/YeWR/V-MCTS.git .\n\n# 1 Introduction\nWhen artificial intelligence was first studied in the 1950s, researchers have sought to find the solution to the question \u201cHow to build an agent with perfect rationality\". The term \u201cperfect rationality\" [7 ,24 ,26 ] here refers to the decision made with infinite amounts of computations. However, one can only solve small-scale problems without considering the practical computation time since classical search algorithms usually exhibit exponential running time. Therefore, recent AI research would no longer seek to achieve \u201cperfect rationality\", but instead carefully trade-off computation versus the level of rationality. People have developed computational models like \u201cbounded optimality\" to model these settings [ 26 ]. The increasing level of rationality under the same computational budget has given us a lot of AI successes. Algorithms include the Monte-Carlo sampling algorithms, the variational inference algorithms, and using DNNs as universal function approximators [9, 8, 13, 30, 17].  \n\nRecently, MCTS-based RL algorithms have achieved much success, mainly on board games. The most notable achievement is that AlphaGo beats Hui Fan in 2015 [ 30 ]. It is the first time a computer program beat a human professional Go player. Afterward, AlphaGo beats two top-ranking human players, Lee Sedol in 2016 and Jie Ke in 2017, the latter of which ranked first worldwide at the time. Later, MCTS-based RL algorithms were further extended to other board games and Atari games [ 27 ]. EfficientZero [ 34 ] significantly improves the sample efficiency of MCTS-based RL algorithms, shedding light on its future applications in the real world like robotics and self-driving.  \n\nDespite the impressive performance of MCTS-based RL algorithms, they require massive amounts of computation to train and evaluate. For example, MuZero [ 27 ] used 1000 TPUs trained for 12 hours to learn the game of Go, and for a single Atari game, it needs 40 TPUs to train 12 hours. Compared to previous algorithms on the Atari games benchmark, it needs around two orders of magnitude more compute. This prohibitively large computational requirement has slowed down both the further development of MCTS-based RL algorithms as well as its practical use.  \n\nUnder the hood, MCTS-based RL algorithms imagine the futures when taking different future action sequences. However, this imaging process for the current method is not computationally efficient. For example, AlphaGo needs to look ahead 1600 game states to place a single stone. On the contrary, top human professional players can only think through around 100-200 game states per minute [ 30 ]. Apart from the inefficiency, the current MCTS algorithm deals with easy and challenging cases with the same computational budget. However, human knows to use their time when it is most needed.  \n\nIn this paper, we aim to design new algorithms that save the computational time of the MCTSbased RL methods. We make three key contributions : (1) We present Virtual MCTS, a variant of MCTS, to approximate the vanilla MCTS search policies with less computation. Moreover, unlike previous pruning-based methods that focus on the selection or evaluation stage in MCTS, our method improves the search loop. It terminates the search iterations earlier adaptively when current states are simpler; (2) Theoretically, we provide some error bounds of the proposed method. Furthermore, the visualization results indicate that Virtual MCTS has a better computation and performance trade-off than vanilla MCTS; (3) Empirically, our method can save more than $50\\%$ of search times on the challenging game Go $9\\times9$ and more than $60\\%$ on the visually complex Atari games while keeping comparable performances to those of vanilla MCTS.\n\n# 2 Related Work\n\n# 2.1 Reinforcement Learning with MCTS\nFor a long time, Computer Go has been regarded as a remarkably challenging game [ 3 ,6 ]. Researchers attempt to use Monte-Carlo techniques that evaluate the value of the node state through random playouts [ 4 ,11 ,12 ,30 ]. Afterward, UCT algorithms have generally been applied in Monte-Carlo tree search (MCTS) algorithms, which use UCB1 to select action at each node of the tree [ 20 ]. Recently, MCTS-based RL methods [ 30 ,32 ,31 ,27 ] have become increasingly popular and achieved super-human performances on board games because of their strong ability to search.  \n\nModern MCTS-based RL algorithms include four stages in the search loop : selection, expansion, evaluation, and backpropagation. The computation bottlenecks in vanilla MCTS come from the search loop, especially for the evaluation stage and the selection stage of each iteration. The selection stage is time-consuming when the search tree becomes wider and deeper. The evaluation stage is quite expensive because people attempt to evaluate the node value by random playouts to the end in previous researches. Due to the search loop, MCTS-based algorithms have multiple model inferences compared to other model-free RL methods like PPO [28] and SAC [16]."}], "task_step_question_answer": "\u5728\u533b\u7597\u8bca\u65ad\u9886\u57df\uff0cMCTS\u4e0ePRM\u7684\u7ed3\u5408\u4e5f\u88ab\u5e7f\u6cdb\u5e94\u7528\u3002PRM\u901a\u8fc7\u5206\u6790\u60a3\u8005\u7684\u75c5\u53f2\u548c\u75c7\u72b6\uff0c\u4e3aMCTS\u63d0\u4f9b\u5148\u9a8c\u4fe1\u606f\uff0c\u5e2e\u52a9\u5176\u5728\u8bca\u65ad\u8fc7\u7a0b\u4e2d\u4f18\u5148\u8003\u8651\u53ef\u80fd\u7684\u75be\u75c5\u3002MCTS\u5219\u901a\u8fc7\u6a21\u62df\u4e0d\u540c\u7684\u8bca\u65ad\u8def\u5f84\u6765\u8bc4\u4f30\u6700\u4f18\u8bca\u65ad\u65b9\u6848\u3002\u8fd9\u79cd\u7ed3\u5408\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u8bca\u65ad\u7684\u51c6\u786e\u6027\uff0c\u8fd8\u901a\u8fc7\u51cf\u5c11\u641c\u7d22\u7a7a\u95f4\u548c\u4f18\u5316\u6a21\u62df\u6548\u7387\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u7684\u8ba1\u7b97\u6548\u7387\u3002\u5728\u91d1\u878d\u9884\u6d4b\u9886\u57df\uff0cMCTS\u4e0ePRM\u7684\u7ed3\u5408\u88ab\u7528\u4e8e\u9884\u6d4b\u5e02\u573a\u8d8b\u52bf\u548c\u6295\u8d44\u51b3\u7b56\u3002PRM\u901a\u8fc7\u5206\u6790\u5386\u53f2\u5e02\u573a\u6570\u636e\u548c\u6295\u8d44\u8005\u884c\u4e3a\uff0c\u4e3aMCTS\u63d0\u4f9b\u5148\u9a8c\u4fe1\u606f\uff0c\u5e2e\u52a9\u5176\u5728\u9884\u6d4b\u8fc7\u7a0b\u4e2d\u4f18\u5148\u8003\u8651\u53ef\u80fd\u7684\u8d8b\u52bf\u3002MCTS\u5219\u901a\u8fc7\u6a21\u62df\u4e0d\u540c\u7684\u6295\u8d44\u8def\u5f84\u6765\u8bc4\u4f30\u6700\u4f18\u6295\u8d44\u7b56\u7565\u3002\u8fd9\u79cd\u7ed3\u5408\u5728\u590d\u6742\u5e02\u573a\u73af\u5883\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "ref_task_step_id": ""}, "__type__": "task_step_node"}, "8a1e6e8e-b71e-46d2-aa53-8f3d04b1d842": {"__data__": {"id_": "8a1e6e8e-b71e-46d2-aa53-8f3d04b1d842", "metadata": {}, "relationships": {}, "hash": "", "start_task_context": "MCTS\u5728PRM\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u7684\u5e94\u7528\u63a2\u7d22\u7efc\u8ff0", "aemo_representation_context": "### Step 1: \u5206\u6790\u8fd1\u51e0\u5e74\u7814\u7a76\u9886\u57df\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba\n\n**\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09**\uff1a\n- **\u57fa\u672c\u539f\u7406**\uff1aMCTS\u662f\u4e00\u79cd\u7528\u4e8e\u51b3\u7b56\u8fc7\u7a0b\u7684\u542f\u53d1\u5f0f\u641c\u7d22\u7b97\u6cd5\uff0c\u5e7f\u6cdb\u5e94\u7528\u4e8e\u6e38\u620fAI\u3001\u8def\u5f84\u89c4\u5212\u7b49\u9886\u57df\u3002\u5176\u6838\u5fc3\u601d\u60f3\u662f\u901a\u8fc7\u6a21\u62df\u591a\u6b21\u968f\u673a\u6e38\u620f\u6765\u8bc4\u4f30\u4e0d\u540c\u51b3\u7b56\u7684\u4f18\u52a3\u3002\n- **\u6280\u672f\u6846\u67b6**\uff1a\u4e3b\u8981\u5305\u62ec\u9009\u62e9\uff08Selection\uff09\u3001\u6269\u5c55\uff08Expansion\uff09\u3001\u6a21\u62df\uff08Simulation\uff09\u548c\u56de\u6eaf\uff08Backpropagation\uff09\u56db\u4e2a\u6b65\u9aa4\u3002\n\n**\u504f\u597d\u7b56\u7565\u6a21\u578b\uff08PRM\uff09**\uff1a\n- **\u57fa\u672c\u539f\u7406**\uff1aPRM\u65e8\u5728\u901a\u8fc7\u5b66\u4e60\u7528\u6237\u7684\u504f\u597d\u6765\u4f18\u5316\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5e38\u7528\u4e8e\u63a8\u8350\u7cfb\u7edf\u3001\u4e2a\u6027\u5316\u670d\u52a1\u7b49\u3002\n- **\u6280\u672f\u6846\u67b6**\uff1a\u901a\u5e38\u5305\u62ec\u504f\u597d\u5efa\u6a21\u3001\u7b56\u7565\u4f18\u5316\u548c\u6a21\u578b\u5fae\u8c03\u7b49\u73af\u8282\u3002\n\n**\u65b9\u6cd5\u8bba**\uff1a\n- **\u96c6\u6210\u5b66\u4e60**\uff1a\u5c06MCTS\u4e0ePRM\u7ed3\u5408\uff0c\u5229\u7528MCTS\u7684\u641c\u7d22\u80fd\u529b\u4f18\u5316PRM\u7684\u51b3\u7b56\u8fc7\u7a0b\u3002\n- **\u5f3a\u5316\u5b66\u4e60**\uff1a\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u673a\u5236\u4e0d\u65ad\u8c03\u6574PRM\u7684\u53c2\u6570\uff0c\u4ee5\u9002\u5e94\u52a8\u6001\u73af\u5883\u3002\n\n### Step 2: \u7814\u7a76\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u4e3b\u8981\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0e\u53d8\u4f53\n\n**\u5e94\u7528\u573a\u666f**\uff1a\n- **\u6e38\u620fAI**\uff1aMCTS\u4e0ePRM\u7ed3\u5408\u7528\u4e8e\u6e38\u620f\u4e2d\u7684\u667a\u80fd\u51b3\u7b56\uff0c\u5982AlphaGo\u3002\n- **\u8def\u5f84\u89c4\u5212**\uff1a\u5728\u673a\u5668\u4eba\u5bfc\u822a\u548c\u81ea\u52a8\u9a7e\u9a76\u4e2d\uff0c\u5229\u7528MCTS\u4f18\u5316\u8def\u5f84\u9009\u62e9\u3002\n- **\u63a8\u8350\u7cfb\u7edf**\uff1a\u901a\u8fc7PRM\u5b66\u4e60\u7528\u6237\u504f\u597d\uff0c\u7ed3\u5408MCTS\u8fdb\u884c\u66f4\u7cbe\u51c6\u7684\u63a8\u8350\u3002\n\n**\u53d8\u4f53**\uff1a\n- **\u6df1\u5ea6MCTS**\uff1a\u7ed3\u5408\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff0c\u63d0\u5347MCTS\u7684\u51b3\u7b56\u80fd\u529b\u3002\n- **\u591a\u6a21\u6001PRM**\uff1a\u878d\u5408\u591a\u79cd\u6570\u636e\u6e90\uff08\u5982\u6587\u672c\u3001\u56fe\u50cf\uff09\uff0c\u589e\u5f3aPRM\u7684\u6cdb\u5316\u80fd\u529b\u3002\n\n### Step 3: \u8bc4\u4f30\u5b66\u672f\u754c\u7684\u6280\u672f\u8fdb\u6b65\u4e0e\u5c40\u9650\u6027\n\n**\u6280\u672f\u8fdb\u6b65**\uff1a\n- **\u6027\u80fd\u63d0\u5347**\uff1aMCTS\u4e0ePRM\u7ed3\u5408\u663e\u8457\u63d0\u5347\u4e86\u51b3\u7b56\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002\n- **\u6cdb\u5316\u80fd\u529b**\uff1a\u591a\u6a21\u6001PRM\u5728\u5904\u7406\u591a\u6837\u5316\u6570\u636e\u65f6\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002\n\n**\u5c40\u9650\u6027**\uff1a\n- **\u8ba1\u7b97\u590d\u6742\u5ea6**\uff1aMCTS\u5728\u5927\u89c4\u6a21\u95ee\u9898\u4e2d\u8ba1\u7b97\u91cf\u5927\uff0c\u6548\u7387\u6709\u5f85\u63d0\u5347\u3002\n- **\u6570\u636e\u4f9d\u8d56**\uff1aPRM\u7684\u6027\u80fd\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u9ad8\u8d28\u91cf\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u6570\u636e\u504f\u5dee\u95ee\u9898\u4f9d\u7136\u5b58\u5728\u3002\n\n### Step 4: \u63a2\u8ba8\u8ba1\u7b97\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\n\n**\u9002\u7528\u6027**\uff1a\n- **\u591a\u9886\u57df\u5e94\u7528**\uff1aMCTS+PRM\u6846\u67b6\u5728\u6e38\u620f\u3001\u8def\u5f84\u89c4\u5212\u3001\u63a8\u8350\u7cfb\u7edf\u7b49\u591a\u4e2a\u9886\u57df\u8868\u73b0\u51fa\u826f\u597d\u7684\u9002\u7528\u6027\u3002\n- **\u591a\u6a21\u6001\u6570\u636e**\uff1a\u591a\u6a21\u6001PRM\u80fd\u591f\u5904\u7406\u591a\u79cd\u7c7b\u578b\u7684\u6570\u636e\uff0c\u9002\u7528\u4e8e\u590d\u6742\u5e94\u7528\u573a\u666f\u3002\n\n**\u6cdb\u5316\u80fd\u529b**\uff1a\n- **\u8de8\u9886\u57df\u6cdb\u5316**\uff1a\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\uff0cMCTS+PRM\u6846\u67b6\u5728\u4e0d\u540c\u9886\u57df\u95f4\u5c55\u73b0\u51fa\u8f83\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002\n- **\u52a8\u6001\u73af\u5883\u9002\u5e94**\uff1a\u5f3a\u5316\u5b66\u4e60\u673a\u5236\u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u9002\u5e94\u52a8\u6001\u53d8\u5316\u7684\u73af\u5883\u3002\n\n### Step 5: \u5206\u6790\u6700\u65b0\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027\n\n**\u7a33\u5b9a\u6027**\uff1a\n- **\u7b97\u6cd5\u4f18\u5316**\uff1a\u901a\u8fc7\u6539\u8fdbMCTS\u7684\u9009\u62e9\u7b56\u7565\u548cPRM\u7684\u53c2\u6570\u8c03\u6574\u673a\u5236\uff0c\u63d0\u5347\u4e86\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u3002\n- **\u9c81\u68d2\u6027\u589e\u5f3a**\uff1a\u5f15\u5165\u5bf9\u6297\u8bad\u7ec3\u548c\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\uff0c\u63d0\u9ad8\u6a21\u578b\u5728\u590d\u6742\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u6027\u3002\n\n**\u5bb9\u9519\u6027**\uff1a\n- **\u9519\u8bef\u5904\u7406**\uff1a\u5728\u6a21\u62df\u548c\u56de\u6eaf\u9636\u6bb5\u5f15\u5165\u5bb9\u9519\u673a\u5236\uff0c\u51cf\u5c11\u9519\u8bef\u51b3\u7b56\u7684\u5f71\u54cd\u3002\n- **\u5927\u89c4\u6a21\u6570\u636e\u9002\u5e94\u6027**\uff1a\u4f18\u5316\u7b97\u6cd5\u7ed3\u6784\uff0c\u63d0\u5347\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\u7684\u5904\u7406\u80fd\u529b\u3002\n\n### Step 6: \u8bc4\u4f30\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218\n\n**\u672a\u6765\u7814\u7a76\u65b9\u5411**\uff1a\n- **\u9ad8\u6548\u8ba1\u7b97**\uff1a\u7814\u7a76\u66f4\u9ad8\u6548\u7684MCTS\u7b97\u6cd5\uff0c\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002\n- **\u6570\u636e\u589e\u5f3a**\uff1a\u63a2\u7d22\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u51cf\u5c11PRM\u5bf9\u9ad8\u8d28\u91cf\u6570\u636e\u7684\u4f9d\u8d56\u3002\n\n**\u6311\u6218**\uff1a\n- **\u8de8\u6a21\u6001\u878d\u5408**\uff1a\u5982\u4f55\u6709\u6548\u878d\u5408\u591a\u79cd\u6a21\u6001\u6570\u636e\uff0c\u63d0\u5347\u6a21\u578b\u7684\u7efc\u5408\u6027\u80fd\u3002\n- **\u52a8\u6001\u73af\u5883\u9002\u5e94**\uff1a\u5728\u52a8\u6001\u73af\u5883\u4e0b\uff0c\u5982\u4f55\u4fdd\u6301\u6a21\u578b\u7684\u7a33\u5b9a\u6027\u548c\u9ad8\u6548\u6027\u3002\n\n**\u63a8\u52a8\u540e\u7eed\u7814\u7a76**\uff1a\n- **\u65b0\u95ee\u9898\u63d0\u51fa**\uff1a\u5982\u591a\u6a21\u6001\u6570\u636e\u4e0b\u7684\u504f\u597d\u5efa\u6a21\u3001\u52a8\u6001\u73af\u5883\u4e0b\u7684\u51b3\u7b56\u4f18\u5316\u7b49\u3002\n- **\u7814\u7a76\u5207\u5165\u70b9**\uff1a\u4ece\u7b97\u6cd5\u4f18\u5316\u3001\u6570\u636e\u589e\u5f3a\u3001\u8de8\u6a21\u6001\u878d\u5408\u7b49\u591a\u4e2a\u89d2\u5ea6\u5207\u5165\uff0c\u63a8\u52a8\u6280\u672f\u8fdb\u6b65\u3002\n\n### \u603b\u7ed3\n\n\u8fd1\u5e74\u6765\uff0cMCTS\u5728PRM\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u7684\u5e94\u7528\u63a2\u7d22\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4e3b\u8981\u4f53\u73b0\u5728\u6280\u672f\u6846\u67b6\u7684\u5b8c\u5584\u3001\u5e94\u7528\u573a\u666f\u7684\u62d3\u5c55\u3001\u6027\u80fd\u7684\u63d0\u5347\u4ee5\u53ca\u7a33\u5b9a\u6027\u548c\u5bb9\u9519\u6027\u7684\u589e\u5f3a\u3002\u7136\u800c\uff0c\u4ecd\u5b58\u5728\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u3001\u6570\u636e\u4f9d\u8d56\u6027\u5f3a\u7b49\u5c40\u9650\u6027\u3002\u672a\u6765\u7684\u7814\u7a76\u5e94\u5173\u6ce8\u9ad8\u6548\u8ba1\u7b97\u3001\u6570\u636e\u589e\u5f3a\u548c\u8de8\u6a21\u6001\u878d\u5408\u7b49\u65b9\u5411\uff0c\u4ee5\u5e94\u5bf9\u52a8\u6001\u73af\u5883\u4e0b\u7684\u51b3\u7b56\u4f18\u5316\u7b49\u6311\u6218\uff0c\u63a8\u52a8\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002", "task_step_name": "\u6e38\u620fAI", "task_step_description": "MCTS\u4e0ePRM\u7ed3\u5408\u7528\u4e8e\u6e38\u620f\u4e2d\u7684\u667a\u80fd\u51b3\u7b56\uff0c\u5982AlphaGo\u3002", "task_step_level": "1>1", "task_step_question": "### \u95ee\u9898\n\n\u5728\u6e38\u620fAI\u9886\u57df\uff0cMCTS\u4e0ePRM\u7ed3\u5408\u7528\u4e8e\u667a\u80fd\u51b3\u7b56\uff08\u5982AlphaGo\uff09\u65f6\uff0c\u5982\u4f55\u6709\u6548\u5e73\u8861MCTS\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u548cPRM\u7684\u504f\u597d\u5efa\u6a21\u7cbe\u5ea6\uff0c\u4ee5\u5728\u4fdd\u8bc1\u51b3\u7b56\u8d28\u91cf\u7684\u540c\u65f6\u63d0\u5347\u7b97\u6cd5\u7684\u5b9e\u65f6\u6027\uff1f", "task_step_question_context": [{"ref_id": "455026805323333778", "chunk_id": "1", "score": 0.404296875, "text": "# Spending Thinking Time Wisely: Accelerating MCTS with Virtual Expansions\nWeirui $\\mathbf{Ye}^{*}$ Pieter Abbeel \u2020Yang Gao $\\ast\\ddag\\S$ \u2217Tsinghua University, \u2020UC Berkeley, \u00a7Shanghai Qi Zhi Institute\n\n# Abstract\nOne of the most important AI research questions is to trade off computation versus performance since \u201cperfect rationality\" exists in theory but is impossible to achieve in practice. Recently, Monte-Carlo tree search (MCTS) has attracted considerable attention due to the significant performance improvement in various challenging domains. However, the expensive time cost during search severely restricts its scope for applications. This paper proposes the Virtual MCTS (V-MCTS), a variant of MCTS that spends more search time on harder states and less search time on simpler states adaptively. We give theoretical bounds of the proposed method and evaluate the performance and computations on $9\\times9$ Go board games and Atari games. Experiments show that our method can achieve comparable performances to the original search algorithm while requiring less than $50\\%$ search time on average. We believe that this approach is a viable alternative for tasks under limited time and resources. The code is available at https://github.com/YeWR/V-MCTS.git .\n\n# 1 Introduction\nWhen artificial intelligence was first studied in the 1950s, researchers have sought to find the solution to the question \u201cHow to build an agent with perfect rationality\". The term \u201cperfect rationality\" [7 ,24 ,26 ] here refers to the decision made with infinite amounts of computations. However, one can only solve small-scale problems without considering the practical computation time since classical search algorithms usually exhibit exponential running time. Therefore, recent AI research would no longer seek to achieve \u201cperfect rationality\", but instead carefully trade-off computation versus the level of rationality. People have developed computational models like \u201cbounded optimality\" to model these settings [ 26 ]. The increasing level of rationality under the same computational budget has given us a lot of AI successes. Algorithms include the Monte-Carlo sampling algorithms, the variational inference algorithms, and using DNNs as universal function approximators [9, 8, 13, 30, 17].  \n\nRecently, MCTS-based RL algorithms have achieved much success, mainly on board games. The most notable achievement is that AlphaGo beats Hui Fan in 2015 [ 30 ]. It is the first time a computer program beat a human professional Go player. Afterward, AlphaGo beats two top-ranking human players, Lee Sedol in 2016 and Jie Ke in 2017, the latter of which ranked first worldwide at the time. Later, MCTS-based RL algorithms were further extended to other board games and Atari games [ 27 ]. EfficientZero [ 34 ] significantly improves the sample efficiency of MCTS-based RL algorithms, shedding light on its future applications in the real world like robotics and self-driving.  \n\nDespite the impressive performance of MCTS-based RL algorithms, they require massive amounts of computation to train and evaluate. For example, MuZero [ 27 ] used 1000 TPUs trained for 12 hours to learn the game of Go, and for a single Atari game, it needs 40 TPUs to train 12 hours. Compared to previous algorithms on the Atari games benchmark, it needs around two orders of magnitude more compute. This prohibitively large computational requirement has slowed down both the further development of MCTS-based RL algorithms as well as its practical use.  \n\nUnder the hood, MCTS-based RL algorithms imagine the futures when taking different future action sequences. However, this imaging process for the current method is not computationally efficient. For example, AlphaGo needs to look ahead 1600 game states to place a single stone. On the contrary, top human professional players can only think through around 100-200 game states per minute [ 30 ]. Apart from the inefficiency, the current MCTS algorithm deals with easy and challenging cases with the same computational budget. However, human knows to use their time when it is most needed.  \n\nIn this paper, we aim to design new algorithms that save the computational time of the MCTSbased RL methods. We make three key contributions : (1) We present Virtual MCTS, a variant of MCTS, to approximate the vanilla MCTS search policies with less computation. Moreover, unlike previous pruning-based methods that focus on the selection or evaluation stage in MCTS, our method improves the search loop. It terminates the search iterations earlier adaptively when current states are simpler; (2) Theoretically, we provide some error bounds of the proposed method. Furthermore, the visualization results indicate that Virtual MCTS has a better computation and performance trade-off than vanilla MCTS; (3) Empirically, our method can save more than $50\\%$ of search times on the challenging game Go $9\\times9$ and more than $60\\%$ on the visually complex Atari games while keeping comparable performances to those of vanilla MCTS.\n\n# 2 Related Work\n\n# 2.1 Reinforcement Learning with MCTS\nFor a long time, Computer Go has been regarded as a remarkably challenging game [ 3 ,6 ]. Researchers attempt to use Monte-Carlo techniques that evaluate the value of the node state through random playouts [ 4 ,11 ,12 ,30 ]. Afterward, UCT algorithms have generally been applied in Monte-Carlo tree search (MCTS) algorithms, which use UCB1 to select action at each node of the tree [ 20 ]. Recently, MCTS-based RL methods [ 30 ,32 ,31 ,27 ] have become increasingly popular and achieved super-human performances on board games because of their strong ability to search.  \n\nModern MCTS-based RL algorithms include four stages in the search loop : selection, expansion, evaluation, and backpropagation. The computation bottlenecks in vanilla MCTS come from the search loop, especially for the evaluation stage and the selection stage of each iteration. The selection stage is time-consuming when the search tree becomes wider and deeper. The evaluation stage is quite expensive because people attempt to evaluate the node value by random playouts to the end in previous researches. Due to the search loop, MCTS-based algorithms have multiple model inferences compared to other model-free RL methods like PPO [28] and SAC [16]."}, {"ref_id": "455026805307867280", "chunk_id": "0", "score": 0.341796875, "text": "# 6 PLAYING CHESS WITH MONTE -C ARLO TREE SEARCH\nFinally, we turn from cooperative to adversarial decision-making tasks. We focus on chess, a popular two-player sequential game widely used as a benchmark for AI systems. Here, we are interested in  \n\n<html><body><table><tr><td>Model</td><td>Type</td><td>Accuracy</td></tr><tr><td>\u03b2runtime = 0</td><td></td><td>83.3</td></tr><tr><td>pruntime =1</td><td>\u4e00</td><td>83.0</td></tr><tr><td>Inferred \u03b2temp</td><td>player skill</td><td>83.9</td></tr><tr><td>Inferred 1pruntime (L-IBM)</td><td>playerskill</td><td>84.0</td></tr><tr><td>Inferred \u03b2temp</td><td>difficulty</td><td>83.5</td></tr><tr><td>Inferred Pruntime (L-IBM)</td><td>difficulty</td><td>82.7</td></tr></table></body></html>  \n\nTable 3: Performance of different RSA models in predicting the speaker target. All models (including literal models and fixed-depth RSA models) achieve similar predictive performance\u2014because even literal models have access to all three referents, all model variants can achieve good task performance. Note that $\\beta_{\\mathrm{runtime}}=0$ represents the base literal listener.  \n\n<html><body><table><tr><td>Model</td><td>Type</td><td>Accuracy</td></tr><tr><td>IL</td><td></td><td>42.06</td></tr><tr><td>\u03b2runtime =100</td><td></td><td>43.64</td></tr><tr><td>Inferred \u03b2puct</td><td>ActiveElo</td><td>43.77</td></tr><tr><td>Inferred Pruntime (L-IBM)</td><td>ActiveElo</td><td>44.17</td></tr><tr><td>Inferred Ppuct</td><td>OpponentElo</td><td>43.84</td></tr><tr><td>Inferred Pruntime (L-IBM)</td><td>OpponentElo</td><td>44.17</td></tr><tr><td>Inferred \u03b2puct</td><td>TimeControl</td><td>43.61</td></tr><tr><td>Inferred \u03b2runtime (L-IBM)</td><td>Time Control</td><td>44.15</td></tr></table></body></html>  \n\nTable 4: Accuracy of predicting an agent\u2019s next action in chess. Models with MCTS outperform the depth-0 (imitation learning) baseline. Learning subpopulation-specific $\\beta$ enhances performance, with L-IBM-based learning of $\\beta_{\\mathrm{runtime}}$ consistently outperforming $\\beta_{\\mathrm{puct}}$ by a slight margin.  \n\nmodeling human chess play\u2014specifically, observing data from a population of sub-optimal agents with a common reward function (winning the game) and attempting to infer those agents\u2019 computational constraints. In human human play, there can be numerous sources of such constraints: a player paired against a strong opponent will likely to plan for longer than against a weaker opponent; some variants (like blitz chess) deliberately limit players\u2019 time-per-move (and, we might expect, the quality of their plans). Given a dataset of human games played under different time constraints and player strengths, can we use L-IBM to model variability in players\u2019 decisions across game states?\n\n# 6.1 A GENT MODEL\nIn this work, we model chess players as selecting actions using Monte Carlo tree search (MCTS). Recent work (Jacob et al., 2022) has shown that MCTS is a good model of strong human players. Here, following (Silver et al., 2018; 2016; Jacob et al., 2022; Grill et al., 2020), we implement one of the most common modern forms of MCTS, which uses a value function $V$ predicting the expected total future reward and a policy prior $\\pi^{0}$ to guide exploration. At a high level, MCTS operates by incrementally growing a game tree starting at the root node, repeatedly picking some path to explore down the tree, performing a value function evaluation and then walking back up the tree updating all the value estimates based on that result. At each node, MCTS treats action selection as a multiarmed bandit problem. We use a standard exploration policy (Kocsis & Szepesv\u00b4ari, 2006): during inference at each node of the search tree, we choose actions according to:  \n\n$$\n\\arg\\operatorname*{max}_{a}\\,Q_{t}(a\\mid s)+\\beta_{\\mathrm{puct}}\\pi^{0}(a\\mid s)\\frac{\\sqrt{\\sum_{b}N(s,b)}}{N(s,a)+1}\n$$  \n\nwhere $Q_{t}(s,a)$ is the estimated expected future reward for $i$ from playing action $a$ in state $s$ at iteration $t$ , the visit c $N(s,a)$ is the number of times $a$ has been explored from $s$ ,$\\pi^{0}(a\\mid s)$ is an \u201canchor\u201d policy, and $\\beta_{\\mathrm{puct}}$ is a tunable parameter trading off exploration versus exploitation. After expanding $\\beta_{\\mathrm{runtime}}$ nodes of this tree, an agent\u2019s final action is sampled from a distribution:  \n\n$$\n\\pi(a\\mid s;\\beta_{\\mathrm{runtime}})=\\beta_{\\mathrm{puct}}{\\frac{\\sqrt{\\beta_{\\mathrm{runtime}}}}{N(s,a)+1}}{\\frac{\\pi^{0}(a|s)}{\\gamma-Q_{\\beta_{\\mathrm{runtime}}}(a\\mid s)}}\n$$  \n\nwhere $\\gamma$ is chosen such that $\\pi$ forms a proper probability distribution.  \n\nProposition 3. Monte-Carlo tree search (MCTS) is an anytime inference algorithm. (Let each inference state $f_{\\beta}$ be the tree of nodes and visitation counts after $\\beta$ evaluations. This tree is refined by evaluating Eq. (15) once.)  \n\nWith $\\pi(a\\mid s;\\beta_{\\mathrm{runtime}})$ as defined above, we may instantiate an L-IBM for MCTS:  \n\n$$\n\\pi^{\\mathrm{runtime}}(t|u;\\eta,\\theta)=\\sum_{\\beta_{\\mathrm{runtime}}}p_{\\mathrm{budget}}(\\beta_{\\mathrm{runtime}}\\mid\\eta_{i})\\cdot\\pi(a;s,\\beta_{\\mathrm{runtime}})\n$$\n\n# 6.2 DATA\nWe use similar data to previous models of human chess play by McIlroy-Young et al. (2020); Jacob et al. (2022); McIlroy-Young et al. (2022). Our experiments use two different datasets. First,  \n\n  \nFigure 6: Inferred distributions over $\\beta$ in Chess using MCTS. X-axis indicates the player Elo rating, opponent elo rating buckets and time control: Ultra Bullet (U), Bullet (B), Blitz (BZ), Rapid (R) and Classical (C). The top row depicts the distributions for $\\beta_{\\mathrm{puct}}$ and the bottom row depicts the distributions for $\\beta_{\\mathrm{runtime}}$ . When the player\u2019s or opponent\u2019s strength increases, $\\beta_{\\mathrm{runtime}}$ infers greater runtime. This pattern also holds true as the time control extends. $\\beta_{\\mathrm{puct}}$ displays a similar pattern, as the agents or opponents get stronger, or as the time control extends, $\\beta_{\\mathrm{puct}}$ suggests lower values, placing greater reliance on the search Q-values.  \n\na dataset $D_{\\mathrm{large}}$ containing roughly 6 million moves; second, a dataset $D_{\\mathrm{small}}$ containing roughly 75,000 moves. $D_{\\mathrm{small}}$ includes metadata describing players\u2019 Elo ratings (a measure of strength) and game formats (the amount of time players had to select moves). See Appendix B for details.\n\n# 6.3 MODELING DETAILS\nWe train the base initial policy $\\pi_{0}$ and a value model $\\tilde{v}_{0}$ as two different output heads of a deep neural network using imitation learning on the large dataset split $D_{\\mathrm{large}}$ . Our architecture is a 4-block residual network similar to those used in prior work (McIlroy-Young et al., 2020; Jacob et al., 2022; McIlroy-Young et al., 2022). Unlike previous sections, we do not learn the value functions jointly $p_{\\mathrm{budget}}$ , we first learn a single value function from $D_{\\mathrm{large}}$ , then fit $p_{\\mathrm{budget}}(\\beta_{\\mathrm{puct}}\\mid\\dot{\\eta}_{i})$ and $p_{\\mathrm{budget}}(\\beta_{\\mathrm{runtime}}^{\\mathsf{\\bar{\\alpha}}}\\mid\\eta_{i})$ Elo (a proxy for player skill), and opponent Elo and time control (both proxies for task difficulty). |. We investigate three ways of stratifying players into sub-populations: player As in Section 5, we estimate a separate $\\eta_{i}$ for each group within each stratified dataset."}, {"ref_id": "454967397448679476", "chunk_id": "1", "score": 0.3125, "text": "# 2.2 Acceleration of MCTS\nMCTS-based methods have proved their strong capability of solving complex games or tasks. However, the high computational cost of MCTS hinders its application to some real-time and more general scenarios. Therefore, numerous works are devoted to accelerating MCTS. For example, to make the selection stage more effective, some heuristic pruning methods [ 14 ,33 ,29 ,1 ,2 ] aim to reduce the width and depth of the search tree with some heuristic functions. Furthermore, for more efficient evaluations, Lorentz [ 22 ] proposed early playout termination of MCTS (MCTS-EPT) to stop the random playouts early and use an evaluation function to assess win or loss. Moreover, Hsueh et al. [18 ] applied MCTS-EPT to the Chinese dark chess and proved its effectiveness. Afterward, similar ideas have been applied in the evaluation stage of AlphaGoZero [ 32 ] and later MCTS-based methods [31 ,27 ,34 ]. They evaluate the $Q$ -values through a learnable evaluation network instead of running playouts to the end. Grill et al. [15 ] propose a novel regularized policy optimization method based on AlphaZero to decrease the search budget of MCTS, which is from the optimization perspective. Danihelka et al. [10 ] propose a policy improvement algorithm based on sampling actions without replacement, named Gumbel trick to achieve better performance when planning with few simulations. However, these methods mentioned above focus on the specific stage of the search iteration or reduce the total budget through pruning and optimization methods, which are orthogonal to us. And few works targets at the search loop. Lan et al. [21 ] propose DS-MCTS, which defines the uncertainty of MCTS and approximates it by extra DNNs with specific features for board games in training. During the evaluation, DS-MCTS will check periodically and stop the search if the state is certain.\n\n# 3 Background\nThe AlphaGo series of work [ 30 ,32 ,31 ,27 ] are all MCTS-based reinforcement learning algorithms. Those algorithms assume the environment transition dynamics are known or learn the environment dynamics. Based on the dynamics, they use the Monte-Carlo tree search (MCTS) as the policy improvement operator. I.e., taking in the current policy, MCTS returns a better policy with the search algorithm. The systematic search allows the MCTS-based RL algorithm to quickly improve the policy and perform much better in the setting where heavy reasoning is required.\n\n# 3.1 MCTS\nThis part briefly introduces the MCTS method implemented in reinforcement learning applications. As mentioned in the related works, modern MCTS-based RL algorithms include four stages in the search loop, namely selection, expansion, evaluation, and backpropagation.  \n\nMCTS takes in the current states and generates a policy after the search loop of $N$ iterations. Here $N$ is a constant number of iterations set by the designer, regarded as the total budget. In the selection stage of each iteration, an action will be selected by maximizing over UCB. Specifically, AlphaZero [31 ] and MuZero [ 27 ] are developed based on a variant of UCB, P-UCT [ 25 ] and have achieved great success on board games and Atari games. The formula of P-UCT is the Eq (1):  \n\n$$\na^{k}=\\arg\\operatorname*{max}_{a\\in\\mathcal{A}}Q(s,a)+P(s,a)\\frac{\\sqrt{\\sum_{b\\in\\mathcal{A}}N(s,b)}}{1+N(s,a)}(c_{1}+\\log((\\sum_{b\\in\\mathcal{A}}N(s,b)+c_{2}+1)/c_{2})),\n$$  \n\nwhere $k$ is the index of iteration, $\\boldsymbol{\\mathcal{A}}$ is the acti $Q(s,a)$ is the estimated Q-value, $P(s,a)$ is the policy prior obtained from neural networks, $N(s,a)$ is the visitations to select the action a from the state $s$ and $c_{1},c_{2}$ are hyper-parameters. The output of MCTS is the visitation of each action of the root node. After $N$ search iterations, the final policy $\\pi(s)$ is defined as the normalized root visitation distribution simplification, we use $\\pi_{N}(s)$ $\\pi_{k}$ in place of , where $\\begin{array}{r}{\\pi_{k}(s,a)=N(s,a)/\\sum_{b\\in\\mathcal{A}}N(s,b)=N(s,a)/k,a\\in\\mathcal{A}}\\end{array}$ $\\pi_{k}(s)$ sometimes. And the detailed procedure of MCTS is \u2208A . For introduced in Appendix. In our method, we propose to approximate the final policy $\\pi_{N}(s)$ with $\\hat{\\pi}_{k}(s)$ ,which we name as a virtual expanded policy, through a new expansion method and a termination rule. In this way, the number of iterations in MCTS can be reduced from $N$ to $k$ .\n\n# 3.2 Computation Requirement\nMost of the computations in MCTS-based RL are in the MCTS procedure. Each action taken by MCTS requires $N$ times neural network evaluations, where $N$ is a constant number of iterations in the search loop. Traditional RL algorithms, such as PPO [ 28 ] or DQN [ 23 ], only need a single neural network evaluation per action. Thus, MCTS-based RL is roughly $N$ times computationally more expensive than traditional RL algorithms. In practice, training a single Atari game needs 12 hours of computation time on 40 TPUs [ 27 ]. The computation need is roughly two orders of magnitude more than traditional RL algorithms [28], although the final performance of MuZero is much better.\n\n# 4 Method\nWe aim to spend more search time on harder states and less on easier states. Intuitively, human knows when to make a quick decision or a slow decision under different circumstances. Unfortunately, this situation-aware behavior is absent in current MCTS algorithms. Therefore, we propose an MCTS variant that terminates the search iteration adaptively. It consists of two components: a novel expansion method named virtual expansion to estimate the final visitation based on the current partial tree; a termination rule that decides when to terminate based on the hardness of the current scenario. And we will display the adaptive mechanism through visualizations in Section 5.5."}], "task_step_question_answer": "8a1e6e8e-b71e-46d2-aa53-8f3d04b1d842:\u300c\u5728\u6e38\u620fAI\u9886\u57df\uff0cMCTS\uff08\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff09\u4e0ePRM\uff08\u504f\u597d\u5efa\u6a21\uff09\u7ed3\u5408\u7528\u4e8e\u667a\u80fd\u51b3\u7b56\u65f6\uff0c\u5e73\u8861\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u504f\u597d\u5efa\u6a21\u7cbe\u5ea6\u662f\u4e00\u4e2a\u5173\u952e\u95ee\u9898\u3002\u4ee5\u4e0b\u662f\u4e00\u4e9b\u53ef\u80fd\u7684\u7b56\u7565\uff1a\n\n1. **\u81ea\u9002\u5e94\u641c\u7d22\u65f6\u95f4\u5206\u914d**\uff1a\u5982V-MCTS\uff08\u865a\u62dfMCTS\uff09\u6240\u63d0\u51fa\u7684\uff0c\u53ef\u4ee5\u6839\u636e\u72b6\u6001\u7684\u590d\u6742\u6027\u81ea\u9002\u5e94\u5730\u5206\u914d\u641c\u7d22\u65f6\u95f4\u3002\u5bf9\u4e8e\u8f83\u7b80\u5355\u7684\u72b6\u6001\uff0c\u51cf\u5c11\u641c\u7d22\u65f6\u95f4\uff1b\u5bf9\u4e8e\u8f83\u590d\u6742\u7684\u72b6\u6001\uff0c\u589e\u52a0\u641c\u7d22\u65f6\u95f4\u3002\u8fd9\u79cd\u65b9\u6cd5\u53ef\u4ee5\u5728\u4fdd\u8bc1\u51b3\u7b56\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u663e\u8457\u51cf\u5c11\u603b\u4f53\u8ba1\u7b97\u65f6\u95f4\u3002\n\n2. **\u65e9\u671f\u7ec8\u6b62\u548c\u526a\u679d**\uff1a\u5728MCTS\u7684\u641c\u7d22\u8fc7\u7a0b\u4e2d\uff0c\u53ef\u4ee5\u5f15\u5165\u65e9\u671f\u7ec8\u6b62\u548c\u526a\u679d\u7b56\u7565\u3002\u4f8b\u5982\uff0c\u5f53\u67d0\u4e2a\u8282\u70b9\u7684\u8bc4\u4f30\u503c\u5df2\u7ecf\u8db3\u591f\u786e\u5b9a\u65f6\uff0c\u53ef\u4ee5\u63d0\u524d\u7ec8\u6b62\u5bf9\u8be5\u8282\u70b9\u7684\u8fdb\u4e00\u6b65\u641c\u7d22\u3002\u8fd9\u53ef\u4ee5\u51cf\u5c11\u4e0d\u5fc5\u8981\u7684\u8ba1\u7b97\uff0c\u540c\u65f6\u4fdd\u6301\u51b3\u7b56\u7684\u51c6\u786e\u6027\u3002\n\n3. **\u4f18\u5316\u504f\u597d\u5efa\u6a21**\uff1a\u5728PRM\u4e2d\uff0c\u53ef\u4ee5\u901a\u8fc7\u66f4\u9ad8\u6548\u7684\u7b97\u6cd5\u548c\u6570\u636e\u7ed3\u6784\u6765\u4f18\u5316\u504f\u597d\u5efa\u6a21\u7684\u7cbe\u5ea6\u548c\u901f\u5ea6\u3002\u4f8b\u5982\uff0c\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u6765\u5feb\u901f\u8bc4\u4f30\u8282\u70b9\u7684\u4ef7\u503c\uff0c\u800c\u4e0d\u662f\u4f9d\u8d56\u4f20\u7edf\u7684\u968f\u673a\u6a21\u62df\u3002\n\n4. **\u5e76\u884c\u8ba1\u7b97**\uff1a\u5229\u7528\u73b0\u4ee3\u8ba1\u7b97\u786c\u4ef6\u7684\u5e76\u884c\u80fd\u529b\uff0c\u53ef\u4ee5\u540c\u65f6\u8fdb\u884c\u591a\u4e2aMCTS\u641c\u7d22\u548cPRM\u8bc4\u4f30\u3002\u8fd9\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u7b97\u6cd5\u7684\u5b9e\u65f6\u6027\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u590d\u6742\u6e38\u620f\u72b6\u6001\u65f6\u3002\n\n5. **\u6df7\u5408\u7b56\u7565**\uff1a\u7ed3\u5408MCTS\u548cPRM\u7684\u4f18\u52bf\uff0c\u8bbe\u8ba1\u6df7\u5408\u7b56\u7565\u3002\u4f8b\u5982\uff0c\u5728\u521d\u59cb\u9636\u6bb5\u4f7f\u7528PRM\u5feb\u901f\u7b5b\u9009\u51fa\u6709\u6f5c\u529b\u7684\u884c\u52a8\uff0c\u7136\u540e\u5728\u540e\u7eed\u9636\u6bb5\u4f7f\u7528MCTS\u8fdb\u884c\u6df1\u5165\u641c\u7d22\u3002\u8fd9\u79cd\u65b9\u6cd5\u53ef\u4ee5\u5728\u4fdd\u8bc1\u51b3\u7b56\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u51cf\u5c11\u8ba1\u7b97\u590d\u6742\u5ea6\u3002\n\n\u901a\u8fc7\u8fd9\u4e9b\u7b56\u7565\uff0c\u53ef\u4ee5\u5728\u4fdd\u8bc1\u51b3\u7b56\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u6709\u6548\u63d0\u5347\u7b97\u6cd5\u7684\u5b9e\u65f6\u6027\uff0c\u4f7f\u5176\u66f4\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u573a\u666f\u3002\u300d", "ref_task_step_id": ""}, "__type__": "task_step_node"}, "0f8fcb2e-9570-4314-99ab-0d3606636375": {"__data__": {"id_": "0f8fcb2e-9570-4314-99ab-0d3606636375", "metadata": {}, "relationships": {}, "hash": "", "start_task_context": "MCTS\u5728PRM\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u7684\u5e94\u7528\u63a2\u7d22\u7efc\u8ff0", "aemo_representation_context": "### Step 1: \u5206\u6790\u8fd1\u51e0\u5e74\u7814\u7a76\u9886\u57df\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba\n\n**\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09**\uff1a\n- **\u57fa\u672c\u539f\u7406**\uff1aMCTS\u662f\u4e00\u79cd\u7528\u4e8e\u51b3\u7b56\u8fc7\u7a0b\u7684\u542f\u53d1\u5f0f\u641c\u7d22\u7b97\u6cd5\uff0c\u5e7f\u6cdb\u5e94\u7528\u4e8e\u6e38\u620fAI\u3001\u8def\u5f84\u89c4\u5212\u7b49\u9886\u57df\u3002\u5176\u6838\u5fc3\u601d\u60f3\u662f\u901a\u8fc7\u6a21\u62df\u591a\u6b21\u968f\u673a\u6e38\u620f\u6765\u8bc4\u4f30\u4e0d\u540c\u51b3\u7b56\u7684\u4f18\u52a3\u3002\n- **\u6280\u672f\u6846\u67b6**\uff1a\u4e3b\u8981\u5305\u62ec\u9009\u62e9\uff08Selection\uff09\u3001\u6269\u5c55\uff08Expansion\uff09\u3001\u6a21\u62df\uff08Simulation\uff09\u548c\u56de\u6eaf\uff08Backpropagation\uff09\u56db\u4e2a\u6b65\u9aa4\u3002\n\n**\u504f\u597d\u7b56\u7565\u6a21\u578b\uff08PRM\uff09**\uff1a\n- **\u57fa\u672c\u539f\u7406**\uff1aPRM\u65e8\u5728\u901a\u8fc7\u5b66\u4e60\u7528\u6237\u7684\u504f\u597d\u6765\u4f18\u5316\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5e38\u7528\u4e8e\u63a8\u8350\u7cfb\u7edf\u3001\u4e2a\u6027\u5316\u670d\u52a1\u7b49\u3002\n- **\u6280\u672f\u6846\u67b6**\uff1a\u901a\u5e38\u5305\u62ec\u504f\u597d\u5efa\u6a21\u3001\u7b56\u7565\u4f18\u5316\u548c\u6a21\u578b\u5fae\u8c03\u7b49\u73af\u8282\u3002\n\n**\u65b9\u6cd5\u8bba**\uff1a\n- **\u96c6\u6210\u5b66\u4e60**\uff1a\u5c06MCTS\u4e0ePRM\u7ed3\u5408\uff0c\u5229\u7528MCTS\u7684\u641c\u7d22\u80fd\u529b\u4f18\u5316PRM\u7684\u51b3\u7b56\u8fc7\u7a0b\u3002\n- **\u5f3a\u5316\u5b66\u4e60**\uff1a\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u673a\u5236\u4e0d\u65ad\u8c03\u6574PRM\u7684\u53c2\u6570\uff0c\u4ee5\u9002\u5e94\u52a8\u6001\u73af\u5883\u3002\n\n### Step 2: \u7814\u7a76\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u4e3b\u8981\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0e\u53d8\u4f53\n\n**\u5e94\u7528\u573a\u666f**\uff1a\n- **\u6e38\u620fAI**\uff1aMCTS\u4e0ePRM\u7ed3\u5408\u7528\u4e8e\u6e38\u620f\u4e2d\u7684\u667a\u80fd\u51b3\u7b56\uff0c\u5982AlphaGo\u3002\n- **\u8def\u5f84\u89c4\u5212**\uff1a\u5728\u673a\u5668\u4eba\u5bfc\u822a\u548c\u81ea\u52a8\u9a7e\u9a76\u4e2d\uff0c\u5229\u7528MCTS\u4f18\u5316\u8def\u5f84\u9009\u62e9\u3002\n- **\u63a8\u8350\u7cfb\u7edf**\uff1a\u901a\u8fc7PRM\u5b66\u4e60\u7528\u6237\u504f\u597d\uff0c\u7ed3\u5408MCTS\u8fdb\u884c\u66f4\u7cbe\u51c6\u7684\u63a8\u8350\u3002\n\n**\u53d8\u4f53**\uff1a\n- **\u6df1\u5ea6MCTS**\uff1a\u7ed3\u5408\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff0c\u63d0\u5347MCTS\u7684\u51b3\u7b56\u80fd\u529b\u3002\n- **\u591a\u6a21\u6001PRM**\uff1a\u878d\u5408\u591a\u79cd\u6570\u636e\u6e90\uff08\u5982\u6587\u672c\u3001\u56fe\u50cf\uff09\uff0c\u589e\u5f3aPRM\u7684\u6cdb\u5316\u80fd\u529b\u3002\n\n### Step 3: \u8bc4\u4f30\u5b66\u672f\u754c\u7684\u6280\u672f\u8fdb\u6b65\u4e0e\u5c40\u9650\u6027\n\n**\u6280\u672f\u8fdb\u6b65**\uff1a\n- **\u6027\u80fd\u63d0\u5347**\uff1aMCTS\u4e0ePRM\u7ed3\u5408\u663e\u8457\u63d0\u5347\u4e86\u51b3\u7b56\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002\n- **\u6cdb\u5316\u80fd\u529b**\uff1a\u591a\u6a21\u6001PRM\u5728\u5904\u7406\u591a\u6837\u5316\u6570\u636e\u65f6\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002\n\n**\u5c40\u9650\u6027**\uff1a\n- **\u8ba1\u7b97\u590d\u6742\u5ea6**\uff1aMCTS\u5728\u5927\u89c4\u6a21\u95ee\u9898\u4e2d\u8ba1\u7b97\u91cf\u5927\uff0c\u6548\u7387\u6709\u5f85\u63d0\u5347\u3002\n- **\u6570\u636e\u4f9d\u8d56**\uff1aPRM\u7684\u6027\u80fd\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u9ad8\u8d28\u91cf\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u6570\u636e\u504f\u5dee\u95ee\u9898\u4f9d\u7136\u5b58\u5728\u3002\n\n### Step 4: \u63a2\u8ba8\u8ba1\u7b97\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\n\n**\u9002\u7528\u6027**\uff1a\n- **\u591a\u9886\u57df\u5e94\u7528**\uff1aMCTS+PRM\u6846\u67b6\u5728\u6e38\u620f\u3001\u8def\u5f84\u89c4\u5212\u3001\u63a8\u8350\u7cfb\u7edf\u7b49\u591a\u4e2a\u9886\u57df\u8868\u73b0\u51fa\u826f\u597d\u7684\u9002\u7528\u6027\u3002\n- **\u591a\u6a21\u6001\u6570\u636e**\uff1a\u591a\u6a21\u6001PRM\u80fd\u591f\u5904\u7406\u591a\u79cd\u7c7b\u578b\u7684\u6570\u636e\uff0c\u9002\u7528\u4e8e\u590d\u6742\u5e94\u7528\u573a\u666f\u3002\n\n**\u6cdb\u5316\u80fd\u529b**\uff1a\n- **\u8de8\u9886\u57df\u6cdb\u5316**\uff1a\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\uff0cMCTS+PRM\u6846\u67b6\u5728\u4e0d\u540c\u9886\u57df\u95f4\u5c55\u73b0\u51fa\u8f83\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002\n- **\u52a8\u6001\u73af\u5883\u9002\u5e94**\uff1a\u5f3a\u5316\u5b66\u4e60\u673a\u5236\u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u9002\u5e94\u52a8\u6001\u53d8\u5316\u7684\u73af\u5883\u3002\n\n### Step 5: \u5206\u6790\u6700\u65b0\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027\n\n**\u7a33\u5b9a\u6027**\uff1a\n- **\u7b97\u6cd5\u4f18\u5316**\uff1a\u901a\u8fc7\u6539\u8fdbMCTS\u7684\u9009\u62e9\u7b56\u7565\u548cPRM\u7684\u53c2\u6570\u8c03\u6574\u673a\u5236\uff0c\u63d0\u5347\u4e86\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u3002\n- **\u9c81\u68d2\u6027\u589e\u5f3a**\uff1a\u5f15\u5165\u5bf9\u6297\u8bad\u7ec3\u548c\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\uff0c\u63d0\u9ad8\u6a21\u578b\u5728\u590d\u6742\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u6027\u3002\n\n**\u5bb9\u9519\u6027**\uff1a\n- **\u9519\u8bef\u5904\u7406**\uff1a\u5728\u6a21\u62df\u548c\u56de\u6eaf\u9636\u6bb5\u5f15\u5165\u5bb9\u9519\u673a\u5236\uff0c\u51cf\u5c11\u9519\u8bef\u51b3\u7b56\u7684\u5f71\u54cd\u3002\n- **\u5927\u89c4\u6a21\u6570\u636e\u9002\u5e94\u6027**\uff1a\u4f18\u5316\u7b97\u6cd5\u7ed3\u6784\uff0c\u63d0\u5347\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\u7684\u5904\u7406\u80fd\u529b\u3002\n\n### Step 6: \u8bc4\u4f30\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218\n\n**\u672a\u6765\u7814\u7a76\u65b9\u5411**\uff1a\n- **\u9ad8\u6548\u8ba1\u7b97**\uff1a\u7814\u7a76\u66f4\u9ad8\u6548\u7684MCTS\u7b97\u6cd5\uff0c\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002\n- **\u6570\u636e\u589e\u5f3a**\uff1a\u63a2\u7d22\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u51cf\u5c11PRM\u5bf9\u9ad8\u8d28\u91cf\u6570\u636e\u7684\u4f9d\u8d56\u3002\n\n**\u6311\u6218**\uff1a\n- **\u8de8\u6a21\u6001\u878d\u5408**\uff1a\u5982\u4f55\u6709\u6548\u878d\u5408\u591a\u79cd\u6a21\u6001\u6570\u636e\uff0c\u63d0\u5347\u6a21\u578b\u7684\u7efc\u5408\u6027\u80fd\u3002\n- **\u52a8\u6001\u73af\u5883\u9002\u5e94**\uff1a\u5728\u52a8\u6001\u73af\u5883\u4e0b\uff0c\u5982\u4f55\u4fdd\u6301\u6a21\u578b\u7684\u7a33\u5b9a\u6027\u548c\u9ad8\u6548\u6027\u3002\n\n**\u63a8\u52a8\u540e\u7eed\u7814\u7a76**\uff1a\n- **\u65b0\u95ee\u9898\u63d0\u51fa**\uff1a\u5982\u591a\u6a21\u6001\u6570\u636e\u4e0b\u7684\u504f\u597d\u5efa\u6a21\u3001\u52a8\u6001\u73af\u5883\u4e0b\u7684\u51b3\u7b56\u4f18\u5316\u7b49\u3002\n- **\u7814\u7a76\u5207\u5165\u70b9**\uff1a\u4ece\u7b97\u6cd5\u4f18\u5316\u3001\u6570\u636e\u589e\u5f3a\u3001\u8de8\u6a21\u6001\u878d\u5408\u7b49\u591a\u4e2a\u89d2\u5ea6\u5207\u5165\uff0c\u63a8\u52a8\u6280\u672f\u8fdb\u6b65\u3002\n\n### \u603b\u7ed3\n\n\u8fd1\u5e74\u6765\uff0cMCTS\u5728PRM\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u7684\u5e94\u7528\u63a2\u7d22\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4e3b\u8981\u4f53\u73b0\u5728\u6280\u672f\u6846\u67b6\u7684\u5b8c\u5584\u3001\u5e94\u7528\u573a\u666f\u7684\u62d3\u5c55\u3001\u6027\u80fd\u7684\u63d0\u5347\u4ee5\u53ca\u7a33\u5b9a\u6027\u548c\u5bb9\u9519\u6027\u7684\u589e\u5f3a\u3002\u7136\u800c\uff0c\u4ecd\u5b58\u5728\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u3001\u6570\u636e\u4f9d\u8d56\u6027\u5f3a\u7b49\u5c40\u9650\u6027\u3002\u672a\u6765\u7684\u7814\u7a76\u5e94\u5173\u6ce8\u9ad8\u6548\u8ba1\u7b97\u3001\u6570\u636e\u589e\u5f3a\u548c\u8de8\u6a21\u6001\u878d\u5408\u7b49\u65b9\u5411\uff0c\u4ee5\u5e94\u5bf9\u52a8\u6001\u73af\u5883\u4e0b\u7684\u51b3\u7b56\u4f18\u5316\u7b49\u6311\u6218\uff0c\u63a8\u52a8\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002", "task_step_name": "\u8def\u5f84\u89c4\u5212", "task_step_description": "\u5728\u673a\u5668\u4eba\u5bfc\u822a\u548c\u81ea\u52a8\u9a7e\u9a76\u4e2d\uff0c\u5229\u7528MCTS\u4f18\u5316\u8def\u5f84\u9009\u62e9\u3002", "task_step_level": "1>2", "task_step_question": "### \u95ee\u9898\u63d0\u51fa\n\n\u5728\u673a\u5668\u4eba\u5bfc\u822a\u548c\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\uff0c\u8def\u5f84\u89c4\u5212\u662f\u4e00\u4e2a\u5173\u952e\u4efb\u52a1\u3002\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09\u4f5c\u4e3a\u4e00\u79cd\u542f\u53d1\u5f0f\u641c\u7d22\u7b97\u6cd5\uff0c\u5df2\u88ab\u8bc1\u660e\u5728\u4f18\u5316\u8def\u5f84\u9009\u62e9\u65b9\u9762\u5177\u6709\u6f5c\u529b\u3002\u7136\u800c\uff0cMCTS\u5728\u5927\u89c4\u6a21\u95ee\u9898\u4e2d\u8ba1\u7b97\u590d\u6742\u5ea6\u8f83\u9ad8\uff0c\u8fd9\u53ef\u80fd\u9650\u5236\u4e86\u5176\u5728\u5b9e\u65f6\u5e94\u7528\u4e2d\u7684\u6548\u7387\u3002\n\n**\u95ee\u9898**\uff1a\u5728\u673a\u5668\u4eba\u5bfc\u822a\u548c\u81ea\u52a8\u9a7e\u9a76\u7684\u8def\u5f84\u89c4\u5212\u4efb\u52a1\u4e2d\uff0c\u5982\u4f55\u6709\u6548\u964d\u4f4eMCTS\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u5176\u8def\u5f84\u9009\u62e9\u7684\u4f18\u5316\u6027\u80fd\uff1f\u5177\u4f53\u6765\u8bf4\uff0c\u6709\u54ea\u4e9b\u7b56\u7565\u6216\u6280\u672f\u53ef\u4ee5\u5728MCTS\u7684\u9009\u62e9\u3001\u6269\u5c55\u3001\u6a21\u62df\u548c\u56de\u6eaf\u9636\u6bb5\u8fdb\u884c\u4f18\u5316\uff0c\u4ee5\u63d0\u9ad8\u5176\u5728\u5b9e\u65f6\u5e94\u7528\u4e2d\u7684\u6548\u7387\u548c\u5b9e\u7528\u6027\uff1f", "task_step_question_context": [{"ref_id": "454845766417543398", "chunk_id": "4", "score": 0.5390625, "text": "# Monte Carlo Tree Search in the Presence of Transition Uncertainty\nFarnaz Kohankhaki , Kiarash Aghakasiri , Hongming Zhang 1 , Ting-Han Wei 1 , Chao Gao 2 ,Martin M\u00a8uller 1  \n\n1 University of Alberta, 2 Edmonton Research Center, Huawei Canada {kohankha, aghakasi, hongmin2, tinghan, mmueller }@ualberta.ca, cgao3 $@$ outlook.com\n\n# Abstract\nMonte Carlo Tree Search (MCTS) is an immensely popular search-based framework used for decision making. It is traditionally applied to domains where a perfect simulation model of the environment is available. We study and improve MCTS in the context where the environment model is given but imperfect. We show that the discrepancy between the model and the actual environment can lead to significant performance degradation with standard MCTS. We therefore develop Uncertainty Adapted MCTS (UA-MCTS), a more robust algorithm within the MCTS framework. We estimate the transition uncertainty in the given model, and direct the search towards more certain transitions in the state space. We modify all four MCTS phases to improve the search behavior by considering these estimates. We prove, in the corrupted bandit case, that adding uncertainty information to adapt UCB leads to tighter regret bound than standard UCB. Empirically, we evaluate UA-MCTS and its individual components on the deterministic domains from the MinAtar test suite. Our results demonstrate that UA-MCTS strongly improves MCTS in the presence of model transition errors.\n\n# 1 Introduction\nThe Monte Carlo Tree Search (MCTS) framework (Browne et al. 2012) approaches sequential decision-making problems by selective lookahead search. It manages the balance of exploration and exploitation with techniques such as UCT (Kocsis, Szepesv\u00b4ari, and Willemson 2006). Often combined with machine learning, it has been enormously successful in both games (Silver et al. 2016; Banerjee 2020; Arneson, Hayward, and Henderson 2010; Saffidine 2008; Nijssen and Winands 2010) and non-game applications (Lu et al. 2016; Mansley, Weinstein, and Littman 2011; Sabharwal, Samulowitz, and Reddy 2012; Cazenave 2010). In these applications, a perfect simulation model allows for efficient lookahead search. However, in many practical applications, only an imperfect model is available to the agent. Yet lookahead using such a model can still be useful. We improve MCTS for this setting.  \n\nOne research area that studies imperfect models of the environment is model-based reinforcement learning (MBRL).  \n\nHere, an agent builds its own model through limited real world interactions. The resulting learned model, when used for lookahead search, can either be for planning or for producing more accurate training targets (Silver, Sutton, and M\u00a8uller 2008). It can also be used to generate simulated training samples for better sample efficiency (Sutton and Barto 2018). The learned model may be inaccurate for many reasons, including stochasticity of the environment, insufficient training, insufficient capacity, non stationary environments, etc. Consequently, there is a rich body of research on uncertainty in MBRL (Abbas et al. 2020; Xiao et al. 2019; Buckman et al. 2018).  \n\nWhile previous approaches to using search with imperfect models exist (Vemula et al. 2020; Vemula, Bagnell, and Likhachev 2021), to the best of our knowledge, there is no prior work that directly adapts MCTS to deal with model uncertainty. In our work, we define transition uncertainty as a measure of difference between the state transitions in the perfect model and in the model that is available to the agent. We use a neural network to estimate this uncertainty.  \n\nOur Uncertainty Adapted MCTS (UA-MCTS) approach implements the main components of the MCTS framework in a way that guides the search away from states with high uncertainty. We compare the performance of our proposed methods with MCTS baselines in three deterministic MinAtar environments (Young and Tian 2019). In each case the search agent \u201cbelieves\u201d it is playing the real game. However, the rules of the game itself have changed, and the agent only learns about this change slowly when it acts in the real environment. The results show that UA-MCTS is able to outperform the baseline MCTS with an imperfect model.  \n\nOur approach is inspired by the work of (Vemula et al. 2020) where a robotic arm has to solve tasks despite being handicapped, e.g. by a broken motor or by an unmodeled weight restriction. To show how an agent should adapt UCB-based exploration strategy in the presence of environment uncertainties, we first consider a case of stochastic bandits (Lattimore and Szepesv\u00b4ari 2020) along with corrupted feedback. We prove that incorporating uncertainty information can enhance the performance of UCB, yielding a regret bound that is more constrained compared to the standard UCB. We also prove that in the general case of tree search, with similar modification of UCT, our UA-MCTS approach maintains its completeness property, ensuring that as the number of iterations goes to infinity, all nodes will be consistently explored. To further motivate our approach, we compare the scenarios of learning to improve the transition function, using MCTS, directly against the easier task of just learning a transition uncertainty function with UA-MCTS. In both cases, learning occurs online; the former is used with MCTS while the latter is used with UA-MCTS. Our results show that learning the transition function is much harder than learning transition uncertainty, which justifies the use of UA-MCTS in such settings."}, {"ref_id": "454845659346651798", "chunk_id": "3", "score": 0.46484375, "text": "# Large-Scale Multi-Robot Coverage Path Planning via Local Search \\*\nJingtao Tang, Hang Ma  \n\nSimon Fraser University {jingtao tang, hangma }@sfu.ca\n\n# Abstract\nWe study graph-based Multi-Robot Coverage Path Planning (MCPP) that aims to compute coverage paths for multiple robots to cover all vertices of a given 2D grid terrain graph $G$ . Existing graph-based MCPP algorithms first compute a tree cover on $G$ \u2014a forest of multiple trees that cover all vertices\u2014and then employ the Spanning Tree Coverage (STC) paradigm to generate coverage paths on the decomposed graph $D$ of the terrain graph $G$ by circumnavigating the edges of the computed trees, aiming to optimize the makespan (i.e., the maximum coverage path cost among all robots). In this paper, we take a different approach by exploring how to systematically search for good coverage paths directly on $D$ . We introduce a new algorithmic framework, called LS-MCPP, which leverages a local search to operate directly on $D$ . We propose a novel standalone paradigm, Extended-STC (ESTC), that extends STC to achieve complete coverage for MCPP on any decomposed graphs, even those resulting from incomplete terrain graphs. Furthermore, we demonstrate how to integrate ESTC with three novel types of neighborhood operators into our framework to effectively guide its search process. Our extensive experiments demonstrate the effectiveness of LS-MCPP, consistently improving the initial solution returned by two state-of-the-art baseline algorithms that compute suboptimal tree covers on $G$ , with a notable reduction in makespan by up to $35.7\\%$ and $30.3\\%$ , respectively. Moreover, LS-MCPP consistently matches or surpasses the results of optimal tree cover computation, achieving these outcomes with orders of magnitude faster runtime, thereby showcasing its significant benefits for large-scale real-world coverage tasks."}, {"ref_id": "454846884311042388", "chunk_id": "1", "score": 0.4453125, "text": "# 1 Introduction\nCoverage path planning (CPP) is a fundamental problem (Galceran and Carreras 2013) in robotics, which aims to find a path for a robot to completely cover a terrain of interest, such as indoor floors (Bormann et al. 2018) and outdoor fields (Torres et al. 2016). Multi-Robot Coverage Path Planning (MCPP) is an extension of CPP tailored for multi-robot systems, aiming to coordinate the paths of multiple robots to completely cover the given terrain. With improved task efficiency and system robustness, MCPP has facilitated diverse real-world applications, including environmental monitoring (Collins et al. 2021) and search and rescue (Song et al. 2022). A fundamental challenge of MCPP lies in generating cost-balancing coverage paths to optimize task efficiency, commonly quantified by the makespan , which is the maximum path cost of all robots. This challenge is further compounded when dealing with large-scale applications where the number of robots and the size of the terrain increase.  \n\n  \nFigure 1: Graph-based CPP and MCPP: Gray squares, black circles, and black stars represent terrain graph vertices, decomposed graph vertices, and initial vertices of robots, respectively; Solid lines and dashed lines represent coverage paths and spanning edges, respectively. (a) The terrain to be covered where all terrain graph edges have uniform weights of 1 . (b) The single-robot coverage path generated by STC. (c)(d) Suboptimal and optimal 2 -robot coverage paths with makespans of 2 and 1 .5 , respectively.  \n\nIn this paper, we follow existing graph-based MCPP algorithms (Zheng et al. 2010; Li et al. 2023) that represent the terrain to be covered as a 4-connected 2D grid graph $G$ , where each edge connects horizontally or vertically adjacent vertices. The robots are required to start at and return to their respective initial vertices, as in the cover and return setting (Zheng and Koenig 2007). The foundation of these graph-based MCPP algorithms lies in the Spanning Tree Coverage (STC) paradigm (Gabriely and Rimon 2001, 2002), initially developed for (single-robot) CPP. STC operates on the terrain graph $G$ but finds a coverage path with minimal makespan on the decomposed graph $D$ derived from $G$ . The decomposed graph $D$ is also a 4-connected 2D grid graph, resulting from decomposing each vertex of $G$ into four decomposed vertices. Fig. 1 shows the terrain graph $G$ and its corresponding decomposed graph $D$ of an example terrain to be covered, where STC generates a single-robot coverage path on $D$ by circumnavigating (i.e., always moving along the right side of the spanning edges) the minimum spanning tree of $G$ .  \n\nLike STC, existing graph-based MCPP algorithms operate on the given terrain graph $G$ exclusively to build a tree cover\u2014a forest of multiple trees, each rooted at the initial vertex of a robot, that jointly cover all vertices of $G$ . The coverage path for each robot is then obtained by circumnavigating its corresponding tree. In essence, these algorithms reduce MCPP to the NP-hard Min-Max Rooted Tree Cover problem (Even et al. 2004; Nagamochi and Okada 2007) on $G$ that aims to optimize the weight of the largest-weighted tree in the tree cover since it determines the makespan of the resulting coverage paths on $D$ . However, operating on the terrain graph $G$ exclusively has two disadvantages. Firstly, it does not work for an incomplete terrain graph $G$ where some of the four decomposed vertices of a vertex might be blocked and thus absent in the decomposed graph $D$ . Secondly, an optimal tree cover on $G$ does not necessarily result in an optimal MCPP solution (as illustrated in Fig. 1-(c) and (d)), which yields an asymptotic suboptimality ratio of four for makespan in the worst case (Zheng et al. 2010), since circumnavigating the trees in a tree cover explores only a portion of the solution space that encompasses all possible sets of coverage paths on the decomposed graph $D$ .  \n\nTherefore, we take a different route to explore how to systematically search for good coverage paths directly on the decomposed graph. Our algorithmic contribution is through the following key aspects: (1) We propose a novel standalone algorithmic paradigm called Extended-STC (ESTC), an extension of STC, to address coverage planning problems on both complete and incomplete terrain graphs by directly operating on decomposed graphs. Importantly, we demonstrate that ESTC guarantees complete coverage for both singleand multi-robot settings, rendering it an efficient and versatile solution to coverage path planning. (2) We propose three types of specialized neighborhood operators to facilitate an effective local search process by identifying costefficient subgraphs of the decomposed graph that are used to generate coverage paths for the robots. The strategic integration of these operators enhances the efficiency of the local search in exploring the solution space. (3) We demonstrate how to combine these neighborhood operators with iterative calls to the ESTC paradigm to establish our proposed LS-MCPP framework for solving MCPP. To validate the effectiveness of LS-MCPP, we conduct extensive experiments, comparing it with three state-of-the-art baseline graph-based MCPP algorithms that operate on complete terrain graphs only. The results show that LS-MCPP achieves makespans that are smaller by up to $35.7\\%$ and $30.3\\%$ than two of the baseline algorithms, respectively, which compute suboptimal tree covers on the terrain graph. Additionally, LS-MCPP consistently achieves makespans comparable to or better than those achieved by the remaining baseline algorithm, which employs mixed integer programming (MIP) to compute optimal tree covers on the terrain graph. While the baseline algorithm takes more than tens of hours to complete, LS-MCPP accomplishes the same task in just a matter of minutes, showcasing its efficiency and practicality for large-scale real-world coverage problems."}], "task_step_question_answer": "\u5728\u8def\u5f84\u89c4\u5212\u7684\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u8fd9\u4e9b\u4f18\u5316\u7b56\u7565\u5df2\u7ecf\u663e\u793a\u51fa\u663e\u8457\u7684\u6548\u679c\u3002\u4f8b\u5982\uff0c\u5728\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\uff0cUA-MCTS\u548c\u542f\u53d1\u5f0f\u5f15\u5bfc\u7684\u7ed3\u5408\u4f7f\u5f97\u8f66\u8f86\u80fd\u591f\u5728\u590d\u6742\u7684\u57ce\u5e02\u73af\u5883\u4e2d\u5feb\u901f\u627e\u5230\u5b89\u5168\u7684\u884c\u9a76\u8def\u5f84\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u8ba1\u7b97\u8d44\u6e90\u7684\u6d88\u8017\u3002\u5728\u673a\u5668\u4eba\u5bfc\u822a\u4e2d\uff0c\u5c40\u90e8\u641c\u7d22\u7b56\u7565\u548c\u52a8\u6001\u526a\u679d\u7684\u5e94\u7528\u4f7f\u5f97\u673a\u5668\u4eba\u80fd\u591f\u5728\u52a8\u6001\u53d8\u5316\u7684\u73af\u5883\u4e2d\u5b9e\u65f6\u8c03\u6574\u8def\u5f84\uff0c\u907f\u514d\u4e86\u4e0d\u5fc5\u8981\u7684\u8ba1\u7b97\u5f00\u9500\u3002\u6b64\u5916\uff0c\u7b80\u5316\u6a21\u62df\u6a21\u578b\u548c\u5e76\u884c\u6a21\u62df\u7684\u7ed3\u5408\u5728\u65e0\u4eba\u673a\u8def\u5f84\u89c4\u5212\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8def\u5f84\u89c4\u5212\u7684\u5b9e\u65f6\u6027\u548c\u51c6\u786e\u6027\u3002\u5c3d\u7ba1\u8fd9\u4e9b\u4f18\u5316\u7b56\u7565\u5728\u4e0d\u540c\u573a\u666f\u4e2d\u53d6\u5f97\u4e86\u6210\u529f\uff0c\u4f46\u5728\u67d0\u4e9b\u6781\u7aef\u73af\u5883\u4e0b\uff0c\u5982\u9ad8\u52a8\u6001\u6216\u9ad8\u4e0d\u786e\u5b9a\u6027\u7684\u73af\u5883\u4e2d\uff0c\u8fd9\u4e9b\u7b56\u7565\u7684\u6548\u679c\u53ef\u80fd\u4f1a\u53d7\u5230\u9650\u5236\u3002\u672a\u6765\u7684\u7814\u7a76\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63a2\u7d22\u5982\u4f55\u5728\u8fd9\u4e9b\u6781\u7aef\u73af\u5883\u4e0b\u63d0\u5347\u7b97\u6cd5\u7684\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\uff0c\u6216\u8005\u901a\u8fc7\u5f15\u5165\u66f4\u5148\u8fdb\u7684\u6a21\u578b\u548c\u7b97\u6cd5\u6765\u8fdb\u4e00\u6b65\u63d0\u5347\u8def\u5f84\u89c4\u5212\u7684\u6027\u80fd\u3002", "ref_task_step_id": ""}, "__type__": "task_step_node"}, "5e563bfc-4301-43c2-9c34-d92ea2c5783a": {"__data__": {"id_": "5e563bfc-4301-43c2-9c34-d92ea2c5783a", "metadata": {}, "relationships": {}, "hash": "", "start_task_context": "MCTS\u5728PRM\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u7684\u5e94\u7528\u63a2\u7d22\u7efc\u8ff0", "aemo_representation_context": "### Step 1: \u5206\u6790\u8fd1\u51e0\u5e74\u7814\u7a76\u9886\u57df\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba\n\n**\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09**\uff1a\n- **\u57fa\u672c\u539f\u7406**\uff1aMCTS\u662f\u4e00\u79cd\u7528\u4e8e\u51b3\u7b56\u8fc7\u7a0b\u7684\u542f\u53d1\u5f0f\u641c\u7d22\u7b97\u6cd5\uff0c\u5e7f\u6cdb\u5e94\u7528\u4e8e\u6e38\u620fAI\u3001\u8def\u5f84\u89c4\u5212\u7b49\u9886\u57df\u3002\u5176\u6838\u5fc3\u601d\u60f3\u662f\u901a\u8fc7\u6a21\u62df\u591a\u6b21\u968f\u673a\u6e38\u620f\u6765\u8bc4\u4f30\u4e0d\u540c\u51b3\u7b56\u7684\u4f18\u52a3\u3002\n- **\u6280\u672f\u6846\u67b6**\uff1a\u4e3b\u8981\u5305\u62ec\u9009\u62e9\uff08Selection\uff09\u3001\u6269\u5c55\uff08Expansion\uff09\u3001\u6a21\u62df\uff08Simulation\uff09\u548c\u56de\u6eaf\uff08Backpropagation\uff09\u56db\u4e2a\u6b65\u9aa4\u3002\n\n**\u504f\u597d\u7b56\u7565\u6a21\u578b\uff08PRM\uff09**\uff1a\n- **\u57fa\u672c\u539f\u7406**\uff1aPRM\u65e8\u5728\u901a\u8fc7\u5b66\u4e60\u7528\u6237\u7684\u504f\u597d\u6765\u4f18\u5316\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5e38\u7528\u4e8e\u63a8\u8350\u7cfb\u7edf\u3001\u4e2a\u6027\u5316\u670d\u52a1\u7b49\u3002\n- **\u6280\u672f\u6846\u67b6**\uff1a\u901a\u5e38\u5305\u62ec\u504f\u597d\u5efa\u6a21\u3001\u7b56\u7565\u4f18\u5316\u548c\u6a21\u578b\u5fae\u8c03\u7b49\u73af\u8282\u3002\n\n**\u65b9\u6cd5\u8bba**\uff1a\n- **\u96c6\u6210\u5b66\u4e60**\uff1a\u5c06MCTS\u4e0ePRM\u7ed3\u5408\uff0c\u5229\u7528MCTS\u7684\u641c\u7d22\u80fd\u529b\u4f18\u5316PRM\u7684\u51b3\u7b56\u8fc7\u7a0b\u3002\n- **\u5f3a\u5316\u5b66\u4e60**\uff1a\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u673a\u5236\u4e0d\u65ad\u8c03\u6574PRM\u7684\u53c2\u6570\uff0c\u4ee5\u9002\u5e94\u52a8\u6001\u73af\u5883\u3002\n\n### Step 2: \u7814\u7a76\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u4e3b\u8981\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0e\u53d8\u4f53\n\n**\u5e94\u7528\u573a\u666f**\uff1a\n- **\u6e38\u620fAI**\uff1aMCTS\u4e0ePRM\u7ed3\u5408\u7528\u4e8e\u6e38\u620f\u4e2d\u7684\u667a\u80fd\u51b3\u7b56\uff0c\u5982AlphaGo\u3002\n- **\u8def\u5f84\u89c4\u5212**\uff1a\u5728\u673a\u5668\u4eba\u5bfc\u822a\u548c\u81ea\u52a8\u9a7e\u9a76\u4e2d\uff0c\u5229\u7528MCTS\u4f18\u5316\u8def\u5f84\u9009\u62e9\u3002\n- **\u63a8\u8350\u7cfb\u7edf**\uff1a\u901a\u8fc7PRM\u5b66\u4e60\u7528\u6237\u504f\u597d\uff0c\u7ed3\u5408MCTS\u8fdb\u884c\u66f4\u7cbe\u51c6\u7684\u63a8\u8350\u3002\n\n**\u53d8\u4f53**\uff1a\n- **\u6df1\u5ea6MCTS**\uff1a\u7ed3\u5408\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff0c\u63d0\u5347MCTS\u7684\u51b3\u7b56\u80fd\u529b\u3002\n- **\u591a\u6a21\u6001PRM**\uff1a\u878d\u5408\u591a\u79cd\u6570\u636e\u6e90\uff08\u5982\u6587\u672c\u3001\u56fe\u50cf\uff09\uff0c\u589e\u5f3aPRM\u7684\u6cdb\u5316\u80fd\u529b\u3002\n\n### Step 3: \u8bc4\u4f30\u5b66\u672f\u754c\u7684\u6280\u672f\u8fdb\u6b65\u4e0e\u5c40\u9650\u6027\n\n**\u6280\u672f\u8fdb\u6b65**\uff1a\n- **\u6027\u80fd\u63d0\u5347**\uff1aMCTS\u4e0ePRM\u7ed3\u5408\u663e\u8457\u63d0\u5347\u4e86\u51b3\u7b56\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002\n- **\u6cdb\u5316\u80fd\u529b**\uff1a\u591a\u6a21\u6001PRM\u5728\u5904\u7406\u591a\u6837\u5316\u6570\u636e\u65f6\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002\n\n**\u5c40\u9650\u6027**\uff1a\n- **\u8ba1\u7b97\u590d\u6742\u5ea6**\uff1aMCTS\u5728\u5927\u89c4\u6a21\u95ee\u9898\u4e2d\u8ba1\u7b97\u91cf\u5927\uff0c\u6548\u7387\u6709\u5f85\u63d0\u5347\u3002\n- **\u6570\u636e\u4f9d\u8d56**\uff1aPRM\u7684\u6027\u80fd\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u9ad8\u8d28\u91cf\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u6570\u636e\u504f\u5dee\u95ee\u9898\u4f9d\u7136\u5b58\u5728\u3002\n\n### Step 4: \u63a2\u8ba8\u8ba1\u7b97\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\n\n**\u9002\u7528\u6027**\uff1a\n- **\u591a\u9886\u57df\u5e94\u7528**\uff1aMCTS+PRM\u6846\u67b6\u5728\u6e38\u620f\u3001\u8def\u5f84\u89c4\u5212\u3001\u63a8\u8350\u7cfb\u7edf\u7b49\u591a\u4e2a\u9886\u57df\u8868\u73b0\u51fa\u826f\u597d\u7684\u9002\u7528\u6027\u3002\n- **\u591a\u6a21\u6001\u6570\u636e**\uff1a\u591a\u6a21\u6001PRM\u80fd\u591f\u5904\u7406\u591a\u79cd\u7c7b\u578b\u7684\u6570\u636e\uff0c\u9002\u7528\u4e8e\u590d\u6742\u5e94\u7528\u573a\u666f\u3002\n\n**\u6cdb\u5316\u80fd\u529b**\uff1a\n- **\u8de8\u9886\u57df\u6cdb\u5316**\uff1a\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\uff0cMCTS+PRM\u6846\u67b6\u5728\u4e0d\u540c\u9886\u57df\u95f4\u5c55\u73b0\u51fa\u8f83\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002\n- **\u52a8\u6001\u73af\u5883\u9002\u5e94**\uff1a\u5f3a\u5316\u5b66\u4e60\u673a\u5236\u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u9002\u5e94\u52a8\u6001\u53d8\u5316\u7684\u73af\u5883\u3002\n\n### Step 5: \u5206\u6790\u6700\u65b0\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027\n\n**\u7a33\u5b9a\u6027**\uff1a\n- **\u7b97\u6cd5\u4f18\u5316**\uff1a\u901a\u8fc7\u6539\u8fdbMCTS\u7684\u9009\u62e9\u7b56\u7565\u548cPRM\u7684\u53c2\u6570\u8c03\u6574\u673a\u5236\uff0c\u63d0\u5347\u4e86\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u3002\n- **\u9c81\u68d2\u6027\u589e\u5f3a**\uff1a\u5f15\u5165\u5bf9\u6297\u8bad\u7ec3\u548c\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\uff0c\u63d0\u9ad8\u6a21\u578b\u5728\u590d\u6742\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u6027\u3002\n\n**\u5bb9\u9519\u6027**\uff1a\n- **\u9519\u8bef\u5904\u7406**\uff1a\u5728\u6a21\u62df\u548c\u56de\u6eaf\u9636\u6bb5\u5f15\u5165\u5bb9\u9519\u673a\u5236\uff0c\u51cf\u5c11\u9519\u8bef\u51b3\u7b56\u7684\u5f71\u54cd\u3002\n- **\u5927\u89c4\u6a21\u6570\u636e\u9002\u5e94\u6027**\uff1a\u4f18\u5316\u7b97\u6cd5\u7ed3\u6784\uff0c\u63d0\u5347\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\u7684\u5904\u7406\u80fd\u529b\u3002\n\n### Step 6: \u8bc4\u4f30\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218\n\n**\u672a\u6765\u7814\u7a76\u65b9\u5411**\uff1a\n- **\u9ad8\u6548\u8ba1\u7b97**\uff1a\u7814\u7a76\u66f4\u9ad8\u6548\u7684MCTS\u7b97\u6cd5\uff0c\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002\n- **\u6570\u636e\u589e\u5f3a**\uff1a\u63a2\u7d22\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u51cf\u5c11PRM\u5bf9\u9ad8\u8d28\u91cf\u6570\u636e\u7684\u4f9d\u8d56\u3002\n\n**\u6311\u6218**\uff1a\n- **\u8de8\u6a21\u6001\u878d\u5408**\uff1a\u5982\u4f55\u6709\u6548\u878d\u5408\u591a\u79cd\u6a21\u6001\u6570\u636e\uff0c\u63d0\u5347\u6a21\u578b\u7684\u7efc\u5408\u6027\u80fd\u3002\n- **\u52a8\u6001\u73af\u5883\u9002\u5e94**\uff1a\u5728\u52a8\u6001\u73af\u5883\u4e0b\uff0c\u5982\u4f55\u4fdd\u6301\u6a21\u578b\u7684\u7a33\u5b9a\u6027\u548c\u9ad8\u6548\u6027\u3002\n\n**\u63a8\u52a8\u540e\u7eed\u7814\u7a76**\uff1a\n- **\u65b0\u95ee\u9898\u63d0\u51fa**\uff1a\u5982\u591a\u6a21\u6001\u6570\u636e\u4e0b\u7684\u504f\u597d\u5efa\u6a21\u3001\u52a8\u6001\u73af\u5883\u4e0b\u7684\u51b3\u7b56\u4f18\u5316\u7b49\u3002\n- **\u7814\u7a76\u5207\u5165\u70b9**\uff1a\u4ece\u7b97\u6cd5\u4f18\u5316\u3001\u6570\u636e\u589e\u5f3a\u3001\u8de8\u6a21\u6001\u878d\u5408\u7b49\u591a\u4e2a\u89d2\u5ea6\u5207\u5165\uff0c\u63a8\u52a8\u6280\u672f\u8fdb\u6b65\u3002\n\n### \u603b\u7ed3\n\n\u8fd1\u5e74\u6765\uff0cMCTS\u5728PRM\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u7684\u5e94\u7528\u63a2\u7d22\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4e3b\u8981\u4f53\u73b0\u5728\u6280\u672f\u6846\u67b6\u7684\u5b8c\u5584\u3001\u5e94\u7528\u573a\u666f\u7684\u62d3\u5c55\u3001\u6027\u80fd\u7684\u63d0\u5347\u4ee5\u53ca\u7a33\u5b9a\u6027\u548c\u5bb9\u9519\u6027\u7684\u589e\u5f3a\u3002\u7136\u800c\uff0c\u4ecd\u5b58\u5728\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u3001\u6570\u636e\u4f9d\u8d56\u6027\u5f3a\u7b49\u5c40\u9650\u6027\u3002\u672a\u6765\u7684\u7814\u7a76\u5e94\u5173\u6ce8\u9ad8\u6548\u8ba1\u7b97\u3001\u6570\u636e\u589e\u5f3a\u548c\u8de8\u6a21\u6001\u878d\u5408\u7b49\u65b9\u5411\uff0c\u4ee5\u5e94\u5bf9\u52a8\u6001\u73af\u5883\u4e0b\u7684\u51b3\u7b56\u4f18\u5316\u7b49\u6311\u6218\uff0c\u63a8\u52a8\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002", "task_step_name": "\u8bc4\u4f30\u5b66\u672f\u754c\u7684\u6280\u672f\u8fdb\u6b65\u4e0e\u5c40\u9650\u6027", "task_step_description": "MCTS\u4e0ePRM\u7ed3\u5408\u663e\u8457\u63d0\u5347\u4e86\u51b3\u7b56\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "task_step_level": "2", "task_step_question": "### \u95ee\u9898\n\n\u5728\u8bc4\u4f30\u5b66\u672f\u754c\u7684\u6280\u672f\u8fdb\u6b65\u4e0e\u5c40\u9650\u6027\u65f6\uff0cMCTS\u4e0ePRM\u7684\u7ed3\u5408\u663e\u8457\u63d0\u5347\u4e86\u51b3\u7b56\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u7ed3\u5408\u4e5f\u5e26\u6765\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u548c\u6570\u636e\u4f9d\u8d56\u6027\u5f3a\u7b49\u5c40\u9650\u6027\u3002\u57fa\u4e8e\u8fd9\u4e9b\u4fe1\u606f\uff0c\u6211\u4eec\u53ef\u4ee5\u63d0\u51fa\u4ee5\u4e0b\u95ee\u9898\uff1a\n\n**\u95ee\u9898**\uff1a\u5728MCTS\u4e0ePRM\u7ed3\u5408\u7684\u5e94\u7528\u4e2d\uff0c\u5982\u4f55\u6709\u6548\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u5e76\u51cf\u5c11\u5bf9\u9ad8\u8d28\u91cf\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u4ee5\u8fdb\u4e00\u6b65\u63d0\u5347\u51b3\u7b56\u7cfb\u7edf\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u548c\u9002\u7528\u6027\uff1f\n\n\u8fd9\u4e2a\u95ee\u9898\u65e8\u5728\u63a2\u8ba8\u5728\u4fdd\u6301MCTS\u4e0ePRM\u7ed3\u5408\u5e26\u6765\u7684\u6027\u80fd\u63d0\u5347\u7684\u540c\u65f6\uff0c\u5982\u4f55\u901a\u8fc7\u6280\u672f\u624b\u6bb5\u6216\u65b9\u6cd5\u8bba\u521b\u65b0\u6765\u514b\u670d\u5176\u5c40\u9650\u6027\uff0c\u4ece\u800c\u63a8\u52a8\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002", "task_step_question_context": [{"ref_id": "454847029383636420", "chunk_id": "9", "score": 0.181640625, "text": "# 4 Theoretical Analysis\nAlthough it is difficult to analyze the regret of MCTS-VS directly, we can theoretically analyze the influence of general variable selection by adopting the acquisition function GP-UCB. The considered general variable selection framework is as follows: after selecting a subset of variables at each iteration, the corresponding observation data (i.e., the data points sampled-so-far where only the selected variables are used) is used to build a GP model, and the next data point is sampled by maximizing GP-UCB. We use $\\mathbb{M}_{t}$ to denote the sampled variable index subset at iteration $t$ , and let $\\left|\\mathbb{M}_{t}\\right|=d_{t}$ .  \n\nRegret Analysis. Let $x^{*}$ denote an optimal solution. We analyze the cumulative regret $R_{T}\\,=$ $\\begin{array}{r}{\\sum_{t=1}^{\\bar{T}}(f(\\pmb{x}^{*})-f(\\pmb{x}^{t}))}\\end{array}$ the selected points by iteration \u2212, i.e., the Tum of the gap between the opti . To derive an upper bound on $R_{T}$ m and the function values of , we pessimistically assume that the worst function value, i.e., $\\begin{array}{r}{\\operatorname*{min}_{\\pmb{x}_{[D]\\setminus\\mathbb{M}_{t}}}f([\\pmb{x}_{\\mathbb{M}_{t}},\\pmb{x}_{[D]\\setminus\\mathbb{M}_{t}}])}\\end{array}$ , given ${\\pmb x}_{\\mathbb{M}_{t}}$ is returned in evaluation. As in [ Lipschitz assumption. 21 ,38 ], we assume that $\\mathcal{X}\\subset[0,r]^{D}$ is convex and compact, and $f$ satisfies the following Assumption 4.1. The function $f$ is a GP sample path. For some $a,b>0$ , given $L>0$ , the partial derivatives of $f$ satisfy that $\\forall i\\in[D]$ ,$\\exists\\alpha_{i}\\geq0$ ,  \n\n$$\nP\\left(\\operatorname*{sup}_{x\\in\\mathcal{X}}\\left|\\partial f/\\partial x_{i}\\right|<\\alpha_{i}L\\right)\\geq1-a e^{-\\left(L/b\\right)^{2}}.\n$$  \n\nBased on Assumption 4.1, we define $\\alpha_{i}^{*}$ to be the minimum value of $\\alpha_{i}$ such that Eq. (3) holds, which characterizes the importance of the $i$ -th variable $x_{i}$ . The larger $\\alpha_{i}^{*}$ , the greater influence of $x_{i}$ on the function $f$ . Let $\\alpha_{\\mathrm{max}}=\\operatorname*{max}_{i\\in[D]}\\alpha_{i}^{*}$ .  \n\nTheorem 4.2 gives an upper bound on the cumulative regret $R_{T}$ with high probability for general variable selection methods. The proof is inspired by that of GP-UCB without variable selection [ 38 ]$\\forall i:\\alpha_{i}^{*}\\leq1$ and provided in Appendix B.1. If we select all variables each time (i.e., \u2264(4) becomes $R_{T}\\leq\\sqrt{C_{1}T\\beta_{T}^{*}\\gamma_{T}}+2$ p, which is consistent with [ $\\forall t:\\mathbb{M}_{t}=[D])$ and assume 38 ]. Note that \u2200$\\forall t:|\\mathbb{M}_{t}|\\,=\\,d_{t}\\,=\\,D$ ||in this case, which implies that $\\beta_{t}$ increases with $t$ , leading to $\\beta_{T}^{*}=\\beta_{T}$ . We can see that usi variable selection will $R_{T}$ by $\\begin{array}{r}{2\\sum_{t=1}^{T}\\sum_{i\\in[D]\\backslash\\mathbb{M}_{t}}\\alpha_{i}^{*}L r}\\end{array}$ P,variables unselected, the larger related to the importance (i.e., $R_{T}$ $\\alpha_{i}^{*}$ . Meanwhile, the term ) of unselected variables at each iteration. The more important $\\sqrt{C_{1}T\\beta_{T}^{*}\\gamma_{T}}$ pwill decrease as \u2208$\\beta_{T}^{*}$ \\relies on the number $d_{t}$ of selected variables positively. Ideally, if the unselected variables at each iteration are always unrelated (i.e., $\\alpha_{i}^{*}\\!=\\!0$ ), the regret bound will be better than that of using all variables [38].  \n\n$b\\sqrt{\\log(4D a/\\delta)}$ Theorem 4.2. p$\\forall\\delta\\ \\in\\ (0,1)$ , where $r$ is the upper bound on each variable, and , let $\\beta_{t}\\ =\\ 2\\log(4\\pi_{t}/\\delta)\\,+\\,2d_{t}\\log(d_{t}t^{2}b r\\sqrt{\\log(4D a/\\delta)})$ {$\\{\\pi_{t}\\}_{t\\ge1}$ }\u2265satisfies $\\textstyle\\sum_{t\\geq1}\\pi_{t}^{-1}=1$ and $L\\;=$ and $\\pi_{t}>0$ . Let $\\beta_{T}^{*}=\\operatorname*{max}_{1\\leq i\\leq T}\\beta_{t}$ . At iteration $T$ , the cumulative regret  \n\n$$\nR_{T}\\leq\\sqrt{C_{1}T\\beta_{T}^{*}\\gamma_{T}}+2\\alpha_{\\operatorname*{max}}+2\\sum_{t=1}^{T}\\sum_{\\substack{i\\in[D]\\backslash\\mathbb{M}_{t}}}\\alpha_{i}^{*}L r\n$$  \n\nholds with probability least $1\\!-\\!\\delta$ , where $C_{1}$ is a constant, $\\begin{array}{r}{\\gamma_{T}\\!=\\!\\operatorname*{max}_{|\\mathcal{D}|=\\!T}I(\\pmb{y}_{\\mathcal{D}},\\pmb{f}_{\\mathcal{D}}),}\\end{array}$ $I(\\cdot,\\cdot)$ is the information gain, and $\\scriptstyle y_{\\mathcal{D}}$ Dand $f_{\\mathcal{D}}$ Dare the noisy and true observations of a set Dof points, respectively.  \n\nBy selecting been proved [21] that the cumulative regret of Dropout satisfies $d$ variables randomly at each iteration and assuming that $r=1$ and $\\forall i:\\alpha_{i}^{*}\\leq1$ \u2264, it has  \n\n$$\nR_{T}\\leq\\sqrt{C_{1}T\\beta_{T}\\gamma_{T}}+2+2T L(D-d).\n$$  \n\nIn this case, we have $d_{t}=|\\mathbb{M}_{t}|=d$ ,$r=1$ and $\\forall i:\\alpha_{i}^{*}\\leq1$ \u2264. Thus, Eq. (4) becomes  \n\n$$\nR_{T}\\leq\\sqrt{C_{1}T\\beta_{T}^{*}\\gamma_{T}}+2+2T L(D-d).\n$$  \n\nNote that $\\beta_{T}^{*}=\\beta_{T}$ here, as $\\beta_{t}$ increases with $t$ given $d_{t}=d$ . This implies that our bound Eq. (4) for general variable selection is a generalization of Eq. (5) for Dropout [ 21 ]. In [ 33 ], a regret bound analysis has also been performed for variable selection, by optimizing over $d$ fixed important variables and using a common parameter $\\alpha$ to characterize the importance of all the other $D-d$ variables.  \n\nComputational Complexity Analysis. The computational complexity of one iteration of BO depends on three critical components: fitting a GP surrogate model, maximizing an acquisition function and evaluating a sampled point. If using the squared exponential kernel, the computational complexity of fitting a GP model at iteration $t$ is $\\bar{O(t^{3}\\!+\\!t^{2}d_{t})}$ . Maximizing an acquisition function is related to the optimization algorithm. If we use the Quasi-Newton method to optimize GP-UCB, the computational complexity is $\\bar{\\mathcal{O}}(m(t^{2}+t d_{t}+d_{t}^{2}))$ [28 ], where $m$ denotes the Quasi-Newton\u2019s running rounds. The cost of evaluating a sampled point is fixed. Thus, by selecting only a subset of variables, instead of all variables, to optimize, the computational complexity can be decreased significantly. The detailed analysis is provided in Appendix B.2.  \n\nInsight. The above regret and computational complexity analyses have shown that variable selection can reduce the computational complexity while increasing the regret. Given the number $d_{t}$ of variables to be selected, a good variable selection method should select as important variables as possible, i.e., variables with as large $\\alpha_{i}^{*}$ as possible, which may help to design and evaluate variable selection methods. The experiments in Section 5.1 will show that MCTS-VS can select a good subset of variables while maintaining a small computational complexity."}, {"ref_id": "454984236281633338", "chunk_id": "4", "score": 0.1787109375, "text": "# DSensitivity Analysis of Hyper-parameters of MCTS-VS\nWe provide further studies to examine the influence of the hyper-parameters of MCTS-VS, including the employed optimization algorithm for optimizing the selected variables in each iteration, the \u201cfillin\u201d strategy, the hyper-parameter $k$ used in the best$k$ strategy, the hyper-parameter $C_{p}$ for calculating UCB in Eq. (1), the number $2\\,\\times\\,N_{v}\\,\\times\\,N_{s}$ sampled data in each iteration, the threshold $N_{b a d}$ for re-initializing a tree, and the threshold $N_{s p l i t}$ for splitting a tree node.  \n\nThe optimization algorithm is employed by MCTS-VS to optimize the selected variables in each iteration. We compare three different optimization algorithms, i.e., random search (RS), BO and TuRBO. First, we conduct experiments similar to \u201cEffectiveness of Variable Selection\u201d in Section 5.1, to show the effectiveness of MCTS-VS even when equipped with RS. Figure 6 shows that MCTSVS-RS is better than Dropout-RS and RS, revealing the advantage of MCTS-VS.  \n\n  \nFigure 6: Effectiveness of MCTS-VS when equipped with RS.  \n\nNext we compare the performance of MCTS-VS equipped with RS, BO and TuRBO, by experiments on the Hartmann functions with increasing ratio of valid variables. Hartmann 6 _500 has 6 valid variables. Hartmann 6 _5 _500 is generated by mixing 5 Hartmann 6 functions as Hartmann 6 $(\\pmb{x}_{1:6})+$ Hartmann 6 $\\backslash(\\pmb{x}_{7:12})+\\cdot\\cdot\\cdot+\\mathrm{Hartmann6}(\\pmb{x}_{25:30})$ , and appending 470 unrelated dimensions, where $\\pmb{x}_{i:j}$ denotes the $i$ -th to j-th variables. Hartmann 6 _10 _500 is generated alike. Thus, Hartmann 6 _5 _500 and Hartmann 6 _10 _500 have 30 and 60 valid variables, respectively. The results in Figure 7 show that as the ratio of valid variables increases, MCTS-VS-TuRBO gradually surpasses MCTS-VS-RS and MCTS-VS-BO, while MCTS-VS-RS becomes worse and worse. This is expected. If the ratio of valid variables is high, MCTS-VS is more likely to select the valid variables, so it is worth to use the expensive optimization algorithm, e.g., TuRBO, to optimize the selected variables. If the ratio is low, unrelated variables are more likely to be selected most of the time, so using a cheap optimization algorithm would be better. These observations also give us some guidance on selecting optimization algorithms in practice.  \n\n\u201cFill-in\u201d strategy is a basic component of variable selection methods, which influences the quality of the value of unselected variables. We compare the employed best$k$ strategy $(k=20)$ ) with the average best$k$ strategy and the random strategy. The average best$k$ strategy uses the average of the best $k$ data points for the unselected variables, and the random strategy samples the value of an unselected variable from its domain randomly. As shown in Figure 8(a), the random strategy leads to the poor performance of MCTS-VS-BO, which may be because it does not utilize the historical information and leads to over-exploration. The best${\\cdot k}$ strategy utilizes the historical points that have high objective values to fill in the unselected variables, thus behaving much better. The performance of the average strategy is between the best$k$ and random strategies. We recommend using the best$k$ strategy in practice.  \n\nThe hyper-parameter $k$ used in the best$k$ strategy controls the degree of exploitation for the unselected variables. As shown in Figure 8(b), a smaller $k$ encourages exploitation, which results in better performance in the early stage, but easily leads to premature convergence. A larger $k$ encourages exploration and behaves worse in the early stage, but may converge to a better value. We recommend using a larger $k$ if allowing enough evaluations.  \n\n  \nFigure 7: Sensitivity analysis of the optimization algorithm.  \n\n  \nFigure 8: Sensitivity analysis of the \u201cfill-in\u201d strategy and the hyper-parameter $k$ of the best$k$ strategy, using MCTS-VS-BO on Hartmann 6 _300 .  \n\nThe hyper-parameter $C_{p}$ for calculating UCB in Eq. (1) balances the exploration and exploitation of MCTS. As shown in Figure 9, a too small $C_{p}$ leads to relatively worse performance, highlighting the importance of exploration. A too large $C_{p}$ may also lead to over-exploration. But overall MCTSVS is not very sensitive to $C_{p}$ . We recommend setting $C_{p}$ between $1\\%$ and $10\\%$ of the optimum (i.e., max $f({\\boldsymbol{x}}))$ ), which is consistent with that for LA-MCTS [40].  \n\n  \nFigure 9: Sensitivity analysis of the hyper-parameter $C_{p}$ for calculating UCB in Eq. (1), using MCTS-VS-BO on Levy and Hartmann.  \n\nThe number $2\\,\\times\\,N_{v}\\,\\times\\,N_{s}$ of sampled data in ch iteration depends on the batch size $N_{v}$ of variable index subset and the sample batch size $N_{s}$ , and will influence the accuracy of estimating the variable score vector in Eq. (2). If we increase $N_{v}$ and $N_{s}$ , we can calculate the variable score more accurately, but also need more evaluations. Figure 10(a) shows that given the same number of evaluations, MCTS-VS-BO achieves the best performance when $N_{v}=2$ and $N_{s}=3$ . Thus, this setting may be a good choice to balance the accuracy of variable score and the number of evaluations, which is also used throughout the experiments.  \n\nThe threshold $N_{b a d}$ for re-initializing a tree controls the tolerance of selecting bad tree nodes (i.e., nodes containing unimportant variables). A smaller $N_{b a d}$ leads to frequent re-initialization, which can adjust quickly but may cause under-exploitation of the tree. A larger $N_{b a d}$ can make full use of the tree, but may optimize too much on unimportant variables. Figure 10(b) shows that MCTS-VS achieves the best performance when $N_{b a d}=5$ . Thus, we recommend to use this setting, to balance the re-initialization and exploitation of the tree.  \n\nThe threshold $N_{s p l i t}$ for splitting a node. If the number of variables in a node is larger than $N_{s p l i t}$ ,the node can be further partitioned. That is, the parameter $N_{s p l i t}$ controls the least number of variables in a leaf node and thus affects the number of selected variables, which has a direct influence on the wall clock time. Note that MCTS-VS selects a leaf node and optimizes the variables contained by this node in each iteration. The smaller $N_{s p l i t}$ , the shorter the time. Figure 10(c) shows that $N_{s p l i t}$ has little influence on the performance of MCTS-VS-BO, and thus we recommend to set $N_{s p l i t}=3$ to reduce the wall clock time.  \n\n  \nFigure 10: S vity analysis of the number $2\\,\\times\\,N_{v}\\,\\times\\,N_{s}$ pled data in each iteration, the threshold $N_{b a d}$ for re-initializing a tree and the threshold $N_{s p l i t}$ for splitting a node, using MCTS-VS-BO on Hartmann 6 _300 .  \n\nInfluence of the hyper-parameters on the runtime of MCTS-VS. We also provide some intuitive explanation about the influence of the hyper-parameters on the runtime. The threshold $N_{s p l i t}$ for splitting a node has a direct impact on the runtime, because it controls the least number of variables to be optimized in a leaf node. That is, the runtime will increase with $N_{s p l i t}$ . Other parameters may affect the depth of the tree and thus the runtime. For the threshold $N_{b a d}$ for re-initializing a tree, if it is set to a small value, MCTS-VS will re-build the tree frequently and the depth of the tree is small. The shallow nodes have more variables, leading to more runtime to optimize. For the hyper-parameter $C_{p}$ for calculating UCB, if it is set to a large value, the exploration is preferred and MCTS-VS will tend to select the right node (regarded as containing unimportant variables). The tree thus will be re-built freq tly, ding to more runtime. For the number $2\\,\\times\\,N_{v}\\,\\times\\,N_{s}$ of sampled data at each iteration, if $N_{v}$ and $N_{s}$ are set to large values, the depth of the tree will be small given the total number of evaluations, and thus lead to more runtime."}, {"ref_id": "454845993914250942", "chunk_id": "7", "score": 0.1708984375, "text": "# 1 Introduction\nPerformance complementarity, a phenomenon where no single algorithm consistently outperforms all others across diverse problem instances, is a well-established reality in the realm of optimization and learning problems [Kerschke et al. , 2019]. Over the past few years, the growing interest in automated algorithm selection techniques has become evident. These techniques aim to tackle the challenge of selecting the most appropriate algorithm from a predefined set for a given problem instance automatically [Ruhkopf et al. , 2022; Heins et al. , 2023]. As depicted in Figure 1(a), existing techniques predominantly rely on two sources of information: (1) the features of each problem instance and (2) the historical performance of various algorithms across problem instances [Pio et al. , 2023]. Machine learning methods are then employed to establish a mapping from problem features to a subset of algorithms that yield optimal performance. Consequently, extensive research has focused on two critical aspects within this field: (1) designing problem feature extraction methods tailored to specific problem categories or tasks [Alissa et al. , 2023], and (2) constructing advanced machine learning models to map problem features to algorithms [Tornede et al. , 2022].  \n\nHowever, it is noteworthy that there has been a conspicuous absence of research focusing on the features of the algorithms themselves. Most current research has centered on problem features, treating algorithm-related information merely as a supervisor. For instance, some studies treat the selected algorithms as labels, modeling the task as either single-label [Brazdil and Giraud-Carrier, 2018] or multi-label [Dantas and Pozo, 2020] classification. Recognizing the inherent complexity and diversity of algorithms, quantifying and describing their features can indeed be a formidable challenge, and a universal representation method applicable across different algorithms remains elusive. Nevertheless, neglecting this critical aspect of algorithm features undeniably affects the overall model performance, posing at least three issues. Firstly, disregarding algorithm features as an essential information source inevitably results in a loss of model accuracy. Furthermore, relying solely on problem features often implies a unidirectional relationship, characterized by a one-way mapping from problems to algorithms. This unidirectional mapping does not align with the underlying bidirectional nature of the relationship between algorithms and problems, potentially missing crucial information that could enhance the model\u2019s performance. Additionally, neglecting algorithm features could potentially slow down the convergence, necessitating larger training data. This contradicts the essence of algorithm selection, which seeks to reduce experimentation costs as a preprocessing step. Requiring substantial and hardto-acquire training data, such as performance data across diverse problems, undermines the original intent of algorithm selection.  \n\n  \nFigure 1: Comparison of the existing framework and the proposed framework.  \n\nOn the other hand, the algorithm set is typically much smaller than the problem set, and the candidate algorithm set usually remains fixed during the training process [Cunha et al. , 2018]. Therefore, once algorithm features can be effectively extracted, they offer a convenient and potent resource. With the advent of the era of the pre-trained large language model (LLM) [Ouyang et al. , 2022], extracting algorithm features has become more achievable. Code text data accurately represents the functionality and characteristics of an algorithm, and, with the assistance of pre-trained LLMs or dedicated pre-trained models for code text [Chen et al. , 2021], we can represent code features with minimal training overhead. The universality of this extraction process may even surpass that of problem feature extraction.  \n\nSpecifically, this paper introduces a novel algorithm selection framework, as depicted in Figure 1(b), which not only leverages information about problem features and algorithm performance but also captures algorithm representations. Accordingly, the learning objective of our model diverges from existing techniques. Instead of the one-way mapping from problems to algorithms, we directly model the matching degree between algorithm representations and problem representations. To achieve this goal, we propose an Algorithm Selection Model based on Large Language Model (AS-LLM) and deploy it in heuristic algorithm selection tasks for continuous optimization problems to demonstrate the merits of considering algorithm features. The AS-LLM model comprises two distinct tracks for extracting features from problems and algorithms. Problem features are traversed from the tree representation of the objective function, while algorithm features are extracted from corresponding code text. After passing through multiple layers of networks, the similarity computation between algorithm and problem representations determines the selected algorithm. The key contributions are summarized as follows:  \n\n\u2022 To the best of our knowledge, this paper pioneers the integration of algorithm features into the algorithm selection process, utilizing pre-trained LLMs for extracting features from algorithm code text. This method holds  \n\ngenerality across various algorithms. \u2022 The proposed AS-LLM offers at least three practical benefits: (i) A more accurate modeling of the bidirectional nature of algorithm selection tasks; (ii) Superior performance with fewer training samples; (iii) Versatility to adapt to various problem types simply by modifying the track used for problem feature extraction. \u2022 Within the AS-LLM framework, the algorithm selection task serves as a valuable benchmark for evaluating the algorithm comprehension capabilities of LLMs. We integrate various LLMs into AS-LLM and empirically demonstrate their performance differences."}], "task_step_question_answer": "", "ref_task_step_id": ""}, "__type__": "task_step_node"}, "589d43ac-52eb-46d5-a6e1-797b6c59a64e": {"__data__": {"id_": "589d43ac-52eb-46d5-a6e1-797b6c59a64e", "metadata": {}, "relationships": {}, "hash": "", "start_task_context": "MCTS\u5728PRM\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u7684\u5e94\u7528\u63a2\u7d22\u7efc\u8ff0", "aemo_representation_context": "### Step 1: \u5206\u6790\u8fd1\u51e0\u5e74\u7814\u7a76\u9886\u57df\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba\n\n**\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09**\uff1a\n- **\u57fa\u672c\u539f\u7406**\uff1aMCTS\u662f\u4e00\u79cd\u7528\u4e8e\u51b3\u7b56\u8fc7\u7a0b\u7684\u542f\u53d1\u5f0f\u641c\u7d22\u7b97\u6cd5\uff0c\u5e7f\u6cdb\u5e94\u7528\u4e8e\u6e38\u620fAI\u3001\u8def\u5f84\u89c4\u5212\u7b49\u9886\u57df\u3002\u5176\u6838\u5fc3\u601d\u60f3\u662f\u901a\u8fc7\u6a21\u62df\u591a\u6b21\u968f\u673a\u6e38\u620f\u6765\u8bc4\u4f30\u4e0d\u540c\u51b3\u7b56\u7684\u4f18\u52a3\u3002\n- **\u6280\u672f\u6846\u67b6**\uff1a\u4e3b\u8981\u5305\u62ec\u9009\u62e9\uff08Selection\uff09\u3001\u6269\u5c55\uff08Expansion\uff09\u3001\u6a21\u62df\uff08Simulation\uff09\u548c\u56de\u6eaf\uff08Backpropagation\uff09\u56db\u4e2a\u6b65\u9aa4\u3002\n\n**\u504f\u597d\u7b56\u7565\u6a21\u578b\uff08PRM\uff09**\uff1a\n- **\u57fa\u672c\u539f\u7406**\uff1aPRM\u65e8\u5728\u901a\u8fc7\u5b66\u4e60\u7528\u6237\u7684\u504f\u597d\u6765\u4f18\u5316\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5e38\u7528\u4e8e\u63a8\u8350\u7cfb\u7edf\u3001\u4e2a\u6027\u5316\u670d\u52a1\u7b49\u3002\n- **\u6280\u672f\u6846\u67b6**\uff1a\u901a\u5e38\u5305\u62ec\u504f\u597d\u5efa\u6a21\u3001\u7b56\u7565\u4f18\u5316\u548c\u6a21\u578b\u5fae\u8c03\u7b49\u73af\u8282\u3002\n\n**\u65b9\u6cd5\u8bba**\uff1a\n- **\u96c6\u6210\u5b66\u4e60**\uff1a\u5c06MCTS\u4e0ePRM\u7ed3\u5408\uff0c\u5229\u7528MCTS\u7684\u641c\u7d22\u80fd\u529b\u4f18\u5316PRM\u7684\u51b3\u7b56\u8fc7\u7a0b\u3002\n- **\u5f3a\u5316\u5b66\u4e60**\uff1a\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u673a\u5236\u4e0d\u65ad\u8c03\u6574PRM\u7684\u53c2\u6570\uff0c\u4ee5\u9002\u5e94\u52a8\u6001\u73af\u5883\u3002\n\n### Step 2: \u7814\u7a76\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u4e3b\u8981\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0e\u53d8\u4f53\n\n**\u5e94\u7528\u573a\u666f**\uff1a\n- **\u6e38\u620fAI**\uff1aMCTS\u4e0ePRM\u7ed3\u5408\u7528\u4e8e\u6e38\u620f\u4e2d\u7684\u667a\u80fd\u51b3\u7b56\uff0c\u5982AlphaGo\u3002\n- **\u8def\u5f84\u89c4\u5212**\uff1a\u5728\u673a\u5668\u4eba\u5bfc\u822a\u548c\u81ea\u52a8\u9a7e\u9a76\u4e2d\uff0c\u5229\u7528MCTS\u4f18\u5316\u8def\u5f84\u9009\u62e9\u3002\n- **\u63a8\u8350\u7cfb\u7edf**\uff1a\u901a\u8fc7PRM\u5b66\u4e60\u7528\u6237\u504f\u597d\uff0c\u7ed3\u5408MCTS\u8fdb\u884c\u66f4\u7cbe\u51c6\u7684\u63a8\u8350\u3002\n\n**\u53d8\u4f53**\uff1a\n- **\u6df1\u5ea6MCTS**\uff1a\u7ed3\u5408\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff0c\u63d0\u5347MCTS\u7684\u51b3\u7b56\u80fd\u529b\u3002\n- **\u591a\u6a21\u6001PRM**\uff1a\u878d\u5408\u591a\u79cd\u6570\u636e\u6e90\uff08\u5982\u6587\u672c\u3001\u56fe\u50cf\uff09\uff0c\u589e\u5f3aPRM\u7684\u6cdb\u5316\u80fd\u529b\u3002\n\n### Step 3: \u8bc4\u4f30\u5b66\u672f\u754c\u7684\u6280\u672f\u8fdb\u6b65\u4e0e\u5c40\u9650\u6027\n\n**\u6280\u672f\u8fdb\u6b65**\uff1a\n- **\u6027\u80fd\u63d0\u5347**\uff1aMCTS\u4e0ePRM\u7ed3\u5408\u663e\u8457\u63d0\u5347\u4e86\u51b3\u7b56\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002\n- **\u6cdb\u5316\u80fd\u529b**\uff1a\u591a\u6a21\u6001PRM\u5728\u5904\u7406\u591a\u6837\u5316\u6570\u636e\u65f6\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002\n\n**\u5c40\u9650\u6027**\uff1a\n- **\u8ba1\u7b97\u590d\u6742\u5ea6**\uff1aMCTS\u5728\u5927\u89c4\u6a21\u95ee\u9898\u4e2d\u8ba1\u7b97\u91cf\u5927\uff0c\u6548\u7387\u6709\u5f85\u63d0\u5347\u3002\n- **\u6570\u636e\u4f9d\u8d56**\uff1aPRM\u7684\u6027\u80fd\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u9ad8\u8d28\u91cf\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u6570\u636e\u504f\u5dee\u95ee\u9898\u4f9d\u7136\u5b58\u5728\u3002\n\n### Step 4: \u63a2\u8ba8\u8ba1\u7b97\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\n\n**\u9002\u7528\u6027**\uff1a\n- **\u591a\u9886\u57df\u5e94\u7528**\uff1aMCTS+PRM\u6846\u67b6\u5728\u6e38\u620f\u3001\u8def\u5f84\u89c4\u5212\u3001\u63a8\u8350\u7cfb\u7edf\u7b49\u591a\u4e2a\u9886\u57df\u8868\u73b0\u51fa\u826f\u597d\u7684\u9002\u7528\u6027\u3002\n- **\u591a\u6a21\u6001\u6570\u636e**\uff1a\u591a\u6a21\u6001PRM\u80fd\u591f\u5904\u7406\u591a\u79cd\u7c7b\u578b\u7684\u6570\u636e\uff0c\u9002\u7528\u4e8e\u590d\u6742\u5e94\u7528\u573a\u666f\u3002\n\n**\u6cdb\u5316\u80fd\u529b**\uff1a\n- **\u8de8\u9886\u57df\u6cdb\u5316**\uff1a\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\uff0cMCTS+PRM\u6846\u67b6\u5728\u4e0d\u540c\u9886\u57df\u95f4\u5c55\u73b0\u51fa\u8f83\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002\n- **\u52a8\u6001\u73af\u5883\u9002\u5e94**\uff1a\u5f3a\u5316\u5b66\u4e60\u673a\u5236\u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u9002\u5e94\u52a8\u6001\u53d8\u5316\u7684\u73af\u5883\u3002\n\n### Step 5: \u5206\u6790\u6700\u65b0\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027\n\n**\u7a33\u5b9a\u6027**\uff1a\n- **\u7b97\u6cd5\u4f18\u5316**\uff1a\u901a\u8fc7\u6539\u8fdbMCTS\u7684\u9009\u62e9\u7b56\u7565\u548cPRM\u7684\u53c2\u6570\u8c03\u6574\u673a\u5236\uff0c\u63d0\u5347\u4e86\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u3002\n- **\u9c81\u68d2\u6027\u589e\u5f3a**\uff1a\u5f15\u5165\u5bf9\u6297\u8bad\u7ec3\u548c\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\uff0c\u63d0\u9ad8\u6a21\u578b\u5728\u590d\u6742\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u6027\u3002\n\n**\u5bb9\u9519\u6027**\uff1a\n- **\u9519\u8bef\u5904\u7406**\uff1a\u5728\u6a21\u62df\u548c\u56de\u6eaf\u9636\u6bb5\u5f15\u5165\u5bb9\u9519\u673a\u5236\uff0c\u51cf\u5c11\u9519\u8bef\u51b3\u7b56\u7684\u5f71\u54cd\u3002\n- **\u5927\u89c4\u6a21\u6570\u636e\u9002\u5e94\u6027**\uff1a\u4f18\u5316\u7b97\u6cd5\u7ed3\u6784\uff0c\u63d0\u5347\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\u7684\u5904\u7406\u80fd\u529b\u3002\n\n### Step 6: \u8bc4\u4f30\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218\n\n**\u672a\u6765\u7814\u7a76\u65b9\u5411**\uff1a\n- **\u9ad8\u6548\u8ba1\u7b97**\uff1a\u7814\u7a76\u66f4\u9ad8\u6548\u7684MCTS\u7b97\u6cd5\uff0c\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002\n- **\u6570\u636e\u589e\u5f3a**\uff1a\u63a2\u7d22\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u51cf\u5c11PRM\u5bf9\u9ad8\u8d28\u91cf\u6570\u636e\u7684\u4f9d\u8d56\u3002\n\n**\u6311\u6218**\uff1a\n- **\u8de8\u6a21\u6001\u878d\u5408**\uff1a\u5982\u4f55\u6709\u6548\u878d\u5408\u591a\u79cd\u6a21\u6001\u6570\u636e\uff0c\u63d0\u5347\u6a21\u578b\u7684\u7efc\u5408\u6027\u80fd\u3002\n- **\u52a8\u6001\u73af\u5883\u9002\u5e94**\uff1a\u5728\u52a8\u6001\u73af\u5883\u4e0b\uff0c\u5982\u4f55\u4fdd\u6301\u6a21\u578b\u7684\u7a33\u5b9a\u6027\u548c\u9ad8\u6548\u6027\u3002\n\n**\u63a8\u52a8\u540e\u7eed\u7814\u7a76**\uff1a\n- **\u65b0\u95ee\u9898\u63d0\u51fa**\uff1a\u5982\u591a\u6a21\u6001\u6570\u636e\u4e0b\u7684\u504f\u597d\u5efa\u6a21\u3001\u52a8\u6001\u73af\u5883\u4e0b\u7684\u51b3\u7b56\u4f18\u5316\u7b49\u3002\n- **\u7814\u7a76\u5207\u5165\u70b9**\uff1a\u4ece\u7b97\u6cd5\u4f18\u5316\u3001\u6570\u636e\u589e\u5f3a\u3001\u8de8\u6a21\u6001\u878d\u5408\u7b49\u591a\u4e2a\u89d2\u5ea6\u5207\u5165\uff0c\u63a8\u52a8\u6280\u672f\u8fdb\u6b65\u3002\n\n### \u603b\u7ed3\n\n\u8fd1\u5e74\u6765\uff0cMCTS\u5728PRM\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u7684\u5e94\u7528\u63a2\u7d22\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4e3b\u8981\u4f53\u73b0\u5728\u6280\u672f\u6846\u67b6\u7684\u5b8c\u5584\u3001\u5e94\u7528\u573a\u666f\u7684\u62d3\u5c55\u3001\u6027\u80fd\u7684\u63d0\u5347\u4ee5\u53ca\u7a33\u5b9a\u6027\u548c\u5bb9\u9519\u6027\u7684\u589e\u5f3a\u3002\u7136\u800c\uff0c\u4ecd\u5b58\u5728\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u3001\u6570\u636e\u4f9d\u8d56\u6027\u5f3a\u7b49\u5c40\u9650\u6027\u3002\u672a\u6765\u7684\u7814\u7a76\u5e94\u5173\u6ce8\u9ad8\u6548\u8ba1\u7b97\u3001\u6570\u636e\u589e\u5f3a\u548c\u8de8\u6a21\u6001\u878d\u5408\u7b49\u65b9\u5411\uff0c\u4ee5\u5e94\u5bf9\u52a8\u6001\u73af\u5883\u4e0b\u7684\u51b3\u7b56\u4f18\u5316\u7b49\u6311\u6218\uff0c\u63a8\u52a8\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002", "task_step_name": "\u6280\u672f\u8fdb\u6b65", "task_step_description": "MCTS\u4e0ePRM\u7ed3\u5408\u663e\u8457\u63d0\u5347\u4e86\u51b3\u7b56\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "task_step_level": "2>1", "task_step_question": "### \u95ee\u9898\u63d0\u51fa\n\n\u5728MCTS\u4e0ePRM\u7ed3\u5408\u663e\u8457\u63d0\u5347\u51b3\u7b56\u7cfb\u7edf\u6027\u80fd\u7684\u80cc\u666f\u4e0b\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u7684\u60c5\u51b5\u4e0b\uff0c\u5982\u4f55\u8fdb\u4e00\u6b65\u4f18\u5316MCTS\u7684\u8ba1\u7b97\u6548\u7387\uff0c\u4ee5\u964d\u4f4e\u5176\u5728\u5904\u7406\u5927\u89c4\u6a21\u95ee\u9898\u65f6\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u4ece\u800c\u66f4\u5e7f\u6cdb\u5730\u5e94\u7528\u4e8e\u5b9e\u9645\u573a\u666f\u4e2d\uff1f\n\n### \u95ee\u9898\u80cc\u666f\n\nMCTS\u4e0ePRM\u7684\u7ed3\u5408\u5728\u63d0\u5347\u51b3\u7b56\u7cfb\u7edf\u6027\u80fd\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u5c24\u5176\u662f\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002\u7136\u800c\uff0cMCTS\u5728\u5927\u89c4\u6a21\u95ee\u9898\u4e2d\u8ba1\u7b97\u91cf\u5927\uff0c\u6548\u7387\u6709\u5f85\u63d0\u5347\u3002\u56e0\u6b64\uff0c\u5982\u4f55\u8fdb\u4e00\u6b65\u4f18\u5316MCTS\u7684\u8ba1\u7b97\u6548\u7387\uff0c\u4ee5\u964d\u4f4e\u5176\u5728\u5904\u7406\u5927\u89c4\u6a21\u95ee\u9898\u65f6\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u6210\u4e3a\u4e00\u4e2a\u4e9f\u5f85\u89e3\u51b3\u7684\u95ee\u9898\u3002\n\n### \u95ee\u9898\u610f\u4e49\n\n\u4f18\u5316MCTS\u7684\u8ba1\u7b97\u6548\u7387\u4e0d\u4ec5\u80fd\u591f\u63d0\u5347\u5176\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u8fd8\u80fd\u4f7f\u5176\u66f4\u5e7f\u6cdb\u5730\u5e94\u7528\u4e8e\u5b9e\u9645\u573a\u666f\u4e2d\uff0c\u5982\u6e38\u620fAI\u3001\u8def\u5f84\u89c4\u5212\u548c\u63a8\u8350\u7cfb\u7edf\u7b49\u3002\u8fd9\u5c06\u6709\u52a9\u4e8e\u63a8\u52a8MCTS\u4e0ePRM\u7ed3\u5408\u6280\u672f\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\uff0c\u63d0\u5347\u51b3\u7b56\u7cfb\u7edf\u7684\u6574\u4f53\u6027\u80fd\u3002\n\n### \u95ee\u9898\u63a2\u8ba8\n\n1. **\u7b97\u6cd5\u4f18\u5316**\uff1a\u5982\u4f55\u901a\u8fc7\u6539\u8fdbMCTS\u7684\u9009\u62e9\u7b56\u7565\u548cPRM\u7684\u53c2\u6570\u8c03\u6574\u673a\u5236\uff0c\u63d0\u5347\u7b97\u6cd5\u7684\u8ba1\u7b97\u6548\u7387\uff1f\n2. **\u5e76\u884c\u8ba1\u7b97**\uff1a\u5982\u4f55\u5229\u7528\u5e76\u884c\u8ba1\u7b97\u6280\u672f\uff0c\u52a0\u901fMCTS\u7684\u6a21\u62df\u548c\u56de\u6eaf\u8fc7\u7a0b\uff0c\u4ece\u800c\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff1f\n3. **\u6570\u636e\u7ed3\u6784\u4f18\u5316**\uff1a\u5982\u4f55\u4f18\u5316MCTS\u7684\u6570\u636e\u7ed3\u6784\uff0c\u51cf\u5c11\u5185\u5b58\u5360\u7528\u548c\u8ba1\u7b97\u65f6\u95f4\uff0c\u63d0\u5347\u6574\u4f53\u6548\u7387\uff1f\n4. **\u542f\u53d1\u5f0f\u65b9\u6cd5**\uff1a\u5982\u4f55\u5f15\u5165\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u51cf\u5c11MCTS\u7684\u641c\u7d22\u7a7a\u95f4\uff0c\u4ece\u800c\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff1f\n\n\u901a\u8fc7\u63a2\u8ba8\u8fd9\u4e9b\u95ee\u9898\uff0c\u53ef\u4ee5\u4e3a\u8fdb\u4e00\u6b65\u4f18\u5316MCTS\u7684\u8ba1\u7b97\u6548\u7387\u63d0\u4f9b\u65b0\u7684\u601d\u8def\u548c\u65b9\u6cd5\uff0c\u63a8\u52a8MCTS\u4e0ePRM\u7ed3\u5408\u6280\u672f\u7684\u5e7f\u6cdb\u5e94\u7528\u3002", "task_step_question_context": [{"ref_id": "454846649385965806", "chunk_id": "1", "score": 0.55859375, "text": "# 2.2 Acceleration of MCTS\nMCTS-based methods have proved their strong capability of solving complex games or tasks. However, the high computational cost of MCTS hinders its application to some real-time and more general scenarios. Therefore, numerous works are devoted to accelerating MCTS. For example, to make the selection stage more effective, some heuristic pruning methods [ 14 ,33 ,29 ,1 ,2 ] aim to reduce the width and depth of the search tree with some heuristic functions. Furthermore, for more efficient evaluations, Lorentz [ 22 ] proposed early playout termination of MCTS (MCTS-EPT) to stop the random playouts early and use an evaluation function to assess win or loss. Moreover, Hsueh et al. [18 ] applied MCTS-EPT to the Chinese dark chess and proved its effectiveness. Afterward, similar ideas have been applied in the evaluation stage of AlphaGoZero [ 32 ] and later MCTS-based methods [31 ,27 ,34 ]. They evaluate the $Q$ -values through a learnable evaluation network instead of running playouts to the end. Grill et al. [15 ] propose a novel regularized policy optimization method based on AlphaZero to decrease the search budget of MCTS, which is from the optimization perspective. Danihelka et al. [10 ] propose a policy improvement algorithm based on sampling actions without replacement, named Gumbel trick to achieve better performance when planning with few simulations. However, these methods mentioned above focus on the specific stage of the search iteration or reduce the total budget through pruning and optimization methods, which are orthogonal to us. And few works targets at the search loop. Lan et al. [21 ] propose DS-MCTS, which defines the uncertainty of MCTS and approximates it by extra DNNs with specific features for board games in training. During the evaluation, DS-MCTS will check periodically and stop the search if the state is certain.\n\n# 3 Background\nThe AlphaGo series of work [ 30 ,32 ,31 ,27 ] are all MCTS-based reinforcement learning algorithms. Those algorithms assume the environment transition dynamics are known or learn the environment dynamics. Based on the dynamics, they use the Monte-Carlo tree search (MCTS) as the policy improvement operator. I.e., taking in the current policy, MCTS returns a better policy with the search algorithm. The systematic search allows the MCTS-based RL algorithm to quickly improve the policy and perform much better in the setting where heavy reasoning is required.\n\n# 3.1 MCTS\nThis part briefly introduces the MCTS method implemented in reinforcement learning applications. As mentioned in the related works, modern MCTS-based RL algorithms include four stages in the search loop, namely selection, expansion, evaluation, and backpropagation.  \n\nMCTS takes in the current states and generates a policy after the search loop of $N$ iterations. Here $N$ is a constant number of iterations set by the designer, regarded as the total budget. In the selection stage of each iteration, an action will be selected by maximizing over UCB. Specifically, AlphaZero [31 ] and MuZero [ 27 ] are developed based on a variant of UCB, P-UCT [ 25 ] and have achieved great success on board games and Atari games. The formula of P-UCT is the Eq (1):  \n\n$$\na^{k}=\\arg\\operatorname*{max}_{a\\in\\mathcal{A}}Q(s,a)+P(s,a)\\frac{\\sqrt{\\sum_{b\\in\\mathcal{A}}N(s,b)}}{1+N(s,a)}(c_{1}+\\log((\\sum_{b\\in\\mathcal{A}}N(s,b)+c_{2}+1)/c_{2})),\n$$  \n\nwhere $k$ is the index of iteration, $\\boldsymbol{\\mathcal{A}}$ is the acti $Q(s,a)$ is the estimated Q-value, $P(s,a)$ is the policy prior obtained from neural networks, $N(s,a)$ is the visitations to select the action a from the state $s$ and $c_{1},c_{2}$ are hyper-parameters. The output of MCTS is the visitation of each action of the root node. After $N$ search iterations, the final policy $\\pi(s)$ is defined as the normalized root visitation distribution simplification, we use $\\pi_{N}(s)$ $\\pi_{k}$ in place of , where $\\begin{array}{r}{\\pi_{k}(s,a)=N(s,a)/\\sum_{b\\in\\mathcal{A}}N(s,b)=N(s,a)/k,a\\in\\mathcal{A}}\\end{array}$ $\\pi_{k}(s)$ sometimes. And the detailed procedure of MCTS is \u2208A . For introduced in Appendix. In our method, we propose to approximate the final policy $\\pi_{N}(s)$ with $\\hat{\\pi}_{k}(s)$ ,which we name as a virtual expanded policy, through a new expansion method and a termination rule. In this way, the number of iterations in MCTS can be reduced from $N$ to $k$ .\n\n# 3.2 Computation Requirement\nMost of the computations in MCTS-based RL are in the MCTS procedure. Each action taken by MCTS requires $N$ times neural network evaluations, where $N$ is a constant number of iterations in the search loop. Traditional RL algorithms, such as PPO [ 28 ] or DQN [ 23 ], only need a single neural network evaluation per action. Thus, MCTS-based RL is roughly $N$ times computationally more expensive than traditional RL algorithms. In practice, training a single Atari game needs 12 hours of computation time on 40 TPUs [ 27 ]. The computation need is roughly two orders of magnitude more than traditional RL algorithms [28], although the final performance of MuZero is much better.\n\n# 4 Method\nWe aim to spend more search time on harder states and less on easier states. Intuitively, human knows when to make a quick decision or a slow decision under different circumstances. Unfortunately, this situation-aware behavior is absent in current MCTS algorithms. Therefore, we propose an MCTS variant that terminates the search iteration adaptively. It consists of two components: a novel expansion method named virtual expansion to estimate the final visitation based on the current partial tree; a termination rule that decides when to terminate based on the hardness of the current scenario. And we will display the adaptive mechanism through visualizations in Section 5.5."}, {"ref_id": "454845581169535994", "chunk_id": "7", "score": 0.484375, "text": "# Spending Thinking Time Wisely: Accelerating MCTS with Virtual Expansions\nWeirui $\\mathbf{Ye}^{*}$ Pieter Abbeel \u2020Yang Gao $\\ast\\ddag\\S$ \u2217Tsinghua University, \u2020UC Berkeley, \u00a7Shanghai Qi Zhi Institute\n\n# Abstract\nOne of the most important AI research questions is to trade off computation versus performance since \u201cperfect rationality\" exists in theory but is impossible to achieve in practice. Recently, Monte-Carlo tree search (MCTS) has attracted considerable attention due to the significant performance improvement in various challenging domains. However, the expensive time cost during search severely restricts its scope for applications. This paper proposes the Virtual MCTS (V-MCTS), a variant of MCTS that spends more search time on harder states and less search time on simpler states adaptively. We give theoretical bounds of the proposed method and evaluate the performance and computations on $9\\times9$ Go board games and Atari games. Experiments show that our method can achieve comparable performances to the original search algorithm while requiring less than $50\\%$ search time on average. We believe that this approach is a viable alternative for tasks under limited time and resources. The code is available at https://github.com/YeWR/V-MCTS.git .\n\n# 1 Introduction\nWhen artificial intelligence was first studied in the 1950s, researchers have sought to find the solution to the question \u201cHow to build an agent with perfect rationality\". The term \u201cperfect rationality\" [7 ,24 ,26 ] here refers to the decision made with infinite amounts of computations. However, one can only solve small-scale problems without considering the practical computation time since classical search algorithms usually exhibit exponential running time. Therefore, recent AI research would no longer seek to achieve \u201cperfect rationality\", but instead carefully trade-off computation versus the level of rationality. People have developed computational models like \u201cbounded optimality\" to model these settings [ 26 ]. The increasing level of rationality under the same computational budget has given us a lot of AI successes. Algorithms include the Monte-Carlo sampling algorithms, the variational inference algorithms, and using DNNs as universal function approximators [9, 8, 13, 30, 17].  \n\nRecently, MCTS-based RL algorithms have achieved much success, mainly on board games. The most notable achievement is that AlphaGo beats Hui Fan in 2015 [ 30 ]. It is the first time a computer program beat a human professional Go player. Afterward, AlphaGo beats two top-ranking human players, Lee Sedol in 2016 and Jie Ke in 2017, the latter of which ranked first worldwide at the time. Later, MCTS-based RL algorithms were further extended to other board games and Atari games [ 27 ]. EfficientZero [ 34 ] significantly improves the sample efficiency of MCTS-based RL algorithms, shedding light on its future applications in the real world like robotics and self-driving.  \n\nDespite the impressive performance of MCTS-based RL algorithms, they require massive amounts of computation to train and evaluate. For example, MuZero [ 27 ] used 1000 TPUs trained for 12 hours to learn the game of Go, and for a single Atari game, it needs 40 TPUs to train 12 hours. Compared to previous algorithms on the Atari games benchmark, it needs around two orders of magnitude more compute. This prohibitively large computational requirement has slowed down both the further development of MCTS-based RL algorithms as well as its practical use.  \n\nUnder the hood, MCTS-based RL algorithms imagine the futures when taking different future action sequences. However, this imaging process for the current method is not computationally efficient. For example, AlphaGo needs to look ahead 1600 game states to place a single stone. On the contrary, top human professional players can only think through around 100-200 game states per minute [ 30 ]. Apart from the inefficiency, the current MCTS algorithm deals with easy and challenging cases with the same computational budget. However, human knows to use their time when it is most needed.  \n\nIn this paper, we aim to design new algorithms that save the computational time of the MCTSbased RL methods. We make three key contributions : (1) We present Virtual MCTS, a variant of MCTS, to approximate the vanilla MCTS search policies with less computation. Moreover, unlike previous pruning-based methods that focus on the selection or evaluation stage in MCTS, our method improves the search loop. It terminates the search iterations earlier adaptively when current states are simpler; (2) Theoretically, we provide some error bounds of the proposed method. Furthermore, the visualization results indicate that Virtual MCTS has a better computation and performance trade-off than vanilla MCTS; (3) Empirically, our method can save more than $50\\%$ of search times on the challenging game Go $9\\times9$ and more than $60\\%$ on the visually complex Atari games while keeping comparable performances to those of vanilla MCTS.\n\n# 2 Related Work\n\n# 2.1 Reinforcement Learning with MCTS\nFor a long time, Computer Go has been regarded as a remarkably challenging game [ 3 ,6 ]. Researchers attempt to use Monte-Carlo techniques that evaluate the value of the node state through random playouts [ 4 ,11 ,12 ,30 ]. Afterward, UCT algorithms have generally been applied in Monte-Carlo tree search (MCTS) algorithms, which use UCB1 to select action at each node of the tree [ 20 ]. Recently, MCTS-based RL methods [ 30 ,32 ,31 ,27 ] have become increasingly popular and achieved super-human performances on board games because of their strong ability to search.  \n\nModern MCTS-based RL algorithms include four stages in the search loop : selection, expansion, evaluation, and backpropagation. The computation bottlenecks in vanilla MCTS come from the search loop, especially for the evaluation stage and the selection stage of each iteration. The selection stage is time-consuming when the search tree becomes wider and deeper. The evaluation stage is quite expensive because people attempt to evaluate the node value by random playouts to the end in previous researches. Due to the search loop, MCTS-based algorithms have multiple model inferences compared to other model-free RL methods like PPO [28] and SAC [16]."}, {"ref_id": "454845771530662550", "chunk_id": "1", "score": 0.458984375, "text": "# 5 Experiment\nTo examine the performance of MCTS-VS, we conduct experiments on different tasks, including synthetic functions, NAS-bench problems and MuJoCo locomotion tasks, to compare MCTS-VS with other black-box optimization methods. For MCTS-VS, we use the same hyper-parameters except $C_{p}$ , which is used for calculating UCB in Eq. (1). For Dropout and embedding-based methods, we set the parameter $d$ to the number of valid dimensions for synthetic functions, and a reasonable value for real-world problems. The hyper-parameters of the same components of different methods are set to the same. We use five identical random seeds (2021\u20132025) for all problems and methods. More details about the settings can be found in Appendix C. Our code is available at https://github.com/lamda-bbo/MCTS-VS ."}], "task_step_question_answer": "", "ref_task_step_id": ""}, "__type__": "task_step_node"}, "64b86178-229d-4430-83ef-e052788e3896": {"__data__": {"id_": "64b86178-229d-4430-83ef-e052788e3896", "metadata": {}, "relationships": {}, "hash": "", "start_task_context": "MCTS\u5728PRM\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u7684\u5e94\u7528\u63a2\u7d22\u7efc\u8ff0", "aemo_representation_context": "### Step 1: \u5206\u6790\u8fd1\u51e0\u5e74\u7814\u7a76\u9886\u57df\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba\n\n**\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09**\uff1a\n- **\u57fa\u672c\u539f\u7406**\uff1aMCTS\u662f\u4e00\u79cd\u7528\u4e8e\u51b3\u7b56\u8fc7\u7a0b\u7684\u542f\u53d1\u5f0f\u641c\u7d22\u7b97\u6cd5\uff0c\u5e7f\u6cdb\u5e94\u7528\u4e8e\u6e38\u620fAI\u3001\u8def\u5f84\u89c4\u5212\u7b49\u9886\u57df\u3002\u5176\u6838\u5fc3\u601d\u60f3\u662f\u901a\u8fc7\u6a21\u62df\u591a\u6b21\u968f\u673a\u6e38\u620f\u6765\u8bc4\u4f30\u4e0d\u540c\u51b3\u7b56\u7684\u4f18\u52a3\u3002\n- **\u6280\u672f\u6846\u67b6**\uff1a\u4e3b\u8981\u5305\u62ec\u9009\u62e9\uff08Selection\uff09\u3001\u6269\u5c55\uff08Expansion\uff09\u3001\u6a21\u62df\uff08Simulation\uff09\u548c\u56de\u6eaf\uff08Backpropagation\uff09\u56db\u4e2a\u6b65\u9aa4\u3002\n\n**\u504f\u597d\u7b56\u7565\u6a21\u578b\uff08PRM\uff09**\uff1a\n- **\u57fa\u672c\u539f\u7406**\uff1aPRM\u65e8\u5728\u901a\u8fc7\u5b66\u4e60\u7528\u6237\u7684\u504f\u597d\u6765\u4f18\u5316\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5e38\u7528\u4e8e\u63a8\u8350\u7cfb\u7edf\u3001\u4e2a\u6027\u5316\u670d\u52a1\u7b49\u3002\n- **\u6280\u672f\u6846\u67b6**\uff1a\u901a\u5e38\u5305\u62ec\u504f\u597d\u5efa\u6a21\u3001\u7b56\u7565\u4f18\u5316\u548c\u6a21\u578b\u5fae\u8c03\u7b49\u73af\u8282\u3002\n\n**\u65b9\u6cd5\u8bba**\uff1a\n- **\u96c6\u6210\u5b66\u4e60**\uff1a\u5c06MCTS\u4e0ePRM\u7ed3\u5408\uff0c\u5229\u7528MCTS\u7684\u641c\u7d22\u80fd\u529b\u4f18\u5316PRM\u7684\u51b3\u7b56\u8fc7\u7a0b\u3002\n- **\u5f3a\u5316\u5b66\u4e60**\uff1a\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u673a\u5236\u4e0d\u65ad\u8c03\u6574PRM\u7684\u53c2\u6570\uff0c\u4ee5\u9002\u5e94\u52a8\u6001\u73af\u5883\u3002\n\n### Step 2: \u7814\u7a76\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u4e3b\u8981\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0e\u53d8\u4f53\n\n**\u5e94\u7528\u573a\u666f**\uff1a\n- **\u6e38\u620fAI**\uff1aMCTS\u4e0ePRM\u7ed3\u5408\u7528\u4e8e\u6e38\u620f\u4e2d\u7684\u667a\u80fd\u51b3\u7b56\uff0c\u5982AlphaGo\u3002\n- **\u8def\u5f84\u89c4\u5212**\uff1a\u5728\u673a\u5668\u4eba\u5bfc\u822a\u548c\u81ea\u52a8\u9a7e\u9a76\u4e2d\uff0c\u5229\u7528MCTS\u4f18\u5316\u8def\u5f84\u9009\u62e9\u3002\n- **\u63a8\u8350\u7cfb\u7edf**\uff1a\u901a\u8fc7PRM\u5b66\u4e60\u7528\u6237\u504f\u597d\uff0c\u7ed3\u5408MCTS\u8fdb\u884c\u66f4\u7cbe\u51c6\u7684\u63a8\u8350\u3002\n\n**\u53d8\u4f53**\uff1a\n- **\u6df1\u5ea6MCTS**\uff1a\u7ed3\u5408\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff0c\u63d0\u5347MCTS\u7684\u51b3\u7b56\u80fd\u529b\u3002\n- **\u591a\u6a21\u6001PRM**\uff1a\u878d\u5408\u591a\u79cd\u6570\u636e\u6e90\uff08\u5982\u6587\u672c\u3001\u56fe\u50cf\uff09\uff0c\u589e\u5f3aPRM\u7684\u6cdb\u5316\u80fd\u529b\u3002\n\n### Step 3: \u8bc4\u4f30\u5b66\u672f\u754c\u7684\u6280\u672f\u8fdb\u6b65\u4e0e\u5c40\u9650\u6027\n\n**\u6280\u672f\u8fdb\u6b65**\uff1a\n- **\u6027\u80fd\u63d0\u5347**\uff1aMCTS\u4e0ePRM\u7ed3\u5408\u663e\u8457\u63d0\u5347\u4e86\u51b3\u7b56\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002\n- **\u6cdb\u5316\u80fd\u529b**\uff1a\u591a\u6a21\u6001PRM\u5728\u5904\u7406\u591a\u6837\u5316\u6570\u636e\u65f6\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002\n\n**\u5c40\u9650\u6027**\uff1a\n- **\u8ba1\u7b97\u590d\u6742\u5ea6**\uff1aMCTS\u5728\u5927\u89c4\u6a21\u95ee\u9898\u4e2d\u8ba1\u7b97\u91cf\u5927\uff0c\u6548\u7387\u6709\u5f85\u63d0\u5347\u3002\n- **\u6570\u636e\u4f9d\u8d56**\uff1aPRM\u7684\u6027\u80fd\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u9ad8\u8d28\u91cf\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u6570\u636e\u504f\u5dee\u95ee\u9898\u4f9d\u7136\u5b58\u5728\u3002\n\n### Step 4: \u63a2\u8ba8\u8ba1\u7b97\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\n\n**\u9002\u7528\u6027**\uff1a\n- **\u591a\u9886\u57df\u5e94\u7528**\uff1aMCTS+PRM\u6846\u67b6\u5728\u6e38\u620f\u3001\u8def\u5f84\u89c4\u5212\u3001\u63a8\u8350\u7cfb\u7edf\u7b49\u591a\u4e2a\u9886\u57df\u8868\u73b0\u51fa\u826f\u597d\u7684\u9002\u7528\u6027\u3002\n- **\u591a\u6a21\u6001\u6570\u636e**\uff1a\u591a\u6a21\u6001PRM\u80fd\u591f\u5904\u7406\u591a\u79cd\u7c7b\u578b\u7684\u6570\u636e\uff0c\u9002\u7528\u4e8e\u590d\u6742\u5e94\u7528\u573a\u666f\u3002\n\n**\u6cdb\u5316\u80fd\u529b**\uff1a\n- **\u8de8\u9886\u57df\u6cdb\u5316**\uff1a\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\uff0cMCTS+PRM\u6846\u67b6\u5728\u4e0d\u540c\u9886\u57df\u95f4\u5c55\u73b0\u51fa\u8f83\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002\n- **\u52a8\u6001\u73af\u5883\u9002\u5e94**\uff1a\u5f3a\u5316\u5b66\u4e60\u673a\u5236\u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u9002\u5e94\u52a8\u6001\u53d8\u5316\u7684\u73af\u5883\u3002\n\n### Step 5: \u5206\u6790\u6700\u65b0\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027\n\n**\u7a33\u5b9a\u6027**\uff1a\n- **\u7b97\u6cd5\u4f18\u5316**\uff1a\u901a\u8fc7\u6539\u8fdbMCTS\u7684\u9009\u62e9\u7b56\u7565\u548cPRM\u7684\u53c2\u6570\u8c03\u6574\u673a\u5236\uff0c\u63d0\u5347\u4e86\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u3002\n- **\u9c81\u68d2\u6027\u589e\u5f3a**\uff1a\u5f15\u5165\u5bf9\u6297\u8bad\u7ec3\u548c\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\uff0c\u63d0\u9ad8\u6a21\u578b\u5728\u590d\u6742\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u6027\u3002\n\n**\u5bb9\u9519\u6027**\uff1a\n- **\u9519\u8bef\u5904\u7406**\uff1a\u5728\u6a21\u62df\u548c\u56de\u6eaf\u9636\u6bb5\u5f15\u5165\u5bb9\u9519\u673a\u5236\uff0c\u51cf\u5c11\u9519\u8bef\u51b3\u7b56\u7684\u5f71\u54cd\u3002\n- **\u5927\u89c4\u6a21\u6570\u636e\u9002\u5e94\u6027**\uff1a\u4f18\u5316\u7b97\u6cd5\u7ed3\u6784\uff0c\u63d0\u5347\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\u7684\u5904\u7406\u80fd\u529b\u3002\n\n### Step 6: \u8bc4\u4f30\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218\n\n**\u672a\u6765\u7814\u7a76\u65b9\u5411**\uff1a\n- **\u9ad8\u6548\u8ba1\u7b97**\uff1a\u7814\u7a76\u66f4\u9ad8\u6548\u7684MCTS\u7b97\u6cd5\uff0c\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002\n- **\u6570\u636e\u589e\u5f3a**\uff1a\u63a2\u7d22\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u51cf\u5c11PRM\u5bf9\u9ad8\u8d28\u91cf\u6570\u636e\u7684\u4f9d\u8d56\u3002\n\n**\u6311\u6218**\uff1a\n- **\u8de8\u6a21\u6001\u878d\u5408**\uff1a\u5982\u4f55\u6709\u6548\u878d\u5408\u591a\u79cd\u6a21\u6001\u6570\u636e\uff0c\u63d0\u5347\u6a21\u578b\u7684\u7efc\u5408\u6027\u80fd\u3002\n- **\u52a8\u6001\u73af\u5883\u9002\u5e94**\uff1a\u5728\u52a8\u6001\u73af\u5883\u4e0b\uff0c\u5982\u4f55\u4fdd\u6301\u6a21\u578b\u7684\u7a33\u5b9a\u6027\u548c\u9ad8\u6548\u6027\u3002\n\n**\u63a8\u52a8\u540e\u7eed\u7814\u7a76**\uff1a\n- **\u65b0\u95ee\u9898\u63d0\u51fa**\uff1a\u5982\u591a\u6a21\u6001\u6570\u636e\u4e0b\u7684\u504f\u597d\u5efa\u6a21\u3001\u52a8\u6001\u73af\u5883\u4e0b\u7684\u51b3\u7b56\u4f18\u5316\u7b49\u3002\n- **\u7814\u7a76\u5207\u5165\u70b9**\uff1a\u4ece\u7b97\u6cd5\u4f18\u5316\u3001\u6570\u636e\u589e\u5f3a\u3001\u8de8\u6a21\u6001\u878d\u5408\u7b49\u591a\u4e2a\u89d2\u5ea6\u5207\u5165\uff0c\u63a8\u52a8\u6280\u672f\u8fdb\u6b65\u3002\n\n### \u603b\u7ed3\n\n\u8fd1\u5e74\u6765\uff0cMCTS\u5728PRM\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u7684\u5e94\u7528\u63a2\u7d22\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4e3b\u8981\u4f53\u73b0\u5728\u6280\u672f\u6846\u67b6\u7684\u5b8c\u5584\u3001\u5e94\u7528\u573a\u666f\u7684\u62d3\u5c55\u3001\u6027\u80fd\u7684\u63d0\u5347\u4ee5\u53ca\u7a33\u5b9a\u6027\u548c\u5bb9\u9519\u6027\u7684\u589e\u5f3a\u3002\u7136\u800c\uff0c\u4ecd\u5b58\u5728\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u3001\u6570\u636e\u4f9d\u8d56\u6027\u5f3a\u7b49\u5c40\u9650\u6027\u3002\u672a\u6765\u7684\u7814\u7a76\u5e94\u5173\u6ce8\u9ad8\u6548\u8ba1\u7b97\u3001\u6570\u636e\u589e\u5f3a\u548c\u8de8\u6a21\u6001\u878d\u5408\u7b49\u65b9\u5411\uff0c\u4ee5\u5e94\u5bf9\u52a8\u6001\u73af\u5883\u4e0b\u7684\u51b3\u7b56\u4f18\u5316\u7b49\u6311\u6218\uff0c\u63a8\u52a8\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002", "task_step_name": "\u5c40\u9650\u6027", "task_step_description": "MCTS\u5728\u5927\u89c4\u6a21\u95ee\u9898\u4e2d\u8ba1\u7b97\u91cf\u5927\uff0c\u6548\u7387\u6709\u5f85\u63d0\u5347\u3002", "task_step_level": "2>2", "task_step_question": "### \u95ee\u9898\n\n\u5728MCTS\u4e0ePRM\u7ed3\u5408\u7684\u5e94\u7528\u4e2d\uff0cMCTS\u5728\u5927\u89c4\u6a21\u95ee\u9898\u4e2d\u8ba1\u7b97\u91cf\u5927\u3001\u6548\u7387\u4f4e\u7684\u95ee\u9898\u5c24\u4e3a\u7a81\u51fa\u3002\u9488\u5bf9\u8fd9\u4e00\u5c40\u9650\u6027\uff0c\u6709\u54ea\u4e9b\u5177\u4f53\u7684\u6280\u672f\u6216\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u964d\u4f4eMCTS\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u5176\u5728PRM\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u4e2d\u7684\u51b3\u7b56\u6027\u80fd\uff1f", "task_step_question_context": [{"ref_id": "454984236281633338", "chunk_id": "4", "score": 0.306640625, "text": "# 2.2 Acceleration of MCTS\nMCTS-based methods have proved their strong capability of solving complex games or tasks. However, the high computational cost of MCTS hinders its application to some real-time and more general scenarios. Therefore, numerous works are devoted to accelerating MCTS. For example, to make the selection stage more effective, some heuristic pruning methods [ 14 ,33 ,29 ,1 ,2 ] aim to reduce the width and depth of the search tree with some heuristic functions. Furthermore, for more efficient evaluations, Lorentz [ 22 ] proposed early playout termination of MCTS (MCTS-EPT) to stop the random playouts early and use an evaluation function to assess win or loss. Moreover, Hsueh et al. [18 ] applied MCTS-EPT to the Chinese dark chess and proved its effectiveness. Afterward, similar ideas have been applied in the evaluation stage of AlphaGoZero [ 32 ] and later MCTS-based methods [31 ,27 ,34 ]. They evaluate the $Q$ -values through a learnable evaluation network instead of running playouts to the end. Grill et al. [15 ] propose a novel regularized policy optimization method based on AlphaZero to decrease the search budget of MCTS, which is from the optimization perspective. Danihelka et al. [10 ] propose a policy improvement algorithm based on sampling actions without replacement, named Gumbel trick to achieve better performance when planning with few simulations. However, these methods mentioned above focus on the specific stage of the search iteration or reduce the total budget through pruning and optimization methods, which are orthogonal to us. And few works targets at the search loop. Lan et al. [21 ] propose DS-MCTS, which defines the uncertainty of MCTS and approximates it by extra DNNs with specific features for board games in training. During the evaluation, DS-MCTS will check periodically and stop the search if the state is certain.\n\n# 3 Background\nThe AlphaGo series of work [ 30 ,32 ,31 ,27 ] are all MCTS-based reinforcement learning algorithms. Those algorithms assume the environment transition dynamics are known or learn the environment dynamics. Based on the dynamics, they use the Monte-Carlo tree search (MCTS) as the policy improvement operator. I.e., taking in the current policy, MCTS returns a better policy with the search algorithm. The systematic search allows the MCTS-based RL algorithm to quickly improve the policy and perform much better in the setting where heavy reasoning is required.\n\n# 3.1 MCTS\nThis part briefly introduces the MCTS method implemented in reinforcement learning applications. As mentioned in the related works, modern MCTS-based RL algorithms include four stages in the search loop, namely selection, expansion, evaluation, and backpropagation.  \n\nMCTS takes in the current states and generates a policy after the search loop of $N$ iterations. Here $N$ is a constant number of iterations set by the designer, regarded as the total budget. In the selection stage of each iteration, an action will be selected by maximizing over UCB. Specifically, AlphaZero [31 ] and MuZero [ 27 ] are developed based on a variant of UCB, P-UCT [ 25 ] and have achieved great success on board games and Atari games. The formula of P-UCT is the Eq (1):  \n\n$$\na^{k}=\\arg\\operatorname*{max}_{a\\in\\mathcal{A}}Q(s,a)+P(s,a)\\frac{\\sqrt{\\sum_{b\\in\\mathcal{A}}N(s,b)}}{1+N(s,a)}(c_{1}+\\log((\\sum_{b\\in\\mathcal{A}}N(s,b)+c_{2}+1)/c_{2})),\n$$  \n\nwhere $k$ is the index of iteration, $\\boldsymbol{\\mathcal{A}}$ is the acti $Q(s,a)$ is the estimated Q-value, $P(s,a)$ is the policy prior obtained from neural networks, $N(s,a)$ is the visitations to select the action a from the state $s$ and $c_{1},c_{2}$ are hyper-parameters. The output of MCTS is the visitation of each action of the root node. After $N$ search iterations, the final policy $\\pi(s)$ is defined as the normalized root visitation distribution simplification, we use $\\pi_{N}(s)$ $\\pi_{k}$ in place of , where $\\begin{array}{r}{\\pi_{k}(s,a)=N(s,a)/\\sum_{b\\in\\mathcal{A}}N(s,b)=N(s,a)/k,a\\in\\mathcal{A}}\\end{array}$ $\\pi_{k}(s)$ sometimes. And the detailed procedure of MCTS is \u2208A . For introduced in Appendix. In our method, we propose to approximate the final policy $\\pi_{N}(s)$ with $\\hat{\\pi}_{k}(s)$ ,which we name as a virtual expanded policy, through a new expansion method and a termination rule. In this way, the number of iterations in MCTS can be reduced from $N$ to $k$ .\n\n# 3.2 Computation Requirement\nMost of the computations in MCTS-based RL are in the MCTS procedure. Each action taken by MCTS requires $N$ times neural network evaluations, where $N$ is a constant number of iterations in the search loop. Traditional RL algorithms, such as PPO [ 28 ] or DQN [ 23 ], only need a single neural network evaluation per action. Thus, MCTS-based RL is roughly $N$ times computationally more expensive than traditional RL algorithms. In practice, training a single Atari game needs 12 hours of computation time on 40 TPUs [ 27 ]. The computation need is roughly two orders of magnitude more than traditional RL algorithms [28], although the final performance of MuZero is much better.\n\n# 4 Method\nWe aim to spend more search time on harder states and less on easier states. Intuitively, human knows when to make a quick decision or a slow decision under different circumstances. Unfortunately, this situation-aware behavior is absent in current MCTS algorithms. Therefore, we propose an MCTS variant that terminates the search iteration adaptively. It consists of two components: a novel expansion method named virtual expansion to estimate the final visitation based on the current partial tree; a termination rule that decides when to terminate based on the hardness of the current scenario. And we will display the adaptive mechanism through visualizations in Section 5.5."}, {"ref_id": "454984236379937352", "chunk_id": "11", "score": 0.306640625, "text": "# 5.4 Ablation Study\nThe results in the previous section suggest that our method reduces the response time of MCTS while keeping comparable performance on challenging tasks. This section tries to figure out which component contributes to the performance and how the hyperparameters affect it. And we also ablate the effects of different normalization criterions in VET-Rule and the larger budget $(N)$ in MCTS.  \n\nVirtual Expansion In Section 4.2, we introduce the virtual expansion. To prove the effectiveness of virtual expansion, we compare it with another two baseline expansion methods. One is the vanilla expansion, mentioned in Algorithm 1, which returns at iteration $k$ and outputs $\\pi_{k}$ . Another is greedy expansion, wh $N-k$ current best action greedily, where indicating th proces $k=r N$ $k=30$ and $\\hat{\\pi}_{k}(s,a)=\\bigl(N_{k}(s,a)+(N-k)\\mathbf{1}_{b=\\arg\\operatorname*{max}N_{k}(s,b)}\\bigr)/N$ $r=0.2$ $N=150$ $N-k$ .\u2212\u2212times virtual expansion or greedy expansion or nothing, . Briefly, we stop the search We compare the winning rate against the same engine, and the results are listed as Table 3 shows. The winning rate of virtual expansion can achieve $32\\%$ , which is much better than the others. Besides, MCTS with greedy expansion does not work because it over-exploits and results in severe exploration issues. Consequently, virtual expansion can generate a better policy distribution because it can balance exploration and exploitation with UCT.  \n\nTermination Rule It is significant to explore a better termination rule to keep the sound performance while decreasing the tree size as much as possible. As mentioned in Section 4.1, VETRule has two hyperparameters $r,\\epsilon$ . Here $r$ is the factor of the minimum budget $r N$ , and $\\epsilon$ is the minimum distance $\\hat{\\Delta}_{s}(k,k/2)$ . To explore the VET-Rule with better computation and performance trade-off, we do ablations for the different values of $r$ and $\\epsilon$ , respectively. The default values of $r,\\epsilon$ are set to 0 .2 ,0 .1 .  \n\nTable 3: Ablation results of different expansion methods on Go $9\\times9$ for 3 separate training runs.   \n\n\n<html><body><table><tr><td>Algorithm</td><td>Size Avg.</td><td>Winning Rate</td></tr><tr><td>Vanilla expansion</td><td>30</td><td>17%\u00b13.2%</td></tr><tr><td>Greedye expansion</td><td>30</td><td>3%\u00b1 2.0%</td></tr><tr><td>Virtual expansion</td><td>30</td><td>32% \u00b1 3.5%</td></tr></table></body></html>  \n\nFigure 2 compares the winning rate as well as the average tree size across the training stage. Firstly, Figure 3(a) gives the results of different minimum search times factor $r$ . The winning probability is not sensitive to $r$ when $r\\geq0.2$ .ertheless, the average tree size is sensitive to $r$ because V is supposed to search for at least $r N$ times. In addition, there is a performance drop between $r=0.1$ and $r=0.2$ . Therefore, it is reasonable to choose $r=0.2$ to balance the speed and the performance.  \n\nBesides, the comparisons of the different minimum distance $\\epsilon$ are shown in Figure 3(b). A larger $\\epsilon$ makes the tree size smaller because $\\hat{\\Delta}_{s}(k,k/2)<\\epsilon$ is easier to satisfy. In practice, the performance is highly correlated with $\\epsilon$ . In terms of the winning rate, a smaller $\\epsilon$ outperforms a larger one. However, better performances are at the cost of more computations. We suggest selecting an appropriate minimum distance to balance the computation and performance $(r=0.2,\\epsilon=0.1)$ ).  \n\nNormalization criterion in VET-Rule The proposed VET-Rule, $||\\hat{\\pi}_{k}(s)-\\hat{\\pi}_{k/2}(s)||\\;<\\;\\epsilon$ is a termination condition for V-MCTS. And L2 norm is another reasonable choice to amplify the bigger deviations. Therefore, we make ablations of the normalization criterion for the policy distributions. Specifically, we take a pretrained model, and compare the different strategies of L1 norm and L2 norm, namely, $\\left|\\left|\\hat{\\pi}_{k}(s)\\right|^{\\bf2}-\\hat{\\pi}_{k/2}(s)\\right|\\right|_{1}<\\epsilon$ and $\\left|\\left|\\hat{\\pi}_{k}\\dot{(s)}-\\hat{\\pi}_{k/2}(s)\\right|\\right|_{2}<\\epsilon.$ . The results are as Tab. 4 shows. We can find that (1) L2 norm can also work for V-MCTS; (2) L1 norm is better than L2 norm. And we attribute this to the formulation of ucb scores. Because the ucb scores have already taken into account the difference in the visitations (see the $\\mathbf{N}(\\mathbf{s},\\mathbf{a})$ in Eq (1)). Therefore, amplifying the deviations may result in some bias.  \n\nTable 4: Comparison of the winning rate and the average budget with different norm strategies in VETRule. L1 Norm means $\\left|\\left|\\hat{\\pi}_{k}(s)-\\check{\\hat{\\pi}}_{k/2}(s)\\right|\\right|_{1}<\\epsilon$ and L2 Norm means $\\left|\\left|\\hat{\\pi}_{k}(s)-\\hat{\\pi}_{k/2}(s)\\right|\\right|_{2}<\\epsilon$ .  \n\n\n<html><body><table><tr><td></td><td>Average budget</td><td>Winningrate</td></tr><tr><td>MCTS (N = 150)</td><td>150</td><td>82.0%</td></tr><tr><td>V-MCTS L1 Norm, N = 150,r = 0.2,E= 0.1</td><td>96.2</td><td>81.5%</td></tr><tr><td>V-MCTS L2 Norm, N = 150,r= 0.2,E= 0.1</td><td>97.1</td><td>79.8%</td></tr><tr><td>V-MCTS L2 Norm, N = 150, r = 0.2, = 0.05</td><td>119.3</td><td>81.0%</td></tr></table></body></html>  \n\nLarger budget $(N)$ in MCTS To investigate whether our method still holds with larger amounts of MCTS expansions, we take a pretrained model and compare two strategies: (1) vanilla expansion with $\\mathrm{N{=}150/400/600/800}$ nodes in MCTS (2) virtual expanded policy with $N=800,r=0.2,\\epsilon=0.1$ .The results are listed in Tab. 5. The result shows that (1) V-MCTS $\\mathit{\\Omega}^{N}=800,r=0.2,\\epsilon=0.1)$ is better than MCTS ( $N=600)$ ) in both the average budget and the winning rate, (2) V-MCTS can achieve comparable performance to the oracle MCTS( $N=800)$ ) while keeping much less average budget. Therefore, V-MCTS works with a larger amount of MCTS expansions.  \n\nTable 5: Comparison of the winning rate and the average budget with larger amounts of MCTS expansions. Here the hyper-parameters of our method are $N=800,r=0.2,\\epsilon=0.1$ .  \n\n\n<html><body><table><tr><td>MCTS</td><td>N = 150</td><td>N = 400</td><td>N=600</td><td>N=800</td><td>Ours</td></tr><tr><td>Average budget Winningrate</td><td>150 82.0%</td><td>400 84.5%</td><td>600 84.9%</td><td>800 85.9%</td><td>431.1 85.0%</td></tr></table></body></html>  \n\n  \nFigure 3: Heatmap of policy distributions from the MCTS ( $N=150)$ ) and the V-MCTS. The agent play as Black in (a) and White in (b) against the GnuGo (level 10). Our agent wins in both of the games. A darker red color represents larger visitations of the corresponding action. The V-MCTS will terminate with different search times $k$ according to the situations and generate a near-oracle policy distribution."}, {"ref_id": "454984236248078902", "chunk_id": "2", "score": 0.236328125, "text": "# DSensitivity Analysis of Hyper-parameters of MCTS-VS\nWe provide further studies to examine the influence of the hyper-parameters of MCTS-VS, including the employed optimization algorithm for optimizing the selected variables in each iteration, the \u201cfillin\u201d strategy, the hyper-parameter $k$ used in the best$k$ strategy, the hyper-parameter $C_{p}$ for calculating UCB in Eq. (1), the number $2\\,\\times\\,N_{v}\\,\\times\\,N_{s}$ sampled data in each iteration, the threshold $N_{b a d}$ for re-initializing a tree, and the threshold $N_{s p l i t}$ for splitting a tree node.  \n\nThe optimization algorithm is employed by MCTS-VS to optimize the selected variables in each iteration. We compare three different optimization algorithms, i.e., random search (RS), BO and TuRBO. First, we conduct experiments similar to \u201cEffectiveness of Variable Selection\u201d in Section 5.1, to show the effectiveness of MCTS-VS even when equipped with RS. Figure 6 shows that MCTSVS-RS is better than Dropout-RS and RS, revealing the advantage of MCTS-VS.  \n\n  \nFigure 6: Effectiveness of MCTS-VS when equipped with RS.  \n\nNext we compare the performance of MCTS-VS equipped with RS, BO and TuRBO, by experiments on the Hartmann functions with increasing ratio of valid variables. Hartmann 6 _500 has 6 valid variables. Hartmann 6 _5 _500 is generated by mixing 5 Hartmann 6 functions as Hartmann 6 $(\\pmb{x}_{1:6})+$ Hartmann 6 $\\backslash(\\pmb{x}_{7:12})+\\cdot\\cdot\\cdot+\\mathrm{Hartmann6}(\\pmb{x}_{25:30})$ , and appending 470 unrelated dimensions, where $\\pmb{x}_{i:j}$ denotes the $i$ -th to j-th variables. Hartmann 6 _10 _500 is generated alike. Thus, Hartmann 6 _5 _500 and Hartmann 6 _10 _500 have 30 and 60 valid variables, respectively. The results in Figure 7 show that as the ratio of valid variables increases, MCTS-VS-TuRBO gradually surpasses MCTS-VS-RS and MCTS-VS-BO, while MCTS-VS-RS becomes worse and worse. This is expected. If the ratio of valid variables is high, MCTS-VS is more likely to select the valid variables, so it is worth to use the expensive optimization algorithm, e.g., TuRBO, to optimize the selected variables. If the ratio is low, unrelated variables are more likely to be selected most of the time, so using a cheap optimization algorithm would be better. These observations also give us some guidance on selecting optimization algorithms in practice.  \n\n\u201cFill-in\u201d strategy is a basic component of variable selection methods, which influences the quality of the value of unselected variables. We compare the employed best$k$ strategy $(k=20)$ ) with the average best$k$ strategy and the random strategy. The average best$k$ strategy uses the average of the best $k$ data points for the unselected variables, and the random strategy samples the value of an unselected variable from its domain randomly. As shown in Figure 8(a), the random strategy leads to the poor performance of MCTS-VS-BO, which may be because it does not utilize the historical information and leads to over-exploration. The best${\\cdot k}$ strategy utilizes the historical points that have high objective values to fill in the unselected variables, thus behaving much better. The performance of the average strategy is between the best$k$ and random strategies. We recommend using the best$k$ strategy in practice.  \n\nThe hyper-parameter $k$ used in the best$k$ strategy controls the degree of exploitation for the unselected variables. As shown in Figure 8(b), a smaller $k$ encourages exploitation, which results in better performance in the early stage, but easily leads to premature convergence. A larger $k$ encourages exploration and behaves worse in the early stage, but may converge to a better value. We recommend using a larger $k$ if allowing enough evaluations.  \n\n  \nFigure 7: Sensitivity analysis of the optimization algorithm.  \n\n  \nFigure 8: Sensitivity analysis of the \u201cfill-in\u201d strategy and the hyper-parameter $k$ of the best$k$ strategy, using MCTS-VS-BO on Hartmann 6 _300 .  \n\nThe hyper-parameter $C_{p}$ for calculating UCB in Eq. (1) balances the exploration and exploitation of MCTS. As shown in Figure 9, a too small $C_{p}$ leads to relatively worse performance, highlighting the importance of exploration. A too large $C_{p}$ may also lead to over-exploration. But overall MCTSVS is not very sensitive to $C_{p}$ . We recommend setting $C_{p}$ between $1\\%$ and $10\\%$ of the optimum (i.e., max $f({\\boldsymbol{x}}))$ ), which is consistent with that for LA-MCTS [40].  \n\n  \nFigure 9: Sensitivity analysis of the hyper-parameter $C_{p}$ for calculating UCB in Eq. (1), using MCTS-VS-BO on Levy and Hartmann.  \n\nThe number $2\\,\\times\\,N_{v}\\,\\times\\,N_{s}$ of sampled data in ch iteration depends on the batch size $N_{v}$ of variable index subset and the sample batch size $N_{s}$ , and will influence the accuracy of estimating the variable score vector in Eq. (2). If we increase $N_{v}$ and $N_{s}$ , we can calculate the variable score more accurately, but also need more evaluations. Figure 10(a) shows that given the same number of evaluations, MCTS-VS-BO achieves the best performance when $N_{v}=2$ and $N_{s}=3$ . Thus, this setting may be a good choice to balance the accuracy of variable score and the number of evaluations, which is also used throughout the experiments.  \n\nThe threshold $N_{b a d}$ for re-initializing a tree controls the tolerance of selecting bad tree nodes (i.e., nodes containing unimportant variables). A smaller $N_{b a d}$ leads to frequent re-initialization, which can adjust quickly but may cause under-exploitation of the tree. A larger $N_{b a d}$ can make full use of the tree, but may optimize too much on unimportant variables. Figure 10(b) shows that MCTS-VS achieves the best performance when $N_{b a d}=5$ . Thus, we recommend to use this setting, to balance the re-initialization and exploitation of the tree.  \n\nThe threshold $N_{s p l i t}$ for splitting a node. If the number of variables in a node is larger than $N_{s p l i t}$ ,the node can be further partitioned. That is, the parameter $N_{s p l i t}$ controls the least number of variables in a leaf node and thus affects the number of selected variables, which has a direct influence on the wall clock time. Note that MCTS-VS selects a leaf node and optimizes the variables contained by this node in each iteration. The smaller $N_{s p l i t}$ , the shorter the time. Figure 10(c) shows that $N_{s p l i t}$ has little influence on the performance of MCTS-VS-BO, and thus we recommend to set $N_{s p l i t}=3$ to reduce the wall clock time.  \n\n  \nFigure 10: S vity analysis of the number $2\\,\\times\\,N_{v}\\,\\times\\,N_{s}$ pled data in each iteration, the threshold $N_{b a d}$ for re-initializing a tree and the threshold $N_{s p l i t}$ for splitting a node, using MCTS-VS-BO on Hartmann 6 _300 .  \n\nInfluence of the hyper-parameters on the runtime of MCTS-VS. We also provide some intuitive explanation about the influence of the hyper-parameters on the runtime. The threshold $N_{s p l i t}$ for splitting a node has a direct impact on the runtime, because it controls the least number of variables to be optimized in a leaf node. That is, the runtime will increase with $N_{s p l i t}$ . Other parameters may affect the depth of the tree and thus the runtime. For the threshold $N_{b a d}$ for re-initializing a tree, if it is set to a small value, MCTS-VS will re-build the tree frequently and the depth of the tree is small. The shallow nodes have more variables, leading to more runtime to optimize. For the hyper-parameter $C_{p}$ for calculating UCB, if it is set to a large value, the exploration is preferred and MCTS-VS will tend to select the right node (regarded as containing unimportant variables). The tree thus will be re-built freq tly, ding to more runtime. For the number $2\\,\\times\\,N_{v}\\,\\times\\,N_{s}$ of sampled data at each iteration, if $N_{v}$ and $N_{s}$ are set to large values, the depth of the tree will be small given the total number of evaluations, and thus lead to more runtime."}], "task_step_question_answer": "", "ref_task_step_id": ""}, "__type__": "task_step_node"}, "6dba87e8-762d-4764-9182-3555b4b50a86": {"__data__": {"id_": "6dba87e8-762d-4764-9182-3555b4b50a86", "metadata": {}, "relationships": {}, "hash": "", "start_task_context": "MCTS\u5728PRM\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u7684\u5e94\u7528\u63a2\u7d22\u7efc\u8ff0", "aemo_representation_context": "### Step 1: \u5206\u6790\u8fd1\u51e0\u5e74\u7814\u7a76\u9886\u57df\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba\n\n**\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09**\uff1a\n- **\u57fa\u672c\u539f\u7406**\uff1aMCTS\u662f\u4e00\u79cd\u7528\u4e8e\u51b3\u7b56\u8fc7\u7a0b\u7684\u542f\u53d1\u5f0f\u641c\u7d22\u7b97\u6cd5\uff0c\u5e7f\u6cdb\u5e94\u7528\u4e8e\u6e38\u620fAI\u3001\u8def\u5f84\u89c4\u5212\u7b49\u9886\u57df\u3002\u5176\u6838\u5fc3\u601d\u60f3\u662f\u901a\u8fc7\u6a21\u62df\u591a\u6b21\u968f\u673a\u6e38\u620f\u6765\u8bc4\u4f30\u4e0d\u540c\u51b3\u7b56\u7684\u4f18\u52a3\u3002\n- **\u6280\u672f\u6846\u67b6**\uff1a\u4e3b\u8981\u5305\u62ec\u9009\u62e9\uff08Selection\uff09\u3001\u6269\u5c55\uff08Expansion\uff09\u3001\u6a21\u62df\uff08Simulation\uff09\u548c\u56de\u6eaf\uff08Backpropagation\uff09\u56db\u4e2a\u6b65\u9aa4\u3002\n\n**\u504f\u597d\u7b56\u7565\u6a21\u578b\uff08PRM\uff09**\uff1a\n- **\u57fa\u672c\u539f\u7406**\uff1aPRM\u65e8\u5728\u901a\u8fc7\u5b66\u4e60\u7528\u6237\u7684\u504f\u597d\u6765\u4f18\u5316\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5e38\u7528\u4e8e\u63a8\u8350\u7cfb\u7edf\u3001\u4e2a\u6027\u5316\u670d\u52a1\u7b49\u3002\n- **\u6280\u672f\u6846\u67b6**\uff1a\u901a\u5e38\u5305\u62ec\u504f\u597d\u5efa\u6a21\u3001\u7b56\u7565\u4f18\u5316\u548c\u6a21\u578b\u5fae\u8c03\u7b49\u73af\u8282\u3002\n\n**\u65b9\u6cd5\u8bba**\uff1a\n- **\u96c6\u6210\u5b66\u4e60**\uff1a\u5c06MCTS\u4e0ePRM\u7ed3\u5408\uff0c\u5229\u7528MCTS\u7684\u641c\u7d22\u80fd\u529b\u4f18\u5316PRM\u7684\u51b3\u7b56\u8fc7\u7a0b\u3002\n- **\u5f3a\u5316\u5b66\u4e60**\uff1a\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u673a\u5236\u4e0d\u65ad\u8c03\u6574PRM\u7684\u53c2\u6570\uff0c\u4ee5\u9002\u5e94\u52a8\u6001\u73af\u5883\u3002\n\n### Step 2: \u7814\u7a76\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u4e3b\u8981\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0e\u53d8\u4f53\n\n**\u5e94\u7528\u573a\u666f**\uff1a\n- **\u6e38\u620fAI**\uff1aMCTS\u4e0ePRM\u7ed3\u5408\u7528\u4e8e\u6e38\u620f\u4e2d\u7684\u667a\u80fd\u51b3\u7b56\uff0c\u5982AlphaGo\u3002\n- **\u8def\u5f84\u89c4\u5212**\uff1a\u5728\u673a\u5668\u4eba\u5bfc\u822a\u548c\u81ea\u52a8\u9a7e\u9a76\u4e2d\uff0c\u5229\u7528MCTS\u4f18\u5316\u8def\u5f84\u9009\u62e9\u3002\n- **\u63a8\u8350\u7cfb\u7edf**\uff1a\u901a\u8fc7PRM\u5b66\u4e60\u7528\u6237\u504f\u597d\uff0c\u7ed3\u5408MCTS\u8fdb\u884c\u66f4\u7cbe\u51c6\u7684\u63a8\u8350\u3002\n\n**\u53d8\u4f53**\uff1a\n- **\u6df1\u5ea6MCTS**\uff1a\u7ed3\u5408\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff0c\u63d0\u5347MCTS\u7684\u51b3\u7b56\u80fd\u529b\u3002\n- **\u591a\u6a21\u6001PRM**\uff1a\u878d\u5408\u591a\u79cd\u6570\u636e\u6e90\uff08\u5982\u6587\u672c\u3001\u56fe\u50cf\uff09\uff0c\u589e\u5f3aPRM\u7684\u6cdb\u5316\u80fd\u529b\u3002\n\n### Step 3: \u8bc4\u4f30\u5b66\u672f\u754c\u7684\u6280\u672f\u8fdb\u6b65\u4e0e\u5c40\u9650\u6027\n\n**\u6280\u672f\u8fdb\u6b65**\uff1a\n- **\u6027\u80fd\u63d0\u5347**\uff1aMCTS\u4e0ePRM\u7ed3\u5408\u663e\u8457\u63d0\u5347\u4e86\u51b3\u7b56\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002\n- **\u6cdb\u5316\u80fd\u529b**\uff1a\u591a\u6a21\u6001PRM\u5728\u5904\u7406\u591a\u6837\u5316\u6570\u636e\u65f6\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002\n\n**\u5c40\u9650\u6027**\uff1a\n- **\u8ba1\u7b97\u590d\u6742\u5ea6**\uff1aMCTS\u5728\u5927\u89c4\u6a21\u95ee\u9898\u4e2d\u8ba1\u7b97\u91cf\u5927\uff0c\u6548\u7387\u6709\u5f85\u63d0\u5347\u3002\n- **\u6570\u636e\u4f9d\u8d56**\uff1aPRM\u7684\u6027\u80fd\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u9ad8\u8d28\u91cf\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u6570\u636e\u504f\u5dee\u95ee\u9898\u4f9d\u7136\u5b58\u5728\u3002\n\n### Step 4: \u63a2\u8ba8\u8ba1\u7b97\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\n\n**\u9002\u7528\u6027**\uff1a\n- **\u591a\u9886\u57df\u5e94\u7528**\uff1aMCTS+PRM\u6846\u67b6\u5728\u6e38\u620f\u3001\u8def\u5f84\u89c4\u5212\u3001\u63a8\u8350\u7cfb\u7edf\u7b49\u591a\u4e2a\u9886\u57df\u8868\u73b0\u51fa\u826f\u597d\u7684\u9002\u7528\u6027\u3002\n- **\u591a\u6a21\u6001\u6570\u636e**\uff1a\u591a\u6a21\u6001PRM\u80fd\u591f\u5904\u7406\u591a\u79cd\u7c7b\u578b\u7684\u6570\u636e\uff0c\u9002\u7528\u4e8e\u590d\u6742\u5e94\u7528\u573a\u666f\u3002\n\n**\u6cdb\u5316\u80fd\u529b**\uff1a\n- **\u8de8\u9886\u57df\u6cdb\u5316**\uff1a\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\uff0cMCTS+PRM\u6846\u67b6\u5728\u4e0d\u540c\u9886\u57df\u95f4\u5c55\u73b0\u51fa\u8f83\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002\n- **\u52a8\u6001\u73af\u5883\u9002\u5e94**\uff1a\u5f3a\u5316\u5b66\u4e60\u673a\u5236\u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u9002\u5e94\u52a8\u6001\u53d8\u5316\u7684\u73af\u5883\u3002\n\n### Step 5: \u5206\u6790\u6700\u65b0\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027\n\n**\u7a33\u5b9a\u6027**\uff1a\n- **\u7b97\u6cd5\u4f18\u5316**\uff1a\u901a\u8fc7\u6539\u8fdbMCTS\u7684\u9009\u62e9\u7b56\u7565\u548cPRM\u7684\u53c2\u6570\u8c03\u6574\u673a\u5236\uff0c\u63d0\u5347\u4e86\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u3002\n- **\u9c81\u68d2\u6027\u589e\u5f3a**\uff1a\u5f15\u5165\u5bf9\u6297\u8bad\u7ec3\u548c\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\uff0c\u63d0\u9ad8\u6a21\u578b\u5728\u590d\u6742\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u6027\u3002\n\n**\u5bb9\u9519\u6027**\uff1a\n- **\u9519\u8bef\u5904\u7406**\uff1a\u5728\u6a21\u62df\u548c\u56de\u6eaf\u9636\u6bb5\u5f15\u5165\u5bb9\u9519\u673a\u5236\uff0c\u51cf\u5c11\u9519\u8bef\u51b3\u7b56\u7684\u5f71\u54cd\u3002\n- **\u5927\u89c4\u6a21\u6570\u636e\u9002\u5e94\u6027**\uff1a\u4f18\u5316\u7b97\u6cd5\u7ed3\u6784\uff0c\u63d0\u5347\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\u7684\u5904\u7406\u80fd\u529b\u3002\n\n### Step 6: \u8bc4\u4f30\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218\n\n**\u672a\u6765\u7814\u7a76\u65b9\u5411**\uff1a\n- **\u9ad8\u6548\u8ba1\u7b97**\uff1a\u7814\u7a76\u66f4\u9ad8\u6548\u7684MCTS\u7b97\u6cd5\uff0c\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002\n- **\u6570\u636e\u589e\u5f3a**\uff1a\u63a2\u7d22\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u51cf\u5c11PRM\u5bf9\u9ad8\u8d28\u91cf\u6570\u636e\u7684\u4f9d\u8d56\u3002\n\n**\u6311\u6218**\uff1a\n- **\u8de8\u6a21\u6001\u878d\u5408**\uff1a\u5982\u4f55\u6709\u6548\u878d\u5408\u591a\u79cd\u6a21\u6001\u6570\u636e\uff0c\u63d0\u5347\u6a21\u578b\u7684\u7efc\u5408\u6027\u80fd\u3002\n- **\u52a8\u6001\u73af\u5883\u9002\u5e94**\uff1a\u5728\u52a8\u6001\u73af\u5883\u4e0b\uff0c\u5982\u4f55\u4fdd\u6301\u6a21\u578b\u7684\u7a33\u5b9a\u6027\u548c\u9ad8\u6548\u6027\u3002\n\n**\u63a8\u52a8\u540e\u7eed\u7814\u7a76**\uff1a\n- **\u65b0\u95ee\u9898\u63d0\u51fa**\uff1a\u5982\u591a\u6a21\u6001\u6570\u636e\u4e0b\u7684\u504f\u597d\u5efa\u6a21\u3001\u52a8\u6001\u73af\u5883\u4e0b\u7684\u51b3\u7b56\u4f18\u5316\u7b49\u3002\n- **\u7814\u7a76\u5207\u5165\u70b9**\uff1a\u4ece\u7b97\u6cd5\u4f18\u5316\u3001\u6570\u636e\u589e\u5f3a\u3001\u8de8\u6a21\u6001\u878d\u5408\u7b49\u591a\u4e2a\u89d2\u5ea6\u5207\u5165\uff0c\u63a8\u52a8\u6280\u672f\u8fdb\u6b65\u3002\n\n### \u603b\u7ed3\n\n\u8fd1\u5e74\u6765\uff0cMCTS\u5728PRM\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u7684\u5e94\u7528\u63a2\u7d22\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4e3b\u8981\u4f53\u73b0\u5728\u6280\u672f\u6846\u67b6\u7684\u5b8c\u5584\u3001\u5e94\u7528\u573a\u666f\u7684\u62d3\u5c55\u3001\u6027\u80fd\u7684\u63d0\u5347\u4ee5\u53ca\u7a33\u5b9a\u6027\u548c\u5bb9\u9519\u6027\u7684\u589e\u5f3a\u3002\u7136\u800c\uff0c\u4ecd\u5b58\u5728\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u3001\u6570\u636e\u4f9d\u8d56\u6027\u5f3a\u7b49\u5c40\u9650\u6027\u3002\u672a\u6765\u7684\u7814\u7a76\u5e94\u5173\u6ce8\u9ad8\u6548\u8ba1\u7b97\u3001\u6570\u636e\u589e\u5f3a\u548c\u8de8\u6a21\u6001\u878d\u5408\u7b49\u65b9\u5411\uff0c\u4ee5\u5e94\u5bf9\u52a8\u6001\u73af\u5883\u4e0b\u7684\u51b3\u7b56\u4f18\u5316\u7b49\u6311\u6218\uff0c\u63a8\u52a8\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002", "task_step_name": "\u63a2\u8ba8\u8ba1\u7b97\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u4e0e\u6cdb\u5316\u80fd\u529b", "task_step_description": "MCTS+PRM\u6846\u67b6\u5728\u6e38\u620f\u3001\u8def\u5f84\u89c4\u5212\u3001\u63a8\u8350\u7cfb\u7edf\u7b49\u591a\u4e2a\u9886\u57df\u8868\u73b0\u51fa\u826f\u597d\u7684\u9002\u7528\u6027\u3002", "task_step_level": "3", "task_step_question": "", "task_step_question_context": [], "task_step_question_answer": "", "ref_task_step_id": ""}, "__type__": "task_step_node"}, "d338c352-b6ad-45ff-b9c3-0418aa6cccc4": {"__data__": {"id_": "d338c352-b6ad-45ff-b9c3-0418aa6cccc4", "metadata": {}, "relationships": {}, "hash": "", "start_task_context": "MCTS\u5728PRM\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u7684\u5e94\u7528\u63a2\u7d22\u7efc\u8ff0", "aemo_representation_context": "### Step 1: \u5206\u6790\u8fd1\u51e0\u5e74\u7814\u7a76\u9886\u57df\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba\n\n**\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09**\uff1a\n- **\u57fa\u672c\u539f\u7406**\uff1aMCTS\u662f\u4e00\u79cd\u7528\u4e8e\u51b3\u7b56\u8fc7\u7a0b\u7684\u542f\u53d1\u5f0f\u641c\u7d22\u7b97\u6cd5\uff0c\u5e7f\u6cdb\u5e94\u7528\u4e8e\u6e38\u620fAI\u3001\u8def\u5f84\u89c4\u5212\u7b49\u9886\u57df\u3002\u5176\u6838\u5fc3\u601d\u60f3\u662f\u901a\u8fc7\u6a21\u62df\u591a\u6b21\u968f\u673a\u6e38\u620f\u6765\u8bc4\u4f30\u4e0d\u540c\u51b3\u7b56\u7684\u4f18\u52a3\u3002\n- **\u6280\u672f\u6846\u67b6**\uff1a\u4e3b\u8981\u5305\u62ec\u9009\u62e9\uff08Selection\uff09\u3001\u6269\u5c55\uff08Expansion\uff09\u3001\u6a21\u62df\uff08Simulation\uff09\u548c\u56de\u6eaf\uff08Backpropagation\uff09\u56db\u4e2a\u6b65\u9aa4\u3002\n\n**\u504f\u597d\u7b56\u7565\u6a21\u578b\uff08PRM\uff09**\uff1a\n- **\u57fa\u672c\u539f\u7406**\uff1aPRM\u65e8\u5728\u901a\u8fc7\u5b66\u4e60\u7528\u6237\u7684\u504f\u597d\u6765\u4f18\u5316\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5e38\u7528\u4e8e\u63a8\u8350\u7cfb\u7edf\u3001\u4e2a\u6027\u5316\u670d\u52a1\u7b49\u3002\n- **\u6280\u672f\u6846\u67b6**\uff1a\u901a\u5e38\u5305\u62ec\u504f\u597d\u5efa\u6a21\u3001\u7b56\u7565\u4f18\u5316\u548c\u6a21\u578b\u5fae\u8c03\u7b49\u73af\u8282\u3002\n\n**\u65b9\u6cd5\u8bba**\uff1a\n- **\u96c6\u6210\u5b66\u4e60**\uff1a\u5c06MCTS\u4e0ePRM\u7ed3\u5408\uff0c\u5229\u7528MCTS\u7684\u641c\u7d22\u80fd\u529b\u4f18\u5316PRM\u7684\u51b3\u7b56\u8fc7\u7a0b\u3002\n- **\u5f3a\u5316\u5b66\u4e60**\uff1a\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u673a\u5236\u4e0d\u65ad\u8c03\u6574PRM\u7684\u53c2\u6570\uff0c\u4ee5\u9002\u5e94\u52a8\u6001\u73af\u5883\u3002\n\n### Step 2: \u7814\u7a76\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u4e3b\u8981\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0e\u53d8\u4f53\n\n**\u5e94\u7528\u573a\u666f**\uff1a\n- **\u6e38\u620fAI**\uff1aMCTS\u4e0ePRM\u7ed3\u5408\u7528\u4e8e\u6e38\u620f\u4e2d\u7684\u667a\u80fd\u51b3\u7b56\uff0c\u5982AlphaGo\u3002\n- **\u8def\u5f84\u89c4\u5212**\uff1a\u5728\u673a\u5668\u4eba\u5bfc\u822a\u548c\u81ea\u52a8\u9a7e\u9a76\u4e2d\uff0c\u5229\u7528MCTS\u4f18\u5316\u8def\u5f84\u9009\u62e9\u3002\n- **\u63a8\u8350\u7cfb\u7edf**\uff1a\u901a\u8fc7PRM\u5b66\u4e60\u7528\u6237\u504f\u597d\uff0c\u7ed3\u5408MCTS\u8fdb\u884c\u66f4\u7cbe\u51c6\u7684\u63a8\u8350\u3002\n\n**\u53d8\u4f53**\uff1a\n- **\u6df1\u5ea6MCTS**\uff1a\u7ed3\u5408\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff0c\u63d0\u5347MCTS\u7684\u51b3\u7b56\u80fd\u529b\u3002\n- **\u591a\u6a21\u6001PRM**\uff1a\u878d\u5408\u591a\u79cd\u6570\u636e\u6e90\uff08\u5982\u6587\u672c\u3001\u56fe\u50cf\uff09\uff0c\u589e\u5f3aPRM\u7684\u6cdb\u5316\u80fd\u529b\u3002\n\n### Step 3: \u8bc4\u4f30\u5b66\u672f\u754c\u7684\u6280\u672f\u8fdb\u6b65\u4e0e\u5c40\u9650\u6027\n\n**\u6280\u672f\u8fdb\u6b65**\uff1a\n- **\u6027\u80fd\u63d0\u5347**\uff1aMCTS\u4e0ePRM\u7ed3\u5408\u663e\u8457\u63d0\u5347\u4e86\u51b3\u7b56\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002\n- **\u6cdb\u5316\u80fd\u529b**\uff1a\u591a\u6a21\u6001PRM\u5728\u5904\u7406\u591a\u6837\u5316\u6570\u636e\u65f6\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002\n\n**\u5c40\u9650\u6027**\uff1a\n- **\u8ba1\u7b97\u590d\u6742\u5ea6**\uff1aMCTS\u5728\u5927\u89c4\u6a21\u95ee\u9898\u4e2d\u8ba1\u7b97\u91cf\u5927\uff0c\u6548\u7387\u6709\u5f85\u63d0\u5347\u3002\n- **\u6570\u636e\u4f9d\u8d56**\uff1aPRM\u7684\u6027\u80fd\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u9ad8\u8d28\u91cf\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u6570\u636e\u504f\u5dee\u95ee\u9898\u4f9d\u7136\u5b58\u5728\u3002\n\n### Step 4: \u63a2\u8ba8\u8ba1\u7b97\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\n\n**\u9002\u7528\u6027**\uff1a\n- **\u591a\u9886\u57df\u5e94\u7528**\uff1aMCTS+PRM\u6846\u67b6\u5728\u6e38\u620f\u3001\u8def\u5f84\u89c4\u5212\u3001\u63a8\u8350\u7cfb\u7edf\u7b49\u591a\u4e2a\u9886\u57df\u8868\u73b0\u51fa\u826f\u597d\u7684\u9002\u7528\u6027\u3002\n- **\u591a\u6a21\u6001\u6570\u636e**\uff1a\u591a\u6a21\u6001PRM\u80fd\u591f\u5904\u7406\u591a\u79cd\u7c7b\u578b\u7684\u6570\u636e\uff0c\u9002\u7528\u4e8e\u590d\u6742\u5e94\u7528\u573a\u666f\u3002\n\n**\u6cdb\u5316\u80fd\u529b**\uff1a\n- **\u8de8\u9886\u57df\u6cdb\u5316**\uff1a\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\uff0cMCTS+PRM\u6846\u67b6\u5728\u4e0d\u540c\u9886\u57df\u95f4\u5c55\u73b0\u51fa\u8f83\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002\n- **\u52a8\u6001\u73af\u5883\u9002\u5e94**\uff1a\u5f3a\u5316\u5b66\u4e60\u673a\u5236\u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u9002\u5e94\u52a8\u6001\u53d8\u5316\u7684\u73af\u5883\u3002\n\n### Step 5: \u5206\u6790\u6700\u65b0\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027\n\n**\u7a33\u5b9a\u6027**\uff1a\n- **\u7b97\u6cd5\u4f18\u5316**\uff1a\u901a\u8fc7\u6539\u8fdbMCTS\u7684\u9009\u62e9\u7b56\u7565\u548cPRM\u7684\u53c2\u6570\u8c03\u6574\u673a\u5236\uff0c\u63d0\u5347\u4e86\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u3002\n- **\u9c81\u68d2\u6027\u589e\u5f3a**\uff1a\u5f15\u5165\u5bf9\u6297\u8bad\u7ec3\u548c\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\uff0c\u63d0\u9ad8\u6a21\u578b\u5728\u590d\u6742\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u6027\u3002\n\n**\u5bb9\u9519\u6027**\uff1a\n- **\u9519\u8bef\u5904\u7406**\uff1a\u5728\u6a21\u62df\u548c\u56de\u6eaf\u9636\u6bb5\u5f15\u5165\u5bb9\u9519\u673a\u5236\uff0c\u51cf\u5c11\u9519\u8bef\u51b3\u7b56\u7684\u5f71\u54cd\u3002\n- **\u5927\u89c4\u6a21\u6570\u636e\u9002\u5e94\u6027**\uff1a\u4f18\u5316\u7b97\u6cd5\u7ed3\u6784\uff0c\u63d0\u5347\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\u7684\u5904\u7406\u80fd\u529b\u3002\n\n### Step 6: \u8bc4\u4f30\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218\n\n**\u672a\u6765\u7814\u7a76\u65b9\u5411**\uff1a\n- **\u9ad8\u6548\u8ba1\u7b97**\uff1a\u7814\u7a76\u66f4\u9ad8\u6548\u7684MCTS\u7b97\u6cd5\uff0c\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002\n- **\u6570\u636e\u589e\u5f3a**\uff1a\u63a2\u7d22\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u51cf\u5c11PRM\u5bf9\u9ad8\u8d28\u91cf\u6570\u636e\u7684\u4f9d\u8d56\u3002\n\n**\u6311\u6218**\uff1a\n- **\u8de8\u6a21\u6001\u878d\u5408**\uff1a\u5982\u4f55\u6709\u6548\u878d\u5408\u591a\u79cd\u6a21\u6001\u6570\u636e\uff0c\u63d0\u5347\u6a21\u578b\u7684\u7efc\u5408\u6027\u80fd\u3002\n- **\u52a8\u6001\u73af\u5883\u9002\u5e94**\uff1a\u5728\u52a8\u6001\u73af\u5883\u4e0b\uff0c\u5982\u4f55\u4fdd\u6301\u6a21\u578b\u7684\u7a33\u5b9a\u6027\u548c\u9ad8\u6548\u6027\u3002\n\n**\u63a8\u52a8\u540e\u7eed\u7814\u7a76**\uff1a\n- **\u65b0\u95ee\u9898\u63d0\u51fa**\uff1a\u5982\u591a\u6a21\u6001\u6570\u636e\u4e0b\u7684\u504f\u597d\u5efa\u6a21\u3001\u52a8\u6001\u73af\u5883\u4e0b\u7684\u51b3\u7b56\u4f18\u5316\u7b49\u3002\n- **\u7814\u7a76\u5207\u5165\u70b9**\uff1a\u4ece\u7b97\u6cd5\u4f18\u5316\u3001\u6570\u636e\u589e\u5f3a\u3001\u8de8\u6a21\u6001\u878d\u5408\u7b49\u591a\u4e2a\u89d2\u5ea6\u5207\u5165\uff0c\u63a8\u52a8\u6280\u672f\u8fdb\u6b65\u3002\n\n### \u603b\u7ed3\n\n\u8fd1\u5e74\u6765\uff0cMCTS\u5728PRM\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u7684\u5e94\u7528\u63a2\u7d22\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4e3b\u8981\u4f53\u73b0\u5728\u6280\u672f\u6846\u67b6\u7684\u5b8c\u5584\u3001\u5e94\u7528\u573a\u666f\u7684\u62d3\u5c55\u3001\u6027\u80fd\u7684\u63d0\u5347\u4ee5\u53ca\u7a33\u5b9a\u6027\u548c\u5bb9\u9519\u6027\u7684\u589e\u5f3a\u3002\u7136\u800c\uff0c\u4ecd\u5b58\u5728\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u3001\u6570\u636e\u4f9d\u8d56\u6027\u5f3a\u7b49\u5c40\u9650\u6027\u3002\u672a\u6765\u7684\u7814\u7a76\u5e94\u5173\u6ce8\u9ad8\u6548\u8ba1\u7b97\u3001\u6570\u636e\u589e\u5f3a\u548c\u8de8\u6a21\u6001\u878d\u5408\u7b49\u65b9\u5411\uff0c\u4ee5\u5e94\u5bf9\u52a8\u6001\u73af\u5883\u4e0b\u7684\u51b3\u7b56\u4f18\u5316\u7b49\u6311\u6218\uff0c\u63a8\u52a8\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002", "task_step_name": "\u9002\u7528\u6027", "task_step_description": "MCTS+PRM\u6846\u67b6\u5728\u6e38\u620f\u3001\u8def\u5f84\u89c4\u5212\u3001\u63a8\u8350\u7cfb\u7edf\u7b49\u591a\u4e2a\u9886\u57df\u8868\u73b0\u51fa\u826f\u597d\u7684\u9002\u7528\u6027\u3002", "task_step_level": "3>1", "task_step_question": "", "task_step_question_context": [], "task_step_question_answer": "", "ref_task_step_id": ""}, "__type__": "task_step_node"}, "edaf9c7c-601d-4068-b117-92e5eaa45af5": {"__data__": {"id_": "edaf9c7c-601d-4068-b117-92e5eaa45af5", "metadata": {}, "relationships": {}, "hash": "", "start_task_context": "MCTS\u5728PRM\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u7684\u5e94\u7528\u63a2\u7d22\u7efc\u8ff0", "aemo_representation_context": "### Step 1: \u5206\u6790\u8fd1\u51e0\u5e74\u7814\u7a76\u9886\u57df\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba\n\n**\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09**\uff1a\n- **\u57fa\u672c\u539f\u7406**\uff1aMCTS\u662f\u4e00\u79cd\u7528\u4e8e\u51b3\u7b56\u8fc7\u7a0b\u7684\u542f\u53d1\u5f0f\u641c\u7d22\u7b97\u6cd5\uff0c\u5e7f\u6cdb\u5e94\u7528\u4e8e\u6e38\u620fAI\u3001\u8def\u5f84\u89c4\u5212\u7b49\u9886\u57df\u3002\u5176\u6838\u5fc3\u601d\u60f3\u662f\u901a\u8fc7\u6a21\u62df\u591a\u6b21\u968f\u673a\u6e38\u620f\u6765\u8bc4\u4f30\u4e0d\u540c\u51b3\u7b56\u7684\u4f18\u52a3\u3002\n- **\u6280\u672f\u6846\u67b6**\uff1a\u4e3b\u8981\u5305\u62ec\u9009\u62e9\uff08Selection\uff09\u3001\u6269\u5c55\uff08Expansion\uff09\u3001\u6a21\u62df\uff08Simulation\uff09\u548c\u56de\u6eaf\uff08Backpropagation\uff09\u56db\u4e2a\u6b65\u9aa4\u3002\n\n**\u504f\u597d\u7b56\u7565\u6a21\u578b\uff08PRM\uff09**\uff1a\n- **\u57fa\u672c\u539f\u7406**\uff1aPRM\u65e8\u5728\u901a\u8fc7\u5b66\u4e60\u7528\u6237\u7684\u504f\u597d\u6765\u4f18\u5316\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5e38\u7528\u4e8e\u63a8\u8350\u7cfb\u7edf\u3001\u4e2a\u6027\u5316\u670d\u52a1\u7b49\u3002\n- **\u6280\u672f\u6846\u67b6**\uff1a\u901a\u5e38\u5305\u62ec\u504f\u597d\u5efa\u6a21\u3001\u7b56\u7565\u4f18\u5316\u548c\u6a21\u578b\u5fae\u8c03\u7b49\u73af\u8282\u3002\n\n**\u65b9\u6cd5\u8bba**\uff1a\n- **\u96c6\u6210\u5b66\u4e60**\uff1a\u5c06MCTS\u4e0ePRM\u7ed3\u5408\uff0c\u5229\u7528MCTS\u7684\u641c\u7d22\u80fd\u529b\u4f18\u5316PRM\u7684\u51b3\u7b56\u8fc7\u7a0b\u3002\n- **\u5f3a\u5316\u5b66\u4e60**\uff1a\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u673a\u5236\u4e0d\u65ad\u8c03\u6574PRM\u7684\u53c2\u6570\uff0c\u4ee5\u9002\u5e94\u52a8\u6001\u73af\u5883\u3002\n\n### Step 2: \u7814\u7a76\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u4e3b\u8981\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0e\u53d8\u4f53\n\n**\u5e94\u7528\u573a\u666f**\uff1a\n- **\u6e38\u620fAI**\uff1aMCTS\u4e0ePRM\u7ed3\u5408\u7528\u4e8e\u6e38\u620f\u4e2d\u7684\u667a\u80fd\u51b3\u7b56\uff0c\u5982AlphaGo\u3002\n- **\u8def\u5f84\u89c4\u5212**\uff1a\u5728\u673a\u5668\u4eba\u5bfc\u822a\u548c\u81ea\u52a8\u9a7e\u9a76\u4e2d\uff0c\u5229\u7528MCTS\u4f18\u5316\u8def\u5f84\u9009\u62e9\u3002\n- **\u63a8\u8350\u7cfb\u7edf**\uff1a\u901a\u8fc7PRM\u5b66\u4e60\u7528\u6237\u504f\u597d\uff0c\u7ed3\u5408MCTS\u8fdb\u884c\u66f4\u7cbe\u51c6\u7684\u63a8\u8350\u3002\n\n**\u53d8\u4f53**\uff1a\n- **\u6df1\u5ea6MCTS**\uff1a\u7ed3\u5408\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff0c\u63d0\u5347MCTS\u7684\u51b3\u7b56\u80fd\u529b\u3002\n- **\u591a\u6a21\u6001PRM**\uff1a\u878d\u5408\u591a\u79cd\u6570\u636e\u6e90\uff08\u5982\u6587\u672c\u3001\u56fe\u50cf\uff09\uff0c\u589e\u5f3aPRM\u7684\u6cdb\u5316\u80fd\u529b\u3002\n\n### Step 3: \u8bc4\u4f30\u5b66\u672f\u754c\u7684\u6280\u672f\u8fdb\u6b65\u4e0e\u5c40\u9650\u6027\n\n**\u6280\u672f\u8fdb\u6b65**\uff1a\n- **\u6027\u80fd\u63d0\u5347**\uff1aMCTS\u4e0ePRM\u7ed3\u5408\u663e\u8457\u63d0\u5347\u4e86\u51b3\u7b56\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002\n- **\u6cdb\u5316\u80fd\u529b**\uff1a\u591a\u6a21\u6001PRM\u5728\u5904\u7406\u591a\u6837\u5316\u6570\u636e\u65f6\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002\n\n**\u5c40\u9650\u6027**\uff1a\n- **\u8ba1\u7b97\u590d\u6742\u5ea6**\uff1aMCTS\u5728\u5927\u89c4\u6a21\u95ee\u9898\u4e2d\u8ba1\u7b97\u91cf\u5927\uff0c\u6548\u7387\u6709\u5f85\u63d0\u5347\u3002\n- **\u6570\u636e\u4f9d\u8d56**\uff1aPRM\u7684\u6027\u80fd\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u9ad8\u8d28\u91cf\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u6570\u636e\u504f\u5dee\u95ee\u9898\u4f9d\u7136\u5b58\u5728\u3002\n\n### Step 4: \u63a2\u8ba8\u8ba1\u7b97\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\n\n**\u9002\u7528\u6027**\uff1a\n- **\u591a\u9886\u57df\u5e94\u7528**\uff1aMCTS+PRM\u6846\u67b6\u5728\u6e38\u620f\u3001\u8def\u5f84\u89c4\u5212\u3001\u63a8\u8350\u7cfb\u7edf\u7b49\u591a\u4e2a\u9886\u57df\u8868\u73b0\u51fa\u826f\u597d\u7684\u9002\u7528\u6027\u3002\n- **\u591a\u6a21\u6001\u6570\u636e**\uff1a\u591a\u6a21\u6001PRM\u80fd\u591f\u5904\u7406\u591a\u79cd\u7c7b\u578b\u7684\u6570\u636e\uff0c\u9002\u7528\u4e8e\u590d\u6742\u5e94\u7528\u573a\u666f\u3002\n\n**\u6cdb\u5316\u80fd\u529b**\uff1a\n- **\u8de8\u9886\u57df\u6cdb\u5316**\uff1a\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\uff0cMCTS+PRM\u6846\u67b6\u5728\u4e0d\u540c\u9886\u57df\u95f4\u5c55\u73b0\u51fa\u8f83\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002\n- **\u52a8\u6001\u73af\u5883\u9002\u5e94**\uff1a\u5f3a\u5316\u5b66\u4e60\u673a\u5236\u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u9002\u5e94\u52a8\u6001\u53d8\u5316\u7684\u73af\u5883\u3002\n\n### Step 5: \u5206\u6790\u6700\u65b0\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027\n\n**\u7a33\u5b9a\u6027**\uff1a\n- **\u7b97\u6cd5\u4f18\u5316**\uff1a\u901a\u8fc7\u6539\u8fdbMCTS\u7684\u9009\u62e9\u7b56\u7565\u548cPRM\u7684\u53c2\u6570\u8c03\u6574\u673a\u5236\uff0c\u63d0\u5347\u4e86\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u3002\n- **\u9c81\u68d2\u6027\u589e\u5f3a**\uff1a\u5f15\u5165\u5bf9\u6297\u8bad\u7ec3\u548c\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\uff0c\u63d0\u9ad8\u6a21\u578b\u5728\u590d\u6742\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u6027\u3002\n\n**\u5bb9\u9519\u6027**\uff1a\n- **\u9519\u8bef\u5904\u7406**\uff1a\u5728\u6a21\u62df\u548c\u56de\u6eaf\u9636\u6bb5\u5f15\u5165\u5bb9\u9519\u673a\u5236\uff0c\u51cf\u5c11\u9519\u8bef\u51b3\u7b56\u7684\u5f71\u54cd\u3002\n- **\u5927\u89c4\u6a21\u6570\u636e\u9002\u5e94\u6027**\uff1a\u4f18\u5316\u7b97\u6cd5\u7ed3\u6784\uff0c\u63d0\u5347\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\u7684\u5904\u7406\u80fd\u529b\u3002\n\n### Step 6: \u8bc4\u4f30\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218\n\n**\u672a\u6765\u7814\u7a76\u65b9\u5411**\uff1a\n- **\u9ad8\u6548\u8ba1\u7b97**\uff1a\u7814\u7a76\u66f4\u9ad8\u6548\u7684MCTS\u7b97\u6cd5\uff0c\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002\n- **\u6570\u636e\u589e\u5f3a**\uff1a\u63a2\u7d22\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u51cf\u5c11PRM\u5bf9\u9ad8\u8d28\u91cf\u6570\u636e\u7684\u4f9d\u8d56\u3002\n\n**\u6311\u6218**\uff1a\n- **\u8de8\u6a21\u6001\u878d\u5408**\uff1a\u5982\u4f55\u6709\u6548\u878d\u5408\u591a\u79cd\u6a21\u6001\u6570\u636e\uff0c\u63d0\u5347\u6a21\u578b\u7684\u7efc\u5408\u6027\u80fd\u3002\n- **\u52a8\u6001\u73af\u5883\u9002\u5e94**\uff1a\u5728\u52a8\u6001\u73af\u5883\u4e0b\uff0c\u5982\u4f55\u4fdd\u6301\u6a21\u578b\u7684\u7a33\u5b9a\u6027\u548c\u9ad8\u6548\u6027\u3002\n\n**\u63a8\u52a8\u540e\u7eed\u7814\u7a76**\uff1a\n- **\u65b0\u95ee\u9898\u63d0\u51fa**\uff1a\u5982\u591a\u6a21\u6001\u6570\u636e\u4e0b\u7684\u504f\u597d\u5efa\u6a21\u3001\u52a8\u6001\u73af\u5883\u4e0b\u7684\u51b3\u7b56\u4f18\u5316\u7b49\u3002\n- **\u7814\u7a76\u5207\u5165\u70b9**\uff1a\u4ece\u7b97\u6cd5\u4f18\u5316\u3001\u6570\u636e\u589e\u5f3a\u3001\u8de8\u6a21\u6001\u878d\u5408\u7b49\u591a\u4e2a\u89d2\u5ea6\u5207\u5165\uff0c\u63a8\u52a8\u6280\u672f\u8fdb\u6b65\u3002\n\n### \u603b\u7ed3\n\n\u8fd1\u5e74\u6765\uff0cMCTS\u5728PRM\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u7684\u5e94\u7528\u63a2\u7d22\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4e3b\u8981\u4f53\u73b0\u5728\u6280\u672f\u6846\u67b6\u7684\u5b8c\u5584\u3001\u5e94\u7528\u573a\u666f\u7684\u62d3\u5c55\u3001\u6027\u80fd\u7684\u63d0\u5347\u4ee5\u53ca\u7a33\u5b9a\u6027\u548c\u5bb9\u9519\u6027\u7684\u589e\u5f3a\u3002\u7136\u800c\uff0c\u4ecd\u5b58\u5728\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u3001\u6570\u636e\u4f9d\u8d56\u6027\u5f3a\u7b49\u5c40\u9650\u6027\u3002\u672a\u6765\u7684\u7814\u7a76\u5e94\u5173\u6ce8\u9ad8\u6548\u8ba1\u7b97\u3001\u6570\u636e\u589e\u5f3a\u548c\u8de8\u6a21\u6001\u878d\u5408\u7b49\u65b9\u5411\uff0c\u4ee5\u5e94\u5bf9\u52a8\u6001\u73af\u5883\u4e0b\u7684\u51b3\u7b56\u4f18\u5316\u7b49\u6311\u6218\uff0c\u63a8\u52a8\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002", "task_step_name": "\u6cdb\u5316\u80fd\u529b", "task_step_description": "\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\uff0cMCTS+PRM\u6846\u67b6\u5728\u4e0d\u540c\u9886\u57df\u95f4\u5c55\u73b0\u51fa\u8f83\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "task_step_level": "3>2", "task_step_question": "", "task_step_question_context": [], "task_step_question_answer": "", "ref_task_step_id": ""}, "__type__": "task_step_node"}, "29d7d9b0-9dd0-42e2-a847-f9e6d0c0b37f": {"__data__": {"id_": "29d7d9b0-9dd0-42e2-a847-f9e6d0c0b37f", "metadata": {}, "relationships": {}, "hash": "", "start_task_context": "MCTS\u5728PRM\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u7684\u5e94\u7528\u63a2\u7d22\u7efc\u8ff0", "aemo_representation_context": "### Step 1: \u5206\u6790\u8fd1\u51e0\u5e74\u7814\u7a76\u9886\u57df\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba\n\n**\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09**\uff1a\n- **\u57fa\u672c\u539f\u7406**\uff1aMCTS\u662f\u4e00\u79cd\u7528\u4e8e\u51b3\u7b56\u8fc7\u7a0b\u7684\u542f\u53d1\u5f0f\u641c\u7d22\u7b97\u6cd5\uff0c\u5e7f\u6cdb\u5e94\u7528\u4e8e\u6e38\u620fAI\u3001\u8def\u5f84\u89c4\u5212\u7b49\u9886\u57df\u3002\u5176\u6838\u5fc3\u601d\u60f3\u662f\u901a\u8fc7\u6a21\u62df\u591a\u6b21\u968f\u673a\u6e38\u620f\u6765\u8bc4\u4f30\u4e0d\u540c\u51b3\u7b56\u7684\u4f18\u52a3\u3002\n- **\u6280\u672f\u6846\u67b6**\uff1a\u4e3b\u8981\u5305\u62ec\u9009\u62e9\uff08Selection\uff09\u3001\u6269\u5c55\uff08Expansion\uff09\u3001\u6a21\u62df\uff08Simulation\uff09\u548c\u56de\u6eaf\uff08Backpropagation\uff09\u56db\u4e2a\u6b65\u9aa4\u3002\n\n**\u504f\u597d\u7b56\u7565\u6a21\u578b\uff08PRM\uff09**\uff1a\n- **\u57fa\u672c\u539f\u7406**\uff1aPRM\u65e8\u5728\u901a\u8fc7\u5b66\u4e60\u7528\u6237\u7684\u504f\u597d\u6765\u4f18\u5316\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5e38\u7528\u4e8e\u63a8\u8350\u7cfb\u7edf\u3001\u4e2a\u6027\u5316\u670d\u52a1\u7b49\u3002\n- **\u6280\u672f\u6846\u67b6**\uff1a\u901a\u5e38\u5305\u62ec\u504f\u597d\u5efa\u6a21\u3001\u7b56\u7565\u4f18\u5316\u548c\u6a21\u578b\u5fae\u8c03\u7b49\u73af\u8282\u3002\n\n**\u65b9\u6cd5\u8bba**\uff1a\n- **\u96c6\u6210\u5b66\u4e60**\uff1a\u5c06MCTS\u4e0ePRM\u7ed3\u5408\uff0c\u5229\u7528MCTS\u7684\u641c\u7d22\u80fd\u529b\u4f18\u5316PRM\u7684\u51b3\u7b56\u8fc7\u7a0b\u3002\n- **\u5f3a\u5316\u5b66\u4e60**\uff1a\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u673a\u5236\u4e0d\u65ad\u8c03\u6574PRM\u7684\u53c2\u6570\uff0c\u4ee5\u9002\u5e94\u52a8\u6001\u73af\u5883\u3002\n\n### Step 2: \u7814\u7a76\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u4e3b\u8981\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0e\u53d8\u4f53\n\n**\u5e94\u7528\u573a\u666f**\uff1a\n- **\u6e38\u620fAI**\uff1aMCTS\u4e0ePRM\u7ed3\u5408\u7528\u4e8e\u6e38\u620f\u4e2d\u7684\u667a\u80fd\u51b3\u7b56\uff0c\u5982AlphaGo\u3002\n- **\u8def\u5f84\u89c4\u5212**\uff1a\u5728\u673a\u5668\u4eba\u5bfc\u822a\u548c\u81ea\u52a8\u9a7e\u9a76\u4e2d\uff0c\u5229\u7528MCTS\u4f18\u5316\u8def\u5f84\u9009\u62e9\u3002\n- **\u63a8\u8350\u7cfb\u7edf**\uff1a\u901a\u8fc7PRM\u5b66\u4e60\u7528\u6237\u504f\u597d\uff0c\u7ed3\u5408MCTS\u8fdb\u884c\u66f4\u7cbe\u51c6\u7684\u63a8\u8350\u3002\n\n**\u53d8\u4f53**\uff1a\n- **\u6df1\u5ea6MCTS**\uff1a\u7ed3\u5408\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff0c\u63d0\u5347MCTS\u7684\u51b3\u7b56\u80fd\u529b\u3002\n- **\u591a\u6a21\u6001PRM**\uff1a\u878d\u5408\u591a\u79cd\u6570\u636e\u6e90\uff08\u5982\u6587\u672c\u3001\u56fe\u50cf\uff09\uff0c\u589e\u5f3aPRM\u7684\u6cdb\u5316\u80fd\u529b\u3002\n\n### Step 3: \u8bc4\u4f30\u5b66\u672f\u754c\u7684\u6280\u672f\u8fdb\u6b65\u4e0e\u5c40\u9650\u6027\n\n**\u6280\u672f\u8fdb\u6b65**\uff1a\n- **\u6027\u80fd\u63d0\u5347**\uff1aMCTS\u4e0ePRM\u7ed3\u5408\u663e\u8457\u63d0\u5347\u4e86\u51b3\u7b56\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002\n- **\u6cdb\u5316\u80fd\u529b**\uff1a\u591a\u6a21\u6001PRM\u5728\u5904\u7406\u591a\u6837\u5316\u6570\u636e\u65f6\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002\n\n**\u5c40\u9650\u6027**\uff1a\n- **\u8ba1\u7b97\u590d\u6742\u5ea6**\uff1aMCTS\u5728\u5927\u89c4\u6a21\u95ee\u9898\u4e2d\u8ba1\u7b97\u91cf\u5927\uff0c\u6548\u7387\u6709\u5f85\u63d0\u5347\u3002\n- **\u6570\u636e\u4f9d\u8d56**\uff1aPRM\u7684\u6027\u80fd\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u9ad8\u8d28\u91cf\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u6570\u636e\u504f\u5dee\u95ee\u9898\u4f9d\u7136\u5b58\u5728\u3002\n\n### Step 4: \u63a2\u8ba8\u8ba1\u7b97\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\n\n**\u9002\u7528\u6027**\uff1a\n- **\u591a\u9886\u57df\u5e94\u7528**\uff1aMCTS+PRM\u6846\u67b6\u5728\u6e38\u620f\u3001\u8def\u5f84\u89c4\u5212\u3001\u63a8\u8350\u7cfb\u7edf\u7b49\u591a\u4e2a\u9886\u57df\u8868\u73b0\u51fa\u826f\u597d\u7684\u9002\u7528\u6027\u3002\n- **\u591a\u6a21\u6001\u6570\u636e**\uff1a\u591a\u6a21\u6001PRM\u80fd\u591f\u5904\u7406\u591a\u79cd\u7c7b\u578b\u7684\u6570\u636e\uff0c\u9002\u7528\u4e8e\u590d\u6742\u5e94\u7528\u573a\u666f\u3002\n\n**\u6cdb\u5316\u80fd\u529b**\uff1a\n- **\u8de8\u9886\u57df\u6cdb\u5316**\uff1a\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\uff0cMCTS+PRM\u6846\u67b6\u5728\u4e0d\u540c\u9886\u57df\u95f4\u5c55\u73b0\u51fa\u8f83\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002\n- **\u52a8\u6001\u73af\u5883\u9002\u5e94**\uff1a\u5f3a\u5316\u5b66\u4e60\u673a\u5236\u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u9002\u5e94\u52a8\u6001\u53d8\u5316\u7684\u73af\u5883\u3002\n\n### Step 5: \u5206\u6790\u6700\u65b0\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027\n\n**\u7a33\u5b9a\u6027**\uff1a\n- **\u7b97\u6cd5\u4f18\u5316**\uff1a\u901a\u8fc7\u6539\u8fdbMCTS\u7684\u9009\u62e9\u7b56\u7565\u548cPRM\u7684\u53c2\u6570\u8c03\u6574\u673a\u5236\uff0c\u63d0\u5347\u4e86\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u3002\n- **\u9c81\u68d2\u6027\u589e\u5f3a**\uff1a\u5f15\u5165\u5bf9\u6297\u8bad\u7ec3\u548c\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\uff0c\u63d0\u9ad8\u6a21\u578b\u5728\u590d\u6742\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u6027\u3002\n\n**\u5bb9\u9519\u6027**\uff1a\n- **\u9519\u8bef\u5904\u7406**\uff1a\u5728\u6a21\u62df\u548c\u56de\u6eaf\u9636\u6bb5\u5f15\u5165\u5bb9\u9519\u673a\u5236\uff0c\u51cf\u5c11\u9519\u8bef\u51b3\u7b56\u7684\u5f71\u54cd\u3002\n- **\u5927\u89c4\u6a21\u6570\u636e\u9002\u5e94\u6027**\uff1a\u4f18\u5316\u7b97\u6cd5\u7ed3\u6784\uff0c\u63d0\u5347\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\u7684\u5904\u7406\u80fd\u529b\u3002\n\n### Step 6: \u8bc4\u4f30\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218\n\n**\u672a\u6765\u7814\u7a76\u65b9\u5411**\uff1a\n- **\u9ad8\u6548\u8ba1\u7b97**\uff1a\u7814\u7a76\u66f4\u9ad8\u6548\u7684MCTS\u7b97\u6cd5\uff0c\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002\n- **\u6570\u636e\u589e\u5f3a**\uff1a\u63a2\u7d22\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u51cf\u5c11PRM\u5bf9\u9ad8\u8d28\u91cf\u6570\u636e\u7684\u4f9d\u8d56\u3002\n\n**\u6311\u6218**\uff1a\n- **\u8de8\u6a21\u6001\u878d\u5408**\uff1a\u5982\u4f55\u6709\u6548\u878d\u5408\u591a\u79cd\u6a21\u6001\u6570\u636e\uff0c\u63d0\u5347\u6a21\u578b\u7684\u7efc\u5408\u6027\u80fd\u3002\n- **\u52a8\u6001\u73af\u5883\u9002\u5e94**\uff1a\u5728\u52a8\u6001\u73af\u5883\u4e0b\uff0c\u5982\u4f55\u4fdd\u6301\u6a21\u578b\u7684\u7a33\u5b9a\u6027\u548c\u9ad8\u6548\u6027\u3002\n\n**\u63a8\u52a8\u540e\u7eed\u7814\u7a76**\uff1a\n- **\u65b0\u95ee\u9898\u63d0\u51fa**\uff1a\u5982\u591a\u6a21\u6001\u6570\u636e\u4e0b\u7684\u504f\u597d\u5efa\u6a21\u3001\u52a8\u6001\u73af\u5883\u4e0b\u7684\u51b3\u7b56\u4f18\u5316\u7b49\u3002\n- **\u7814\u7a76\u5207\u5165\u70b9**\uff1a\u4ece\u7b97\u6cd5\u4f18\u5316\u3001\u6570\u636e\u589e\u5f3a\u3001\u8de8\u6a21\u6001\u878d\u5408\u7b49\u591a\u4e2a\u89d2\u5ea6\u5207\u5165\uff0c\u63a8\u52a8\u6280\u672f\u8fdb\u6b65\u3002\n\n### \u603b\u7ed3\n\n\u8fd1\u5e74\u6765\uff0cMCTS\u5728PRM\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u7684\u5e94\u7528\u63a2\u7d22\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4e3b\u8981\u4f53\u73b0\u5728\u6280\u672f\u6846\u67b6\u7684\u5b8c\u5584\u3001\u5e94\u7528\u573a\u666f\u7684\u62d3\u5c55\u3001\u6027\u80fd\u7684\u63d0\u5347\u4ee5\u53ca\u7a33\u5b9a\u6027\u548c\u5bb9\u9519\u6027\u7684\u589e\u5f3a\u3002\u7136\u800c\uff0c\u4ecd\u5b58\u5728\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u3001\u6570\u636e\u4f9d\u8d56\u6027\u5f3a\u7b49\u5c40\u9650\u6027\u3002\u672a\u6765\u7684\u7814\u7a76\u5e94\u5173\u6ce8\u9ad8\u6548\u8ba1\u7b97\u3001\u6570\u636e\u589e\u5f3a\u548c\u8de8\u6a21\u6001\u878d\u5408\u7b49\u65b9\u5411\uff0c\u4ee5\u5e94\u5bf9\u52a8\u6001\u73af\u5883\u4e0b\u7684\u51b3\u7b56\u4f18\u5316\u7b49\u6311\u6218\uff0c\u63a8\u52a8\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002", "task_step_name": "\u5206\u6790\u6700\u65b0\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027", "task_step_description": "\u901a\u8fc7\u6539\u8fdbMCTS\u7684\u9009\u62e9\u7b56\u7565\u548cPRM\u7684\u53c2\u6570\u8c03\u6574\u673a\u5236\uff0c\u63d0\u5347\u4e86\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u3002", "task_step_level": "4", "task_step_question": "", "task_step_question_context": [], "task_step_question_answer": "", "ref_task_step_id": ""}, "__type__": "task_step_node"}, "c438cc1b-a630-4359-b8d9-abeaf797d240": {"__data__": {"id_": "c438cc1b-a630-4359-b8d9-abeaf797d240", "metadata": {}, "relationships": {}, "hash": "", "start_task_context": "MCTS\u5728PRM\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u7684\u5e94\u7528\u63a2\u7d22\u7efc\u8ff0", "aemo_representation_context": "### Step 1: \u5206\u6790\u8fd1\u51e0\u5e74\u7814\u7a76\u9886\u57df\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba\n\n**\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09**\uff1a\n- **\u57fa\u672c\u539f\u7406**\uff1aMCTS\u662f\u4e00\u79cd\u7528\u4e8e\u51b3\u7b56\u8fc7\u7a0b\u7684\u542f\u53d1\u5f0f\u641c\u7d22\u7b97\u6cd5\uff0c\u5e7f\u6cdb\u5e94\u7528\u4e8e\u6e38\u620fAI\u3001\u8def\u5f84\u89c4\u5212\u7b49\u9886\u57df\u3002\u5176\u6838\u5fc3\u601d\u60f3\u662f\u901a\u8fc7\u6a21\u62df\u591a\u6b21\u968f\u673a\u6e38\u620f\u6765\u8bc4\u4f30\u4e0d\u540c\u51b3\u7b56\u7684\u4f18\u52a3\u3002\n- **\u6280\u672f\u6846\u67b6**\uff1a\u4e3b\u8981\u5305\u62ec\u9009\u62e9\uff08Selection\uff09\u3001\u6269\u5c55\uff08Expansion\uff09\u3001\u6a21\u62df\uff08Simulation\uff09\u548c\u56de\u6eaf\uff08Backpropagation\uff09\u56db\u4e2a\u6b65\u9aa4\u3002\n\n**\u504f\u597d\u7b56\u7565\u6a21\u578b\uff08PRM\uff09**\uff1a\n- **\u57fa\u672c\u539f\u7406**\uff1aPRM\u65e8\u5728\u901a\u8fc7\u5b66\u4e60\u7528\u6237\u7684\u504f\u597d\u6765\u4f18\u5316\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5e38\u7528\u4e8e\u63a8\u8350\u7cfb\u7edf\u3001\u4e2a\u6027\u5316\u670d\u52a1\u7b49\u3002\n- **\u6280\u672f\u6846\u67b6**\uff1a\u901a\u5e38\u5305\u62ec\u504f\u597d\u5efa\u6a21\u3001\u7b56\u7565\u4f18\u5316\u548c\u6a21\u578b\u5fae\u8c03\u7b49\u73af\u8282\u3002\n\n**\u65b9\u6cd5\u8bba**\uff1a\n- **\u96c6\u6210\u5b66\u4e60**\uff1a\u5c06MCTS\u4e0ePRM\u7ed3\u5408\uff0c\u5229\u7528MCTS\u7684\u641c\u7d22\u80fd\u529b\u4f18\u5316PRM\u7684\u51b3\u7b56\u8fc7\u7a0b\u3002\n- **\u5f3a\u5316\u5b66\u4e60**\uff1a\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u673a\u5236\u4e0d\u65ad\u8c03\u6574PRM\u7684\u53c2\u6570\uff0c\u4ee5\u9002\u5e94\u52a8\u6001\u73af\u5883\u3002\n\n### Step 2: \u7814\u7a76\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u4e3b\u8981\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0e\u53d8\u4f53\n\n**\u5e94\u7528\u573a\u666f**\uff1a\n- **\u6e38\u620fAI**\uff1aMCTS\u4e0ePRM\u7ed3\u5408\u7528\u4e8e\u6e38\u620f\u4e2d\u7684\u667a\u80fd\u51b3\u7b56\uff0c\u5982AlphaGo\u3002\n- **\u8def\u5f84\u89c4\u5212**\uff1a\u5728\u673a\u5668\u4eba\u5bfc\u822a\u548c\u81ea\u52a8\u9a7e\u9a76\u4e2d\uff0c\u5229\u7528MCTS\u4f18\u5316\u8def\u5f84\u9009\u62e9\u3002\n- **\u63a8\u8350\u7cfb\u7edf**\uff1a\u901a\u8fc7PRM\u5b66\u4e60\u7528\u6237\u504f\u597d\uff0c\u7ed3\u5408MCTS\u8fdb\u884c\u66f4\u7cbe\u51c6\u7684\u63a8\u8350\u3002\n\n**\u53d8\u4f53**\uff1a\n- **\u6df1\u5ea6MCTS**\uff1a\u7ed3\u5408\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff0c\u63d0\u5347MCTS\u7684\u51b3\u7b56\u80fd\u529b\u3002\n- **\u591a\u6a21\u6001PRM**\uff1a\u878d\u5408\u591a\u79cd\u6570\u636e\u6e90\uff08\u5982\u6587\u672c\u3001\u56fe\u50cf\uff09\uff0c\u589e\u5f3aPRM\u7684\u6cdb\u5316\u80fd\u529b\u3002\n\n### Step 3: \u8bc4\u4f30\u5b66\u672f\u754c\u7684\u6280\u672f\u8fdb\u6b65\u4e0e\u5c40\u9650\u6027\n\n**\u6280\u672f\u8fdb\u6b65**\uff1a\n- **\u6027\u80fd\u63d0\u5347**\uff1aMCTS\u4e0ePRM\u7ed3\u5408\u663e\u8457\u63d0\u5347\u4e86\u51b3\u7b56\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002\n- **\u6cdb\u5316\u80fd\u529b**\uff1a\u591a\u6a21\u6001PRM\u5728\u5904\u7406\u591a\u6837\u5316\u6570\u636e\u65f6\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002\n\n**\u5c40\u9650\u6027**\uff1a\n- **\u8ba1\u7b97\u590d\u6742\u5ea6**\uff1aMCTS\u5728\u5927\u89c4\u6a21\u95ee\u9898\u4e2d\u8ba1\u7b97\u91cf\u5927\uff0c\u6548\u7387\u6709\u5f85\u63d0\u5347\u3002\n- **\u6570\u636e\u4f9d\u8d56**\uff1aPRM\u7684\u6027\u80fd\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u9ad8\u8d28\u91cf\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u6570\u636e\u504f\u5dee\u95ee\u9898\u4f9d\u7136\u5b58\u5728\u3002\n\n### Step 4: \u63a2\u8ba8\u8ba1\u7b97\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\n\n**\u9002\u7528\u6027**\uff1a\n- **\u591a\u9886\u57df\u5e94\u7528**\uff1aMCTS+PRM\u6846\u67b6\u5728\u6e38\u620f\u3001\u8def\u5f84\u89c4\u5212\u3001\u63a8\u8350\u7cfb\u7edf\u7b49\u591a\u4e2a\u9886\u57df\u8868\u73b0\u51fa\u826f\u597d\u7684\u9002\u7528\u6027\u3002\n- **\u591a\u6a21\u6001\u6570\u636e**\uff1a\u591a\u6a21\u6001PRM\u80fd\u591f\u5904\u7406\u591a\u79cd\u7c7b\u578b\u7684\u6570\u636e\uff0c\u9002\u7528\u4e8e\u590d\u6742\u5e94\u7528\u573a\u666f\u3002\n\n**\u6cdb\u5316\u80fd\u529b**\uff1a\n- **\u8de8\u9886\u57df\u6cdb\u5316**\uff1a\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\uff0cMCTS+PRM\u6846\u67b6\u5728\u4e0d\u540c\u9886\u57df\u95f4\u5c55\u73b0\u51fa\u8f83\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002\n- **\u52a8\u6001\u73af\u5883\u9002\u5e94**\uff1a\u5f3a\u5316\u5b66\u4e60\u673a\u5236\u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u9002\u5e94\u52a8\u6001\u53d8\u5316\u7684\u73af\u5883\u3002\n\n### Step 5: \u5206\u6790\u6700\u65b0\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027\n\n**\u7a33\u5b9a\u6027**\uff1a\n- **\u7b97\u6cd5\u4f18\u5316**\uff1a\u901a\u8fc7\u6539\u8fdbMCTS\u7684\u9009\u62e9\u7b56\u7565\u548cPRM\u7684\u53c2\u6570\u8c03\u6574\u673a\u5236\uff0c\u63d0\u5347\u4e86\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u3002\n- **\u9c81\u68d2\u6027\u589e\u5f3a**\uff1a\u5f15\u5165\u5bf9\u6297\u8bad\u7ec3\u548c\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\uff0c\u63d0\u9ad8\u6a21\u578b\u5728\u590d\u6742\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u6027\u3002\n\n**\u5bb9\u9519\u6027**\uff1a\n- **\u9519\u8bef\u5904\u7406**\uff1a\u5728\u6a21\u62df\u548c\u56de\u6eaf\u9636\u6bb5\u5f15\u5165\u5bb9\u9519\u673a\u5236\uff0c\u51cf\u5c11\u9519\u8bef\u51b3\u7b56\u7684\u5f71\u54cd\u3002\n- **\u5927\u89c4\u6a21\u6570\u636e\u9002\u5e94\u6027**\uff1a\u4f18\u5316\u7b97\u6cd5\u7ed3\u6784\uff0c\u63d0\u5347\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\u7684\u5904\u7406\u80fd\u529b\u3002\n\n### Step 6: \u8bc4\u4f30\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218\n\n**\u672a\u6765\u7814\u7a76\u65b9\u5411**\uff1a\n- **\u9ad8\u6548\u8ba1\u7b97**\uff1a\u7814\u7a76\u66f4\u9ad8\u6548\u7684MCTS\u7b97\u6cd5\uff0c\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002\n- **\u6570\u636e\u589e\u5f3a**\uff1a\u63a2\u7d22\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u51cf\u5c11PRM\u5bf9\u9ad8\u8d28\u91cf\u6570\u636e\u7684\u4f9d\u8d56\u3002\n\n**\u6311\u6218**\uff1a\n- **\u8de8\u6a21\u6001\u878d\u5408**\uff1a\u5982\u4f55\u6709\u6548\u878d\u5408\u591a\u79cd\u6a21\u6001\u6570\u636e\uff0c\u63d0\u5347\u6a21\u578b\u7684\u7efc\u5408\u6027\u80fd\u3002\n- **\u52a8\u6001\u73af\u5883\u9002\u5e94**\uff1a\u5728\u52a8\u6001\u73af\u5883\u4e0b\uff0c\u5982\u4f55\u4fdd\u6301\u6a21\u578b\u7684\u7a33\u5b9a\u6027\u548c\u9ad8\u6548\u6027\u3002\n\n**\u63a8\u52a8\u540e\u7eed\u7814\u7a76**\uff1a\n- **\u65b0\u95ee\u9898\u63d0\u51fa**\uff1a\u5982\u591a\u6a21\u6001\u6570\u636e\u4e0b\u7684\u504f\u597d\u5efa\u6a21\u3001\u52a8\u6001\u73af\u5883\u4e0b\u7684\u51b3\u7b56\u4f18\u5316\u7b49\u3002\n- **\u7814\u7a76\u5207\u5165\u70b9**\uff1a\u4ece\u7b97\u6cd5\u4f18\u5316\u3001\u6570\u636e\u589e\u5f3a\u3001\u8de8\u6a21\u6001\u878d\u5408\u7b49\u591a\u4e2a\u89d2\u5ea6\u5207\u5165\uff0c\u63a8\u52a8\u6280\u672f\u8fdb\u6b65\u3002\n\n### \u603b\u7ed3\n\n\u8fd1\u5e74\u6765\uff0cMCTS\u5728PRM\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u7684\u5e94\u7528\u63a2\u7d22\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4e3b\u8981\u4f53\u73b0\u5728\u6280\u672f\u6846\u67b6\u7684\u5b8c\u5584\u3001\u5e94\u7528\u573a\u666f\u7684\u62d3\u5c55\u3001\u6027\u80fd\u7684\u63d0\u5347\u4ee5\u53ca\u7a33\u5b9a\u6027\u548c\u5bb9\u9519\u6027\u7684\u589e\u5f3a\u3002\u7136\u800c\uff0c\u4ecd\u5b58\u5728\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u3001\u6570\u636e\u4f9d\u8d56\u6027\u5f3a\u7b49\u5c40\u9650\u6027\u3002\u672a\u6765\u7684\u7814\u7a76\u5e94\u5173\u6ce8\u9ad8\u6548\u8ba1\u7b97\u3001\u6570\u636e\u589e\u5f3a\u548c\u8de8\u6a21\u6001\u878d\u5408\u7b49\u65b9\u5411\uff0c\u4ee5\u5e94\u5bf9\u52a8\u6001\u73af\u5883\u4e0b\u7684\u51b3\u7b56\u4f18\u5316\u7b49\u6311\u6218\uff0c\u63a8\u52a8\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002", "task_step_name": "\u7a33\u5b9a\u6027", "task_step_description": "\u901a\u8fc7\u6539\u8fdbMCTS\u7684\u9009\u62e9\u7b56\u7565\u548cPRM\u7684\u53c2\u6570\u8c03\u6574\u673a\u5236\uff0c\u63d0\u5347\u4e86\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u3002", "task_step_level": "4>1", "task_step_question": "", "task_step_question_context": [], "task_step_question_answer": "", "ref_task_step_id": ""}, "__type__": "task_step_node"}, "a2b27547-c4b3-4bf8-ae68-c913858a2752": {"__data__": {"id_": "a2b27547-c4b3-4bf8-ae68-c913858a2752", "metadata": {}, "relationships": {}, "hash": "", "start_task_context": "MCTS\u5728PRM\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u7684\u5e94\u7528\u63a2\u7d22\u7efc\u8ff0", "aemo_representation_context": "### Step 1: \u5206\u6790\u8fd1\u51e0\u5e74\u7814\u7a76\u9886\u57df\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba\n\n**\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09**\uff1a\n- **\u57fa\u672c\u539f\u7406**\uff1aMCTS\u662f\u4e00\u79cd\u7528\u4e8e\u51b3\u7b56\u8fc7\u7a0b\u7684\u542f\u53d1\u5f0f\u641c\u7d22\u7b97\u6cd5\uff0c\u5e7f\u6cdb\u5e94\u7528\u4e8e\u6e38\u620fAI\u3001\u8def\u5f84\u89c4\u5212\u7b49\u9886\u57df\u3002\u5176\u6838\u5fc3\u601d\u60f3\u662f\u901a\u8fc7\u6a21\u62df\u591a\u6b21\u968f\u673a\u6e38\u620f\u6765\u8bc4\u4f30\u4e0d\u540c\u51b3\u7b56\u7684\u4f18\u52a3\u3002\n- **\u6280\u672f\u6846\u67b6**\uff1a\u4e3b\u8981\u5305\u62ec\u9009\u62e9\uff08Selection\uff09\u3001\u6269\u5c55\uff08Expansion\uff09\u3001\u6a21\u62df\uff08Simulation\uff09\u548c\u56de\u6eaf\uff08Backpropagation\uff09\u56db\u4e2a\u6b65\u9aa4\u3002\n\n**\u504f\u597d\u7b56\u7565\u6a21\u578b\uff08PRM\uff09**\uff1a\n- **\u57fa\u672c\u539f\u7406**\uff1aPRM\u65e8\u5728\u901a\u8fc7\u5b66\u4e60\u7528\u6237\u7684\u504f\u597d\u6765\u4f18\u5316\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5e38\u7528\u4e8e\u63a8\u8350\u7cfb\u7edf\u3001\u4e2a\u6027\u5316\u670d\u52a1\u7b49\u3002\n- **\u6280\u672f\u6846\u67b6**\uff1a\u901a\u5e38\u5305\u62ec\u504f\u597d\u5efa\u6a21\u3001\u7b56\u7565\u4f18\u5316\u548c\u6a21\u578b\u5fae\u8c03\u7b49\u73af\u8282\u3002\n\n**\u65b9\u6cd5\u8bba**\uff1a\n- **\u96c6\u6210\u5b66\u4e60**\uff1a\u5c06MCTS\u4e0ePRM\u7ed3\u5408\uff0c\u5229\u7528MCTS\u7684\u641c\u7d22\u80fd\u529b\u4f18\u5316PRM\u7684\u51b3\u7b56\u8fc7\u7a0b\u3002\n- **\u5f3a\u5316\u5b66\u4e60**\uff1a\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u673a\u5236\u4e0d\u65ad\u8c03\u6574PRM\u7684\u53c2\u6570\uff0c\u4ee5\u9002\u5e94\u52a8\u6001\u73af\u5883\u3002\n\n### Step 2: \u7814\u7a76\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u4e3b\u8981\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0e\u53d8\u4f53\n\n**\u5e94\u7528\u573a\u666f**\uff1a\n- **\u6e38\u620fAI**\uff1aMCTS\u4e0ePRM\u7ed3\u5408\u7528\u4e8e\u6e38\u620f\u4e2d\u7684\u667a\u80fd\u51b3\u7b56\uff0c\u5982AlphaGo\u3002\n- **\u8def\u5f84\u89c4\u5212**\uff1a\u5728\u673a\u5668\u4eba\u5bfc\u822a\u548c\u81ea\u52a8\u9a7e\u9a76\u4e2d\uff0c\u5229\u7528MCTS\u4f18\u5316\u8def\u5f84\u9009\u62e9\u3002\n- **\u63a8\u8350\u7cfb\u7edf**\uff1a\u901a\u8fc7PRM\u5b66\u4e60\u7528\u6237\u504f\u597d\uff0c\u7ed3\u5408MCTS\u8fdb\u884c\u66f4\u7cbe\u51c6\u7684\u63a8\u8350\u3002\n\n**\u53d8\u4f53**\uff1a\n- **\u6df1\u5ea6MCTS**\uff1a\u7ed3\u5408\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff0c\u63d0\u5347MCTS\u7684\u51b3\u7b56\u80fd\u529b\u3002\n- **\u591a\u6a21\u6001PRM**\uff1a\u878d\u5408\u591a\u79cd\u6570\u636e\u6e90\uff08\u5982\u6587\u672c\u3001\u56fe\u50cf\uff09\uff0c\u589e\u5f3aPRM\u7684\u6cdb\u5316\u80fd\u529b\u3002\n\n### Step 3: \u8bc4\u4f30\u5b66\u672f\u754c\u7684\u6280\u672f\u8fdb\u6b65\u4e0e\u5c40\u9650\u6027\n\n**\u6280\u672f\u8fdb\u6b65**\uff1a\n- **\u6027\u80fd\u63d0\u5347**\uff1aMCTS\u4e0ePRM\u7ed3\u5408\u663e\u8457\u63d0\u5347\u4e86\u51b3\u7b56\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002\n- **\u6cdb\u5316\u80fd\u529b**\uff1a\u591a\u6a21\u6001PRM\u5728\u5904\u7406\u591a\u6837\u5316\u6570\u636e\u65f6\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002\n\n**\u5c40\u9650\u6027**\uff1a\n- **\u8ba1\u7b97\u590d\u6742\u5ea6**\uff1aMCTS\u5728\u5927\u89c4\u6a21\u95ee\u9898\u4e2d\u8ba1\u7b97\u91cf\u5927\uff0c\u6548\u7387\u6709\u5f85\u63d0\u5347\u3002\n- **\u6570\u636e\u4f9d\u8d56**\uff1aPRM\u7684\u6027\u80fd\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u9ad8\u8d28\u91cf\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u6570\u636e\u504f\u5dee\u95ee\u9898\u4f9d\u7136\u5b58\u5728\u3002\n\n### Step 4: \u63a2\u8ba8\u8ba1\u7b97\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\n\n**\u9002\u7528\u6027**\uff1a\n- **\u591a\u9886\u57df\u5e94\u7528**\uff1aMCTS+PRM\u6846\u67b6\u5728\u6e38\u620f\u3001\u8def\u5f84\u89c4\u5212\u3001\u63a8\u8350\u7cfb\u7edf\u7b49\u591a\u4e2a\u9886\u57df\u8868\u73b0\u51fa\u826f\u597d\u7684\u9002\u7528\u6027\u3002\n- **\u591a\u6a21\u6001\u6570\u636e**\uff1a\u591a\u6a21\u6001PRM\u80fd\u591f\u5904\u7406\u591a\u79cd\u7c7b\u578b\u7684\u6570\u636e\uff0c\u9002\u7528\u4e8e\u590d\u6742\u5e94\u7528\u573a\u666f\u3002\n\n**\u6cdb\u5316\u80fd\u529b**\uff1a\n- **\u8de8\u9886\u57df\u6cdb\u5316**\uff1a\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\uff0cMCTS+PRM\u6846\u67b6\u5728\u4e0d\u540c\u9886\u57df\u95f4\u5c55\u73b0\u51fa\u8f83\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002\n- **\u52a8\u6001\u73af\u5883\u9002\u5e94**\uff1a\u5f3a\u5316\u5b66\u4e60\u673a\u5236\u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u9002\u5e94\u52a8\u6001\u53d8\u5316\u7684\u73af\u5883\u3002\n\n### Step 5: \u5206\u6790\u6700\u65b0\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027\n\n**\u7a33\u5b9a\u6027**\uff1a\n- **\u7b97\u6cd5\u4f18\u5316**\uff1a\u901a\u8fc7\u6539\u8fdbMCTS\u7684\u9009\u62e9\u7b56\u7565\u548cPRM\u7684\u53c2\u6570\u8c03\u6574\u673a\u5236\uff0c\u63d0\u5347\u4e86\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u3002\n- **\u9c81\u68d2\u6027\u589e\u5f3a**\uff1a\u5f15\u5165\u5bf9\u6297\u8bad\u7ec3\u548c\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\uff0c\u63d0\u9ad8\u6a21\u578b\u5728\u590d\u6742\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u6027\u3002\n\n**\u5bb9\u9519\u6027**\uff1a\n- **\u9519\u8bef\u5904\u7406**\uff1a\u5728\u6a21\u62df\u548c\u56de\u6eaf\u9636\u6bb5\u5f15\u5165\u5bb9\u9519\u673a\u5236\uff0c\u51cf\u5c11\u9519\u8bef\u51b3\u7b56\u7684\u5f71\u54cd\u3002\n- **\u5927\u89c4\u6a21\u6570\u636e\u9002\u5e94\u6027**\uff1a\u4f18\u5316\u7b97\u6cd5\u7ed3\u6784\uff0c\u63d0\u5347\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\u7684\u5904\u7406\u80fd\u529b\u3002\n\n### Step 6: \u8bc4\u4f30\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218\n\n**\u672a\u6765\u7814\u7a76\u65b9\u5411**\uff1a\n- **\u9ad8\u6548\u8ba1\u7b97**\uff1a\u7814\u7a76\u66f4\u9ad8\u6548\u7684MCTS\u7b97\u6cd5\uff0c\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002\n- **\u6570\u636e\u589e\u5f3a**\uff1a\u63a2\u7d22\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u51cf\u5c11PRM\u5bf9\u9ad8\u8d28\u91cf\u6570\u636e\u7684\u4f9d\u8d56\u3002\n\n**\u6311\u6218**\uff1a\n- **\u8de8\u6a21\u6001\u878d\u5408**\uff1a\u5982\u4f55\u6709\u6548\u878d\u5408\u591a\u79cd\u6a21\u6001\u6570\u636e\uff0c\u63d0\u5347\u6a21\u578b\u7684\u7efc\u5408\u6027\u80fd\u3002\n- **\u52a8\u6001\u73af\u5883\u9002\u5e94**\uff1a\u5728\u52a8\u6001\u73af\u5883\u4e0b\uff0c\u5982\u4f55\u4fdd\u6301\u6a21\u578b\u7684\u7a33\u5b9a\u6027\u548c\u9ad8\u6548\u6027\u3002\n\n**\u63a8\u52a8\u540e\u7eed\u7814\u7a76**\uff1a\n- **\u65b0\u95ee\u9898\u63d0\u51fa**\uff1a\u5982\u591a\u6a21\u6001\u6570\u636e\u4e0b\u7684\u504f\u597d\u5efa\u6a21\u3001\u52a8\u6001\u73af\u5883\u4e0b\u7684\u51b3\u7b56\u4f18\u5316\u7b49\u3002\n- **\u7814\u7a76\u5207\u5165\u70b9**\uff1a\u4ece\u7b97\u6cd5\u4f18\u5316\u3001\u6570\u636e\u589e\u5f3a\u3001\u8de8\u6a21\u6001\u878d\u5408\u7b49\u591a\u4e2a\u89d2\u5ea6\u5207\u5165\uff0c\u63a8\u52a8\u6280\u672f\u8fdb\u6b65\u3002\n\n### \u603b\u7ed3\n\n\u8fd1\u5e74\u6765\uff0cMCTS\u5728PRM\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u7684\u5e94\u7528\u63a2\u7d22\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4e3b\u8981\u4f53\u73b0\u5728\u6280\u672f\u6846\u67b6\u7684\u5b8c\u5584\u3001\u5e94\u7528\u573a\u666f\u7684\u62d3\u5c55\u3001\u6027\u80fd\u7684\u63d0\u5347\u4ee5\u53ca\u7a33\u5b9a\u6027\u548c\u5bb9\u9519\u6027\u7684\u589e\u5f3a\u3002\u7136\u800c\uff0c\u4ecd\u5b58\u5728\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u3001\u6570\u636e\u4f9d\u8d56\u6027\u5f3a\u7b49\u5c40\u9650\u6027\u3002\u672a\u6765\u7684\u7814\u7a76\u5e94\u5173\u6ce8\u9ad8\u6548\u8ba1\u7b97\u3001\u6570\u636e\u589e\u5f3a\u548c\u8de8\u6a21\u6001\u878d\u5408\u7b49\u65b9\u5411\uff0c\u4ee5\u5e94\u5bf9\u52a8\u6001\u73af\u5883\u4e0b\u7684\u51b3\u7b56\u4f18\u5316\u7b49\u6311\u6218\uff0c\u63a8\u52a8\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002", "task_step_name": "\u5bb9\u9519\u6027", "task_step_description": "\u5728\u6a21\u62df\u548c\u56de\u6eaf\u9636\u6bb5\u5f15\u5165\u5bb9\u9519\u673a\u5236\uff0c\u51cf\u5c11\u9519\u8bef\u51b3\u7b56\u7684\u5f71\u54cd\u3002", "task_step_level": "4>2", "task_step_question": "", "task_step_question_context": [], "task_step_question_answer": "", "ref_task_step_id": ""}, "__type__": "task_step_node"}, "ac68fbc2-cec5-4dc0-9a52-1d0003ba3731": {"__data__": {"id_": "ac68fbc2-cec5-4dc0-9a52-1d0003ba3731", "metadata": {}, "relationships": {}, "hash": "", "start_task_context": "MCTS\u5728PRM\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u7684\u5e94\u7528\u63a2\u7d22\u7efc\u8ff0", "aemo_representation_context": "### Step 1: \u5206\u6790\u8fd1\u51e0\u5e74\u7814\u7a76\u9886\u57df\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba\n\n**\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09**\uff1a\n- **\u57fa\u672c\u539f\u7406**\uff1aMCTS\u662f\u4e00\u79cd\u7528\u4e8e\u51b3\u7b56\u8fc7\u7a0b\u7684\u542f\u53d1\u5f0f\u641c\u7d22\u7b97\u6cd5\uff0c\u5e7f\u6cdb\u5e94\u7528\u4e8e\u6e38\u620fAI\u3001\u8def\u5f84\u89c4\u5212\u7b49\u9886\u57df\u3002\u5176\u6838\u5fc3\u601d\u60f3\u662f\u901a\u8fc7\u6a21\u62df\u591a\u6b21\u968f\u673a\u6e38\u620f\u6765\u8bc4\u4f30\u4e0d\u540c\u51b3\u7b56\u7684\u4f18\u52a3\u3002\n- **\u6280\u672f\u6846\u67b6**\uff1a\u4e3b\u8981\u5305\u62ec\u9009\u62e9\uff08Selection\uff09\u3001\u6269\u5c55\uff08Expansion\uff09\u3001\u6a21\u62df\uff08Simulation\uff09\u548c\u56de\u6eaf\uff08Backpropagation\uff09\u56db\u4e2a\u6b65\u9aa4\u3002\n\n**\u504f\u597d\u7b56\u7565\u6a21\u578b\uff08PRM\uff09**\uff1a\n- **\u57fa\u672c\u539f\u7406**\uff1aPRM\u65e8\u5728\u901a\u8fc7\u5b66\u4e60\u7528\u6237\u7684\u504f\u597d\u6765\u4f18\u5316\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5e38\u7528\u4e8e\u63a8\u8350\u7cfb\u7edf\u3001\u4e2a\u6027\u5316\u670d\u52a1\u7b49\u3002\n- **\u6280\u672f\u6846\u67b6**\uff1a\u901a\u5e38\u5305\u62ec\u504f\u597d\u5efa\u6a21\u3001\u7b56\u7565\u4f18\u5316\u548c\u6a21\u578b\u5fae\u8c03\u7b49\u73af\u8282\u3002\n\n**\u65b9\u6cd5\u8bba**\uff1a\n- **\u96c6\u6210\u5b66\u4e60**\uff1a\u5c06MCTS\u4e0ePRM\u7ed3\u5408\uff0c\u5229\u7528MCTS\u7684\u641c\u7d22\u80fd\u529b\u4f18\u5316PRM\u7684\u51b3\u7b56\u8fc7\u7a0b\u3002\n- **\u5f3a\u5316\u5b66\u4e60**\uff1a\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u673a\u5236\u4e0d\u65ad\u8c03\u6574PRM\u7684\u53c2\u6570\uff0c\u4ee5\u9002\u5e94\u52a8\u6001\u73af\u5883\u3002\n\n### Step 2: \u7814\u7a76\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u4e3b\u8981\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0e\u53d8\u4f53\n\n**\u5e94\u7528\u573a\u666f**\uff1a\n- **\u6e38\u620fAI**\uff1aMCTS\u4e0ePRM\u7ed3\u5408\u7528\u4e8e\u6e38\u620f\u4e2d\u7684\u667a\u80fd\u51b3\u7b56\uff0c\u5982AlphaGo\u3002\n- **\u8def\u5f84\u89c4\u5212**\uff1a\u5728\u673a\u5668\u4eba\u5bfc\u822a\u548c\u81ea\u52a8\u9a7e\u9a76\u4e2d\uff0c\u5229\u7528MCTS\u4f18\u5316\u8def\u5f84\u9009\u62e9\u3002\n- **\u63a8\u8350\u7cfb\u7edf**\uff1a\u901a\u8fc7PRM\u5b66\u4e60\u7528\u6237\u504f\u597d\uff0c\u7ed3\u5408MCTS\u8fdb\u884c\u66f4\u7cbe\u51c6\u7684\u63a8\u8350\u3002\n\n**\u53d8\u4f53**\uff1a\n- **\u6df1\u5ea6MCTS**\uff1a\u7ed3\u5408\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff0c\u63d0\u5347MCTS\u7684\u51b3\u7b56\u80fd\u529b\u3002\n- **\u591a\u6a21\u6001PRM**\uff1a\u878d\u5408\u591a\u79cd\u6570\u636e\u6e90\uff08\u5982\u6587\u672c\u3001\u56fe\u50cf\uff09\uff0c\u589e\u5f3aPRM\u7684\u6cdb\u5316\u80fd\u529b\u3002\n\n### Step 3: \u8bc4\u4f30\u5b66\u672f\u754c\u7684\u6280\u672f\u8fdb\u6b65\u4e0e\u5c40\u9650\u6027\n\n**\u6280\u672f\u8fdb\u6b65**\uff1a\n- **\u6027\u80fd\u63d0\u5347**\uff1aMCTS\u4e0ePRM\u7ed3\u5408\u663e\u8457\u63d0\u5347\u4e86\u51b3\u7b56\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002\n- **\u6cdb\u5316\u80fd\u529b**\uff1a\u591a\u6a21\u6001PRM\u5728\u5904\u7406\u591a\u6837\u5316\u6570\u636e\u65f6\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002\n\n**\u5c40\u9650\u6027**\uff1a\n- **\u8ba1\u7b97\u590d\u6742\u5ea6**\uff1aMCTS\u5728\u5927\u89c4\u6a21\u95ee\u9898\u4e2d\u8ba1\u7b97\u91cf\u5927\uff0c\u6548\u7387\u6709\u5f85\u63d0\u5347\u3002\n- **\u6570\u636e\u4f9d\u8d56**\uff1aPRM\u7684\u6027\u80fd\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u9ad8\u8d28\u91cf\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u6570\u636e\u504f\u5dee\u95ee\u9898\u4f9d\u7136\u5b58\u5728\u3002\n\n### Step 4: \u63a2\u8ba8\u8ba1\u7b97\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\n\n**\u9002\u7528\u6027**\uff1a\n- **\u591a\u9886\u57df\u5e94\u7528**\uff1aMCTS+PRM\u6846\u67b6\u5728\u6e38\u620f\u3001\u8def\u5f84\u89c4\u5212\u3001\u63a8\u8350\u7cfb\u7edf\u7b49\u591a\u4e2a\u9886\u57df\u8868\u73b0\u51fa\u826f\u597d\u7684\u9002\u7528\u6027\u3002\n- **\u591a\u6a21\u6001\u6570\u636e**\uff1a\u591a\u6a21\u6001PRM\u80fd\u591f\u5904\u7406\u591a\u79cd\u7c7b\u578b\u7684\u6570\u636e\uff0c\u9002\u7528\u4e8e\u590d\u6742\u5e94\u7528\u573a\u666f\u3002\n\n**\u6cdb\u5316\u80fd\u529b**\uff1a\n- **\u8de8\u9886\u57df\u6cdb\u5316**\uff1a\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\uff0cMCTS+PRM\u6846\u67b6\u5728\u4e0d\u540c\u9886\u57df\u95f4\u5c55\u73b0\u51fa\u8f83\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002\n- **\u52a8\u6001\u73af\u5883\u9002\u5e94**\uff1a\u5f3a\u5316\u5b66\u4e60\u673a\u5236\u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u9002\u5e94\u52a8\u6001\u53d8\u5316\u7684\u73af\u5883\u3002\n\n### Step 5: \u5206\u6790\u6700\u65b0\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027\n\n**\u7a33\u5b9a\u6027**\uff1a\n- **\u7b97\u6cd5\u4f18\u5316**\uff1a\u901a\u8fc7\u6539\u8fdbMCTS\u7684\u9009\u62e9\u7b56\u7565\u548cPRM\u7684\u53c2\u6570\u8c03\u6574\u673a\u5236\uff0c\u63d0\u5347\u4e86\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u3002\n- **\u9c81\u68d2\u6027\u589e\u5f3a**\uff1a\u5f15\u5165\u5bf9\u6297\u8bad\u7ec3\u548c\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\uff0c\u63d0\u9ad8\u6a21\u578b\u5728\u590d\u6742\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u6027\u3002\n\n**\u5bb9\u9519\u6027**\uff1a\n- **\u9519\u8bef\u5904\u7406**\uff1a\u5728\u6a21\u62df\u548c\u56de\u6eaf\u9636\u6bb5\u5f15\u5165\u5bb9\u9519\u673a\u5236\uff0c\u51cf\u5c11\u9519\u8bef\u51b3\u7b56\u7684\u5f71\u54cd\u3002\n- **\u5927\u89c4\u6a21\u6570\u636e\u9002\u5e94\u6027**\uff1a\u4f18\u5316\u7b97\u6cd5\u7ed3\u6784\uff0c\u63d0\u5347\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\u7684\u5904\u7406\u80fd\u529b\u3002\n\n### Step 6: \u8bc4\u4f30\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218\n\n**\u672a\u6765\u7814\u7a76\u65b9\u5411**\uff1a\n- **\u9ad8\u6548\u8ba1\u7b97**\uff1a\u7814\u7a76\u66f4\u9ad8\u6548\u7684MCTS\u7b97\u6cd5\uff0c\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002\n- **\u6570\u636e\u589e\u5f3a**\uff1a\u63a2\u7d22\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u51cf\u5c11PRM\u5bf9\u9ad8\u8d28\u91cf\u6570\u636e\u7684\u4f9d\u8d56\u3002\n\n**\u6311\u6218**\uff1a\n- **\u8de8\u6a21\u6001\u878d\u5408**\uff1a\u5982\u4f55\u6709\u6548\u878d\u5408\u591a\u79cd\u6a21\u6001\u6570\u636e\uff0c\u63d0\u5347\u6a21\u578b\u7684\u7efc\u5408\u6027\u80fd\u3002\n- **\u52a8\u6001\u73af\u5883\u9002\u5e94**\uff1a\u5728\u52a8\u6001\u73af\u5883\u4e0b\uff0c\u5982\u4f55\u4fdd\u6301\u6a21\u578b\u7684\u7a33\u5b9a\u6027\u548c\u9ad8\u6548\u6027\u3002\n\n**\u63a8\u52a8\u540e\u7eed\u7814\u7a76**\uff1a\n- **\u65b0\u95ee\u9898\u63d0\u51fa**\uff1a\u5982\u591a\u6a21\u6001\u6570\u636e\u4e0b\u7684\u504f\u597d\u5efa\u6a21\u3001\u52a8\u6001\u73af\u5883\u4e0b\u7684\u51b3\u7b56\u4f18\u5316\u7b49\u3002\n- **\u7814\u7a76\u5207\u5165\u70b9**\uff1a\u4ece\u7b97\u6cd5\u4f18\u5316\u3001\u6570\u636e\u589e\u5f3a\u3001\u8de8\u6a21\u6001\u878d\u5408\u7b49\u591a\u4e2a\u89d2\u5ea6\u5207\u5165\uff0c\u63a8\u52a8\u6280\u672f\u8fdb\u6b65\u3002\n\n### \u603b\u7ed3\n\n\u8fd1\u5e74\u6765\uff0cMCTS\u5728PRM\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u7684\u5e94\u7528\u63a2\u7d22\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4e3b\u8981\u4f53\u73b0\u5728\u6280\u672f\u6846\u67b6\u7684\u5b8c\u5584\u3001\u5e94\u7528\u573a\u666f\u7684\u62d3\u5c55\u3001\u6027\u80fd\u7684\u63d0\u5347\u4ee5\u53ca\u7a33\u5b9a\u6027\u548c\u5bb9\u9519\u6027\u7684\u589e\u5f3a\u3002\u7136\u800c\uff0c\u4ecd\u5b58\u5728\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u3001\u6570\u636e\u4f9d\u8d56\u6027\u5f3a\u7b49\u5c40\u9650\u6027\u3002\u672a\u6765\u7684\u7814\u7a76\u5e94\u5173\u6ce8\u9ad8\u6548\u8ba1\u7b97\u3001\u6570\u636e\u589e\u5f3a\u548c\u8de8\u6a21\u6001\u878d\u5408\u7b49\u65b9\u5411\uff0c\u4ee5\u5e94\u5bf9\u52a8\u6001\u73af\u5883\u4e0b\u7684\u51b3\u7b56\u4f18\u5316\u7b49\u6311\u6218\uff0c\u63a8\u52a8\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002", "task_step_name": "\u8bc4\u4f30\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218", "task_step_description": "\u7814\u7a76\u66f4\u9ad8\u6548\u7684MCTS\u7b97\u6cd5\uff0c\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "task_step_level": "5", "task_step_question": "", "task_step_question_context": [], "task_step_question_answer": "", "ref_task_step_id": ""}, "__type__": "task_step_node"}, "7895a383-ce04-42fa-9517-e0cea2b11f50": {"__data__": {"id_": "7895a383-ce04-42fa-9517-e0cea2b11f50", "metadata": {}, "relationships": {}, "hash": "", "start_task_context": "MCTS\u5728PRM\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u7684\u5e94\u7528\u63a2\u7d22\u7efc\u8ff0", "aemo_representation_context": "### Step 1: \u5206\u6790\u8fd1\u51e0\u5e74\u7814\u7a76\u9886\u57df\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba\n\n**\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09**\uff1a\n- **\u57fa\u672c\u539f\u7406**\uff1aMCTS\u662f\u4e00\u79cd\u7528\u4e8e\u51b3\u7b56\u8fc7\u7a0b\u7684\u542f\u53d1\u5f0f\u641c\u7d22\u7b97\u6cd5\uff0c\u5e7f\u6cdb\u5e94\u7528\u4e8e\u6e38\u620fAI\u3001\u8def\u5f84\u89c4\u5212\u7b49\u9886\u57df\u3002\u5176\u6838\u5fc3\u601d\u60f3\u662f\u901a\u8fc7\u6a21\u62df\u591a\u6b21\u968f\u673a\u6e38\u620f\u6765\u8bc4\u4f30\u4e0d\u540c\u51b3\u7b56\u7684\u4f18\u52a3\u3002\n- **\u6280\u672f\u6846\u67b6**\uff1a\u4e3b\u8981\u5305\u62ec\u9009\u62e9\uff08Selection\uff09\u3001\u6269\u5c55\uff08Expansion\uff09\u3001\u6a21\u62df\uff08Simulation\uff09\u548c\u56de\u6eaf\uff08Backpropagation\uff09\u56db\u4e2a\u6b65\u9aa4\u3002\n\n**\u504f\u597d\u7b56\u7565\u6a21\u578b\uff08PRM\uff09**\uff1a\n- **\u57fa\u672c\u539f\u7406**\uff1aPRM\u65e8\u5728\u901a\u8fc7\u5b66\u4e60\u7528\u6237\u7684\u504f\u597d\u6765\u4f18\u5316\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5e38\u7528\u4e8e\u63a8\u8350\u7cfb\u7edf\u3001\u4e2a\u6027\u5316\u670d\u52a1\u7b49\u3002\n- **\u6280\u672f\u6846\u67b6**\uff1a\u901a\u5e38\u5305\u62ec\u504f\u597d\u5efa\u6a21\u3001\u7b56\u7565\u4f18\u5316\u548c\u6a21\u578b\u5fae\u8c03\u7b49\u73af\u8282\u3002\n\n**\u65b9\u6cd5\u8bba**\uff1a\n- **\u96c6\u6210\u5b66\u4e60**\uff1a\u5c06MCTS\u4e0ePRM\u7ed3\u5408\uff0c\u5229\u7528MCTS\u7684\u641c\u7d22\u80fd\u529b\u4f18\u5316PRM\u7684\u51b3\u7b56\u8fc7\u7a0b\u3002\n- **\u5f3a\u5316\u5b66\u4e60**\uff1a\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u673a\u5236\u4e0d\u65ad\u8c03\u6574PRM\u7684\u53c2\u6570\uff0c\u4ee5\u9002\u5e94\u52a8\u6001\u73af\u5883\u3002\n\n### Step 2: \u7814\u7a76\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u4e3b\u8981\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0e\u53d8\u4f53\n\n**\u5e94\u7528\u573a\u666f**\uff1a\n- **\u6e38\u620fAI**\uff1aMCTS\u4e0ePRM\u7ed3\u5408\u7528\u4e8e\u6e38\u620f\u4e2d\u7684\u667a\u80fd\u51b3\u7b56\uff0c\u5982AlphaGo\u3002\n- **\u8def\u5f84\u89c4\u5212**\uff1a\u5728\u673a\u5668\u4eba\u5bfc\u822a\u548c\u81ea\u52a8\u9a7e\u9a76\u4e2d\uff0c\u5229\u7528MCTS\u4f18\u5316\u8def\u5f84\u9009\u62e9\u3002\n- **\u63a8\u8350\u7cfb\u7edf**\uff1a\u901a\u8fc7PRM\u5b66\u4e60\u7528\u6237\u504f\u597d\uff0c\u7ed3\u5408MCTS\u8fdb\u884c\u66f4\u7cbe\u51c6\u7684\u63a8\u8350\u3002\n\n**\u53d8\u4f53**\uff1a\n- **\u6df1\u5ea6MCTS**\uff1a\u7ed3\u5408\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff0c\u63d0\u5347MCTS\u7684\u51b3\u7b56\u80fd\u529b\u3002\n- **\u591a\u6a21\u6001PRM**\uff1a\u878d\u5408\u591a\u79cd\u6570\u636e\u6e90\uff08\u5982\u6587\u672c\u3001\u56fe\u50cf\uff09\uff0c\u589e\u5f3aPRM\u7684\u6cdb\u5316\u80fd\u529b\u3002\n\n### Step 3: \u8bc4\u4f30\u5b66\u672f\u754c\u7684\u6280\u672f\u8fdb\u6b65\u4e0e\u5c40\u9650\u6027\n\n**\u6280\u672f\u8fdb\u6b65**\uff1a\n- **\u6027\u80fd\u63d0\u5347**\uff1aMCTS\u4e0ePRM\u7ed3\u5408\u663e\u8457\u63d0\u5347\u4e86\u51b3\u7b56\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002\n- **\u6cdb\u5316\u80fd\u529b**\uff1a\u591a\u6a21\u6001PRM\u5728\u5904\u7406\u591a\u6837\u5316\u6570\u636e\u65f6\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002\n\n**\u5c40\u9650\u6027**\uff1a\n- **\u8ba1\u7b97\u590d\u6742\u5ea6**\uff1aMCTS\u5728\u5927\u89c4\u6a21\u95ee\u9898\u4e2d\u8ba1\u7b97\u91cf\u5927\uff0c\u6548\u7387\u6709\u5f85\u63d0\u5347\u3002\n- **\u6570\u636e\u4f9d\u8d56**\uff1aPRM\u7684\u6027\u80fd\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u9ad8\u8d28\u91cf\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u6570\u636e\u504f\u5dee\u95ee\u9898\u4f9d\u7136\u5b58\u5728\u3002\n\n### Step 4: \u63a2\u8ba8\u8ba1\u7b97\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\n\n**\u9002\u7528\u6027**\uff1a\n- **\u591a\u9886\u57df\u5e94\u7528**\uff1aMCTS+PRM\u6846\u67b6\u5728\u6e38\u620f\u3001\u8def\u5f84\u89c4\u5212\u3001\u63a8\u8350\u7cfb\u7edf\u7b49\u591a\u4e2a\u9886\u57df\u8868\u73b0\u51fa\u826f\u597d\u7684\u9002\u7528\u6027\u3002\n- **\u591a\u6a21\u6001\u6570\u636e**\uff1a\u591a\u6a21\u6001PRM\u80fd\u591f\u5904\u7406\u591a\u79cd\u7c7b\u578b\u7684\u6570\u636e\uff0c\u9002\u7528\u4e8e\u590d\u6742\u5e94\u7528\u573a\u666f\u3002\n\n**\u6cdb\u5316\u80fd\u529b**\uff1a\n- **\u8de8\u9886\u57df\u6cdb\u5316**\uff1a\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\uff0cMCTS+PRM\u6846\u67b6\u5728\u4e0d\u540c\u9886\u57df\u95f4\u5c55\u73b0\u51fa\u8f83\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002\n- **\u52a8\u6001\u73af\u5883\u9002\u5e94**\uff1a\u5f3a\u5316\u5b66\u4e60\u673a\u5236\u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u9002\u5e94\u52a8\u6001\u53d8\u5316\u7684\u73af\u5883\u3002\n\n### Step 5: \u5206\u6790\u6700\u65b0\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027\n\n**\u7a33\u5b9a\u6027**\uff1a\n- **\u7b97\u6cd5\u4f18\u5316**\uff1a\u901a\u8fc7\u6539\u8fdbMCTS\u7684\u9009\u62e9\u7b56\u7565\u548cPRM\u7684\u53c2\u6570\u8c03\u6574\u673a\u5236\uff0c\u63d0\u5347\u4e86\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u3002\n- **\u9c81\u68d2\u6027\u589e\u5f3a**\uff1a\u5f15\u5165\u5bf9\u6297\u8bad\u7ec3\u548c\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\uff0c\u63d0\u9ad8\u6a21\u578b\u5728\u590d\u6742\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u6027\u3002\n\n**\u5bb9\u9519\u6027**\uff1a\n- **\u9519\u8bef\u5904\u7406**\uff1a\u5728\u6a21\u62df\u548c\u56de\u6eaf\u9636\u6bb5\u5f15\u5165\u5bb9\u9519\u673a\u5236\uff0c\u51cf\u5c11\u9519\u8bef\u51b3\u7b56\u7684\u5f71\u54cd\u3002\n- **\u5927\u89c4\u6a21\u6570\u636e\u9002\u5e94\u6027**\uff1a\u4f18\u5316\u7b97\u6cd5\u7ed3\u6784\uff0c\u63d0\u5347\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\u7684\u5904\u7406\u80fd\u529b\u3002\n\n### Step 6: \u8bc4\u4f30\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218\n\n**\u672a\u6765\u7814\u7a76\u65b9\u5411**\uff1a\n- **\u9ad8\u6548\u8ba1\u7b97**\uff1a\u7814\u7a76\u66f4\u9ad8\u6548\u7684MCTS\u7b97\u6cd5\uff0c\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002\n- **\u6570\u636e\u589e\u5f3a**\uff1a\u63a2\u7d22\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u51cf\u5c11PRM\u5bf9\u9ad8\u8d28\u91cf\u6570\u636e\u7684\u4f9d\u8d56\u3002\n\n**\u6311\u6218**\uff1a\n- **\u8de8\u6a21\u6001\u878d\u5408**\uff1a\u5982\u4f55\u6709\u6548\u878d\u5408\u591a\u79cd\u6a21\u6001\u6570\u636e\uff0c\u63d0\u5347\u6a21\u578b\u7684\u7efc\u5408\u6027\u80fd\u3002\n- **\u52a8\u6001\u73af\u5883\u9002\u5e94**\uff1a\u5728\u52a8\u6001\u73af\u5883\u4e0b\uff0c\u5982\u4f55\u4fdd\u6301\u6a21\u578b\u7684\u7a33\u5b9a\u6027\u548c\u9ad8\u6548\u6027\u3002\n\n**\u63a8\u52a8\u540e\u7eed\u7814\u7a76**\uff1a\n- **\u65b0\u95ee\u9898\u63d0\u51fa**\uff1a\u5982\u591a\u6a21\u6001\u6570\u636e\u4e0b\u7684\u504f\u597d\u5efa\u6a21\u3001\u52a8\u6001\u73af\u5883\u4e0b\u7684\u51b3\u7b56\u4f18\u5316\u7b49\u3002\n- **\u7814\u7a76\u5207\u5165\u70b9**\uff1a\u4ece\u7b97\u6cd5\u4f18\u5316\u3001\u6570\u636e\u589e\u5f3a\u3001\u8de8\u6a21\u6001\u878d\u5408\u7b49\u591a\u4e2a\u89d2\u5ea6\u5207\u5165\uff0c\u63a8\u52a8\u6280\u672f\u8fdb\u6b65\u3002\n\n### \u603b\u7ed3\n\n\u8fd1\u5e74\u6765\uff0cMCTS\u5728PRM\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u7684\u5e94\u7528\u63a2\u7d22\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4e3b\u8981\u4f53\u73b0\u5728\u6280\u672f\u6846\u67b6\u7684\u5b8c\u5584\u3001\u5e94\u7528\u573a\u666f\u7684\u62d3\u5c55\u3001\u6027\u80fd\u7684\u63d0\u5347\u4ee5\u53ca\u7a33\u5b9a\u6027\u548c\u5bb9\u9519\u6027\u7684\u589e\u5f3a\u3002\u7136\u800c\uff0c\u4ecd\u5b58\u5728\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u3001\u6570\u636e\u4f9d\u8d56\u6027\u5f3a\u7b49\u5c40\u9650\u6027\u3002\u672a\u6765\u7684\u7814\u7a76\u5e94\u5173\u6ce8\u9ad8\u6548\u8ba1\u7b97\u3001\u6570\u636e\u589e\u5f3a\u548c\u8de8\u6a21\u6001\u878d\u5408\u7b49\u65b9\u5411\uff0c\u4ee5\u5e94\u5bf9\u52a8\u6001\u73af\u5883\u4e0b\u7684\u51b3\u7b56\u4f18\u5316\u7b49\u6311\u6218\uff0c\u63a8\u52a8\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002", "task_step_name": "\u672a\u6765\u7814\u7a76\u65b9\u5411", "task_step_description": "\u7814\u7a76\u66f4\u9ad8\u6548\u7684MCTS\u7b97\u6cd5\uff0c\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "task_step_level": "5>1", "task_step_question": "", "task_step_question_context": [], "task_step_question_answer": "", "ref_task_step_id": ""}, "__type__": "task_step_node"}, "595b81fe-b868-420f-a43e-e551a8cbcce1": {"__data__": {"id_": "595b81fe-b868-420f-a43e-e551a8cbcce1", "metadata": {}, "relationships": {}, "hash": "", "start_task_context": "MCTS\u5728PRM\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u7684\u5e94\u7528\u63a2\u7d22\u7efc\u8ff0", "aemo_representation_context": "### Step 1: \u5206\u6790\u8fd1\u51e0\u5e74\u7814\u7a76\u9886\u57df\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba\n\n**\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09**\uff1a\n- **\u57fa\u672c\u539f\u7406**\uff1aMCTS\u662f\u4e00\u79cd\u7528\u4e8e\u51b3\u7b56\u8fc7\u7a0b\u7684\u542f\u53d1\u5f0f\u641c\u7d22\u7b97\u6cd5\uff0c\u5e7f\u6cdb\u5e94\u7528\u4e8e\u6e38\u620fAI\u3001\u8def\u5f84\u89c4\u5212\u7b49\u9886\u57df\u3002\u5176\u6838\u5fc3\u601d\u60f3\u662f\u901a\u8fc7\u6a21\u62df\u591a\u6b21\u968f\u673a\u6e38\u620f\u6765\u8bc4\u4f30\u4e0d\u540c\u51b3\u7b56\u7684\u4f18\u52a3\u3002\n- **\u6280\u672f\u6846\u67b6**\uff1a\u4e3b\u8981\u5305\u62ec\u9009\u62e9\uff08Selection\uff09\u3001\u6269\u5c55\uff08Expansion\uff09\u3001\u6a21\u62df\uff08Simulation\uff09\u548c\u56de\u6eaf\uff08Backpropagation\uff09\u56db\u4e2a\u6b65\u9aa4\u3002\n\n**\u504f\u597d\u7b56\u7565\u6a21\u578b\uff08PRM\uff09**\uff1a\n- **\u57fa\u672c\u539f\u7406**\uff1aPRM\u65e8\u5728\u901a\u8fc7\u5b66\u4e60\u7528\u6237\u7684\u504f\u597d\u6765\u4f18\u5316\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5e38\u7528\u4e8e\u63a8\u8350\u7cfb\u7edf\u3001\u4e2a\u6027\u5316\u670d\u52a1\u7b49\u3002\n- **\u6280\u672f\u6846\u67b6**\uff1a\u901a\u5e38\u5305\u62ec\u504f\u597d\u5efa\u6a21\u3001\u7b56\u7565\u4f18\u5316\u548c\u6a21\u578b\u5fae\u8c03\u7b49\u73af\u8282\u3002\n\n**\u65b9\u6cd5\u8bba**\uff1a\n- **\u96c6\u6210\u5b66\u4e60**\uff1a\u5c06MCTS\u4e0ePRM\u7ed3\u5408\uff0c\u5229\u7528MCTS\u7684\u641c\u7d22\u80fd\u529b\u4f18\u5316PRM\u7684\u51b3\u7b56\u8fc7\u7a0b\u3002\n- **\u5f3a\u5316\u5b66\u4e60**\uff1a\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u673a\u5236\u4e0d\u65ad\u8c03\u6574PRM\u7684\u53c2\u6570\uff0c\u4ee5\u9002\u5e94\u52a8\u6001\u73af\u5883\u3002\n\n### Step 2: \u7814\u7a76\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u4e3b\u8981\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0e\u53d8\u4f53\n\n**\u5e94\u7528\u573a\u666f**\uff1a\n- **\u6e38\u620fAI**\uff1aMCTS\u4e0ePRM\u7ed3\u5408\u7528\u4e8e\u6e38\u620f\u4e2d\u7684\u667a\u80fd\u51b3\u7b56\uff0c\u5982AlphaGo\u3002\n- **\u8def\u5f84\u89c4\u5212**\uff1a\u5728\u673a\u5668\u4eba\u5bfc\u822a\u548c\u81ea\u52a8\u9a7e\u9a76\u4e2d\uff0c\u5229\u7528MCTS\u4f18\u5316\u8def\u5f84\u9009\u62e9\u3002\n- **\u63a8\u8350\u7cfb\u7edf**\uff1a\u901a\u8fc7PRM\u5b66\u4e60\u7528\u6237\u504f\u597d\uff0c\u7ed3\u5408MCTS\u8fdb\u884c\u66f4\u7cbe\u51c6\u7684\u63a8\u8350\u3002\n\n**\u53d8\u4f53**\uff1a\n- **\u6df1\u5ea6MCTS**\uff1a\u7ed3\u5408\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff0c\u63d0\u5347MCTS\u7684\u51b3\u7b56\u80fd\u529b\u3002\n- **\u591a\u6a21\u6001PRM**\uff1a\u878d\u5408\u591a\u79cd\u6570\u636e\u6e90\uff08\u5982\u6587\u672c\u3001\u56fe\u50cf\uff09\uff0c\u589e\u5f3aPRM\u7684\u6cdb\u5316\u80fd\u529b\u3002\n\n### Step 3: \u8bc4\u4f30\u5b66\u672f\u754c\u7684\u6280\u672f\u8fdb\u6b65\u4e0e\u5c40\u9650\u6027\n\n**\u6280\u672f\u8fdb\u6b65**\uff1a\n- **\u6027\u80fd\u63d0\u5347**\uff1aMCTS\u4e0ePRM\u7ed3\u5408\u663e\u8457\u63d0\u5347\u4e86\u51b3\u7b56\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002\n- **\u6cdb\u5316\u80fd\u529b**\uff1a\u591a\u6a21\u6001PRM\u5728\u5904\u7406\u591a\u6837\u5316\u6570\u636e\u65f6\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002\n\n**\u5c40\u9650\u6027**\uff1a\n- **\u8ba1\u7b97\u590d\u6742\u5ea6**\uff1aMCTS\u5728\u5927\u89c4\u6a21\u95ee\u9898\u4e2d\u8ba1\u7b97\u91cf\u5927\uff0c\u6548\u7387\u6709\u5f85\u63d0\u5347\u3002\n- **\u6570\u636e\u4f9d\u8d56**\uff1aPRM\u7684\u6027\u80fd\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u9ad8\u8d28\u91cf\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u6570\u636e\u504f\u5dee\u95ee\u9898\u4f9d\u7136\u5b58\u5728\u3002\n\n### Step 4: \u63a2\u8ba8\u8ba1\u7b97\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\n\n**\u9002\u7528\u6027**\uff1a\n- **\u591a\u9886\u57df\u5e94\u7528**\uff1aMCTS+PRM\u6846\u67b6\u5728\u6e38\u620f\u3001\u8def\u5f84\u89c4\u5212\u3001\u63a8\u8350\u7cfb\u7edf\u7b49\u591a\u4e2a\u9886\u57df\u8868\u73b0\u51fa\u826f\u597d\u7684\u9002\u7528\u6027\u3002\n- **\u591a\u6a21\u6001\u6570\u636e**\uff1a\u591a\u6a21\u6001PRM\u80fd\u591f\u5904\u7406\u591a\u79cd\u7c7b\u578b\u7684\u6570\u636e\uff0c\u9002\u7528\u4e8e\u590d\u6742\u5e94\u7528\u573a\u666f\u3002\n\n**\u6cdb\u5316\u80fd\u529b**\uff1a\n- **\u8de8\u9886\u57df\u6cdb\u5316**\uff1a\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\uff0cMCTS+PRM\u6846\u67b6\u5728\u4e0d\u540c\u9886\u57df\u95f4\u5c55\u73b0\u51fa\u8f83\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002\n- **\u52a8\u6001\u73af\u5883\u9002\u5e94**\uff1a\u5f3a\u5316\u5b66\u4e60\u673a\u5236\u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u9002\u5e94\u52a8\u6001\u53d8\u5316\u7684\u73af\u5883\u3002\n\n### Step 5: \u5206\u6790\u6700\u65b0\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027\n\n**\u7a33\u5b9a\u6027**\uff1a\n- **\u7b97\u6cd5\u4f18\u5316**\uff1a\u901a\u8fc7\u6539\u8fdbMCTS\u7684\u9009\u62e9\u7b56\u7565\u548cPRM\u7684\u53c2\u6570\u8c03\u6574\u673a\u5236\uff0c\u63d0\u5347\u4e86\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u3002\n- **\u9c81\u68d2\u6027\u589e\u5f3a**\uff1a\u5f15\u5165\u5bf9\u6297\u8bad\u7ec3\u548c\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\uff0c\u63d0\u9ad8\u6a21\u578b\u5728\u590d\u6742\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u6027\u3002\n\n**\u5bb9\u9519\u6027**\uff1a\n- **\u9519\u8bef\u5904\u7406**\uff1a\u5728\u6a21\u62df\u548c\u56de\u6eaf\u9636\u6bb5\u5f15\u5165\u5bb9\u9519\u673a\u5236\uff0c\u51cf\u5c11\u9519\u8bef\u51b3\u7b56\u7684\u5f71\u54cd\u3002\n- **\u5927\u89c4\u6a21\u6570\u636e\u9002\u5e94\u6027**\uff1a\u4f18\u5316\u7b97\u6cd5\u7ed3\u6784\uff0c\u63d0\u5347\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\u7684\u5904\u7406\u80fd\u529b\u3002\n\n### Step 6: \u8bc4\u4f30\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218\n\n**\u672a\u6765\u7814\u7a76\u65b9\u5411**\uff1a\n- **\u9ad8\u6548\u8ba1\u7b97**\uff1a\u7814\u7a76\u66f4\u9ad8\u6548\u7684MCTS\u7b97\u6cd5\uff0c\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002\n- **\u6570\u636e\u589e\u5f3a**\uff1a\u63a2\u7d22\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u51cf\u5c11PRM\u5bf9\u9ad8\u8d28\u91cf\u6570\u636e\u7684\u4f9d\u8d56\u3002\n\n**\u6311\u6218**\uff1a\n- **\u8de8\u6a21\u6001\u878d\u5408**\uff1a\u5982\u4f55\u6709\u6548\u878d\u5408\u591a\u79cd\u6a21\u6001\u6570\u636e\uff0c\u63d0\u5347\u6a21\u578b\u7684\u7efc\u5408\u6027\u80fd\u3002\n- **\u52a8\u6001\u73af\u5883\u9002\u5e94**\uff1a\u5728\u52a8\u6001\u73af\u5883\u4e0b\uff0c\u5982\u4f55\u4fdd\u6301\u6a21\u578b\u7684\u7a33\u5b9a\u6027\u548c\u9ad8\u6548\u6027\u3002\n\n**\u63a8\u52a8\u540e\u7eed\u7814\u7a76**\uff1a\n- **\u65b0\u95ee\u9898\u63d0\u51fa**\uff1a\u5982\u591a\u6a21\u6001\u6570\u636e\u4e0b\u7684\u504f\u597d\u5efa\u6a21\u3001\u52a8\u6001\u73af\u5883\u4e0b\u7684\u51b3\u7b56\u4f18\u5316\u7b49\u3002\n- **\u7814\u7a76\u5207\u5165\u70b9**\uff1a\u4ece\u7b97\u6cd5\u4f18\u5316\u3001\u6570\u636e\u589e\u5f3a\u3001\u8de8\u6a21\u6001\u878d\u5408\u7b49\u591a\u4e2a\u89d2\u5ea6\u5207\u5165\uff0c\u63a8\u52a8\u6280\u672f\u8fdb\u6b65\u3002\n\n### \u603b\u7ed3\n\n\u8fd1\u5e74\u6765\uff0cMCTS\u5728PRM\u504f\u597d\u7b56\u7565\u6a21\u578b\u5fae\u8c03\u7684\u5e94\u7528\u63a2\u7d22\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4e3b\u8981\u4f53\u73b0\u5728\u6280\u672f\u6846\u67b6\u7684\u5b8c\u5584\u3001\u5e94\u7528\u573a\u666f\u7684\u62d3\u5c55\u3001\u6027\u80fd\u7684\u63d0\u5347\u4ee5\u53ca\u7a33\u5b9a\u6027\u548c\u5bb9\u9519\u6027\u7684\u589e\u5f3a\u3002\u7136\u800c\uff0c\u4ecd\u5b58\u5728\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u3001\u6570\u636e\u4f9d\u8d56\u6027\u5f3a\u7b49\u5c40\u9650\u6027\u3002\u672a\u6765\u7684\u7814\u7a76\u5e94\u5173\u6ce8\u9ad8\u6548\u8ba1\u7b97\u3001\u6570\u636e\u589e\u5f3a\u548c\u8de8\u6a21\u6001\u878d\u5408\u7b49\u65b9\u5411\uff0c\u4ee5\u5e94\u5bf9\u52a8\u6001\u73af\u5883\u4e0b\u7684\u51b3\u7b56\u4f18\u5316\u7b49\u6311\u6218\uff0c\u63a8\u52a8\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002", "task_step_name": "\u6311\u6218", "task_step_description": "\u5982\u4f55\u6709\u6548\u878d\u5408\u591a\u79cd\u6a21\u6001\u6570\u636e\uff0c\u63d0\u5347\u6a21\u578b\u7684\u7efc\u5408\u6027\u80fd\u3002", "task_step_level": "5>2", "task_step_question": "", "task_step_question_context": [], "task_step_question_answer": "", "ref_task_step_id": ""}, "__type__": "task_step_node"}}, "task_step_store/ref_task_step_info": {"": {"node_ids": ["276fc94b-2aa5-4712-ab27-c1e4f31af69f", "3fc9a567-9ad0-439c-bd1f-3954fb72323f", "372a76b7-8c9e-42aa-b783-cf530fb1852d", "d6916989-efb3-47e8-978b-9f9e079e1eaf", "8a1e6e8e-b71e-46d2-aa53-8f3d04b1d842", "0f8fcb2e-9570-4314-99ab-0d3606636375", "5e563bfc-4301-43c2-9c34-d92ea2c5783a", "589d43ac-52eb-46d5-a6e1-797b6c59a64e", "64b86178-229d-4430-83ef-e052788e3896", "6dba87e8-762d-4764-9182-3555b4b50a86", "d338c352-b6ad-45ff-b9c3-0418aa6cccc4", "edaf9c7c-601d-4068-b117-92e5eaa45af5", "29d7d9b0-9dd0-42e2-a847-f9e6d0c0b37f", "c438cc1b-a630-4359-b8d9-abeaf797d240", "a2b27547-c4b3-4bf8-ae68-c913858a2752", "ac68fbc2-cec5-4dc0-9a52-1d0003ba3731", "7895a383-ce04-42fa-9517-e0cea2b11f50", "595b81fe-b868-420f-a43e-e551a8cbcce1", "8a1e6e8e-b71e-46d2-aa53-8f3d04b1d842", "3fc9a567-9ad0-439c-bd1f-3954fb72323f", "276fc94b-2aa5-4712-ab27-c1e4f31af69f", "0f8fcb2e-9570-4314-99ab-0d3606636375", "d6916989-efb3-47e8-978b-9f9e079e1eaf", "372a76b7-8c9e-42aa-b783-cf530fb1852d", "8a1e6e8e-b71e-46d2-aa53-8f3d04b1d842", "3fc9a567-9ad0-439c-bd1f-3954fb72323f", "276fc94b-2aa5-4712-ab27-c1e4f31af69f", "0f8fcb2e-9570-4314-99ab-0d3606636375", "d6916989-efb3-47e8-978b-9f9e079e1eaf", "372a76b7-8c9e-42aa-b783-cf530fb1852d", "d6916989-efb3-47e8-978b-9f9e079e1eaf", "276fc94b-2aa5-4712-ab27-c1e4f31af69f", "8a1e6e8e-b71e-46d2-aa53-8f3d04b1d842", "3fc9a567-9ad0-439c-bd1f-3954fb72323f", "0f8fcb2e-9570-4314-99ab-0d3606636375", "372a76b7-8c9e-42aa-b783-cf530fb1852d", "d6916989-efb3-47e8-978b-9f9e079e1eaf", "d6916989-efb3-47e8-978b-9f9e079e1eaf", "5e563bfc-4301-43c2-9c34-d92ea2c5783a", "372a76b7-8c9e-42aa-b783-cf530fb1852d", "372a76b7-8c9e-42aa-b783-cf530fb1852d", "0f8fcb2e-9570-4314-99ab-0d3606636375", "0f8fcb2e-9570-4314-99ab-0d3606636375", "64b86178-229d-4430-83ef-e052788e3896", "5e563bfc-4301-43c2-9c34-d92ea2c5783a", "589d43ac-52eb-46d5-a6e1-797b6c59a64e", "64b86178-229d-4430-83ef-e052788e3896", "589d43ac-52eb-46d5-a6e1-797b6c59a64e", "3fc9a567-9ad0-439c-bd1f-3954fb72323f"], "metadata": {}}}, "task_step_store/metadata": {"276fc94b-2aa5-4712-ab27-c1e4f31af69f": {"task_step_hash": "", "ref_task_step_id": ""}, "3fc9a567-9ad0-439c-bd1f-3954fb72323f": {"task_step_hash": "", "ref_task_step_id": ""}, "372a76b7-8c9e-42aa-b783-cf530fb1852d": {"task_step_hash": "", "ref_task_step_id": ""}, "d6916989-efb3-47e8-978b-9f9e079e1eaf": {"task_step_hash": "", "ref_task_step_id": ""}, "8a1e6e8e-b71e-46d2-aa53-8f3d04b1d842": {"task_step_hash": "", "ref_task_step_id": ""}, "0f8fcb2e-9570-4314-99ab-0d3606636375": {"task_step_hash": "", "ref_task_step_id": ""}, "5e563bfc-4301-43c2-9c34-d92ea2c5783a": {"task_step_hash": "", "ref_task_step_id": ""}, "589d43ac-52eb-46d5-a6e1-797b6c59a64e": {"task_step_hash": "", "ref_task_step_id": ""}, "64b86178-229d-4430-83ef-e052788e3896": {"task_step_hash": "", "ref_task_step_id": ""}, "6dba87e8-762d-4764-9182-3555b4b50a86": {"task_step_hash": "", "ref_task_step_id": ""}, "d338c352-b6ad-45ff-b9c3-0418aa6cccc4": {"task_step_hash": "", "ref_task_step_id": ""}, "edaf9c7c-601d-4068-b117-92e5eaa45af5": {"task_step_hash": "", "ref_task_step_id": ""}, "29d7d9b0-9dd0-42e2-a847-f9e6d0c0b37f": {"task_step_hash": "", "ref_task_step_id": ""}, "c438cc1b-a630-4359-b8d9-abeaf797d240": {"task_step_hash": "", "ref_task_step_id": ""}, "a2b27547-c4b3-4bf8-ae68-c913858a2752": {"task_step_hash": "", "ref_task_step_id": ""}, "ac68fbc2-cec5-4dc0-9a52-1d0003ba3731": {"task_step_hash": "", "ref_task_step_id": ""}, "7895a383-ce04-42fa-9517-e0cea2b11f50": {"task_step_hash": "", "ref_task_step_id": ""}, "595b81fe-b868-420f-a43e-e551a8cbcce1": {"task_step_hash": "", "ref_task_step_id": ""}}}