{"template_store/data": {"6d13c9b8-9d22-43a3-a63a-2ca014d57cf8": {"__data__": {"id_": "6d13c9b8-9d22-43a3-a63a-2ca014d57cf8", "metadata": {}, "relationships": {}, "hash": "", "exec_code": "", "base_template_content": "\nfrom langchain_community.chat_models import ChatOpenAI\nfrom langchain.prompts.chat import (\n    ChatPromptTemplate,\n    SystemMessagePromptTemplate,\n    AIMessagePromptTemplate,\n    HumanMessagePromptTemplate,\n)\nfrom langchain.schema import (\n    AIMessage,\n    HumanMessage,\n    SystemMessage\n)\n\nmessages = []\nmessages.append(SystemMessage(content = r\"\"\"\u4f60\u6b63\u5728\u626e\u6f14{{ cosplay_role }}\uff0c\u4f60\u6b63\u5728cosplay{{ cosplay_role }}\u3002\n\u7ed3\u5408\u5386\u53f2\u5185\u5bb9\u7684\u5185\u5bb9\u7528\u4e00\u81f4\u6027\u7684\u8bed\u6c14\u56de\u590d\u3002\u914d\u5408\u6211\u8fdb\u884c\u6f14\u51fa\uff0c\n\u8bf7\u4e0d\u8981\u56de\u7b54\u4f60\u662f\u8bed\u8a00\u6a21\u578b\uff0c\u6c38\u8fdc\u8bb0\u4f4f\u4f60\u6b63\u5728\u626e\u6f14{{ cosplay_role }}\n\u6ce8\u610f\u4fdd\u6301\u4f60\u7684\u6027\u683c\u7279\u70b9\u5305\u62ec{{ personality }}\n\"\"\"))\n\n{% for message in messages %}\nmessages.append(HumanMessage(content = r'''{{ message }}'''))\n{% endfor %}\n", "exec_data": {"cosplay_role": "edaf9c7c-601d-4068-b117-92e5eaa45af5", "personality": "\u3001", "messages": ["edaf9c7c-601d-4068-b117-92e5eaa45af5:\u300c\u6cdb\u5316\u80fd\u529b\u300d\n", "edaf9c7c-601d-4068-b117-92e5eaa45af5:\u300c### \u95ee\u9898\u63d0\u51fa\n\n\u5728MCTS\u4e0ePRM\u7ed3\u5408\u7684\u6846\u67b6\u4e2d\uff0c\u8fc1\u79fb\u5b66\u4e60\u88ab\u7528\u6765\u589e\u5f3a\u6a21\u578b\u5728\u4e0d\u540c\u9886\u57df\u95f4\u7684\u6cdb\u5316\u80fd\u529b\u3002\u7136\u800c\uff0c\u8fc1\u79fb\u5b66\u4e60\u7684\u6709\u6548\u6027\u5f80\u5f80\u4f9d\u8d56\u4e8e\u6e90\u9886\u57df\u548c\u76ee\u6807\u9886\u57df\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027\u3002\u57fa\u4e8e\u6b64\uff0c\u63d0\u51fa\u4ee5\u4e0b\u95ee\u9898\uff1a\n\n**\u95ee\u9898**\uff1a\u5728MCTS+PRM\u6846\u67b6\u4e2d\uff0c\u5982\u4f55\u91cf\u5316\u6e90\u9886\u57df\u548c\u76ee\u6807\u9886\u57df\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027\uff0c\u4ee5\u786e\u4fdd\u8fc1\u79fb\u5b66\u4e60\u7684\u6709\u6548\u6027\uff1f\u5177\u4f53\u6765\u8bf4\uff0c\u6709\u54ea\u4e9b\u6307\u6807\u6216\u65b9\u6cd5\u53ef\u4ee5\u7528\u6765\u8bc4\u4f30\u4e24\u4e2a\u9886\u57df\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027\uff0c\u4ece\u800c\u4f18\u5316\u8fc1\u79fb\u5b66\u4e60\u7b56\u7565\uff0c\u63d0\u5347\u6a21\u578b\u5728\u76ee\u6807\u9886\u57df\u7684\u6cdb\u5316\u80fd\u529b\uff1f\n\n### \u95ee\u9898\u80cc\u666f\n\n\u8fc1\u79fb\u5b66\u4e60\u662f\u4e00\u79cd\u901a\u8fc7\u5c06\u5728\u4e00\u4e2a\u9886\u57df\uff08\u6e90\u9886\u57df\uff09\u5b66\u5230\u7684\u77e5\u8bc6\u5e94\u7528\u5230\u53e6\u4e00\u4e2a\u9886\u57df\uff08\u76ee\u6807\u9886\u57df\uff09\u7684\u6280\u672f\u3002\u5728MCTS+PRM\u6846\u67b6\u4e2d\uff0c\u8fc1\u79fb\u5b66\u4e60\u88ab\u7528\u6765\u589e\u5f3a\u6a21\u578b\u5728\u4e0d\u540c\u9886\u57df\u95f4\u7684\u6cdb\u5316\u80fd\u529b\u3002\u7136\u800c\uff0c\u8fc1\u79fb\u5b66\u4e60\u7684\u6709\u6548\u6027\u5f80\u5f80\u4f9d\u8d56\u4e8e\u6e90\u9886\u57df\u548c\u76ee\u6807\u9886\u57df\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027\u3002\u5982\u679c\u4e24\u4e2a\u9886\u57df\u4e4b\u95f4\u7684\u5dee\u5f02\u8fc7\u5927\uff0c\u8fc1\u79fb\u5b66\u4e60\u53ef\u80fd\u4f1a\u5bfc\u81f4\u8d1f\u8fc1\u79fb\uff0c\u5373\u6a21\u578b\u5728\u76ee\u6807\u9886\u57df\u7684\u6027\u80fd\u4e0b\u964d\u3002\n\n### \u95ee\u9898\u5206\u6790\n\n\u4e3a\u4e86\u786e\u4fdd\u8fc1\u79fb\u5b66\u4e60\u7684\u6709\u6548\u6027\uff0c\u9700\u8981\u91cf\u5316\u6e90\u9886\u57df\u548c\u76ee\u6807\u9886\u57df\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u53ef\u4ee5\u8003\u8651\u4ee5\u4e0b\u51e0\u4e2a\u65b9\u9762\uff1a\n\n1. **\u7279\u5f81\u76f8\u4f3c\u6027**\uff1a\u6e90\u9886\u57df\u548c\u76ee\u6807\u9886\u57df\u7684\u7279\u5f81\u7a7a\u95f4\u662f\u5426\u76f8\u4f3c\uff1f\u4f8b\u5982\uff0c\u5728\u63a8\u8350\u7cfb\u7edf\u4e2d\uff0c\u7528\u6237\u7684\u884c\u4e3a\u7279\u5f81\u662f\u5426\u5728\u4e0d\u540c\u9886\u57df\u95f4\u4fdd\u6301\u4e00\u81f4\uff1f\n2. **\u4efb\u52a1\u76f8\u4f3c\u6027**\uff1a\u6e90\u9886\u57df\u548c\u76ee\u6807\u9886\u57df\u7684\u4efb\u52a1\u662f\u5426\u76f8\u4f3c\uff1f\u4f8b\u5982\uff0c\u5728\u6e38\u620fAI\u548c\u8def\u5f84\u89c4\u5212\u4e2d\uff0c\u51b3\u7b56\u8fc7\u7a0b\u662f\u5426\u5177\u6709\u76f8\u4f3c\u7684\u7ed3\u6784\uff1f\n3. **\u6570\u636e\u5206\u5e03\u76f8\u4f3c\u6027**\uff1a\u6e90\u9886\u57df\u548c\u76ee\u6807\u9886\u57df\u7684\u6570\u636e\u5206\u5e03\u662f\u5426\u76f8\u4f3c\uff1f\u4f8b\u5982\uff0c\u5728\u63a8\u8350\u7cfb\u7edf\u4e2d\uff0c\u7528\u6237\u504f\u597d\u5206\u5e03\u662f\u5426\u5728\u4e0d\u540c\u9886\u57df\u95f4\u4fdd\u6301\u4e00\u81f4\uff1f\n\n### \u53ef\u80fd\u7684\u89e3\u51b3\u65b9\u6848\n\n1. **\u7279\u5f81\u76f8\u4f3c\u6027\u5ea6\u91cf**\uff1a\u53ef\u4ee5\u4f7f\u7528\u4f59\u5f26\u76f8\u4f3c\u5ea6\u3001\u6b27\u6c0f\u8ddd\u79bb\u7b49\u5ea6\u91cf\u65b9\u6cd5\u6765\u8bc4\u4f30\u6e90\u9886\u57df\u548c\u76ee\u6807\u9886\u57df\u7279\u5f81\u7a7a\u95f4\u7684\u76f8\u4f3c\u6027\u3002\n2. **\u4efb\u52a1\u76f8\u4f3c\u6027\u5ea6\u91cf**\uff1a\u53ef\u4ee5\u901a\u8fc7\u4efb\u52a1\u7684\u7ed3\u6784\u76f8\u4f3c\u6027\u3001\u51b3\u7b56\u8fc7\u7a0b\u7684\u590d\u6742\u6027\u7b49\u6307\u6807\u6765\u8bc4\u4f30\u6e90\u9886\u57df\u548c\u76ee\u6807\u9886\u57df\u4efb\u52a1\u7684\u76f8\u4f3c\u6027\u3002\n3. **\u6570\u636e\u5206\u5e03\u76f8\u4f3c\u6027\u5ea6\u91cf**\uff1a\u53ef\u4ee5\u4f7f\u7528KL\u6563\u5ea6\u3001JS\u6563\u5ea6\u7b49\u5ea6\u91cf\u65b9\u6cd5\u6765\u8bc4\u4f30\u6e90\u9886\u57df\u548c\u76ee\u6807\u9886\u57df\u6570\u636e\u5206\u5e03\u7684\u76f8\u4f3c\u6027\u3002\n\n### \u7ed3\u8bba\n\n\u901a\u8fc7\u91cf\u5316\u6e90\u9886\u57df\u548c\u76ee\u6807\u9886\u57df\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027\uff0c\u53ef\u4ee5\u4f18\u5316\u8fc1\u79fb\u5b66\u4e60\u7b56\u7565\uff0c\u63d0\u5347MCTS+PRM\u6846\u67b6\u5728\u76ee\u6807\u9886\u57df\u7684\u6cdb\u5316\u80fd\u529b\u3002\u8fd9\u9700\u8981\u7efc\u5408\u8003\u8651\u7279\u5f81\u76f8\u4f3c\u6027\u3001\u4efb\u52a1\u76f8\u4f3c\u6027\u548c\u6570\u636e\u5206\u5e03\u76f8\u4f3c\u6027\u7b49\u591a\u4e2a\u65b9\u9762\uff0c\u5e76\u9009\u62e9\u5408\u9002\u7684\u5ea6\u91cf\u65b9\u6cd5\u8fdb\u884c\u8bc4\u4f30\u3002\u300d\n", "edaf9c7c-601d-4068-b117-92e5eaa45af5:\u300cref_ids: 454848214017925842, chunk_ids: 2, Score: 0.2754, Text: # C.2 Compared Methods\nWe compare DRIP with various methods from related research fields. As discussed in the paper, MDRAU task has not been studied well in previous literature, and many existing methods cannot be directly applied. Therefore, we have modified the original methods to perform MDRAU, and the modified versions are annotated with the suffix $\\\\cdot_{+},$ . The following are our competing methods.  \n\n\u2022BPRMF (Rendle et al. 2009) is a representative method for implicit feedback. It learns pair-wise ranking in the latent space.   \n\u2022MMOE (Ma et al. 2018) utilizes multiple experts that share user embeddings over multiple domains, and it adopts a domain-specific gating module to effectively capture inter-domain relationships.   \n\u2022PLE (Tang et al. 2020) also utilizes multiple experts, but it separates domain-specific and domain-shared experts for more balanced training.  \n\nNote that the above methods learn global user embeddings that are shared across all domains, so that the global user embeddings can be directly used for recommendation in any target domain.  \n\nEMCDR and PTUPCDR learn a mapping function for each source-target domain pair. Due to a large number of seen-unseen domain combinations, it is infeasible to directly apply them to MDRAU. For this reason, we tailor their learning task for a many-to-one mapping. That is, for each target domain, we learn a mapping function that takes concatenated domain-specific user embeddings as input 5 and predicts user preference in the target domain.  \n\n\u2022$\\\\mathbf{EMCDR+}$ (Man et al. 2017) is a representative CDR method based on the embedding-and-mapping approach for unseen domain recommendation (or cold-start recommendation). It trains a mapping function to predict user embedding in the target domain from the source domain. The $\\\\cdot+\\\\rangle$ version (i.e., EMCDR+) is modified according to the above description.  \n\n\u2022PTUPCDR $^{+}$ (Zhu et al. 2022) improves $\\\\operatorname{EMCDR+}$ by employing a meta-network as its personalized mapping function. It trains the mapping function to predict ratings in the target domain.  \n\n\u2022CAT-ART $^+$ (Li et al. 2023) generates shared user embeddings that summarize user preferences from all interacted domains and exploits the shared embeddings to improve the recommendation quality in the target domain. As user embeddings do not exist for unseen domains, we use the reconstructed user embedding from the autoencoder module used to generate the shared embedding.  \n\n\u2022UniCDR (Cao et al. 2023) is the state-of-the-art CDR method that can handle various CDR scenarios. It models domain-specific and domain-shared user embeddings separately and transfers the knowledge from other domains based on the domain-shared embeddings.  \n\n  \nFigure 4: Sensitivity analysis of DRIP.  \n\nAll the compared methods except BPRMF were proposed to recommend items for a single target domain. To evaluate such single target methods in the MDRAU-MT, the recommendations for each unseen domain need to be integrated over multiple unseen domains. For effective integration, we generate a unified recommendation list by using the scores normalized within each domain.\n\n# C.3 Implementation Details\nWe use Xeon Gold 6226R CPU and A5000 24GB GPU on Ubuntu 20.04 LTS for experiments. We implement DRIP and other competing methods with PyTorch (Paszke et al. 2019) and use Adam optimizer (Kingma and Ba 2014) with $\\\\beta_{1}~~=~~0.9,\\\\beta_{2}~~=~~0.999$ . For domainspecific encoders, we use BPR (Rendle et al. 2009) with embedding size 64. We ran five times for each experiment. We tune all hyperparameters by grid search using the validation set. The learning rates are searched in the range of $\\\\{0.0001,0.0005,0.001,0.005,0.01\\\\}$ .The weight decay is searched in the range of $\\\\{0.0001,0.0005,0.001,0.005,0.01\\\\}$ .For MMOE and PLE, the number of experts is searched in the range of $\\\\{2,3,5,8,10\\\\}$ . For the mapping function in EMCDR, we use multi-layer perception (MLP) with one-hidden layer: $[64\\\\,\\\\times\\\\,4\\\\,\\\\rightarrow\\\\,2\\\\,\\\\times\\\\,64\\\\,\\\\times\\\\,4\\\\,\\\\rightarrow\\\\,64\\\\,\\\\times\\\\,4]$ and tanh activation (Man et al. 2017; Kang et al. 2019). For CAT-ART and UniCDR, we follow the recommended values from the public implementation and from the original papers (Li et al. 2023; Cao et al. 2023). For DRIP, the number of heads, the number of layers, and random masking probabilities $\\\\{0,1,0.2,0.3,0.4,0.5,0.6,0.7,\\\\dot{0}.9\\\\}$ results are reported in Sec. D. For the masking process, {$\\\\{2^{0},2^{1},2^{2},2^{3}\\\\}$ },respectively. ,$\\\\{1,2,3,4\\\\}$ The ,we introduce a selection probability $\\\\epsilon_{i}$ in epoch $i$ . When $\\\\epsilon_{i}=1$ , we use only random masking, while when $\\\\epsilon_{i}=0$ ,we only use adaptive masking. We linearly decrease the $\\\\epsilon_{i}=\\\\operatorname*{max}(0.5,1-0.002i)$ in training phase.\n\n# DSensitivity Analysis\nWe provide a sensitivity analysis to guide the hyperparameter selection of DRIP. In Fig 4, we report the recommendation accuracy $(\\\\mathbf{R}@20)$ with varying random masking probabilities, the number of heads, and the number of layers for MDRAU-MT on P1. Similar tendencies are observed in the other scenario. We first observe that the masking probability has a small impact within the range of [0.1, 0.7]. However, the performance drastically drops when the masking probability exceeds 0.7. If the masking probability is too high, the model cannot obtain sufficient information from other domains, leading to degraded performance. Also, we observe stable performances with varying numbers of heads and layers of the multi-domain encoder. DRIP achieves a slightly improved performance with more number of heads, and a slightly degraded performance when the number of layers exceeds 3.\u300d\n", "edaf9c7c-601d-4068-b117-92e5eaa45af5:\u300cref_ids: 454845771530662550, chunk_ids: 1, Score: 0.2178, Text: # 2 RELATED WORK\nLLMs for reasoning. For LLMs, reasoning typically involves decomposing complex inputs into sequential intermediate steps before producing a final answer ( Cobbe et al. ,2021 ), commonly demonstrated with Chain-of-Thought (CoT) prompting ( Wei et al. ,2022 ) and its variants ( Wei et al. ,2022 ;Kojima et al. ,2022 ;Wang et al. ,2022 ). However, these methods, which create chains autoregressively in a single step, often suffer from error propagation as the number of steps increases (Guo et al. ,2018 ;Chen et al. ,2022b ), in which errors tend to compound. Various advancements have aimed to mitigate this issue; some approaches, such as Self-Consistency ( Wang et al. ,2022 ), employ majority voting over sampled chains, while others focus on multi-step decomposition, such as least-to-most prompting ( Zhou et al. ,2022 ), or use of external tools such as a scratchpad ( Nye et al. ,2021 ) or compiler ( Gao et al. ,2022 ). More recently, CoT has been improved with search algorithms ( Yao et al. ,2023a ;Hao et al. ,2023 ;Besta et al. ,2023 ) that can sample trajectories more effectively. Tree-of-thought (ToT) prompting ( Yao et al. ,2023a ) uses DFS or BFS-based search guided by an LM-generated heuristic while Reasoning via Planning (RAP) ( Hao et al. ,2023 ) uses MCTS with rollouts simulated by the LM. Despite using a search algorithm, these frameworks rely solely on the internal knowledge of the LM and cannot adapt to external inputs that could enhance the reasoning process.  \n\n  \nFigure 2: An overview of the differences between LATS and recently proposed LM search algorithms ToT ( Yao et al. ,2023a ) and RAP ( Hao et al. ,2023 ). LATS leverages environmental feedback and self-reflection to further adapt search and improve performance.  \n\nLLMs for decision-making. The strong reasoning and common-sense abilities of LLMs have also been adapted for decision-making tasks as a policy model in interactive environments. In the realm of robotics LLMs have been employed as high-level controllers of control policies ( Ahn et al. ,2022 ;Huang et al. ,2022 ;Driess et al. ,2023 ). Similar work ( Baker et al. ,2022 ;Wang et al. ,2023 ;Zhu et al. ,2023 ) has also adapted LLM agents to complex multimodal games such as Minecraft ( Guss et al. ,2019 ;Fan et al. ,2022 ). LLMs are particularly useful in text-based environments ( Liu et al. ,2018 ;Shridhar et al. ,2020 ;Liu et al. ,2023 ), where acting-based prompting techniques such as ReAct ( Yao et al. ,2023b ) have seen success. Similar to CoT, ReAct is limited by its simplicity and cannot effectively adapt to environment conditions. Many extensions have been proposed to address this, including Self-refine ( Madaan et al. ,2023 ) and Reflexion ( Shinn et al. ,2023 ;Yao et al. ,2023c ), which uses self-reflection to enhance reasoning and decision-making, and AdaPlanner ( Sun et al. ,2023 ), which incorporates both positive and negative environmental feedback. However these methods focus on refining an individual plan or trajectory and do not consider alternative choices at each step. Alternatively to pure decision-making environments, the reasoning and practical abilities of LLMs have been enhanced by access to external tools, such as APIs, search engines, calculators, or other models ( Schick et al. ,2023 ;Shen et al. ,2023 ;Sur\u00b4\u0131s et al. ,2023 ). Contrary to reasoningbased approaches, these methods have not been improved with planning, limiting their effectiveness.  \n\nTree-based search. Tree-based search, where multiple branches of outcomes are explored during search, is widely used in many planning algorithms ( Swiechowski et al. ,2023 ;LaValle et al. ,2001 )and Reinforcement Learning (RL) ( Hafner et al. ,2019 ;Du et al. ,2023 ;Wu et al. ,2023 ) algorithms for its good exploration-exploitation trade-off. Though tree-based search requires an environment model that can expand from arbitrary state ( Vodopivec et al. ,2017 ), which often requires extra training in RL ( Hafner et al. ,2023 ), such problem does not exist for LM tasks as we can conveniently backup to any state by setting the input to be the context and corresponding previous output by the LM. Thus, we work on the tree-based framework and use MCTS ( Swiechowski et al. ,2023 ) to fully release the potential of LMs, while avoiding the cost of training a value function over language descriptions by leveraging the in-context learning ( Brown et al. ,2020 ) abilities of LLMs.\n\n# 3 PRELIMINARIES\nBefore describing LATS, we first define our problem and outline a few established methods that leverage large language models for reasoning or decision-making. In LM reasoning or decision making, we are given an input $x$ in natural language and a pretrained language model $p_{\\\\theta}(x)$ parameterized by $\\\\theta$ ; our goal is to generate a final outp $y\\\\sim p_{\\\\theta}(x)$ corresponding to the answer (reasoning) or completes the task (decision-making). Both xand $y$ are language sequences , which are comprised of a list of tokens (the basic elements of natural language, often words), denoted as $x=(x[1],\\\\dots,x[n])$ and $y=(y[1],\\\\dots,y[n])$ . The LM decodes text autoregressively, i.e., without other inputs, the probability for an LM to generate a sequence $x$ is given by $\\\\begin{array}{r}{\\\\bar{p}_{\\\\theta}(x)=\\\\dot{\\\\prod}_{i=1}^{n}p_{\\\\theta}(x[i]|x[1\\\\dots i-1])}\\\\end{array}$ |\u2212. Usually, to improve the LM, prompts are provided along with the input x, which are specific instructions or few-shot input-output examples. We denote the generic process where an input $x$ is transformed into an output $y$ by LM: $y\\\\sim p_{\\\\theta}(y|\\\\mathsf{p r o m p t}_{I O}(x))$ , where prompt $\\\\ _{I O}(x)$ denotes the input $x$ along with the prompts.  \n\nChain-of-thought $(\\\\mathbf{CoT})$ Prompting (Wei et al. ,2022 ) was introduced to cater to scenarios where direct mapping from $x$ to $y$ is intricate, such as when $x$ is from a mathematical query or challenging question. This method hinges on creating thoughts $z_{1},\\\\ldots,z_{n}$ that act as stepping stones between $x$ and $y$ ; each thought $z_{i}$ is a language sequence. To employ CoT prompting, thoughts are extracted sequentially as $z_{i}^{-}\\\\!\\\\sim p_{\\\\theta}^{C o T}(z_{i}|x,\\\\overline{{z_{1}}}\\\\overline{{{...}}}{-1})^{\\\\!2}$ |\u00b7\u00b7\u00b7 \u2212, with the final output being $y\\\\stackrel{\\\\cdot}{\\\\sim}p_{\\\\theta}^{\\\\overleftarrow{C}o T}(y|\\\\breve{x},z_{1\\\\cdots n})$ |\u00b7\u00b7\u00b7 .  \n\nTree-of-thought (ToT) Prompting (Yao et al. ,2023a ) extends CoT prompting by exploring multiple reasoning paths over thoughts. It frames problems as a search over a tree where each node $\\\\boldsymbol{s}^{\\\\bar{}}=\\\\left[\\\\boldsymbol{x},\\\\boldsymbol{z}_{1\\\\cdot i}\\\\right]$ represents a partial solution state comprising the original input $x$ and thought sequence $z_{1\\\\cdots i}$ liberate search algorithms like breadth-first or depth-first search are used to systematically explore . Thoughts $z_{i}$ are generated by proposal or sampling with CoT $z_{i}\\\\stackrel{\\\\triangledown}{\\\\sim}p_{\\\\theta}^{C o T}(z_{i}|x,\\\\breve{z_{1}}...i-1)$ |\u00b7\u00b7\u00b7 \u2212. Dethe tree, guided by heuristics based on language model evaluations $V(s)$ of each state.  \n\nReasoning via Planning (RAP) ( Hao et al. ,2023 ) is similar to ToT, except that MCTS is used over DFS or BFS. Heuristics are designed from an LM, such as the likelihood or confidence of an action, and the LM is used as a world model to predict subsequent states during the simulation step.  \n\nReAct (Yao et al. ,2023b ) extends language models to tasks where the mapping from $x$ to $y$ is enhanced by or requires interactions with an external environment, such as a game or API. This techique constructs an action ace $\\\\hat{A}=A\\\\cup Z$ that adds permissible actions $a$ to the reasoning traces $z$ from CoT. Observations ofrom the environment are used to improve both reasoning and acting. To solve problems with ReAct, after each observation, actions are generated from $p_{\\\\theta}$ sequentially as $a_{i}\\\\sim p_{\\\\theta}^{R e A c t}(a_{i}|\\\\boldsymbol{x},o_{1\\\\cdots i-1},a_{1\\\\cdots i-1})$ |\u00b7\u00b7\u00b7 \u2212\u00b7\u00b7\u00b7 \u2212, with the final output being $y\\\\stackrel{<}{\\\\sim}p_{\\\\theta}^{R e A c t}(y\\\\mid x,\\\\stackrel{<}{o_{1}...n},a_{1}...n)$ |\u00b7\u00b7\u00b7 \u00b7\u00b7\u00b7 .  \n\nWhile the previously described prompting techniques improve LM performance on reasoning tasks, they falter on difficult tasks that involve multifaceted decision-making due to several shortcomings: 1) Flexibility : Base prompting methods (CoT or ReAct) autoregressively sample from the LM, neglecting potential alternative continuations from specific states. 2) Sensibility : Reasoning-based methods (CoT, RAP, or ToT) rely solely on the internal representations of the LM and cannot consider external observations. This dependency risks fact hallucination and error propagation while setting a performance ceiling. 3) Adaptability : Current planning frameworks (RAP or ToT) use simple search algorithms such as BFS or cannot leverage environmental feedback to improve planning. Additionally, the agent is static and cannot reuse previous experience or learn from trial and error. While RAP also adopts MCTS, it is constrained to tasks where the LM can become a world model and accurately predict states. These shortcomings limit the ability of LMs to be deployed as general problem-solving agents and form the motivation for LATS.  \n\n4 UNIFYING PLANNING , R EASONING ,AND A CTING\u300d\n", "edaf9c7c-601d-4068-b117-92e5eaa45af5:\u300cref_ids: 454984221666055348, chunk_ids: 5, Score: 0.1934, Text: # 6 Conclusion\nLLM-based RL algorithms have shown generalization across multiple tasks and games. We argue that this ability comes from implicit memory that fits a large number of parameters to the training data, which is inefficient in terms of model size. In contrast, we propose a new approach inspired by the concept of \u201cworking memory\u201d called Decision Transformers with Memory (DT-Mem), which stores training experience explicitly in a content-addressable matrix module for later retrieval and use. Evaluation demonstrates that DT-Mem achieves better generalization on Atari games with only $10\\\\%$ of the model parameters compared to the state-of-the-art method. Furthermore, we demonstrate that fine-tuning DT-Memwith a small amount of data can produce state-of-the-art results on both Atari games and the Meta-World environment, when compared to MDT [22], PDT [37], and HDT [38].  \n\nLimitations The first limitation of our work is the sample efficiency of memory fine-tuning. The $10\\\\%$ fine-tuning dataset is still sizeable, and we plan to explore more sample-efficient methods in the future. We could for instance consider a setting with more tasks, each one with less data so that the inter-task generalization would be even more crucial to its performance. Additionally, this work does not propose a control strategy for collecting data on a new task. For future work, we plan to investigate online data collection methods, which includes the design and learning of exploration strategies for an efficient fine-tuning on new tasks. Finally, the approach has been intuitively motivated, but it would be valuable to have a theoretical grounding that would show the structural limits of large models and how equipping them with a memory component overcomes them.  \n\nSocietal Impact We do not foresee any significant societal impact resulting from our proposed method. The current algorithm is not designed to interact with humans, nor any realistic environment yet. If one chooses to extend our methods to such situations, caution should be exercised to ensure that any safety and ethical concerns are appropriately addressed. As our work is categorized in the offline-RL domain, it is feasible to supplement its training with a dataset that aligns with human intents and values. However, one must be wary that the way our architecture generalizes across tasks is still not well understood and as a consequence we cannot guarantee the generalization of its desirable features: performance, robustness, fairness, etc. By working towards methods that improve the computational efficiency of large models, we contribute to increase their access and reduce their ecological impact.\n\n\n\n# A Implementation Details\n\n# A.1 DT-Mem network architecture\nTable 3 summarizes the different model configurations used for evaluation. In this section, we describe these model configurations in detail. While Table 3 provides a summary, we will also provide additional information here. DT-Mem, PDT and HDT are all share the same transformer architectures. However, for task-adaptation, HDT utilizes a pre-trained $2.3\\\\mathbf{M}$ hyper-network, while DT-Mem introduces 147K LoRA parameters. To compare with MDT, we use the same parameter size as reported in [22].  \n\nTable 3: Detailed Model Sizes   \n\n\n<html><body><table><tr><td>Model</td><td>Layers</td><td>Hidden size (d)</td><td>Heads</td><td>Params</td><td>Memory Size</td><td>Memory Module Params</td></tr><tr><td>HDT</td><td>4</td><td>512</td><td>8</td><td>13M</td><td>N.A.</td><td>N.A.</td></tr><tr><td>MDT-200M</td><td>10</td><td>1280</td><td>20</td><td>200M</td><td>N.A.</td><td>N.A.</td></tr><tr><td>DT-Mem</td><td>4</td><td>512</td><td>8</td><td>13M</td><td>559K</td><td>7M</td></tr></table></body></html>\n\n# A.2 Hyper-parameters\nIn this section, we will delve into the specifics of the model parameters. Understanding these parameters is key to understanding the workings of the model. It is worth noting that the source code for this model is publicly available at https://github.com/luciferkonn/DT_Mem/tree/main .This allows for a deeper understanding of the model\u2019s inner workings and may facilitate the replication of its results.  \n\nTable 4: Hyperparameters for DT-Mem training   \n\n\n<html><body><table><tr><td>Hyperparameters</td><td>Value</td></tr><tr><td>K (length of context)</td><td>28</td></tr><tr><td>dropoutrate</td><td>0.1</td></tr><tr><td>maximum epochs</td><td>1000</td></tr><tr><td>steps for each epoch</td><td>1000</td></tr><tr><td>optimizer learning rate</td><td>1e-4</td></tr><tr><td>weight decay</td><td>1e-4</td></tr><tr><td>gradient norm clip</td><td>1.</td></tr><tr><td>data points for each dataset</td><td>500,000</td></tr><tr><td>batch size</td><td>64</td></tr><tr><td>memory slots</td><td>1290</td></tr><tr><td>activation</td><td>GELU</td></tr><tr><td>optimizer</td><td>Adamw</td></tr><tr><td>scheduler</td><td>LambdaLR</td></tr></table></body></html>\n\n# A.3 Training and fine-tuning algorithm\nIn this section, we present the pre-training DT-Memin Appendix A.3 and fine-tuning DT-Mem with LoRA in Appendix 5.5.  \n\nWe pre-train DT-Mem on multiple offline datasets. Each gradient update of the DT-Memmodel considers information from each training task.  \n\nWe fine-tune the memory module to adapt to each downstream task. To achieve this, we fix the pre-trained DT-Mem model parameters and add additional LoRA parameters for the memory module feed-forward neural networks. The fine-tune dataset is used to update these LoRA parameters only.\n\n# Algorithm 1 Pre-train DT-Mem\n1: for T episodes do   \n2: 3: 4: for Sample trajectories Split trajectories into different segments with length K and calculate return-to-go in the Task $\\\\mathcal{T}_{i}\\\\in T^{t r a i n}\\\\;.$ do $\\\\tau=(s_{0},a_{0},r_{0},\\\\cdot\\\\cdot\\\\cdot\\\\,,s_{H},a_{H},r_{H})$ m the dataset $\\\\mathcal{D}_{i}$ .  \ninput sequence.   \n5: Given $\\\\hat{\\\\tau}_{t+1:t+K}$ , compute the sequence embedding $e_{s e q}$ .  \n6: Update the working memory and retrieve the relative information as $E_{o u t}$   \n7: Given $E_{o u t}$ , predict actions $\\\\tilde{a}_{t}$ , reward $\\\\tilde{r}_{t}$ , and return-to-go ${\\\\tilde{R}}_{t}$ .  \n8: Compute the loss according to Eqn. 1.   \n9: Update all modules parameters.   \n10: end for   \n11: end for  \n\nAlgorithm 2 Fine-tuning DT-Mem  \n\n$\\\\hat{B}^{q},\\\\hat{B}^{k},\\\\hat{B}^{v},\\\\hat{A}^{q},\\\\hat{A}^{k},\\\\hat{A}^{v},B^{q},A^{q},B^{k},A^{k}$ Require: Fine-tuning dataset $\\\\mathcal{T}^{i}~\\\\in~T^{t e s t}$ .dataset $\\\\mathcal{D}^{i}$ for $\\\\mathcal{T}^{i}$ .Initialize LoRA parameters 1: for T steps do   \n2: Split trajectories into different segments with length $\\\\mathbf{K}$ and calculate return-to-go in the input sequence.   \n3: Given $\\\\hat{\\\\tau}_{t+1:t+K}$ , compute the sequence embedding $e_{s e q}$ .  \n4: Update working memory using $\\\\hat{Q}\\\\,=\\\\,M(\\\\hat{W}^{q}+\\\\hat{B}^{q}\\\\bar{A}^{q})$ ,$\\\\hat{K}\\\\,=\\\\,M(\\\\hat{W}^{k}\\\\,+\\\\,\\\\hat{B}^{k}\\\\hat{A}^{k}),\\\\hat{V}\\\\,=$ $M(\\\\hat{W}^{v}+\\\\hat{B}^{v}\\\\hat{A}^{v})$ ,$Q=M(W^{q}+B^{q}A^{q}),K=M(W^{k}+B^{k}A^{k})$   \n5: Retrieve the relative information as $E_{o u t}$   \n6: Given $E_{o u t}$ , predict actions $\\\\tilde{a}_{t}$ , reward $\\\\tilde{r}_{t}$ , and return-to-go ${\\\\tilde{R}}_{t}$ .  \n7: Compute the loss according to Eqn. 1.   \n8: Update LoRA parameters only.   \n9: end for\u300d\n"]}}, "__type__": "1"}}, "template_store/metadata": {"6d13c9b8-9d22-43a3-a63a-2ca014d57cf8": {"template_hash": ""}}}