角色,内容,分镜
276fc94b-2aa5-4712-ab27-c1e4f31af69f,分析近几年研究领域的技术框架与方法论,0
276fc94b-2aa5-4712-ab27-c1e4f31af69f,"### 问题

在分析近几年研究领域的技术框架与方法论时，蒙特卡洛树搜索（MCTS）作为一种启发式搜索算法，其核心思想是通过模拟多次随机游戏来评估不同决策的优劣。然而，MCTS在实际应用中面临计算复杂度高的问题，尤其是在大规模问题中。基于此，**如何通过改进MCTS的技术框架或引入新的方法论来降低其计算复杂度，同时保持或提升其决策的准确性和效率？**",0
276fc94b-2aa5-4712-ab27-c1e4f31af69f,"ref_ids: 455026805323333778, chunk_ids: 1, Score: 0.5391, Text: # Layered and Staged Monte Carlo Tree Search for SMT Strategy Synthesis
Zhengyang $\\left(\\mathbf{Johm}\\right)\\mathbf{Lu}^{1}$ ,Stefan Siemer 2 ,Piyush Jha 3 ,Joel Day 4 ,Florin Manea 2 and Vijay Ganesh 3 1 University of Waterloo 2 University of G¨ottingen 3 Georgia Institute of Technology 4 Loughborough University

# Abstract
Modern SMT solvers, such as Z3 , offer usercontrollable strategies, enabling users to tailor them for their unique set of instances, thus dramatically enhancing solver performance for their use case. However, this approach of strategy customization presents a significant challenge: handcrafting an optimized strategy for a class of SMT instances remains a complex and demanding task for both solver developers and users alike.  

In this paper, we address this problem of automatic SMT strategy synthesis via a novel Monte Carlo Tree Search (MCTS) based method. Our method treats strategy synthesis as a sequential decisionmaking process, whose search tree corresponds to the strategy space, and employs MCTS to navigate this vast search space. The key innovations that enable our method to identify effective strategies, while keeping costs low, are the ideas of layered and staged MCTS search. These novel approaches allow for a deeper and more efficient exploration of the strategy space, enabling us to synthesize more effective strategies than the default ones in state-ofthe-art (SOTA) SMT solvers. We implement our method, dubbed Z3alpha , as part of the Z3 SMT solver. Through extensive evaluations across 6 important SMT logics, Z3alpha demonstrates superior performance compared to the SOTA synthesis tool FastSMT , the default Z3 solver, and the CVC5 solver on most benchmarks. Remarkably, on a challenging QF BV benchmark set, Z3alpha solves $42.7\\%$ more instances than the default strategy in the Z3 SMT solver.

# 1 Introduction
Satisfiability Modulo Theories (SMT) solvers [De Moura and Bjørner, 2011] are key tools in diverse fields such as software engineering [Cadar et al. , 2008], verification [Gurfinkel et al. , 2015], security [Song et al. , 2008], and artificial intelligence [Pulina and Tacchella, 2012]. It has long been observed that no single solver or algorithm excels across all instances of a given SMT logic or of a problem class. As a result, modern SMT solvers, such as Z3 [De Moura and Bjørner, 2008], offer user-controllable strategies [De Moura and Passmore, 2013], enabling users to customize a sub-solver for their class of instances. A strategy can be thought of as an algorithmic recipe for selecting, sequencing, and parameterizing tactics .Each tactic is a well-defined algorithmic proof rule or symbolic reasoning step, provided by the solver. For example, propagate-values is a Z3 tactic that propagates equalities, while sat and smt are the tactic wrappers of the main SAT and SMT solver in Z3 . A strategy builds a decision procedure by combining tactics, as shown in an exemplar strategy (if is-pb (then propagate-values sat) smt) . This strategy specifies a solver algorithm that, given an input instance, applies propagate-values followed by sat if the instance is a pseudo-boolean problem (as checked using is-pb ), or applies smt otherwise.  

Default solver strategies are typically optimized for wellestablished benchmarks, such as those in the SMT-LIB library [Barrett et al. , 2016]. However, as the scope of SMT applications continues to grow rapidly, users frequently encounter specialized, evolving, and unprecedented classes of instances. In these scenarios, the default or the existing customized strategies might not be as effective. Consequently, there arises a need for novel customized strategies, specifically designed to efficiently address the unique challenges posed by users’ specific problems. Traditionally, this task of strategy customization has been undertaken by human experts through extensive experimentation and benchmark analysis. However, even with their expertise and efforts, the task remains challenging due to the intricate interactions among tactics and the vast search space for potential strategies.  

Early attempts have been made to automatically synthesize SMT strategies. For instance, StratEVO [Ram´ırez et al. , 2016] searches for an optimal strategy using evolutionary algorithms, while FastSMT [Balunovic et al. , 2018] synthesizes a tailored strategy using imitation learning and decision tree learning techniques. While these methods show promise in automating strategy customization, they suffer from issues such as a lack of robustness, limited interpretability, and extensive training times.  

To address these issues, we introduce a novel SMT strategy synthesis method that employs Monte Carlo Tree Search (MCTS). MCTS is a heuristic search algorithm, widely applied in computer board game players as a planning algorithm [Browne et al. , 2012]. Its prominence further escalated following its successful integration into the groundbreaking deep reinforcement learning systems AlphaGo [Silver et al. ,2016] and AlphaZero [Silver et al. , 2017], where MCTS was employed as a policy improvement operator. Recently, MCTS has shown remarkable success as a standalone algorithm in solving complex symbolic or combinatorial search problems, as evidenced in Khalil et al. [2022] and Sun et al. [2023]. Its key strengths, including its ability to effectively balance exploration and exploitation and adapt to the nuances of varied search problems, making it an excellent method for such challenging tasks. Our work is the first to apply MCTS to the SMT strategy synthesis problem.",0
276fc94b-2aa5-4712-ab27-c1e4f31af69f,"ref_ids: 454845587396505206, chunk_ids: 0, Score: 0.5312, Text: # BLIMITATION AND FUTURE WORK
Currently, our method TS-LLM still cannot scale to really large-scale scenarios due to the extra computation burdens introduced by node expansion and value evaluation. Additional engineering work such as key value caching is required to accelerate the tree-search. In addition, we do not cover all feasible action-space designs for tree search and it is flexible to propose advanced algorithms to automatically construct a tree mixed with both sentence-level expansion and token-level expansion, etc. We leave such exploration for future work. For MCTS aggregation, the current method still struggles to improve under large aggregation numbers. some new algorithms that can encourage multi-search diversity might be needed. Currently, we are still actively working on scaling up our method both during inference and training (especially multi-iteration training).

# CBACKGROUND OF MONTE CARLO TREE -SEARCH A LGORIHTMS
Once we build the tree, we can use various search algorithms to find a high-reward trace. However, it’s not easy to balance between exploration and exploitation during the search process, especially when the tree is sufficiently deep. Therefore we adopt Monte Carlo Tree Search(MCTS) variants as choices for strategic and principled search. Instead of the four operations in traditional MCTS (Kocsis & Szepesv´ari, 2006; Coulom, 2006), we refer to the search process in AlphaZero (Silver et al., 2017a) and introduce 3 basic operations of a standard search simulation in it as follows, when searching actions from current state node $s_{0}$ :  

Select It begins at the root node of the search tree, of the current state, $s_{0}$ , and finishes when reaching a leaf node $s_{L}$ at timestep $L$ . At each of these $L$ timesteps(internal nodes), an action(child node) is selected according to $a_{t}=\\arg\\operatorname*{max}_{a}\\left(Q(s_{t},a)+U(s_{t},a)\\right)$ where $U(s_{t},a)$ is calculated by a variant of PUCT algorithm (Rosin, 2011):  

$$
U(s,a)=c_{\\mathrm{puct}}\\cdot\\pi_{\\theta}(s,a)\\frac{\\sqrt{\\sum_{b}N(s,b)}}{1+N(s,a)}
$$  

$N(s,a)$ isit count of selecting action $a$ at node $s$ , and $\\begin{array}{r}{c_{\\mathrm{puct}}\\,=\\,\\log((\\sum_{b}N(s,b)\\,+\\,}\\end{array}$ P$c_{\\mathrm{base}}+1)/c_{\\mathrm{base}})+c_{\\mathrm{init}}$ is controlled by visit count and two constants. This search control strategy initially prefers actions with high prior probability and low visit count, but asymptotically prefers actions with high action-value.  

Expand and evaluate After encountering a leaf node $s_{L}$ by select , if $s_{L}$ is not a terminal node, it will be expanded by the language model policy. The state of the leaf node is evaluated by the value network, noted as $v(s_{L})$ . If $s_{L}$ is a terminal node, if there is an oracle reward function $R$ , then $v(s_{L})=R(s_{L})$ , otherwise, in this paper, we use an ORM $\\hat{r}$ as an approximation of it.  

Backup After expand and evaluate on a leaf node, backward the statistics through the path $s_{L},s_{L-1},\\ldots,s_{0}$ , for each node, increase the visit count by $N(s_{t},a_{t})\\,=\\,N(s_{t},a_{t})+1$ , and the total action-value are updated as $W(s_{t},a_{t})\\,=\\,W(s_{t},a_{t})\\,\\dot{+}\\,v(s_{L})$ , the mean action-value are updated as $Q(s_{t},a_{t})=W(s_{t},a_{t})/N(s_{t},a_{t})$ .  

In this paper, we introduce 3 variants of MCTS based on the above basic operations.

# DEXPERIMENT DETAILS

# D.1 TASK SETUPS
GSM8k GSM8k (Cobbe et al., 2021) is a commonly used numerical reasoning dataset, Given a context description and a question, it takes steps of mathematical reasoning and computation to arrive at a final answer. There are about $7.5\\mathrm{k}$ problems in the training dataset and $1.3\\mathrm{k}$ problems in the test dataset.  

Game24 We also test our methods on Game24(Yao et al., 2023) which has been proven to be hard even for state-of-the-art LLMs like GPT-4. Each problem in Game24 consists of 4 integers between 1 and 13. And LLMs are required to use each number exactly once by $(+\\mathrm{~-~}\\times\\div)$ to get a result equal to 24 We follow Yao et al. (2023) by using a set of 1362 problems sorted from easy to hard according to human solving time. We split the first 1k problems as the training dataset and the last 362 hard problems as the test dataset. For each problem in the training dataset, we collect data for SFT by enumerating all possible correct answers.  

PrOntoQA PrOntoQA (Saparov & He, 2022) is a typical logical reasoning task in which a language model is required to verify whether a hypothesis is true or false given a set of facts and logical rules. There are 4k problems in the training dataset and 500 problems in the test dataset.  

RLHF We choose a synthetic RLHF dataset Dahoas 1 serving as the query data. We split the dataset to 30000/3000 as training and test set respectively. For the reward model, we choose reward-modeldeberta-v3-large$\\cdot\\mathbf{V}2^{2}$ from OpenAssistant, which is trained from several RLHF datasets.

# D.2 SFT AND VALUE TRAINING DETAILS
SFT in GSM8k, Game24 and PrOntoQA : For GSM8k, Game24 and PrOntoQA, we first train LLaMA2-7b on the training dataset The training is conducted on 8 NVIDIA A800 GPUs, using a cosine scheduler decaying from $\\scriptstyle{\\mathrm{lr}=2\\ e-5}$ to 0.0 with a warmup ratio of 0.03, batch size 128 for 3 epochs. For GSM8k and Game24 we use the checkpoint at the last epoch as the direct policy in experiments, while for PrOntoQA we use the checkpoint at the 1st epoch since the others overfit.  

Value training in GSM8k, Game24 and PrOntoQA : Then we train the value function on the data rollout by the SFT policy. In GSM8k and Game24, For each model checkpoints of 3 epochs during SFT, we first collect 100 outputs per problem in the training dataset, then duplicate the overlapped answers, labeled each answer with our training set outcome reward ocracle. For data sampled by ech model checkpoint, we subsample 17 answers per problem, which is in total at most 51 answers per problem after deduplication. In PrOntoQA, we only sample 50 answers per problem with the first epoch model checkpoint and then do deduplication.  

The value functions are trained in the same setting as supervised finetuning. We set the reward to be 1 when the output answer is correct and -1 otherwise. Then we use MC with $\\gamma=1$ to compute the returns. We do model selection on a validation dataset sampled from the direct policy model. For GSM8k, we train the value function and ORM for one epoch, while for Game24 and PrOntoQA we train the value function and ORM for 3 epochs.  

SFT in RLHF alignment : We utilize GPT2-open-instruct 3 , a GPT2-Small model supervisedfinetuned over several instruction-tuning dataset.  

Value training in RLHF alignment : Based on the SFT model, we collect 50 rollouts by the SFT policy for each question in the training set and label their final reward with the reward model. Then we train the value function and ORM for 2 epochs.  

Note that here we start training the value function and ORM from the data sampled by the SFT policy model through direct decoding just as an initialization of the value function and ORM. After that TS-LLM can optimize the policy model, value function, and ORM simultaneously by adding new data sampled from tree search into the training buffer.",0
276fc94b-2aa5-4712-ab27-c1e4f31af69f,"ref_ids: 454845580969520102, chunk_ids: 5, Score: 0.5195, Text: # Spending Thinking Time Wisely: Accelerating MCTS with Virtual Expansions
Weirui $\\mathbf{Ye}^{*}$ Pieter Abbeel †Yang Gao $\\ast\\ddag\\S$ ∗Tsinghua University, †UC Berkeley, §Shanghai Qi Zhi Institute

# Abstract
One of the most important AI research questions is to trade off computation versus performance since “perfect rationality"" exists in theory but is impossible to achieve in practice. Recently, Monte-Carlo tree search (MCTS) has attracted considerable attention due to the significant performance improvement in various challenging domains. However, the expensive time cost during search severely restricts its scope for applications. This paper proposes the Virtual MCTS (V-MCTS), a variant of MCTS that spends more search time on harder states and less search time on simpler states adaptively. We give theoretical bounds of the proposed method and evaluate the performance and computations on $9\\times9$ Go board games and Atari games. Experiments show that our method can achieve comparable performances to the original search algorithm while requiring less than $50\\%$ search time on average. We believe that this approach is a viable alternative for tasks under limited time and resources. The code is available at https://github.com/YeWR/V-MCTS.git .

# 1 Introduction
When artificial intelligence was first studied in the 1950s, researchers have sought to find the solution to the question “How to build an agent with perfect rationality"". The term “perfect rationality"" [7 ,24 ,26 ] here refers to the decision made with infinite amounts of computations. However, one can only solve small-scale problems without considering the practical computation time since classical search algorithms usually exhibit exponential running time. Therefore, recent AI research would no longer seek to achieve “perfect rationality"", but instead carefully trade-off computation versus the level of rationality. People have developed computational models like “bounded optimality"" to model these settings [ 26 ]. The increasing level of rationality under the same computational budget has given us a lot of AI successes. Algorithms include the Monte-Carlo sampling algorithms, the variational inference algorithms, and using DNNs as universal function approximators [9, 8, 13, 30, 17].  

Recently, MCTS-based RL algorithms have achieved much success, mainly on board games. The most notable achievement is that AlphaGo beats Hui Fan in 2015 [ 30 ]. It is the first time a computer program beat a human professional Go player. Afterward, AlphaGo beats two top-ranking human players, Lee Sedol in 2016 and Jie Ke in 2017, the latter of which ranked first worldwide at the time. Later, MCTS-based RL algorithms were further extended to other board games and Atari games [ 27 ]. EfficientZero [ 34 ] significantly improves the sample efficiency of MCTS-based RL algorithms, shedding light on its future applications in the real world like robotics and self-driving.  

Despite the impressive performance of MCTS-based RL algorithms, they require massive amounts of computation to train and evaluate. For example, MuZero [ 27 ] used 1000 TPUs trained for 12 hours to learn the game of Go, and for a single Atari game, it needs 40 TPUs to train 12 hours. Compared to previous algorithms on the Atari games benchmark, it needs around two orders of magnitude more compute. This prohibitively large computational requirement has slowed down both the further development of MCTS-based RL algorithms as well as its practical use.  

Under the hood, MCTS-based RL algorithms imagine the futures when taking different future action sequences. However, this imaging process for the current method is not computationally efficient. For example, AlphaGo needs to look ahead 1600 game states to place a single stone. On the contrary, top human professional players can only think through around 100-200 game states per minute [ 30 ]. Apart from the inefficiency, the current MCTS algorithm deals with easy and challenging cases with the same computational budget. However, human knows to use their time when it is most needed.  

In this paper, we aim to design new algorithms that save the computational time of the MCTSbased RL methods. We make three key contributions : (1) We present Virtual MCTS, a variant of MCTS, to approximate the vanilla MCTS search policies with less computation. Moreover, unlike previous pruning-based methods that focus on the selection or evaluation stage in MCTS, our method improves the search loop. It terminates the search iterations earlier adaptively when current states are simpler; (2) Theoretically, we provide some error bounds of the proposed method. Furthermore, the visualization results indicate that Virtual MCTS has a better computation and performance trade-off than vanilla MCTS; (3) Empirically, our method can save more than $50\\%$ of search times on the challenging game Go $9\\times9$ and more than $60\\%$ on the visually complex Atari games while keeping comparable performances to those of vanilla MCTS.

# 2 Related Work

# 2.1 Reinforcement Learning with MCTS
For a long time, Computer Go has been regarded as a remarkably challenging game [ 3 ,6 ]. Researchers attempt to use Monte-Carlo techniques that evaluate the value of the node state through random playouts [ 4 ,11 ,12 ,30 ]. Afterward, UCT algorithms have generally been applied in Monte-Carlo tree search (MCTS) algorithms, which use UCB1 to select action at each node of the tree [ 20 ]. Recently, MCTS-based RL methods [ 30 ,32 ,31 ,27 ] have become increasingly popular and achieved super-human performances on board games because of their strong ability to search.  

Modern MCTS-based RL algorithms include four stages in the search loop : selection, expansion, evaluation, and backpropagation. The computation bottlenecks in vanilla MCTS come from the search loop, especially for the evaluation stage and the selection stage of each iteration. The selection stage is time-consuming when the search tree becomes wider and deeper. The evaluation stage is quite expensive because people attempt to evaluate the node value by random playouts to the end in previous researches. Due to the search loop, MCTS-based algorithms have multiple model inferences compared to other model-free RL methods like PPO [28] and SAC [16].",0
