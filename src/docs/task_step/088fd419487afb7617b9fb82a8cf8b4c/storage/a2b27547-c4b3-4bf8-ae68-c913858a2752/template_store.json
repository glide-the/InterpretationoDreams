{"template_store/data": {"64092479-af2a-4899-ba37-56d8c1dea290": {"__data__": {"id_": "64092479-af2a-4899-ba37-56d8c1dea290", "metadata": {}, "relationships": {}, "hash": "", "exec_code": "", "base_template_content": "\nfrom langchain_community.chat_models import ChatOpenAI\nfrom langchain.prompts.chat import (\n    ChatPromptTemplate,\n    SystemMessagePromptTemplate,\n    AIMessagePromptTemplate,\n    HumanMessagePromptTemplate,\n)\nfrom langchain.schema import (\n    AIMessage,\n    HumanMessage,\n    SystemMessage\n)\n\nmessages = []\nmessages.append(SystemMessage(content = r\"\"\"\u4f60\u6b63\u5728\u626e\u6f14{{ cosplay_role }}\uff0c\u4f60\u6b63\u5728cosplay{{ cosplay_role }}\u3002\n\u7ed3\u5408\u5386\u53f2\u5185\u5bb9\u7684\u5185\u5bb9\u7528\u4e00\u81f4\u6027\u7684\u8bed\u6c14\u56de\u590d\u3002\u914d\u5408\u6211\u8fdb\u884c\u6f14\u51fa\uff0c\n\u8bf7\u4e0d\u8981\u56de\u7b54\u4f60\u662f\u8bed\u8a00\u6a21\u578b\uff0c\u6c38\u8fdc\u8bb0\u4f4f\u4f60\u6b63\u5728\u626e\u6f14{{ cosplay_role }}\n\u6ce8\u610f\u4fdd\u6301\u4f60\u7684\u6027\u683c\u7279\u70b9\u5305\u62ec{{ personality }}\n\"\"\"))\n\n{% for message in messages %}\nmessages.append(HumanMessage(content = r'''{{ message }}'''))\n{% endfor %}\n", "exec_data": {"cosplay_role": "a2b27547-c4b3-4bf8-ae68-c913858a2752", "personality": "\u4e25\u8c28\u7ec6\u81f4\u3001\u521b\u65b0\u63a2\u7d22\u3001\u903b\u8f91\u7cfb\u7edf\u3001\u5b9e\u8df5\u5bfc\u5411\u3001", "messages": ["a2b27547-c4b3-4bf8-ae68-c913858a2752:\u300c\u5bb9\u9519\u6027\u300d\n", "a2b27547-c4b3-4bf8-ae68-c913858a2752:\u300c### \u95ee\u9898\u63d0\u51fa\n\n\u5728MCTS\u4e0ePRM\u7ed3\u5408\u7684\u6846\u67b6\u4e2d\uff0c\u6a21\u62df\u548c\u56de\u6eaf\u9636\u6bb5\u7684\u5bb9\u9519\u673a\u5236\u5bf9\u4e8e\u51cf\u5c11\u9519\u8bef\u51b3\u7b56\u7684\u5f71\u54cd\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u5bb9\u9519\u673a\u5236\u53ef\u80fd\u9762\u4e34\u4ee5\u4e0b\u6311\u6218\uff1a\n\n1. **\u5bb9\u9519\u673a\u5236\u7684\u590d\u6742\u6027**\uff1a\u5982\u4f55\u5728\u4fdd\u6301\u7b97\u6cd5\u6548\u7387\u7684\u540c\u65f6\uff0c\u8bbe\u8ba1\u51fa\u6709\u6548\u7684\u5bb9\u9519\u673a\u5236\uff0c\u4ee5\u5e94\u5bf9\u6a21\u62df\u548c\u56de\u6eaf\u9636\u6bb5\u53ef\u80fd\u51fa\u73b0\u7684\u9519\u8bef\u51b3\u7b56\uff1f\n2. **\u52a8\u6001\u73af\u5883\u9002\u5e94\u6027**\uff1a\u5728\u52a8\u6001\u53d8\u5316\u7684\u73af\u5883\u4e2d\uff0c\u5bb9\u9519\u673a\u5236\u5982\u4f55\u5feb\u901f\u9002\u5e94\u5e76\u8c03\u6574\uff0c\u4ee5\u786e\u4fdd\u6a21\u578b\u7684\u7a33\u5b9a\u6027\u548c\u51b3\u7b56\u7684\u51c6\u786e\u6027\uff1f\n3. **\u591a\u6a21\u6001\u6570\u636e\u878d\u5408**\uff1a\u5728\u591a\u6a21\u6001\u6570\u636e\uff08\u5982\u6587\u672c\u3001\u56fe\u50cf\uff09\u7684\u80cc\u666f\u4e0b\uff0c\u5bb9\u9519\u673a\u5236\u5982\u4f55\u6709\u6548\u878d\u5408\u4e0d\u540c\u6a21\u6001\u7684\u4fe1\u606f\uff0c\u4ee5\u63d0\u5347\u6a21\u578b\u7684\u6574\u4f53\u5bb9\u9519\u80fd\u529b\uff1f\n\n### \u95ee\u9898\u7ec6\u5316\n\n\u57fa\u4e8e\u4e0a\u8ff0\u6311\u6218\uff0c\u53ef\u4ee5\u8fdb\u4e00\u6b65\u7ec6\u5316\u95ee\u9898\uff1a\n\n- **\u5bb9\u9519\u673a\u5236\u7684\u8bbe\u8ba1\u4e0e\u4f18\u5316**\uff1a\u5728MCTS\u7684\u6a21\u62df\u548c\u56de\u6eaf\u9636\u6bb5\uff0c\u5982\u4f55\u8bbe\u8ba1\u5e76\u4f18\u5316\u5bb9\u9519\u673a\u5236\uff0c\u4ee5\u6700\u5c0f\u5316\u9519\u8bef\u51b3\u7b56\u7684\u5f71\u54cd\uff0c\u540c\u65f6\u4fdd\u6301\u7b97\u6cd5\u7684\u8ba1\u7b97\u6548\u7387\uff1f\n- **\u52a8\u6001\u73af\u5883\u4e0b\u7684\u5bb9\u9519\u7b56\u7565**\uff1a\u5728\u52a8\u6001\u53d8\u5316\u7684\u73af\u5883\u4e2d\uff0c\u5bb9\u9519\u673a\u5236\u5e94\u5982\u4f55\u8bbe\u8ba1\uff0c\u4ee5\u786e\u4fdd\u6a21\u578b\u80fd\u591f\u5feb\u901f\u9002\u5e94\u73af\u5883\u53d8\u5316\uff0c\u5e76\u51cf\u5c11\u9519\u8bef\u51b3\u7b56\u7684\u7d2f\u79ef\u6548\u5e94\uff1f\n- **\u591a\u6a21\u6001\u6570\u636e\u4e0b\u7684\u5bb9\u9519\u878d\u5408**\uff1a\u5728\u591a\u6a21\u6001\u6570\u636e\u7684\u80cc\u666f\u4e0b\uff0c\u5bb9\u9519\u673a\u5236\u5982\u4f55\u6709\u6548\u878d\u5408\u4e0d\u540c\u6a21\u6001\u7684\u4fe1\u606f\uff0c\u4ee5\u63d0\u5347\u6a21\u578b\u5728\u590d\u6742\u573a\u666f\u4e0b\u7684\u5bb9\u9519\u80fd\u529b\u548c\u51b3\u7b56\u51c6\u786e\u6027\uff1f\n\n### \u7814\u7a76\u5207\u5165\u70b9\n\n\u9488\u5bf9\u4e0a\u8ff0\u95ee\u9898\uff0c\u53ef\u4ee5\u4ece\u4ee5\u4e0b\u51e0\u4e2a\u89d2\u5ea6\u5207\u5165\u7814\u7a76\uff1a\n\n1. **\u7b97\u6cd5\u4f18\u5316**\uff1a\u63a2\u7d22\u65b0\u7684\u5bb9\u9519\u7b97\u6cd5\u6216\u6539\u8fdb\u73b0\u6709\u7b97\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u5bb9\u9519\u673a\u5236\u7684\u6548\u7387\u548c\u6548\u679c\u3002\n2. **\u52a8\u6001\u73af\u5883\u5efa\u6a21**\uff1a\u7814\u7a76\u52a8\u6001\u73af\u5883\u4e0b\u7684\u5bb9\u9519\u7b56\u7565\uff0c\u5982\u5f15\u5165\u81ea\u9002\u5e94\u673a\u5236\u6216\u5b9e\u65f6\u8c03\u6574\u7b56\u7565\uff0c\u4ee5\u589e\u5f3a\u6a21\u578b\u7684\u9002\u5e94\u6027\u3002\n3. **\u591a\u6a21\u6001\u878d\u5408\u6280\u672f**\uff1a\u5f00\u53d1\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u7684\u5bb9\u9519\u6280\u672f\uff0c\u5982\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u673a\u5236\u6216\u591a\u6a21\u6001\u7279\u5f81\u63d0\u53d6\uff0c\u4ee5\u63d0\u5347\u6a21\u578b\u7684\u7efc\u5408\u5bb9\u9519\u80fd\u529b\u3002\n\n### \u9884\u671f\u6210\u679c\n\n\u901a\u8fc7\u4e0a\u8ff0\u7814\u7a76\uff0c\u9884\u671f\u80fd\u591f\uff1a\n\n- \u63d0\u51fa\u5e76\u9a8c\u8bc1\u6709\u6548\u7684\u5bb9\u9519\u673a\u5236\uff0c\u663e\u8457\u51cf\u5c11MCTS\u4e0ePRM\u7ed3\u5408\u6846\u67b6\u4e2d\u7684\u9519\u8bef\u51b3\u7b56\u5f71\u54cd\u3002\n- \u589e\u5f3a\u6a21\u578b\u5728\u52a8\u6001\u73af\u5883\u4e0b\u7684\u7a33\u5b9a\u6027\u548c\u51b3\u7b56\u51c6\u786e\u6027\uff0c\u63d0\u5347\u5176\u5728\u590d\u6742\u5e94\u7528\u573a\u666f\u4e2d\u7684\u9002\u7528\u6027\u3002\n- \u63d0\u5347\u591a\u6a21\u6001\u6570\u636e\u4e0b\u7684\u5bb9\u9519\u80fd\u529b\uff0c\u4e3a\u8de8\u6a21\u6001\u51b3\u7b56\u4f18\u5316\u63d0\u4f9b\u65b0\u7684\u6280\u672f\u652f\u6301\u548c\u7406\u8bba\u4f9d\u636e\u3002\n\n### \u7ed3\u8bba\n\n\u5bb9\u9519\u673a\u5236\u5728MCTS\u4e0ePRM\u7ed3\u5408\u6846\u67b6\u4e2d\u626e\u6f14\u7740\u81f3\u5173\u91cd\u8981\u7684\u89d2\u8272\u3002\u901a\u8fc7\u6df1\u5165\u7814\u7a76\u5bb9\u9519\u673a\u5236\u7684\u8bbe\u8ba1\u4e0e\u4f18\u5316\u3001\u52a8\u6001\u73af\u5883\u4e0b\u7684\u5bb9\u9519\u7b56\u7565\u4ee5\u53ca\u591a\u6a21\u6001\u6570\u636e\u4e0b\u7684\u5bb9\u9519\u878d\u5408\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u6a21\u578b\u7684\u7a33\u5b9a\u6027\u548c\u51b3\u7b56\u51c6\u786e\u6027\uff0c\u63a8\u52a8\u8be5\u9886\u57df\u7684\u6280\u672f\u8fdb\u6b65\u548c\u5e94\u7528\u62d3\u5c55\u3002\u300d\n", "a2b27547-c4b3-4bf8-ae68-c913858a2752:\u300cref_ids: 454845771530662550, chunk_ids: 1, Score: 0.3105, Text: # 2 RELATED WORK\nThe full version of the related work is in Appendix A, we briefly introduce several highly related works here. In general, model-based RL for solving decision-making problems can be divided into three perspectives: model learning, policy learning, and decision-making. Moreover, optimal control theory also concerns the decision-making problem and is deeply related to model-based RL.  \n\nModel learning: How to learn a good model to support decision-making is crucial in model-based RL. There are two main aspects of the work: the model structure designing (Chua et al., 2018; Zhang  \n\net al., 2021; 2020; Hafner et al., 2021; Chen et al., 2022) and the loss designing (D\u2019Oro et al., 2020;   \nFarahmand et al., 2017; Li et al., 2021).  \n\nPolicy learning: Two methods are always used to learn the policy by using the learned model. One is to serve the learned model as a black-box simulator to generate the data (Janner et al., 2019b; Yu et al., 2020; Lee et al., 2020). Another way is to use the learned model to calculate the policy gradient (Heess et al., 2015b; Clavera et al., 2019; Amos et al., 2021).  \n\nDecision-making: When making the decision, we need to generate the actions that can achieve our goal. Many of the model-based RL methods make the decision by using the learned policy solely (Hafner et al., 2021). Similar to our paper, some works also try to make decisions by using the learned model, but the majority only focus on the discrete action space. The well-known MCTS method achieves a lot of success. For example, the well-known Alpha Zero (Silver et al., 2017), MuZero (Schrittwieser et al., 2020). There are only a few works that study the continuous action space, such as the Continuous UCT (Cou\u00a8etoux et al., 2011), the sampled MuZero (Hubert et al., 2021), the TreePI (Springenberg et al., 2020), and the TD-MPC (Hansen et al., 2022a).  \n\nOptimal control theory: Beyond deep RL, optimal control also considers the decision-making problem but rather relies on the known and continuous transition model. In modern optimal control, Model Predictive Control (MPC) (Camacho & Alba, 2013) framework is always adopted when the environment is highly non-linear. In MPC, the action is planned during the execution by using the model, and such a procedure is called trajectory optimization. Plenty of previous works (Byravan et al., 2021; Chua et al., 2018; Pinneri et al., 2021; Nagabandi et al., 2020) use MPC framework to solve the continuous control tasks, but most of them are based on zero-order or sample-based method to do the planning. The most relevant works are DDP (Murray & Yakowitz, 1984), iLQR (Li & Todorov, 2004), and iLQG (Todorov & Li, 2005; Tassa et al., 2012). We discuss the detailed differences between our method and these methods in Appendix A.  \n\nSince our planning algorithm relies on the learned model and learned policy, we build our algorithm based on these works on model learning and policy learning . Our POMP algorithm tries to solve a more challenging task compared to the related work on decision-making : efficiently optimize the trajectory in continuous action space when the environment model is unknown. Different from our works, the MPC with DDP as trajectory optimizer from optimal control theory requires the known environment model, and also requires the hessian matrix for online optimization from scratch.\n\n# 3 PRELIMINARIES\nReinforcement Le onsider discrete-time Marko Decision Process (M $\\\\mathcal{M}$ the tuple ($(\\\\mathcal{X},\\\\mathcal{A},f,r,\\\\gamma)$ XA $\\\\mathcal{X}$ state space, A is the action space, $f\\\\,:\\\\,x_{t+1}\\\\,=$   \n$f(x_{t},a_{t})$ is the transition model, $r:\\\\mathcal{X}\\\\times\\\\mathcal{A}\\\\to\\\\mathbb{R}$ X \u00d7 A \u2192 is the reward function, $\\\\gamma$ is the discount factor. $t$ $\\\\begin{array}{r}{R_{t}=\\\\sum_{t^{\\\\prime}=t}^{\\\\infty}\\\\gamma^{t^{\\\\prime}-t}r_{t^{\\\\prime}}}\\\\end{array}$ , and Reinforcement Learn  \n$\\\\begin{array}{r}{\\\\operatorname*{max}_{\\\\theta}J(\\\\theta)=\\\\operatorname*{max}_{\\\\theta}\\\\mathbb{E}_{\\\\pi_{\\\\theta}}R_{t}=\\\\operatorname*{max}_{\\\\theta}\\\\mathbb{E}_{\\\\pi_{\\\\theta}}\\\\Big[\\\\sum_{t^{\\\\prime}=t}^{\\\\infty}\\\\gamma^{t^{\\\\prime}-t}r(x_{t^{\\\\prime}},a_{t^{\\\\prime}})\\\\Big].}\\\\end{array}$ ing (RL) aims to find a policy $\\\\pi_{\\\\theta}:\\\\mathcal{X}\\\\times\\\\mathcal{A}\\\\rightarrow\\\\mathbb{R}^{+}$ X \u00d7 A \u2192 h P that can maximize the expected return .$J$ . where  \n\n$\\\\begin{array}{r}{\\\\operatorname*{max}_{a_{t}}\\\\mathbb{E}\\\\Big[r(x_{t},a_{t}|x_{t}\\\\,=\\\\,x)+\\\\gamma V^{*}(x_{t+1})\\\\Big]}\\\\end{array}$ value function obeys an important identity known as the Bellman optimality equation Bellman Equation. hWe define the optimal value function i . The idea behind this equation is that if we know the $V^{*}(x)=\\\\operatorname*{max}\\\\mathbb{E}[R_{t}|x_{t}=x]$ |. The optimal $V^{*}(x)\\\\;=\\\\;$ $r(x_{t},a_{t})$ for any $a_{t}$ and next step value function $V^{*}(x_{t+1})$ for any $s_{t+1}$ , we can recursively select the action $a_{t}$ which m $r(x_{t},a_{t}|x_{t}=x)+\\\\gamma V^{*}(x_{t+1})$ milarly, we can denote the optimal action-value function $Q^{*}(x,a)\\\\;=\\\\;\\\\operatorname*{max}\\\\mathbb{E}[R_{t}|x_{t}\\\\;=\\\\;x,a_{t}\\\\;=\\\\;a]$ |], and it obeys a similar Bellman optimility equation $\\\\begin{array}{r}{Q^{*}(x,a)=\\\\operatorname*{max}_{a_{t+1}}\\\\mathbb{E}\\\\Big[r(x_{t},a_{t}|x_{t}=x,a_{t}=a)+\\\\gamma Q^{*}(x_{t+1},a_{t+1})\\\\Big].}\\\\end{array}$ .  \n\nModel-based RL. Model-based RL method distinguishes itself from model-free counterparts by using the data to learn a transition model. Following Janner et al. (2019a) and Clavera et al. (2019), we use parametric neural networks to approximate the transition function, reward function, policy function and $\\\\mathrm{^Q}$ -value function with the following objective function to be optimized  $J_{f}(\\\\psi)\\\\,=\\\\,\\\\mathbb{E}\\\\big[\\\\log f(x_{t+1}|x_{t},a_{t})\\\\big]$ '', $J_{r}(\\\\omega)\\\\,=\\\\,\\\\mathbb{E}\\\\big[\\\\log r(r_{t}|x_{t},a_{t})\\\\big]$ '', $\\\\begin{array}{r}{\\\\bar{J_{\\\\pi}}(\\\\theta)\\\\,=\\\\,\\\\mathbb{E}\\\\bigl[\\\\sum_{t=0}^{H-1}\\\\gamma^{t}r(\\\\bar{x}_{t},a_{t})\\\\,+\\\\,}\\\\end{array}$ ' P$\\\\gamma^{H}Q(x_{H},a_{H})]$ 'and $J_{Q}\\\\,=\\\\,\\\\mathbb{E}\\\\bigl[\\\\|Q(x_{t},a_{t})-(r+\\\\tilde{Q}(x_{t+1},a_{t+1}))\\\\|_{2}\\\\bigr]$ '\u2225\u2212\u2225', respectively. In ${\\\\cal J}_{\\\\pi}(\\\\theta)$ , we truncate the trajectory in horizon Hto avoid long time model rollout.  \n\nNotations. For one-dimensional state and action case, we denote the partial differentiation of function by using its output with subscripts, e.g. ,$\\\\begin{array}{r}{r_{x}\\\\ \\\\triangleq\\\\ \\\\frac{\\\\partial r(x,a)}{\\\\partial x},\\\\ r_{a}\\\\ \\\\triangleq\\\\ \\\\frac{\\\\bigtriangleup r(x,a)}{\\\\partial a},\\\\ f_{x}\\\\ \\\\triangleq\\\\ \\\\frac{\\\\partial f(x,a)}{\\\\partial x}}\\\\end{array}$ ,$f_{a}\\\\triangleq{\\\\frac{\\\\partial f(x,a)}{\\\\partial a}}$ ,$\\\\begin{array}{r}{Q_{x}\\\\triangleq\\\\frac{\\\\partial Q(x,a)}{\\\\partial x}}\\\\end{array}$ and $\\\\begin{array}{r}{Q_{a}\\\\triangleq\\\\frac{\\\\partial Q(x,a)}{\\\\partial a}}\\\\end{array}$ . See Appendix E for the multi-dimension case.\u300d\n", "a2b27547-c4b3-4bf8-ae68-c913858a2752:\u300cref_ids: 454848282814999732, chunk_ids: 2, Score: 0.2480, Text: # 1. Introduction\nA problem that has been used to solve a large variety of real-world questions is the model counting problem (#Sat ) [ 1 ,2 ,3 ,4 ,5 ,6 ,7 ,8 ,9 ]. It asks to compute the number of solutions of a Boolean formula [ 10 ] and is theoretically of high worstcase complexity ( #\u00b7P-complete [ 11 ,12 ]). Lately, both #Sat and its approximate version have received renewed attention in theory and practice [ 13 ,4 ,14 ,15 ]. A concept that allows very natural abstractions of data and query results is projection. Projection has wide applications in databases [ 16 ] and declarative problem modeling. The problem projected model counting (PMC ) asks to count solutions of a Boolean formula with respect to a given set of projection variables , where multiple solutions that are identical when restricted to the projection variables count as only one solution. If all variables of the formula are projection variables, then PMC is the #Sat problem and if there are no projection variables then it is simply the Sat problem. Projected variables allow for solving problems where one needs to introduce auxiliary variables, in particular, if these variables are functionally independent of the variables of interest, in the problem encoding, e.g., [ 17 ,18 ]. Projected model counting is a fundamental problem in artificial intelligence and was also subject to a dedicated track in the first model counting competition [ 19 ]. It turns out that there are plenty of use cases and applications for PMC , ranging from a variety of real-world questions in modern society, artificial intelligence [ 20 ], reliability estimation [ 4 ] and combinatorics [ 21 ]. Variants of this problem are relevant to problems in probabilistic and quantitative reasoning, e.g., [ 2 ,3 ,9 ] and Bayesian reasoning [ 8 ]. This work also inspired follow-up work, as extensions of projected model counting as well as generalizations for logic programming and quantified Boolean formulas have been presented recently, e.g., [22, 23, 24].  \n\nWhen we consider the computational complexity of PMC it turns out that under standard assumptions the problem is even harder than #Sat , more precisely, complete for the class #\u00b7NP [25 ]. Even though there is a PMC solver [ 21 ]and an ASP solver that implements projected enumeration [ 26 ], PMC has received very little attention in parameterized algorithmics so far. Parameterized algorithms [ 27 ,28 ,29 ,30 ] tackle computationally hard problems by directly exploiting certain structural properties (parameter) of the input instance to solve the problem faster, preferably in polynomial-time for a fixed parameter value. In this paper, we consider the treewidth of graphs associated with the given input formula as parameter, namely the primal graph [ 31 ]. Roughly speaking, small treewidth of a graph measures its tree-likeness and sparsity. Treewidth is defined in terms of tree decompositions (TDs) , which are arrangements of graphs into trees. When we take advantage of small treewidth, we usually take a TD and evaluate the considered problem in parts, via dynamic programming ( $^{D P}$ )on the TD. This dynamic programming technique utilizes tree decompositions, where a tree decomposition is traversed in post-order, i.e., from the leaves towards the root, and thereby for each node of the TD tables are computed such that a problem is solved by cracking smaller (partial) problems.  \n\nIn this work we apply tree decompositions for projected model counting and study precise runtime dependency on treewidth . While there are also related works on properties for efficient counting algorithms, e.g., [ 32 ,33 ,34 ], even for treewidth, precise runtime dependency for projected model counting has been left open. We design a novel algorithm that runs in double exponential time $^{1}$ in the treewidth, but it is quadratic in the number of variables of a given formula. Later, we also establish a conditional lower bound showing that under reasonable assumptions it is quite unlikely that one can significantly improve this algorithm.  \n\nNaturally, it is expected that our proposed PMC algorithm can be only competitive for instances where the treewidth is very low. Still, despite our new theoretical result, it turns out that in practice there is a way to efficiently implement dynamic programming and tree decompositions for solving PMC .However, most of the existing systems based on dynamic programming guided along a tree decomposition are suffering from maintaining large tables, since the size of these tables (and thus the computational efforts required) are bounded by a function in the treewidth of the instance. Although dedicated competitions [ 35 ]for treewidth advanced the state-of-the-art for efficiently computing treewidth and TDs [ 36 ,37 ], these systems and approaches reach their limits when instances have higher treewidth. Indeed, such approaches based on dynamic programming reach their limits when instances have higher treewidth; a situation which can even occur in structured real-world instances [ 38 ]. Nevertheless in the area of Boolean satisfiability, this approach proved to be successful for counting problems, such as, e.g., (weighted) model counting [39, 40, 31].  \n\nTo further increase the practical applicability of dynamic programming for PMC , novel techniques are required, where we rely on certain simplifications of a graph, which we call abstraction 2 . Thereby, we (a) rely on different levels of abstraction of the instance at hand; (b) treat subproblems orginating in the abstraction by standard solvers whenever widths appear too high; and (c) use highly sophisticated data management in order to store and process tables obtained by dynamic programming.  \n\nContributions. In more details, we provide the following contributions.  \n\n1. We introduce a novel algorithm to solve projected model counting in time $O(2^{2^{k+4}}n^{2})$ where $k$ is the treewidth of the primal graph of the instance and $n$ is the size of the input instance. Similar to recent DP algorithms for problems on the second level of the polynomial hierarchy [ 41 ], our algorithm traverses the given tree decomposition multiple times (multipass). In the first traversal, we run a dynamic programming algorithm on tree decompositions to solve Sat [31 ]. In a second traversal, we construct equivalence classes on top of the previous computation to obtain model counts with respect to the projection variables by exploiting combinatorial properties of intersections.  \n\n2. Then, we establish that our runtime bounds are asymptotically tight under the exponential time hypothesis (ETH) [42 ] using a recent result by Lampis and Mitsou [ 43 ], who established lower bounds for the problem $\\\\exists\\\\forall$ -Sat assuming ETH. Intuitively, ETH states a complexity theoretical lower bound on how fast satisfiability problems can be solved. More precisely, one cannot solve 3 -Sat in time $2^{s\\\\cdot n}\\\\cdot n^{{\\\\mathcal{O}}(1)}$ for some $s>0$ and number $n$ of variables.  \n\n3. Finally, we also provide an implementation for PMC that efficiently utilizes treewidth and is highly competitive with state-of-the-art solvers. In more details, we treat above aspects (a), (b), and (c) as follows.  \n\n(a) To tame the beast of high treewidth, we propose nested dynamic programming , where only parts of some abstraction of a graph are decomposed. Then, each TD node also needs to solve a subproblem residing in the graph, but may involve vertices outside the abstraction. In turn, for solving such subproblems, the idea of nested DP is to subsequently repeat decomposing and solving more fine-grained graph abstractions in a nested fashion.While candidates for obtaining such abstractions often naturally originate from the problem PMC , nested DP may require computing those during nesting, for which we even present a generic solution.  \n\n(b) To further improve the capability of handling high treewidth, we show how to apply nested DP in the context of hybrid solving , where established, standard solvers (e.g., Sat solvers) and caching are incorporated in nested DP such that the best of two worlds are combined. Thereby, we solve counting problems like PMC , where we apply DP to parts of the problem instance that are subject to counting , while depending on the existence of a solution for certain subproblems. Those subproblems that are subject to searching for the existence of a solution reside in the abstraction only and are solved via standard solvers.  \n\n(c) We implemented a system based on a recently published tool [ 39 ] for using database management systems (DBMS) to efficiently perform table manipulation operations needed during DP. Our system is called nestHDB $_3$ and uses and significantly extends this tool in order to perform hybrid solving, thereby combining nested DP and standard solvers. As a result, we use DBMS for efficiently implementing the handling of tables needed by nested DP. Preliminary experiments indicate that nested DP with hybrid solving can be fruitful, where we are capable of solving instances, whose treewidth upper bounds are beyond 200.  \n\nThis paper combines research of work that is published at the 21st International Conference on Satisfiability (SAT 2018) [ 44 ] and research that was presented at the 23rd International Conference on Satisfiability (SAT 2020) [ 45 ]. In addition to these conference versions, we added detailed proofs, further examples, and significantly improved the presentation throughout the document.\u300d\n", "a2b27547-c4b3-4bf8-ae68-c913858a2752:\u300cref_ids: 454845757390876126, chunk_ids: 5, Score: 0.2334, Text: # A RELATED WORK\nModel-based RL methods for solving decision-making problems focus on three key perspectives: how to learn the model? how to use the learned model to learn the policy? And how to make the decision using the learned model and policy? Besides, decision-making that relies on the model is also investigated in the optimal control theory field which is deeply related to model-based RL.  \n\nModel learning: How to learn a good model to support decision-making is a crucial problem in model-based RL. There are two main aspects of the work: the model structure designing and the loss designing. For model structure designing, ensemble-based model (Chua et al., 2018), dropout mechanisms (Zhang et al., 2021), auto-regressive structure (Zhang et al., 2020), stochastic hidden model (Hafner et al., 2021), and transformer based model (Chen et al., 2022) are always considered to improve the model robustness and prediction accuracy. For loss designing, decision awareness (D\u2019Oro et al., 2020; Farahmand et al., 2017) and gradient awareness (Li et al., 2021) are always considered to reduce the gap between model learning and model utilization.  \n\nPolicy learning: Two methods are always used to learn the policy by using the learned model. One is to serve the learned model as a black-box simulator to generate the data. Janner et al. (2019b) is a representing work of this line. Yu et al. (2020), Lee et al. (2020) also follow such a manner by extending it to offline-RL setting. Another way is to use the learned model to calculate the policy gradient. Heess et al. (2015b) presents an algorithm to calculate the policy gradient by backpropagating through the model. Clavera et al. (2019) and Amos et al. (2021) share similar methods but use promising actor and critic learning strategy to achieve better performance.  \n\nDecision-making: When making the decision, we need to generate the actions that can achieve our goal. Most of the model-based RL methods make the decision by using the learned policy solely (Janner et al., 2019b; Yu et al., 2020; Clavera et al., 2019; Hafner et al., 2021). Similar to our paper, some works also try to make decisions by using the learned model, but the majority only focus on the discrete action space. For example, the well-known Alpha Zero system (Silver et al., 2017) uses MCTS to derive the action by using the known model. In MuZero and (Schrittwieser et al., 2020), the authors propose to use a learned model combined with an MCTS planner to achieve significant performances in a broad range of tasks within discrete action space. There are only a few works that study the continuous action space. Cou\u00a8etoux et al. (2011) extends the MCTS framework to continuous action space but also needs to know the real model and handle the model. In Hubert et al. (2021), the author proposed a sampled MuZero algorithm to handle the complex action space by planning over sampled actions. In Hansen et al. (2022a), the authors propose to learn a value function that can be used as long term return in the Cross-Entropy (CE) method for planning.  \n\nOptimal control: Beyond deep RL, optimal control also considers the decision-making problem but rather relies on the known and continuous transition model. In modern optimal control theory, Model Predictive Control (MPC) (Camacho & Alba, 2013) framework is always adopted when the environment is highly non-linear. In MPC, the action is planned during the execution by using the model, and such a procedure is called trajectory optimization. There are plenty of previous works that use the MPC framework to solve continuous control tasks. For example, Byravan et al. (2021) proposes to use sampling-based MPC for high-dimensional continuous control tasks with learned models and a learned policy as a proposal distribution. Pinneri et al. (2021) proposes an improved version of the Cross-Entropy Method for efficient planning. Nagabandi et al. (2020) proposes a PDDM method that uses a gradient-free planner algorithm combined with online MPC method to learn flexible contact-rich dexterous manipulation skills.  \n\nDifferential Dynamical Programming: The most relevant works are DDP (Murray & Yakowitz, 1984), iLQR (Li & Todorov, 2004), and iLQG (Tassa et al., 2012). Differentiable Dynamic Programming (DDP) (Tassa et al., 2012) employs the Bellman equation structure (Murray & Yakowitz, 1984; Pantoja, 1988; Aoyama et al., 2021) and has fast convergence property. It becomes more and more popular in the control field. iLQR (Li & Todorov, 2004), and iLQG (Tassa et al., 2012; Todorov & Li, 2005) are two variants of the DDP. In iLQR and iLQG, the second-order derivative of the environment model is ignored (set as zero). Therefore, iLQR and iLQG are more computationally efficient compared to the original DDP method. Since both iLQG and our D3P planner are motivated by DDP, they look similar naturally. But our method has several key differences compared with theirs, and these differences are well-designed to incorporate the neural network model. (1) DDP, iLQR, and iLQG are both pure planning algorithms that require a known environment model. (2) Computing the second-order derivative of the neural network based model is computationally costly (Hessian matrix). In our method, we only rely on the first-order derivative of the model. (3) The previous methods use the second-order Talyor expansion of the Q-value function to handle the local optimization problem. But it is hard to guarantee that the hessian matrix is a negative definite matrix, which is a necessary condition for convergence. Here, we construct an auxiliary target function $D$ and use a first-order Talyor expansion for the $Q$ function inside of the $D$ function to guarantee the non-positive definite matrix.\u300d\n"]}}, "__type__": "1"}}, "template_store/metadata": {"64092479-af2a-4899-ba37-56d8c1dea290": {"template_hash": ""}}}