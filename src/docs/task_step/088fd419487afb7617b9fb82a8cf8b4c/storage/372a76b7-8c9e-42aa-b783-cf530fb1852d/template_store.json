{"template_store/data": {"742ac35b-1bb0-4778-a65c-50a78973e3d3": {"__data__": {"id_": "742ac35b-1bb0-4778-a65c-50a78973e3d3", "metadata": {}, "relationships": {}, "hash": "", "exec_code": "", "base_template_content": "\nfrom langchain_community.chat_models import ChatOpenAI\nfrom langchain.prompts.chat import (\n    ChatPromptTemplate,\n    SystemMessagePromptTemplate,\n    AIMessagePromptTemplate,\n    HumanMessagePromptTemplate,\n)\nfrom langchain.schema import (\n    AIMessage,\n    HumanMessage,\n    SystemMessage\n)\n\nmessages = []\nmessages.append(SystemMessage(content = r\"\"\"\u4f60\u6b63\u5728\u626e\u6f14{{ cosplay_role }}\uff0c\u4f60\u6b63\u5728cosplay{{ cosplay_role }}\u3002\n\u7ed3\u5408\u5386\u53f2\u5185\u5bb9\u7684\u5185\u5bb9\u7528\u4e00\u81f4\u6027\u7684\u8bed\u6c14\u56de\u590d\u3002\u914d\u5408\u6211\u8fdb\u884c\u6f14\u51fa\uff0c\n\u8bf7\u4e0d\u8981\u56de\u7b54\u4f60\u662f\u8bed\u8a00\u6a21\u578b\uff0c\u6c38\u8fdc\u8bb0\u4f4f\u4f60\u6b63\u5728\u626e\u6f14{{ cosplay_role }}\n\u6ce8\u610f\u4fdd\u6301\u4f60\u7684\u6027\u683c\u7279\u70b9\u5305\u62ec{{ personality }}\n\"\"\"))\n\n{% for message in messages %}\nmessages.append(HumanMessage(content = r'''{{ message }}'''))\n{% endfor %}\n", "exec_data": {"cosplay_role": "372a76b7-8c9e-42aa-b783-cf530fb1852d", "personality": "\u3001", "messages": ["372a76b7-8c9e-42aa-b783-cf530fb1852d:\u300c\u504f\u597d\u7b56\u7565\u6a21\u578b\uff08PRM\uff09\u300d\n", "372a76b7-8c9e-42aa-b783-cf530fb1852d:\u300c### \u95ee\u9898\u63d0\u51fa\n\n\u5728MCTS\u4e0ePRM\u7ed3\u5408\u7684\u6846\u67b6\u4e2d\uff0c\u5982\u4f55\u6709\u6548\u5229\u7528MCTS\u7684\u641c\u7d22\u80fd\u529b\u6765\u4f18\u5316PRM\u7684\u504f\u597d\u5efa\u6a21\u8fc7\u7a0b\uff0c\u7279\u522b\u662f\u5728\u63a8\u8350\u7cfb\u7edf\u548c\u4e2a\u6027\u5316\u670d\u52a1\u4e2d\uff0c\u5982\u4f55\u786e\u4fdd\u6a21\u578b\u80fd\u591f\u51c6\u786e\u6355\u6349\u7528\u6237\u7684\u52a8\u6001\u504f\u597d\u5e76\u505a\u51fa\u5b9e\u65f6\u51b3\u7b56\uff1f\n\n### \u95ee\u9898\u80cc\u666f\n\nPRM\u901a\u8fc7\u5b66\u4e60\u7528\u6237\u7684\u504f\u597d\u6765\u4f18\u5316\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5e38\u7528\u4e8e\u63a8\u8350\u7cfb\u7edf\u548c\u4e2a\u6027\u5316\u670d\u52a1\u3002\u7136\u800c\uff0c\u7528\u6237\u7684\u504f\u597d\u5f80\u5f80\u662f\u52a8\u6001\u53d8\u5316\u7684\uff0c\u4f20\u7edf\u7684PRM\u6a21\u578b\u53ef\u80fd\u96be\u4ee5\u5b9e\u65f6\u6355\u6349\u8fd9\u4e9b\u53d8\u5316\u3002MCTS\u4f5c\u4e3a\u4e00\u79cd\u542f\u53d1\u5f0f\u641c\u7d22\u7b97\u6cd5\uff0c\u5177\u6709\u5f3a\u5927\u7684\u51b3\u7b56\u4f18\u5316\u80fd\u529b\uff0c\u4f46\u5176\u5728\u5927\u89c4\u6a21\u95ee\u9898\u4e2d\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u8f83\u9ad8\u3002\u56e0\u6b64\uff0c\u5982\u4f55\u5c06MCTS\u4e0ePRM\u6709\u6548\u7ed3\u5408\uff0c\u65e2\u5229\u7528MCTS\u7684\u641c\u7d22\u80fd\u529b\u4f18\u5316PRM\u7684\u51b3\u7b56\u8fc7\u7a0b\uff0c\u53c8\u786e\u4fdd\u6a21\u578b\u80fd\u591f\u5b9e\u65f6\u6355\u6349\u7528\u6237\u7684\u52a8\u6001\u504f\u597d\uff0c\u662f\u4e00\u4e2a\u503c\u5f97\u63a2\u8ba8\u7684\u95ee\u9898\u3002\n\n### \u95ee\u9898\u7ec6\u5316\n\n1. **\u52a8\u6001\u504f\u597d\u6355\u6349**\uff1a\u5728\u63a8\u8350\u7cfb\u7edf\u548c\u4e2a\u6027\u5316\u670d\u52a1\u4e2d\uff0c\u7528\u6237\u7684\u504f\u597d\u53ef\u80fd\u4f1a\u968f\u65f6\u95f4\u3001\u60c5\u5883\u7b49\u56e0\u7d20\u53d1\u751f\u53d8\u5316\u3002\u5982\u4f55\u8bbe\u8ba1MCTS\u4e0ePRM\u7ed3\u5408\u7684\u6846\u67b6\uff0c\u4f7f\u5176\u80fd\u591f\u5b9e\u65f6\u6355\u6349\u8fd9\u4e9b\u52a8\u6001\u53d8\u5316\uff1f\n   \n2. **\u8ba1\u7b97\u6548\u7387\u4f18\u5316**\uff1aMCTS\u5728\u5927\u89c4\u6a21\u95ee\u9898\u4e2d\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u8f83\u9ad8\uff0c\u5982\u4f55\u4f18\u5316MCTS\u7684\u641c\u7d22\u7b56\u7565\uff0c\u4f7f\u5176\u5728PRM\u7684\u504f\u597d\u5efa\u6a21\u8fc7\u7a0b\u4e2d\u4fdd\u6301\u9ad8\u6548\uff1f\n\n3. **\u6a21\u578b\u6cdb\u5316\u80fd\u529b**\uff1a\u5982\u4f55\u786e\u4fddMCTS\u4e0ePRM\u7ed3\u5408\u7684\u6a21\u578b\u5728\u4e0d\u540c\u5e94\u7528\u573a\u666f\u4e0b\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u591a\u6a21\u6001\u6570\u636e\u65f6\uff1f\n\n4. **\u5b9e\u65f6\u51b3\u7b56\u652f\u6301**\uff1a\u5728\u63a8\u8350\u7cfb\u7edf\u548c\u4e2a\u6027\u5316\u670d\u52a1\u4e2d\uff0c\u5982\u4f55\u786e\u4fddMCTS\u4e0ePRM\u7ed3\u5408\u7684\u6a21\u578b\u80fd\u591f\u5feb\u901f\u505a\u51fa\u51b3\u7b56\uff0c\u6ee1\u8db3\u5b9e\u65f6\u6027\u8981\u6c42\uff1f\n\n### \u95ee\u9898\u610f\u4e49\n\n\u901a\u8fc7\u89e3\u51b3\u4e0a\u8ff0\u95ee\u9898\uff0c\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63d0\u5347MCTS\u4e0ePRM\u7ed3\u5408\u6846\u67b6\u5728\u63a8\u8350\u7cfb\u7edf\u548c\u4e2a\u6027\u5316\u670d\u52a1\u4e2d\u7684\u5e94\u7528\u6548\u679c\uff0c\u4f7f\u5176\u80fd\u591f\u66f4\u51c6\u786e\u5730\u6355\u6349\u7528\u6237\u7684\u52a8\u6001\u504f\u597d\uff0c\u5e76\u505a\u51fa\u5b9e\u65f6\u3001\u9ad8\u6548\u7684\u51b3\u7b56\u3002\u8fd9\u4e0d\u4ec5\u6709\u52a9\u4e8e\u63d0\u5347\u7528\u6237\u4f53\u9a8c\uff0c\u8fd8\u80fd\u63a8\u52a8\u76f8\u5173\u9886\u57df\u7684\u6280\u672f\u8fdb\u6b65\u3002\u300d\n", "372a76b7-8c9e-42aa-b783-cf530fb1852d:\u300cref_ids: 454848282814999732, chunk_ids: 2, Score: 0.3359, Text: # 2 RELATED WORK\nThe full version of the related work is in Appendix A, we briefly introduce several highly related works here. In general, model-based RL for solving decision-making problems can be divided into three perspectives: model learning, policy learning, and decision-making. Moreover, optimal control theory also concerns the decision-making problem and is deeply related to model-based RL.  \n\nModel learning: How to learn a good model to support decision-making is crucial in model-based RL. There are two main aspects of the work: the model structure designing (Chua et al., 2018; Zhang  \n\net al., 2021; 2020; Hafner et al., 2021; Chen et al., 2022) and the loss designing (D\u2019Oro et al., 2020;   \nFarahmand et al., 2017; Li et al., 2021).  \n\nPolicy learning: Two methods are always used to learn the policy by using the learned model. One is to serve the learned model as a black-box simulator to generate the data (Janner et al., 2019b; Yu et al., 2020; Lee et al., 2020). Another way is to use the learned model to calculate the policy gradient (Heess et al., 2015b; Clavera et al., 2019; Amos et al., 2021).  \n\nDecision-making: When making the decision, we need to generate the actions that can achieve our goal. Many of the model-based RL methods make the decision by using the learned policy solely (Hafner et al., 2021). Similar to our paper, some works also try to make decisions by using the learned model, but the majority only focus on the discrete action space. The well-known MCTS method achieves a lot of success. For example, the well-known Alpha Zero (Silver et al., 2017), MuZero (Schrittwieser et al., 2020). There are only a few works that study the continuous action space, such as the Continuous UCT (Cou\u00a8etoux et al., 2011), the sampled MuZero (Hubert et al., 2021), the TreePI (Springenberg et al., 2020), and the TD-MPC (Hansen et al., 2022a).  \n\nOptimal control theory: Beyond deep RL, optimal control also considers the decision-making problem but rather relies on the known and continuous transition model. In modern optimal control, Model Predictive Control (MPC) (Camacho & Alba, 2013) framework is always adopted when the environment is highly non-linear. In MPC, the action is planned during the execution by using the model, and such a procedure is called trajectory optimization. Plenty of previous works (Byravan et al., 2021; Chua et al., 2018; Pinneri et al., 2021; Nagabandi et al., 2020) use MPC framework to solve the continuous control tasks, but most of them are based on zero-order or sample-based method to do the planning. The most relevant works are DDP (Murray & Yakowitz, 1984), iLQR (Li & Todorov, 2004), and iLQG (Todorov & Li, 2005; Tassa et al., 2012). We discuss the detailed differences between our method and these methods in Appendix A.  \n\nSince our planning algorithm relies on the learned model and learned policy, we build our algorithm based on these works on model learning and policy learning . Our POMP algorithm tries to solve a more challenging task compared to the related work on decision-making : efficiently optimize the trajectory in continuous action space when the environment model is unknown. Different from our works, the MPC with DDP as trajectory optimizer from optimal control theory requires the known environment model, and also requires the hessian matrix for online optimization from scratch.\n\n# 3 PRELIMINARIES\nReinforcement Le onsider discrete-time Marko Decision Process (M $\\\\mathcal{M}$ the tuple ($(\\\\mathcal{X},\\\\mathcal{A},f,r,\\\\gamma)$ XA $\\\\mathcal{X}$ state space, A is the action space, $f\\\\,:\\\\,x_{t+1}\\\\,=$   \n$f(x_{t},a_{t})$ is the transition model, $r:\\\\mathcal{X}\\\\times\\\\mathcal{A}\\\\to\\\\mathbb{R}$ X \u00d7 A \u2192 is the reward function, $\\\\gamma$ is the discount factor. $t$ $\\\\begin{array}{r}{R_{t}=\\\\sum_{t^{\\\\prime}=t}^{\\\\infty}\\\\gamma^{t^{\\\\prime}-t}r_{t^{\\\\prime}}}\\\\end{array}$ , and Reinforcement Learn  \n$\\\\begin{array}{r}{\\\\operatorname*{max}_{\\\\theta}J(\\\\theta)=\\\\operatorname*{max}_{\\\\theta}\\\\mathbb{E}_{\\\\pi_{\\\\theta}}R_{t}=\\\\operatorname*{max}_{\\\\theta}\\\\mathbb{E}_{\\\\pi_{\\\\theta}}\\\\Big[\\\\sum_{t^{\\\\prime}=t}^{\\\\infty}\\\\gamma^{t^{\\\\prime}-t}r(x_{t^{\\\\prime}},a_{t^{\\\\prime}})\\\\Big].}\\\\end{array}$ ing (RL) aims to find a policy $\\\\pi_{\\\\theta}:\\\\mathcal{X}\\\\times\\\\mathcal{A}\\\\rightarrow\\\\mathbb{R}^{+}$ X \u00d7 A \u2192 h P that can maximize the expected return .$J$ . where  \n\n$\\\\begin{array}{r}{\\\\operatorname*{max}_{a_{t}}\\\\mathbb{E}\\\\Big[r(x_{t},a_{t}|x_{t}\\\\,=\\\\,x)+\\\\gamma V^{*}(x_{t+1})\\\\Big]}\\\\end{array}$ value function obeys an important identity known as the Bellman optimality equation Bellman Equation. hWe define the optimal value function i . The idea behind this equation is that if we know the $V^{*}(x)=\\\\operatorname*{max}\\\\mathbb{E}[R_{t}|x_{t}=x]$ |. The optimal $V^{*}(x)\\\\;=\\\\;$ $r(x_{t},a_{t})$ for any $a_{t}$ and next step value function $V^{*}(x_{t+1})$ for any $s_{t+1}$ , we can recursively select the action $a_{t}$ which m $r(x_{t},a_{t}|x_{t}=x)+\\\\gamma V^{*}(x_{t+1})$ milarly, we can denote the optimal action-value function $Q^{*}(x,a)\\\\;=\\\\;\\\\operatorname*{max}\\\\mathbb{E}[R_{t}|x_{t}\\\\;=\\\\;x,a_{t}\\\\;=\\\\;a]$ |], and it obeys a similar Bellman optimility equation $\\\\begin{array}{r}{Q^{*}(x,a)=\\\\operatorname*{max}_{a_{t+1}}\\\\mathbb{E}\\\\Big[r(x_{t},a_{t}|x_{t}=x,a_{t}=a)+\\\\gamma Q^{*}(x_{t+1},a_{t+1})\\\\Big].}\\\\end{array}$ .  \n\nModel-based RL. Model-based RL method distinguishes itself from model-free counterparts by using the data to learn a transition model. Following Janner et al. (2019a) and Clavera et al. (2019), we use parametric neural networks to approximate the transition function, reward function, policy function and $\\\\mathrm{^Q}$ -value function with the following objective function to be optimized  $J_{f}(\\\\psi)\\\\,=\\\\,\\\\mathbb{E}\\\\big[\\\\log f(x_{t+1}|x_{t},a_{t})\\\\big]$ '', $J_{r}(\\\\omega)\\\\,=\\\\,\\\\mathbb{E}\\\\big[\\\\log r(r_{t}|x_{t},a_{t})\\\\big]$ '', $\\\\begin{array}{r}{\\\\bar{J_{\\\\pi}}(\\\\theta)\\\\,=\\\\,\\\\mathbb{E}\\\\bigl[\\\\sum_{t=0}^{H-1}\\\\gamma^{t}r(\\\\bar{x}_{t},a_{t})\\\\,+\\\\,}\\\\end{array}$ ' P$\\\\gamma^{H}Q(x_{H},a_{H})]$ 'and $J_{Q}\\\\,=\\\\,\\\\mathbb{E}\\\\bigl[\\\\|Q(x_{t},a_{t})-(r+\\\\tilde{Q}(x_{t+1},a_{t+1}))\\\\|_{2}\\\\bigr]$ '\u2225\u2212\u2225', respectively. In ${\\\\cal J}_{\\\\pi}(\\\\theta)$ , we truncate the trajectory in horizon Hto avoid long time model rollout.  \n\nNotations. For one-dimensional state and action case, we denote the partial differentiation of function by using its output with subscripts, e.g. ,$\\\\begin{array}{r}{r_{x}\\\\ \\\\triangleq\\\\ \\\\frac{\\\\partial r(x,a)}{\\\\partial x},\\\\ r_{a}\\\\ \\\\triangleq\\\\ \\\\frac{\\\\bigtriangleup r(x,a)}{\\\\partial a},\\\\ f_{x}\\\\ \\\\triangleq\\\\ \\\\frac{\\\\partial f(x,a)}{\\\\partial x}}\\\\end{array}$ ,$f_{a}\\\\triangleq{\\\\frac{\\\\partial f(x,a)}{\\\\partial a}}$ ,$\\\\begin{array}{r}{Q_{x}\\\\triangleq\\\\frac{\\\\partial Q(x,a)}{\\\\partial x}}\\\\end{array}$ and $\\\\begin{array}{r}{Q_{a}\\\\triangleq\\\\frac{\\\\partial Q(x,a)}{\\\\partial a}}\\\\end{array}$ . See Appendix E for the multi-dimension case.\u300d\n", "372a76b7-8c9e-42aa-b783-cf530fb1852d:\u300cref_ids: 454845581169535994, chunk_ids: 7, Score: 0.3066, Text: # 4 Theoretical Analysis\nAlthough it is difficult to analyze the regret of MCTS-VS directly, we can theoretically analyze the influence of general variable selection by adopting the acquisition function GP-UCB. The considered general variable selection framework is as follows: after selecting a subset of variables at each iteration, the corresponding observation data (i.e., the data points sampled-so-far where only the selected variables are used) is used to build a GP model, and the next data point is sampled by maximizing GP-UCB. We use $\\\\mathbb{M}_{t}$ to denote the sampled variable index subset at iteration $t$ , and let $\\\\left|\\\\mathbb{M}_{t}\\\\right|=d_{t}$ .  \n\nRegret Analysis. Let $x^{*}$ denote an optimal solution. We analyze the cumulative regret $R_{T}\\\\,=$ $\\\\begin{array}{r}{\\\\sum_{t=1}^{\\\\bar{T}}(f(\\\\pmb{x}^{*})-f(\\\\pmb{x}^{t}))}\\\\end{array}$ the selected points by iteration \u2212, i.e., the Tum of the gap between the opti . To derive an upper bound on $R_{T}$ m and the function values of , we pessimistically assume that the worst function value, i.e., $\\\\begin{array}{r}{\\\\operatorname*{min}_{\\\\pmb{x}_{[D]\\\\setminus\\\\mathbb{M}_{t}}}f([\\\\pmb{x}_{\\\\mathbb{M}_{t}},\\\\pmb{x}_{[D]\\\\setminus\\\\mathbb{M}_{t}}])}\\\\end{array}$ , given ${\\\\pmb x}_{\\\\mathbb{M}_{t}}$ is returned in evaluation. As in [ Lipschitz assumption. 21 ,38 ], we assume that $\\\\mathcal{X}\\\\subset[0,r]^{D}$ is convex and compact, and $f$ satisfies the following Assumption 4.1. The function $f$ is a GP sample path. For some $a,b>0$ , given $L>0$ , the partial derivatives of $f$ satisfy that $\\\\forall i\\\\in[D]$ ,$\\\\exists\\\\alpha_{i}\\\\geq0$ ,  \n\n$$\nP\\\\left(\\\\operatorname*{sup}_{x\\\\in\\\\mathcal{X}}\\\\left|\\\\partial f/\\\\partial x_{i}\\\\right|<\\\\alpha_{i}L\\\\right)\\\\geq1-a e^{-\\\\left(L/b\\\\right)^{2}}.\n$$  \n\nBased on Assumption 4.1, we define $\\\\alpha_{i}^{*}$ to be the minimum value of $\\\\alpha_{i}$ such that Eq. (3) holds, which characterizes the importance of the $i$ -th variable $x_{i}$ . The larger $\\\\alpha_{i}^{*}$ , the greater influence of $x_{i}$ on the function $f$ . Let $\\\\alpha_{\\\\mathrm{max}}=\\\\operatorname*{max}_{i\\\\in[D]}\\\\alpha_{i}^{*}$ .  \n\nTheorem 4.2 gives an upper bound on the cumulative regret $R_{T}$ with high probability for general variable selection methods. The proof is inspired by that of GP-UCB without variable selection [ 38 ]$\\\\forall i:\\\\alpha_{i}^{*}\\\\leq1$ and provided in Appendix B.1. If we select all variables each time (i.e., \u2264(4) becomes $R_{T}\\\\leq\\\\sqrt{C_{1}T\\\\beta_{T}^{*}\\\\gamma_{T}}+2$ p, which is consistent with [ $\\\\forall t:\\\\mathbb{M}_{t}=[D])$ and assume 38 ]. Note that \u2200$\\\\forall t:|\\\\mathbb{M}_{t}|\\\\,=\\\\,d_{t}\\\\,=\\\\,D$ ||in this case, which implies that $\\\\beta_{t}$ increases with $t$ , leading to $\\\\beta_{T}^{*}=\\\\beta_{T}$ . We can see that usi variable selection will $R_{T}$ by $\\\\begin{array}{r}{2\\\\sum_{t=1}^{T}\\\\sum_{i\\\\in[D]\\\\backslash\\\\mathbb{M}_{t}}\\\\alpha_{i}^{*}L r}\\\\end{array}$ P,variables unselected, the larger related to the importance (i.e., $R_{T}$ $\\\\alpha_{i}^{*}$ . Meanwhile, the term ) of unselected variables at each iteration. The more important $\\\\sqrt{C_{1}T\\\\beta_{T}^{*}\\\\gamma_{T}}$ pwill decrease as \u2208$\\\\beta_{T}^{*}$ \\\\relies on the number $d_{t}$ of selected variables positively. Ideally, if the unselected variables at each iteration are always unrelated (i.e., $\\\\alpha_{i}^{*}\\\\!=\\\\!0$ ), the regret bound will be better than that of using all variables [38].  \n\n$b\\\\sqrt{\\\\log(4D a/\\\\delta)}$ Theorem 4.2. p$\\\\forall\\\\delta\\\\ \\\\in\\\\ (0,1)$ , where $r$ is the upper bound on each variable, and , let $\\\\beta_{t}\\\\ =\\\\ 2\\\\log(4\\\\pi_{t}/\\\\delta)\\\\,+\\\\,2d_{t}\\\\log(d_{t}t^{2}b r\\\\sqrt{\\\\log(4D a/\\\\delta)})$ {$\\\\{\\\\pi_{t}\\\\}_{t\\\\ge1}$ }\u2265satisfies $\\\\textstyle\\\\sum_{t\\\\geq1}\\\\pi_{t}^{-1}=1$ and $L\\\\;=$ and $\\\\pi_{t}>0$ . Let $\\\\beta_{T}^{*}=\\\\operatorname*{max}_{1\\\\leq i\\\\leq T}\\\\beta_{t}$ . At iteration $T$ , the cumulative regret  \n\n$$\nR_{T}\\\\leq\\\\sqrt{C_{1}T\\\\beta_{T}^{*}\\\\gamma_{T}}+2\\\\alpha_{\\\\operatorname*{max}}+2\\\\sum_{t=1}^{T}\\\\sum_{\\\\substack{i\\\\in[D]\\\\backslash\\\\mathbb{M}_{t}}}\\\\alpha_{i}^{*}L r\n$$  \n\nholds with probability least $1\\\\!-\\\\!\\\\delta$ , where $C_{1}$ is a constant, $\\\\begin{array}{r}{\\\\gamma_{T}\\\\!=\\\\!\\\\operatorname*{max}_{|\\\\mathcal{D}|=\\\\!T}I(\\\\pmb{y}_{\\\\mathcal{D}},\\\\pmb{f}_{\\\\mathcal{D}}),}\\\\end{array}$ $I(\\\\cdot,\\\\cdot)$ is the information gain, and $\\\\scriptstyle y_{\\\\mathcal{D}}$ Dand $f_{\\\\mathcal{D}}$ Dare the noisy and true observations of a set Dof points, respectively.  \n\nBy selecting been proved [21] that the cumulative regret of Dropout satisfies $d$ variables randomly at each iteration and assuming that $r=1$ and $\\\\forall i:\\\\alpha_{i}^{*}\\\\leq1$ \u2264, it has  \n\n$$\nR_{T}\\\\leq\\\\sqrt{C_{1}T\\\\beta_{T}\\\\gamma_{T}}+2+2T L(D-d).\n$$  \n\nIn this case, we have $d_{t}=|\\\\mathbb{M}_{t}|=d$ ,$r=1$ and $\\\\forall i:\\\\alpha_{i}^{*}\\\\leq1$ \u2264. Thus, Eq. (4) becomes  \n\n$$\nR_{T}\\\\leq\\\\sqrt{C_{1}T\\\\beta_{T}^{*}\\\\gamma_{T}}+2+2T L(D-d).\n$$  \n\nNote that $\\\\beta_{T}^{*}=\\\\beta_{T}$ here, as $\\\\beta_{t}$ increases with $t$ given $d_{t}=d$ . This implies that our bound Eq. (4) for general variable selection is a generalization of Eq. (5) for Dropout [ 21 ]. In [ 33 ], a regret bound analysis has also been performed for variable selection, by optimizing over $d$ fixed important variables and using a common parameter $\\\\alpha$ to characterize the importance of all the other $D-d$ variables.  \n\nComputational Complexity Analysis. The computational complexity of one iteration of BO depends on three critical components: fitting a GP surrogate model, maximizing an acquisition function and evaluating a sampled point. If using the squared exponential kernel, the computational complexity of fitting a GP model at iteration $t$ is $\\\\bar{O(t^{3}\\\\!+\\\\!t^{2}d_{t})}$ . Maximizing an acquisition function is related to the optimization algorithm. If we use the Quasi-Newton method to optimize GP-UCB, the computational complexity is $\\\\bar{\\\\mathcal{O}}(m(t^{2}+t d_{t}+d_{t}^{2}))$ [28 ], where $m$ denotes the Quasi-Newton\u2019s running rounds. The cost of evaluating a sampled point is fixed. Thus, by selecting only a subset of variables, instead of all variables, to optimize, the computational complexity can be decreased significantly. The detailed analysis is provided in Appendix B.2.  \n\nInsight. The above regret and computational complexity analyses have shown that variable selection can reduce the computational complexity while increasing the regret. Given the number $d_{t}$ of variables to be selected, a good variable selection method should select as important variables as possible, i.e., variables with as large $\\\\alpha_{i}^{*}$ as possible, which may help to design and evaluate variable selection methods. The experiments in Section 5.1 will show that MCTS-VS can select a good subset of variables while maintaining a small computational complexity.\u300d\n", "372a76b7-8c9e-42aa-b783-cf530fb1852d:\u300cref_ids: 454847097820262972, chunk_ids: 6, Score: 0.2988, Text: # 4.1 User Modeling\nWe firstly encode the state $s_{t}$ , which contains all the conversational information of the prior $t\\\\!-\\\\!1$ turns. The current state includes six components: $s_{t}\\\\,=\\\\,\\\\{\\\\dot{u},\\\\dot{\\\\mathcal{P}}_{u}^{(t)},\\\\mathcal{P}_{r e j}^{(t)},\\\\mathcal{V}_{r e j}^{(t)},\\\\mathcal{P}_{c a n d}^{(t)},\\\\mathcal{V}_{c a n d}^{(t)}\\\\}$ . Previous methods [ 8,13 ,15 ] for MCR only extract the user\u2019s interest from the current state, ignoring the complements of historical interactions to the current user\u2019s preference. To this end, we construct a current graph and a global graph to jointly learn user representations. Moreover, we develop an iterative multi-interest extractor to obtain multiple interests of the user, which will be discussed in subsection 5.1.\n\n# 4.2 Consultation\nOnce the system finishes the user modeling step, it will move to the consultation step, with the purpose to decide whether to ask attribute instances or to recommend items. To make the next action more profitable and recommend successfully with the fewer turns, we employ a reinforcement learning (RL) method based on the extracted multiple interests of the user to learn the policy. The action space includes all candidate items and candidate attribute instances. However, in the real world, the number of items and attribute instances is very large, which severely limits the efficiency of CRS. To improve the efficiency, we sample $K_{v}$ items and $K_{p}$ attribute instances as action space $\\\\mathcal{A}_{t}$ . We develop a novel dueling Q-network [ 34 ] to calculate the Q-value of each action in $\\\\mathcal{A}_{t}$ . If CRS decides to ask a question, our method will select $K_{a}$ attribute instances in ${\\\\mathcal{A}}_{t}$ with the same attribute type to generate attribute type-based multiple choice questions . The user can choose zero (the option \"Others\" as shown in conversation (b) of Figure 1), one, or more attribute instances with the given attribute type. If CRS decides to recommend items, the system will select $K$ items in ${\\\\mathcal{A}}_{t}$ to recommend. We will discuss the details of sampling strategies and policy learning in subsection 5.2.\n\n# 4.3 Transition\nWhen the user responds to the action of agent, the transition step will be triggered. This step will transition the current state to the next state $s_{t+1}$ . If the user responds to the question, attribute instance sets that the user accepts and rejects in this turn can be defined as $\\\\mathcal{P}_{c u r\\\\_a c c}^{(t)}$ and $\\\\mathcal{P}_{c u r\\\\_r e j}^{(t)}$ respectively. Some components are updated by $\\\\mathcal{P}_{c a n d}^{(t+1)}=\\\\mathcal{P}_{c a n d}^{(t)}-\\\\overset{-}{\\\\mathcal{P}}_{c u r\\\\_r e j}^{(t)}-\\\\mathcal{P}_{c u r\\\\_a c c}^{(t)},\\\\mathcal{P}_{r e j}^{(t+1)}=\\\\mathcal{P}_{r e j}^{(t)}\\\\cup\\\\mathcal{P}_{c u r\\\\_r e j}^{(t)}$ and $\\\\mathcal{P}_{u}^{(t+1)}\\\\;=\\\\;\\\\mathcal{P}_{u}^{(t)}\\\\;\\\\cup\\\\;\\\\mathcal{P}_{c u r\\\\_a c c}^{(t)}$ . When the user is recommended items, if the set $\\\\mathcal{V}_{r e c}^{(t)}$ of recommended items are all rejected, the next state can be updated by $\\\\mathcal{V}_{r e j}^{(t+1)}=\\\\mathcal{V}_{r e j}^{(t)}\\\\cup\\\\mathcal{V}_{r e c}^{(t)}$ . Otherwise, this conversation session ends. Finally, we need to update the candidate item set $\\\\mathcal{V}_{c a n d}^{(t+1)}$ based on the user\u2019s feedback. Previous works [ 8,15 ]update candidate items based the intersection set strategy, that is, only the items satisfying all the accepted attribute instances in $\\\\mathcal{P}_{u}^{(t+1)}$ remain, which obviously deviates from the scenario. In fact, the user might not prefer the combination of all attribute instances, but rather part of them. To this end, we propose the attribute instance-based union set strategy to update $\\\\bar{\\\\mathcal{V}}_{c a n d}^{(t+\\\\bar{1})}$ as follows:  \n\n  \nFigure 2: The overview of Multi-Interest Policy Learning (MIPL).  \n\n$$\n\\\\begin{array}{r}{\\\\mathcal{V}_{c a n d}^{(t+1)}=\\\\{v\\\\vert v\\\\in\\\\mathcal{V}_{p_{0}}-\\\\mathcal{V}_{r e j}^{(t+1)}\\\\ \\\\mathrm{~and~}\\\\,\\\\mathcal{P}_{v}\\\\cap\\\\mathcal{P}_{u}^{(t+1)}\\\\neq\\\\varnothing}\\\\\\\\ {\\\\qquad\\\\qquad\\\\qquad\\\\qquad\\\\qquad\\\\mathrm{~and~}\\\\,\\\\mathcal{P}_{v}\\\\cap\\\\mathcal{P}_{r e j}^{(t+1)}=\\\\varnothing\\\\}}\\\\end{array}\n$$  \n\nwhere $\\\\mathcal{V}_{p_{0}}$ is the item set in which all items are associated to attribute instance $\\\\scriptstyle{\\\\mathcal{P}}0$ which initializes the conversation session. In this way, we can get the next state, which will be updated as $s_{t+1}~=$ $\\\\{\\\\Bar{u_{*}}^{\\\\prime}\\\\mathcal{P}_{u}^{(t+1)},\\\\mathcal{P}_{r e j}^{(\\\\Bar{t}+1)},\\\\mathcal{V}_{r e j}^{(t+1)},\\\\mathcal{P}_{c a n d}^{(t+1)},\\\\mathcal{V}_{c a n d}^{(t+1)}\\\\}$ .\n\n# 4.4 Reward\nIn this work, five kinds of rewards are defined following [ 8,15 ],   \nnamely, (1) $r_{r e c\\\\_s u c}$ , a strongly positive reward when the recommen  \ndation succeeds, (2) $r_{r e c\\\\_f a i l}$ , a strongly negative reward when the $r_{a s k\\\\_s u c}$ , a slightly positive reward when   \nthe user accepts an asked attribute instance, $(4)\\\\,r_{a s k\\\\_f a i l}$ , a negative $r_{q u i t}$ , a   \nstrongly negative reward if the session reaches the maximum number   \nof turns. In addition, since our method asks multiple choice ques  \ntions, we design the reward from the user\u2019s feedback on a question   \nin the form of sum as $\\\\begin{array}{r}{r_{t}=\\\\sum_{\\\\mathcal{P}_{c u r_{-}a c c}^{(t)}}r_{a s k_{-}s u c}+\\\\sum_{\\\\mathcal{P}_{c u r_{-}r e j}^{(t)}}r_{a s k_{-}r e j}.}\\\\end{array}$\n\n# 5MULTI-INTEREST POLICY LEARNING\nIn this section, we detail the design of Multi-Interest Policy Learning (MIPL) module. As shown in Figure 2, to obtain more comprehensive user representations, we establish a current graph to capture user current preferences, and a global graph to capture long-term preferences. Based on the learned node representations of the two graphs, we propose an iterative multi-interest extractor to model user\u2019s preferences for different combinations of attribute instances. Moreover, we design a new dueling Q-network [ 34 ] to decide the next action based on the extracted multiple interests.\u300d\n"]}}, "__type__": "1"}}, "template_store/metadata": {"742ac35b-1bb0-4778-a65c-50a78973e3d3": {"template_hash": ""}}}