角色,内容,分镜
372a76b7-8c9e-42aa-b783-cf530fb1852d,偏好策略模型（PRM）,0>2
372a76b7-8c9e-42aa-b783-cf530fb1852d,"### 问题提出

在MCTS与PRM结合的框架中，如何有效利用MCTS的搜索能力来优化PRM的偏好建模过程，特别是在推荐系统和个性化服务中，如何确保模型能够准确捕捉用户的动态偏好并做出实时决策？

### 问题背景

PRM通过学习用户的偏好来优化决策过程，常用于推荐系统和个性化服务。然而，用户的偏好往往是动态变化的，传统的PRM模型可能难以实时捕捉这些变化。MCTS作为一种启发式搜索算法，具有强大的决策优化能力，但其在大规模问题中的计算复杂度较高。因此，如何将MCTS与PRM有效结合，既利用MCTS的搜索能力优化PRM的决策过程，又确保模型能够实时捕捉用户的动态偏好，是一个值得探讨的问题。

### 问题细化

1. **动态偏好捕捉**：在推荐系统和个性化服务中，用户的偏好可能会随时间、情境等因素发生变化。如何设计MCTS与PRM结合的框架，使其能够实时捕捉这些动态变化？
   
2. **计算效率优化**：MCTS在大规模问题中的计算复杂度较高，如何优化MCTS的搜索策略，使其在PRM的偏好建模过程中保持高效？

3. **模型泛化能力**：如何确保MCTS与PRM结合的模型在不同应用场景下具有良好的泛化能力，特别是在处理多模态数据时？

4. **实时决策支持**：在推荐系统和个性化服务中，如何确保MCTS与PRM结合的模型能够快速做出决策，满足实时性要求？

### 问题意义

通过解决上述问题，可以进一步提升MCTS与PRM结合框架在推荐系统和个性化服务中的应用效果，使其能够更准确地捕捉用户的动态偏好，并做出实时、高效的决策。这不仅有助于提升用户体验，还能推动相关领域的技术进步。",0>2
372a76b7-8c9e-42aa-b783-cf530fb1852d,"ref_ids: 454848282814999732, chunk_ids: 2, Score: 0.3359, Text: # 2 RELATED WORK
The full version of the related work is in Appendix A, we briefly introduce several highly related works here. In general, model-based RL for solving decision-making problems can be divided into three perspectives: model learning, policy learning, and decision-making. Moreover, optimal control theory also concerns the decision-making problem and is deeply related to model-based RL.  

Model learning: How to learn a good model to support decision-making is crucial in model-based RL. There are two main aspects of the work: the model structure designing (Chua et al., 2018; Zhang  

et al., 2021; 2020; Hafner et al., 2021; Chen et al., 2022) and the loss designing (D’Oro et al., 2020;   
Farahmand et al., 2017; Li et al., 2021).  

Policy learning: Two methods are always used to learn the policy by using the learned model. One is to serve the learned model as a black-box simulator to generate the data (Janner et al., 2019b; Yu et al., 2020; Lee et al., 2020). Another way is to use the learned model to calculate the policy gradient (Heess et al., 2015b; Clavera et al., 2019; Amos et al., 2021).  

Decision-making: When making the decision, we need to generate the actions that can achieve our goal. Many of the model-based RL methods make the decision by using the learned policy solely (Hafner et al., 2021). Similar to our paper, some works also try to make decisions by using the learned model, but the majority only focus on the discrete action space. The well-known MCTS method achieves a lot of success. For example, the well-known Alpha Zero (Silver et al., 2017), MuZero (Schrittwieser et al., 2020). There are only a few works that study the continuous action space, such as the Continuous UCT (Cou¨etoux et al., 2011), the sampled MuZero (Hubert et al., 2021), the TreePI (Springenberg et al., 2020), and the TD-MPC (Hansen et al., 2022a).  

Optimal control theory: Beyond deep RL, optimal control also considers the decision-making problem but rather relies on the known and continuous transition model. In modern optimal control, Model Predictive Control (MPC) (Camacho & Alba, 2013) framework is always adopted when the environment is highly non-linear. In MPC, the action is planned during the execution by using the model, and such a procedure is called trajectory optimization. Plenty of previous works (Byravan et al., 2021; Chua et al., 2018; Pinneri et al., 2021; Nagabandi et al., 2020) use MPC framework to solve the continuous control tasks, but most of them are based on zero-order or sample-based method to do the planning. The most relevant works are DDP (Murray & Yakowitz, 1984), iLQR (Li & Todorov, 2004), and iLQG (Todorov & Li, 2005; Tassa et al., 2012). We discuss the detailed differences between our method and these methods in Appendix A.  

Since our planning algorithm relies on the learned model and learned policy, we build our algorithm based on these works on model learning and policy learning . Our POMP algorithm tries to solve a more challenging task compared to the related work on decision-making : efficiently optimize the trajectory in continuous action space when the environment model is unknown. Different from our works, the MPC with DDP as trajectory optimizer from optimal control theory requires the known environment model, and also requires the hessian matrix for online optimization from scratch.

# 3 PRELIMINARIES
Reinforcement Le onsider discrete-time Marko Decision Process (M $\\mathcal{M}$ the tuple ($(\\mathcal{X},\\mathcal{A},f,r,\\gamma)$ XA $\\mathcal{X}$ state space, A is the action space, $f\\,:\\,x_{t+1}\\,=$   
$f(x_{t},a_{t})$ is the transition model, $r:\\mathcal{X}\\times\\mathcal{A}\\to\\mathbb{R}$ X × A → is the reward function, $\\gamma$ is the discount factor. $t$ $\\begin{array}{r}{R_{t}=\\sum_{t^{\\prime}=t}^{\\infty}\\gamma^{t^{\\prime}-t}r_{t^{\\prime}}}\\end{array}$ , and Reinforcement Learn  
$\\begin{array}{r}{\\operatorname*{max}_{\\theta}J(\\theta)=\\operatorname*{max}_{\\theta}\\mathbb{E}_{\\pi_{\\theta}}R_{t}=\\operatorname*{max}_{\\theta}\\mathbb{E}_{\\pi_{\\theta}}\\Big[\\sum_{t^{\\prime}=t}^{\\infty}\\gamma^{t^{\\prime}-t}r(x_{t^{\\prime}},a_{t^{\\prime}})\\Big].}\\end{array}$ ing (RL) aims to find a policy $\\pi_{\\theta}:\\mathcal{X}\\times\\mathcal{A}\\rightarrow\\mathbb{R}^{+}$ X × A → h P that can maximize the expected return .$J$ . where  

$\\begin{array}{r}{\\operatorname*{max}_{a_{t}}\\mathbb{E}\\Big[r(x_{t},a_{t}|x_{t}\\,=\\,x)+\\gamma V^{*}(x_{t+1})\\Big]}\\end{array}$ value function obeys an important identity known as the Bellman optimality equation Bellman Equation. hWe define the optimal value function i . The idea behind this equation is that if we know the $V^{*}(x)=\\operatorname*{max}\\mathbb{E}[R_{t}|x_{t}=x]$ |. The optimal $V^{*}(x)\\;=\\;$ $r(x_{t},a_{t})$ for any $a_{t}$ and next step value function $V^{*}(x_{t+1})$ for any $s_{t+1}$ , we can recursively select the action $a_{t}$ which m $r(x_{t},a_{t}|x_{t}=x)+\\gamma V^{*}(x_{t+1})$ milarly, we can denote the optimal action-value function $Q^{*}(x,a)\\;=\\;\\operatorname*{max}\\mathbb{E}[R_{t}|x_{t}\\;=\\;x,a_{t}\\;=\\;a]$ |], and it obeys a similar Bellman optimility equation $\\begin{array}{r}{Q^{*}(x,a)=\\operatorname*{max}_{a_{t+1}}\\mathbb{E}\\Big[r(x_{t},a_{t}|x_{t}=x,a_{t}=a)+\\gamma Q^{*}(x_{t+1},a_{t+1})\\Big].}\\end{array}$ .  

Model-based RL. Model-based RL method distinguishes itself from model-free counterparts by using the data to learn a transition model. Following Janner et al. (2019a) and Clavera et al. (2019), we use parametric neural networks to approximate the transition function, reward function, policy function and $\\mathrm{^Q}$ -value function with the following objective function to be optimized  $J_{f}(\\psi)\\,=\\,\\mathbb{E}\\big[\\log f(x_{t+1}|x_{t},a_{t})\\big]$ '', $J_{r}(\\omega)\\,=\\,\\mathbb{E}\\big[\\log r(r_{t}|x_{t},a_{t})\\big]$ '', $\\begin{array}{r}{\\bar{J_{\\pi}}(\\theta)\\,=\\,\\mathbb{E}\\bigl[\\sum_{t=0}^{H-1}\\gamma^{t}r(\\bar{x}_{t},a_{t})\\,+\\,}\\end{array}$ ' P$\\gamma^{H}Q(x_{H},a_{H})]$ 'and $J_{Q}\\,=\\,\\mathbb{E}\\bigl[\\|Q(x_{t},a_{t})-(r+\\tilde{Q}(x_{t+1},a_{t+1}))\\|_{2}\\bigr]$ '∥−∥', respectively. In ${\\cal J}_{\\pi}(\\theta)$ , we truncate the trajectory in horizon Hto avoid long time model rollout.  

Notations. For one-dimensional state and action case, we denote the partial differentiation of function by using its output with subscripts, e.g. ,$\\begin{array}{r}{r_{x}\\ \\triangleq\\ \\frac{\\partial r(x,a)}{\\partial x},\\ r_{a}\\ \\triangleq\\ \\frac{\\bigtriangleup r(x,a)}{\\partial a},\\ f_{x}\\ \\triangleq\\ \\frac{\\partial f(x,a)}{\\partial x}}\\end{array}$ ,$f_{a}\\triangleq{\\frac{\\partial f(x,a)}{\\partial a}}$ ,$\\begin{array}{r}{Q_{x}\\triangleq\\frac{\\partial Q(x,a)}{\\partial x}}\\end{array}$ and $\\begin{array}{r}{Q_{a}\\triangleq\\frac{\\partial Q(x,a)}{\\partial a}}\\end{array}$ . See Appendix E for the multi-dimension case.",0>2
372a76b7-8c9e-42aa-b783-cf530fb1852d,"ref_ids: 454845581169535994, chunk_ids: 7, Score: 0.3066, Text: # 4 Theoretical Analysis
Although it is difficult to analyze the regret of MCTS-VS directly, we can theoretically analyze the influence of general variable selection by adopting the acquisition function GP-UCB. The considered general variable selection framework is as follows: after selecting a subset of variables at each iteration, the corresponding observation data (i.e., the data points sampled-so-far where only the selected variables are used) is used to build a GP model, and the next data point is sampled by maximizing GP-UCB. We use $\\mathbb{M}_{t}$ to denote the sampled variable index subset at iteration $t$ , and let $\\left|\\mathbb{M}_{t}\\right|=d_{t}$ .  

Regret Analysis. Let $x^{*}$ denote an optimal solution. We analyze the cumulative regret $R_{T}\\,=$ $\\begin{array}{r}{\\sum_{t=1}^{\\bar{T}}(f(\\pmb{x}^{*})-f(\\pmb{x}^{t}))}\\end{array}$ the selected points by iteration −, i.e., the Tum of the gap between the opti . To derive an upper bound on $R_{T}$ m and the function values of , we pessimistically assume that the worst function value, i.e., $\\begin{array}{r}{\\operatorname*{min}_{\\pmb{x}_{[D]\\setminus\\mathbb{M}_{t}}}f([\\pmb{x}_{\\mathbb{M}_{t}},\\pmb{x}_{[D]\\setminus\\mathbb{M}_{t}}])}\\end{array}$ , given ${\\pmb x}_{\\mathbb{M}_{t}}$ is returned in evaluation. As in [ Lipschitz assumption. 21 ,38 ], we assume that $\\mathcal{X}\\subset[0,r]^{D}$ is convex and compact, and $f$ satisfies the following Assumption 4.1. The function $f$ is a GP sample path. For some $a,b>0$ , given $L>0$ , the partial derivatives of $f$ satisfy that $\\forall i\\in[D]$ ,$\\exists\\alpha_{i}\\geq0$ ,  

$$
P\\left(\\operatorname*{sup}_{x\\in\\mathcal{X}}\\left|\\partial f/\\partial x_{i}\\right|<\\alpha_{i}L\\right)\\geq1-a e^{-\\left(L/b\\right)^{2}}.
$$  

Based on Assumption 4.1, we define $\\alpha_{i}^{*}$ to be the minimum value of $\\alpha_{i}$ such that Eq. (3) holds, which characterizes the importance of the $i$ -th variable $x_{i}$ . The larger $\\alpha_{i}^{*}$ , the greater influence of $x_{i}$ on the function $f$ . Let $\\alpha_{\\mathrm{max}}=\\operatorname*{max}_{i\\in[D]}\\alpha_{i}^{*}$ .  

Theorem 4.2 gives an upper bound on the cumulative regret $R_{T}$ with high probability for general variable selection methods. The proof is inspired by that of GP-UCB without variable selection [ 38 ]$\\forall i:\\alpha_{i}^{*}\\leq1$ and provided in Appendix B.1. If we select all variables each time (i.e., ≤(4) becomes $R_{T}\\leq\\sqrt{C_{1}T\\beta_{T}^{*}\\gamma_{T}}+2$ p, which is consistent with [ $\\forall t:\\mathbb{M}_{t}=[D])$ and assume 38 ]. Note that ∀$\\forall t:|\\mathbb{M}_{t}|\\,=\\,d_{t}\\,=\\,D$ ||in this case, which implies that $\\beta_{t}$ increases with $t$ , leading to $\\beta_{T}^{*}=\\beta_{T}$ . We can see that usi variable selection will $R_{T}$ by $\\begin{array}{r}{2\\sum_{t=1}^{T}\\sum_{i\\in[D]\\backslash\\mathbb{M}_{t}}\\alpha_{i}^{*}L r}\\end{array}$ P,variables unselected, the larger related to the importance (i.e., $R_{T}$ $\\alpha_{i}^{*}$ . Meanwhile, the term ) of unselected variables at each iteration. The more important $\\sqrt{C_{1}T\\beta_{T}^{*}\\gamma_{T}}$ pwill decrease as ∈$\\beta_{T}^{*}$ \\relies on the number $d_{t}$ of selected variables positively. Ideally, if the unselected variables at each iteration are always unrelated (i.e., $\\alpha_{i}^{*}\\!=\\!0$ ), the regret bound will be better than that of using all variables [38].  

$b\\sqrt{\\log(4D a/\\delta)}$ Theorem 4.2. p$\\forall\\delta\\ \\in\\ (0,1)$ , where $r$ is the upper bound on each variable, and , let $\\beta_{t}\\ =\\ 2\\log(4\\pi_{t}/\\delta)\\,+\\,2d_{t}\\log(d_{t}t^{2}b r\\sqrt{\\log(4D a/\\delta)})$ {$\\{\\pi_{t}\\}_{t\\ge1}$ }≥satisfies $\\textstyle\\sum_{t\\geq1}\\pi_{t}^{-1}=1$ and $L\\;=$ and $\\pi_{t}>0$ . Let $\\beta_{T}^{*}=\\operatorname*{max}_{1\\leq i\\leq T}\\beta_{t}$ . At iteration $T$ , the cumulative regret  

$$
R_{T}\\leq\\sqrt{C_{1}T\\beta_{T}^{*}\\gamma_{T}}+2\\alpha_{\\operatorname*{max}}+2\\sum_{t=1}^{T}\\sum_{\\substack{i\\in[D]\\backslash\\mathbb{M}_{t}}}\\alpha_{i}^{*}L r
$$  

holds with probability least $1\\!-\\!\\delta$ , where $C_{1}$ is a constant, $\\begin{array}{r}{\\gamma_{T}\\!=\\!\\operatorname*{max}_{|\\mathcal{D}|=\\!T}I(\\pmb{y}_{\\mathcal{D}},\\pmb{f}_{\\mathcal{D}}),}\\end{array}$ $I(\\cdot,\\cdot)$ is the information gain, and $\\scriptstyle y_{\\mathcal{D}}$ Dand $f_{\\mathcal{D}}$ Dare the noisy and true observations of a set Dof points, respectively.  

By selecting been proved [21] that the cumulative regret of Dropout satisfies $d$ variables randomly at each iteration and assuming that $r=1$ and $\\forall i:\\alpha_{i}^{*}\\leq1$ ≤, it has  

$$
R_{T}\\leq\\sqrt{C_{1}T\\beta_{T}\\gamma_{T}}+2+2T L(D-d).
$$  

In this case, we have $d_{t}=|\\mathbb{M}_{t}|=d$ ,$r=1$ and $\\forall i:\\alpha_{i}^{*}\\leq1$ ≤. Thus, Eq. (4) becomes  

$$
R_{T}\\leq\\sqrt{C_{1}T\\beta_{T}^{*}\\gamma_{T}}+2+2T L(D-d).
$$  

Note that $\\beta_{T}^{*}=\\beta_{T}$ here, as $\\beta_{t}$ increases with $t$ given $d_{t}=d$ . This implies that our bound Eq. (4) for general variable selection is a generalization of Eq. (5) for Dropout [ 21 ]. In [ 33 ], a regret bound analysis has also been performed for variable selection, by optimizing over $d$ fixed important variables and using a common parameter $\\alpha$ to characterize the importance of all the other $D-d$ variables.  

Computational Complexity Analysis. The computational complexity of one iteration of BO depends on three critical components: fitting a GP surrogate model, maximizing an acquisition function and evaluating a sampled point. If using the squared exponential kernel, the computational complexity of fitting a GP model at iteration $t$ is $\\bar{O(t^{3}\\!+\\!t^{2}d_{t})}$ . Maximizing an acquisition function is related to the optimization algorithm. If we use the Quasi-Newton method to optimize GP-UCB, the computational complexity is $\\bar{\\mathcal{O}}(m(t^{2}+t d_{t}+d_{t}^{2}))$ [28 ], where $m$ denotes the Quasi-Newton’s running rounds. The cost of evaluating a sampled point is fixed. Thus, by selecting only a subset of variables, instead of all variables, to optimize, the computational complexity can be decreased significantly. The detailed analysis is provided in Appendix B.2.  

Insight. The above regret and computational complexity analyses have shown that variable selection can reduce the computational complexity while increasing the regret. Given the number $d_{t}$ of variables to be selected, a good variable selection method should select as important variables as possible, i.e., variables with as large $\\alpha_{i}^{*}$ as possible, which may help to design and evaluate variable selection methods. The experiments in Section 5.1 will show that MCTS-VS can select a good subset of variables while maintaining a small computational complexity.",0>2
372a76b7-8c9e-42aa-b783-cf530fb1852d,"ref_ids: 454847097820262972, chunk_ids: 6, Score: 0.2988, Text: # 4.1 User Modeling
We firstly encode the state $s_{t}$ , which contains all the conversational information of the prior $t\\!-\\!1$ turns. The current state includes six components: $s_{t}\\,=\\,\\{\\dot{u},\\dot{\\mathcal{P}}_{u}^{(t)},\\mathcal{P}_{r e j}^{(t)},\\mathcal{V}_{r e j}^{(t)},\\mathcal{P}_{c a n d}^{(t)},\\mathcal{V}_{c a n d}^{(t)}\\}$ . Previous methods [ 8,13 ,15 ] for MCR only extract the user’s interest from the current state, ignoring the complements of historical interactions to the current user’s preference. To this end, we construct a current graph and a global graph to jointly learn user representations. Moreover, we develop an iterative multi-interest extractor to obtain multiple interests of the user, which will be discussed in subsection 5.1.

# 4.2 Consultation
Once the system finishes the user modeling step, it will move to the consultation step, with the purpose to decide whether to ask attribute instances or to recommend items. To make the next action more profitable and recommend successfully with the fewer turns, we employ a reinforcement learning (RL) method based on the extracted multiple interests of the user to learn the policy. The action space includes all candidate items and candidate attribute instances. However, in the real world, the number of items and attribute instances is very large, which severely limits the efficiency of CRS. To improve the efficiency, we sample $K_{v}$ items and $K_{p}$ attribute instances as action space $\\mathcal{A}_{t}$ . We develop a novel dueling Q-network [ 34 ] to calculate the Q-value of each action in $\\mathcal{A}_{t}$ . If CRS decides to ask a question, our method will select $K_{a}$ attribute instances in ${\\mathcal{A}}_{t}$ with the same attribute type to generate attribute type-based multiple choice questions . The user can choose zero (the option ""Others"" as shown in conversation (b) of Figure 1), one, or more attribute instances with the given attribute type. If CRS decides to recommend items, the system will select $K$ items in ${\\mathcal{A}}_{t}$ to recommend. We will discuss the details of sampling strategies and policy learning in subsection 5.2.

# 4.3 Transition
When the user responds to the action of agent, the transition step will be triggered. This step will transition the current state to the next state $s_{t+1}$ . If the user responds to the question, attribute instance sets that the user accepts and rejects in this turn can be defined as $\\mathcal{P}_{c u r\\_a c c}^{(t)}$ and $\\mathcal{P}_{c u r\\_r e j}^{(t)}$ respectively. Some components are updated by $\\mathcal{P}_{c a n d}^{(t+1)}=\\mathcal{P}_{c a n d}^{(t)}-\\overset{-}{\\mathcal{P}}_{c u r\\_r e j}^{(t)}-\\mathcal{P}_{c u r\\_a c c}^{(t)},\\mathcal{P}_{r e j}^{(t+1)}=\\mathcal{P}_{r e j}^{(t)}\\cup\\mathcal{P}_{c u r\\_r e j}^{(t)}$ and $\\mathcal{P}_{u}^{(t+1)}\\;=\\;\\mathcal{P}_{u}^{(t)}\\;\\cup\\;\\mathcal{P}_{c u r\\_a c c}^{(t)}$ . When the user is recommended items, if the set $\\mathcal{V}_{r e c}^{(t)}$ of recommended items are all rejected, the next state can be updated by $\\mathcal{V}_{r e j}^{(t+1)}=\\mathcal{V}_{r e j}^{(t)}\\cup\\mathcal{V}_{r e c}^{(t)}$ . Otherwise, this conversation session ends. Finally, we need to update the candidate item set $\\mathcal{V}_{c a n d}^{(t+1)}$ based on the user’s feedback. Previous works [ 8,15 ]update candidate items based the intersection set strategy, that is, only the items satisfying all the accepted attribute instances in $\\mathcal{P}_{u}^{(t+1)}$ remain, which obviously deviates from the scenario. In fact, the user might not prefer the combination of all attribute instances, but rather part of them. To this end, we propose the attribute instance-based union set strategy to update $\\bar{\\mathcal{V}}_{c a n d}^{(t+\\bar{1})}$ as follows:  

  
Figure 2: The overview of Multi-Interest Policy Learning (MIPL).  

$$
\\begin{array}{r}{\\mathcal{V}_{c a n d}^{(t+1)}=\\{v\\vert v\\in\\mathcal{V}_{p_{0}}-\\mathcal{V}_{r e j}^{(t+1)}\\ \\mathrm{~and~}\\,\\mathcal{P}_{v}\\cap\\mathcal{P}_{u}^{(t+1)}\\neq\\varnothing}\\\\ {\\qquad\\qquad\\qquad\\qquad\\qquad\\mathrm{~and~}\\,\\mathcal{P}_{v}\\cap\\mathcal{P}_{r e j}^{(t+1)}=\\varnothing\\}}\\end{array}
$$  

where $\\mathcal{V}_{p_{0}}$ is the item set in which all items are associated to attribute instance $\\scriptstyle{\\mathcal{P}}0$ which initializes the conversation session. In this way, we can get the next state, which will be updated as $s_{t+1}~=$ $\\{\\Bar{u_{*}}^{\\prime}\\mathcal{P}_{u}^{(t+1)},\\mathcal{P}_{r e j}^{(\\Bar{t}+1)},\\mathcal{V}_{r e j}^{(t+1)},\\mathcal{P}_{c a n d}^{(t+1)},\\mathcal{V}_{c a n d}^{(t+1)}\\}$ .

# 4.4 Reward
In this work, five kinds of rewards are defined following [ 8,15 ],   
namely, (1) $r_{r e c\\_s u c}$ , a strongly positive reward when the recommen  
dation succeeds, (2) $r_{r e c\\_f a i l}$ , a strongly negative reward when the $r_{a s k\\_s u c}$ , a slightly positive reward when   
the user accepts an asked attribute instance, $(4)\\,r_{a s k\\_f a i l}$ , a negative $r_{q u i t}$ , a   
strongly negative reward if the session reaches the maximum number   
of turns. In addition, since our method asks multiple choice ques  
tions, we design the reward from the user’s feedback on a question   
in the form of sum as $\\begin{array}{r}{r_{t}=\\sum_{\\mathcal{P}_{c u r_{-}a c c}^{(t)}}r_{a s k_{-}s u c}+\\sum_{\\mathcal{P}_{c u r_{-}r e j}^{(t)}}r_{a s k_{-}r e j}.}\\end{array}$

# 5MULTI-INTEREST POLICY LEARNING
In this section, we detail the design of Multi-Interest Policy Learning (MIPL) module. As shown in Figure 2, to obtain more comprehensive user representations, we establish a current graph to capture user current preferences, and a global graph to capture long-term preferences. Based on the learned node representations of the two graphs, we propose an iterative multi-interest extractor to model user’s preferences for different combinations of attribute instances. Moreover, we design a new dueling Q-network [ 34 ] to decide the next action based on the extracted multiple interests.",0>2
