角色,内容,分镜
49abb75e-21b0-4504-8500-80c6e0bd9a07,分析近几年研究领域的技术框架与方法论,0
49abb75e-21b0-4504-8500-80c6e0bd9a07,近几年在将 MCTS 与 PRM 相结合探索偏好策略模型微调的有效技术框架时，具体是如何通过实验对比不同参数设置和改进策略来优化技术框架的？ ,0
49abb75e-21b0-4504-8500-80c6e0bd9a07,"ref_ids: 454984236281633338, chunk_ids: 4, Score: 0.3711, Text: # 5 Experiment
To examine the performance of MCTS-VS, we conduct experiments on different tasks, including synthetic functions, NAS-bench problems and MuJoCo locomotion tasks, to compare MCTS-VS with other black-box optimization methods. For MCTS-VS, we use the same hyper-parameters except $C_{p}$ , which is used for calculating UCB in Eq. (1). For Dropout and embedding-based methods, we set the parameter $d$ to the number of valid dimensions for synthetic functions, and a reasonable value for real-world problems. The hyper-parameters of the same components of different methods are set to the same. We use five identical random seeds (2021–2025) for all problems and methods. More details about the settings can be found in Appendix C. Our code is available at https://github.com/lamda-bbo/MCTS-VS .",0
49abb75e-21b0-4504-8500-80c6e0bd9a07,"ref_ids: 454984236293691964, chunk_ids: 5, Score: 0.2676, Text: # DSensitivity Analysis of Hyper-parameters of MCTS-VS
We provide further studies to examine the influence of the hyper-parameters of MCTS-VS, including the employed optimization algorithm for optimizing the selected variables in each iteration, the “fillin” strategy, the hyper-parameter $k$ used in the best$k$ strategy, the hyper-parameter $C_{p}$ for calculating UCB in Eq. (1), the number $2\\,\\times\\,N_{v}\\,\\times\\,N_{s}$ sampled data in each iteration, the threshold $N_{b a d}$ for re-initializing a tree, and the threshold $N_{s p l i t}$ for splitting a tree node.  

The optimization algorithm is employed by MCTS-VS to optimize the selected variables in each iteration. We compare three different optimization algorithms, i.e., random search (RS), BO and TuRBO. First, we conduct experiments similar to “Effectiveness of Variable Selection” in Section 5.1, to show the effectiveness of MCTS-VS even when equipped with RS. Figure 6 shows that MCTSVS-RS is better than Dropout-RS and RS, revealing the advantage of MCTS-VS.  

  
Figure 6: Effectiveness of MCTS-VS when equipped with RS.  

Next we compare the performance of MCTS-VS equipped with RS, BO and TuRBO, by experiments on the Hartmann functions with increasing ratio of valid variables. Hartmann 6 _500 has 6 valid variables. Hartmann 6 _5 _500 is generated by mixing 5 Hartmann 6 functions as Hartmann 6 $(\\pmb{x}_{1:6})+$ Hartmann 6 $\\backslash(\\pmb{x}_{7:12})+\\cdot\\cdot\\cdot+\\mathrm{Hartmann6}(\\pmb{x}_{25:30})$ , and appending 470 unrelated dimensions, where $\\pmb{x}_{i:j}$ denotes the $i$ -th to j-th variables. Hartmann 6 _10 _500 is generated alike. Thus, Hartmann 6 _5 _500 and Hartmann 6 _10 _500 have 30 and 60 valid variables, respectively. The results in Figure 7 show that as the ratio of valid variables increases, MCTS-VS-TuRBO gradually surpasses MCTS-VS-RS and MCTS-VS-BO, while MCTS-VS-RS becomes worse and worse. This is expected. If the ratio of valid variables is high, MCTS-VS is more likely to select the valid variables, so it is worth to use the expensive optimization algorithm, e.g., TuRBO, to optimize the selected variables. If the ratio is low, unrelated variables are more likely to be selected most of the time, so using a cheap optimization algorithm would be better. These observations also give us some guidance on selecting optimization algorithms in practice.  

“Fill-in” strategy is a basic component of variable selection methods, which influences the quality of the value of unselected variables. We compare the employed best$k$ strategy $(k=20)$ ) with the average best$k$ strategy and the random strategy. The average best$k$ strategy uses the average of the best $k$ data points for the unselected variables, and the random strategy samples the value of an unselected variable from its domain randomly. As shown in Figure 8(a), the random strategy leads to the poor performance of MCTS-VS-BO, which may be because it does not utilize the historical information and leads to over-exploration. The best${\\cdot k}$ strategy utilizes the historical points that have high objective values to fill in the unselected variables, thus behaving much better. The performance of the average strategy is between the best$k$ and random strategies. We recommend using the best$k$ strategy in practice.  

The hyper-parameter $k$ used in the best$k$ strategy controls the degree of exploitation for the unselected variables. As shown in Figure 8(b), a smaller $k$ encourages exploitation, which results in better performance in the early stage, but easily leads to premature convergence. A larger $k$ encourages exploration and behaves worse in the early stage, but may converge to a better value. We recommend using a larger $k$ if allowing enough evaluations.  

  
Figure 7: Sensitivity analysis of the optimization algorithm.  

  
Figure 8: Sensitivity analysis of the “fill-in” strategy and the hyper-parameter $k$ of the best$k$ strategy, using MCTS-VS-BO on Hartmann 6 _300 .  

The hyper-parameter $C_{p}$ for calculating UCB in Eq. (1) balances the exploration and exploitation of MCTS. As shown in Figure 9, a too small $C_{p}$ leads to relatively worse performance, highlighting the importance of exploration. A too large $C_{p}$ may also lead to over-exploration. But overall MCTSVS is not very sensitive to $C_{p}$ . We recommend setting $C_{p}$ between $1\\%$ and $10\\%$ of the optimum (i.e., max $f({\\boldsymbol{x}}))$ ), which is consistent with that for LA-MCTS [40].  

  
Figure 9: Sensitivity analysis of the hyper-parameter $C_{p}$ for calculating UCB in Eq. (1), using MCTS-VS-BO on Levy and Hartmann.  

The number $2\\,\\times\\,N_{v}\\,\\times\\,N_{s}$ of sampled data in ch iteration depends on the batch size $N_{v}$ of variable index subset and the sample batch size $N_{s}$ , and will influence the accuracy of estimating the variable score vector in Eq. (2). If we increase $N_{v}$ and $N_{s}$ , we can calculate the variable score more accurately, but also need more evaluations. Figure 10(a) shows that given the same number of evaluations, MCTS-VS-BO achieves the best performance when $N_{v}=2$ and $N_{s}=3$ . Thus, this setting may be a good choice to balance the accuracy of variable score and the number of evaluations, which is also used throughout the experiments.  

The threshold $N_{b a d}$ for re-initializing a tree controls the tolerance of selecting bad tree nodes (i.e., nodes containing unimportant variables). A smaller $N_{b a d}$ leads to frequent re-initialization, which can adjust quickly but may cause under-exploitation of the tree. A larger $N_{b a d}$ can make full use of the tree, but may optimize too much on unimportant variables. Figure 10(b) shows that MCTS-VS achieves the best performance when $N_{b a d}=5$ . Thus, we recommend to use this setting, to balance the re-initialization and exploitation of the tree.  

The threshold $N_{s p l i t}$ for splitting a node. If the number of variables in a node is larger than $N_{s p l i t}$ ,the node can be further partitioned. That is, the parameter $N_{s p l i t}$ controls the least number of variables in a leaf node and thus affects the number of selected variables, which has a direct influence on the wall clock time. Note that MCTS-VS selects a leaf node and optimizes the variables contained by this node in each iteration. The smaller $N_{s p l i t}$ , the shorter the time. Figure 10(c) shows that $N_{s p l i t}$ has little influence on the performance of MCTS-VS-BO, and thus we recommend to set $N_{s p l i t}=3$ to reduce the wall clock time.  

  
Figure 10: S vity analysis of the number $2\\,\\times\\,N_{v}\\,\\times\\,N_{s}$ pled data in each iteration, the threshold $N_{b a d}$ for re-initializing a tree and the threshold $N_{s p l i t}$ for splitting a node, using MCTS-VS-BO on Hartmann 6 _300 .  

Influence of the hyper-parameters on the runtime of MCTS-VS. We also provide some intuitive explanation about the influence of the hyper-parameters on the runtime. The threshold $N_{s p l i t}$ for splitting a node has a direct impact on the runtime, because it controls the least number of variables to be optimized in a leaf node. That is, the runtime will increase with $N_{s p l i t}$ . Other parameters may affect the depth of the tree and thus the runtime. For the threshold $N_{b a d}$ for re-initializing a tree, if it is set to a small value, MCTS-VS will re-build the tree frequently and the depth of the tree is small. The shallow nodes have more variables, leading to more runtime to optimize. For the hyper-parameter $C_{p}$ for calculating UCB, if it is set to a large value, the exploration is preferred and MCTS-VS will tend to select the right node (regarded as containing unimportant variables). The tree thus will be re-built freq tly, ding to more runtime. For the number $2\\,\\times\\,N_{v}\\,\\times\\,N_{s}$ of sampled data at each iteration, if $N_{v}$ and $N_{s}$ are set to large values, the depth of the tree will be small given the total number of evaluations, and thus lead to more runtime.",0
49abb75e-21b0-4504-8500-80c6e0bd9a07,"ref_ids: 454845604178965768, chunk_ids: 4, Score: 0.2383, Text: # 4.2 The MCTS Framework for Strategy Synthesis
We instantiate MCTS for this optimal strategy search problem. We use UCT as the tree policy in the selection phase and rollout randomly in the rollout phase. Notably, in the backup phase, we apply the max-backup rule [Sabharwal et al. , 2012; Sun et al. , 2023]. This approach updates the action values with the best return observed, rather than the average. It encourages more aggressive exploitation towards the bestperforming strategy observed, aligning with our goal. Therefore, in each MCTS simulation, the agent explores and assesses a single strategy, continually updating and retaining the best strategy seen so far. The process continues until a predetermined number of simulations have been run. At the conclusion of this process, the strategy that has achieved the highest reward $R_{T}$ is selected and presented as the synthesized SMT strategy for the specified instance set $P$ .  

Figure 2 illustrates our basic MCTS framework, using a simplified CFG $G^{\\prime}$ for illustrative purposes. $G^{\\prime}$ is defined as $\\mathrm{~S~}\\dot{\\rightarrow}\\mathrm{~\\tiny~T~S~}|$ symbolize variables for strategy and tactic, respectively. smt and $\\mathrm{~S~}\\rightarrow$ Tsimplify |aig , where S and T  

The primary challenge in synthesizing strategies through MCTS is the extensive time required to evaluate each strategy, which involves calling an SMT solver on all instances in $P$ . This situation leads to a very limited exploration of potential paths, particularly given the immense search space created by the rich strategy language. To address this issue, we first add additional rules restricting valid actions based on domain knowledge. For example, no tactic could be applied sequentially following a solver tactic such as smt . We refer readers to the Appendix for a comprehensive list of such rules. More importantly, we have introduced two heuristic methods, namely layered search and the staged search methods, into conventional MCTS, facilitating a deeper and more effective exploration of the strategy space.

# 4.3 Layered Search
To solve the above-mentioned challenge, we propose a layered search method to optimize the tactic parameters within strategy synthesis. As shown in our CFG $G$ , each tactic can be paired with multiple parameters. In traditional MCTS, the selection of each candidate value for a parameter is represented by one production rule in $G$ , and the agent needs to make sequential production-rule decisions to configure all parameters for a given tactic, leading to exponential growth in the problem search tree.  

  
Figure 3: Comparison of the conventional MCTS and the layersearch in treating tactic parameter tuning  

To address this issue, our layered search method approaches the tuning of each tactic parameter as a separate Multi-Armed Bandit (MAB) problem [Bubeck et al. , 2012]. In an MAB scenario, an agent repeatedly chooses from multiple actions (arms) with unknown rewards. The major difference between MAB and MDP is that, in MAB, an action does not affect subsequent state transitions. For these MABs, we select actions according to UCB1 [Auer et al. , 2002].  

In the layered search, we build an individual MAB for each parameter to be tuned, with each arm representing one candidate value. The MABs are associated with a tree edge (tactic) but themselves are not part of the tree. They are engaged to select parameter values when their associated tree edge is traversed, and they are updated based on the episode reward during the Backup phase. However, such MABs do not expand the MCTS search tree after the parameter configuration, since they are separate components from the main search tree. This is in contrast to conventional MCTS, which also employs MAB principles to select among child nodes to explore, where these nodes are part of the search tree.  

This concept is well explained in Figure 3 with an example based on the simplified grammar $G^{\\prime}$ . Here, the tactic simplify has two Boolean parameters, namely som and flat to be tuned. As shown in Figure 3(a), the search tree is expanded fourfold to accommodate all possible combinations of these two parameters in the conventional MCTS framework. In contrast, in the layered search framework, MABs for the two parameters are isolated from the search tree, creating no additional branches in the tree.  

The rationale behind the layer search is twofold. Firstly, tactics such as simplify may have dozens of parameters, and it is common for a tactic to be used multiple times within a strategy. Thus, navigating a search space that is fully expanded by all possible parameter combinations becomes impractical, especially given the time-intensive nature of strategy evaluation. Secondly, we argue that parameter tuning, although important, serves more as an auxiliary task in comparison to the tasks of tactic selection and sequencing. By employing the layered search method, we maintain the primary focus on more influential tasks. At the same time, the isolated MABs efficiently optimize the parameters, thus effectively managing these secondary tasks without overwhelming the main search process.",0
