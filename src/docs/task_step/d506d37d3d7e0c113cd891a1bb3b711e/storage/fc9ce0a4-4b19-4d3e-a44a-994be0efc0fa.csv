角色,内容,分镜
fc9ce0a4-4b19-4d3e-a44a-994be0efc0fa,评估学术界的技术进步与局限性,2
fc9ce0a4-4b19-4d3e-a44a-994be0efc0fa,学术界在模型性能提升和新架构、训练方法涌现方面取得了技术进步，但存在模型偏差、数据依赖、泛化能力不足以及模型解释性差等局限性，那么如何在利用新架构和训练方法提升模型性能的同时，有效解决模型偏差和数据依赖问题，提高模型的泛化能力和解释性？ ,2
fc9ce0a4-4b19-4d3e-a44a-994be0efc0fa,"ref_ids: 454847062641060886, chunk_ids: 6, Score: 0.3027, Text: # CFurther Discussions
We summarize some empirical observations as follows.  

1) The CLIP with four training tricks yields about $4\\%$ improvement at Rank-1 in Table 1 of the main paper. It can inspire future works in which the model performance could be boosted by applying these training tricks.  

2) Data augmentation and loss function are common technologies used in various methods. The investigation of more than 20 data augmentations and about 10 loss functions on performance in Tables 2-5 of the main paper provides valuable guidance on future works. Researchers can select proper and effective augmentations and losses into the model for improving performance.   
3) We explore the internal properties and functionalities of the model for the first time. These results can light future works on model compression, so as to develop a more lightweight and effective TBPS method.   
4) There are very little research on few-shot TBPS, while this paper makes a preliminary study on CLIP-based fewshot TBPS, providing valuable observation for future research direction.",2
fc9ce0a4-4b19-4d3e-a44a-994be0efc0fa,"ref_ids: 454846731731167836, chunk_ids: 9, Score: 0.2217, Text: # 4.3. Backward-Congruent Re-ranking
Unlike model ensemble, knowledge distillation attempts to explicitly align the behaviors of the new model to the old model during training . Alternatively, we propose BackwardCongruent Re-ranking (BCR), which does not impose any constraint to the training of the new model and only takes effect during inference .  

Re-ranking is a popular approach in structured prediction to combine the strengths of two different models ( Collins & Koo ,2005 ;Socher et al. ,2013 ;Le & Zuidema ,2014 ;Do & Rehbein ,2020 ). It suggests to use one model as the candidate generator for creating a candidate pool and use the other model as the re-ranker for picking the best candidate. While previous work has been focused on developing powerful re-rankers for overall performance improvement, the purpose of BCR is to reduce model update regression. BCR treats the new model as the candidate generator and the old model as the re-ranker. Our motivations are two-fold. First, structured prediction models can produce a number of possible predictions. Second, different predictions may achieve similar error rates but differ by the mistakes they made. Among them, the most likely one according to the old model should have the least prediction flips. These make re-ranking particularly useful for structured prediction.  

Formally, we have the old model and the new model parameterized by $\\phi_{\\mathrm{new}}$ and $\\phi_{\\mathrm{old}}$ respectively. For a given input $x$ ,the new model first generates a set of candidates $\\mathrm{GEN}_{\\phi_{\\mathrm{new}}}(x)$ .Then we choose the prediction $y^{\\ast}$ with the highest score computed by the old model.  

$$
y^{*}=\\operatorname*{arg\\,max}_{y\\in\\mathrm{GEN}_{\\phi_{\\mathrm{new}}}(x)}p_{\\phi_{\\mathrm{old}}}(y|x),
$$  

where $p_{\\phi_{\\mathrm{old}}}(y|x)$ is the old odel’s generation probability (score) of $y$ given the input xand GEN can be implemented by various decoding methods :  

•Maximization-based Search The de facto decoding objective, particular for structured prediction tasks is, maximization-based search. This strategy is based on the assumption that the model assigns higher scores to better output. When finding the exact best $\\cdot k$ output is intractable, the common practice is to use beam search for approximation instead. Hence, we use the $k$ -best MST algorithm ( Zmigrod et al. ,2020 ;2021 ) for the biaffine parser and use beam search for other parsers.  

•Top$k$ Sampling Truncated stochastic sampling such as top-k sampling ( Fan et al. ,2018 ;Radford et al. ,2019 ) is a popular alternative for generating multiple outputs in open-ended text generation tasks. At each decoding step, the top $k$ possible local predictions are selected and the sampling is based on their relative probabilities.  

•Top $\\cdot p$ Sampling Similar to top$k$ sampling, top$\\boldsymbol{p}$ sampling ( Holtzman et al. ,2019 ) is another sampling-based decoding method. The only difference is that top$\\boldsymbol{p}$ sampling samples from the smallest set of top local predictions whose cumulative probability mass exceeds $p$ .  

•Dropout$\\boldsymbol{p}$ Sampling We explore a special sampling method, dropout$\\cdot p$ sampling, that has attracted little attention in the literature. Dropout ( Srivastava et al. ,2014 ) is a regularization technique used in almost all modern neural network-based models. The key idea is to randomly drop some neurons from the neural network during training. Normally, dropout is turned off during inference. However, in dropout$\\cdot p$ sampling, we keep using dropout with dropout rate $p$ . Compared to other methods, dropout$\\boldsymbol{p}$ sampling is unique in that (1) it changes the scoring function instead of the decoding objective; (2) unlike traditional truncated sampling that conducts local sampling at each decoding step, it can be regarded as a global sampling. Dropout$\\boldsymbol{p}$ sampling also has a broader applicable scope than top$k$ and top$p$ sampling as the latter two are designed for sequence generation models.  

Discussion Following previous work ( Shen et al. ,2004 ;Yan et al. ,2021 ;Xie et al. ,2021 ), our discussion has been focused on handling one model update. However, BCR can be extended to handle multiple turns of model updates. To do so, one can keep the most recent $k$ models and use a weighted combination of their scores as the re-ranking metric. In practice, $k$ can be set to trade-off between performance and runtime cost.  

One downside of BCR is that we must maintain and deploy both the old and new models. Because the re-ranking step has less time complexity compared to the decoding algorithms and the computation is fully parallelizable, this does not create much additional inference latency. However, the increase in memory footprint does entail an increase in inference hosting cost. One remedy could be to use knowldge distillation to distill the old model(s) into a smaller one, which we leave for future work.  

Measuring and Reducing Model Update Regression in Structured Prediction for NLP   


<html><body><table><tr><td rowspan=""3""></td><td colspan=""4"">deepbiafdeepbiaf</td><td colspan=""3"">stackptrstackptr</td><td colspan=""3"">deepbiaf=stackptr</td></tr><tr><td colspan=""2"">UCM</td><td>UAS</td><td></td><td>UCM</td><td>UAS</td><td></td><td>UCM</td><td>UAS</td><td></td></tr><tr><td>NFR</td><td>NFIA ACC NFR</td><td></td><td>NFIACCNFR</td><td>NFIACC NFR</td><td>NFIACC</td><td>NFR</td><td></td><td>NFI ACC NFR</td><td>NFI ACC</td></tr><tr><td>PIO</td><td></td><td>-63.88</td><td></td><td>-91.76</td><td>-65.83</td><td>-91.81</td><td></td><td>-63.88</td><td></td><td>-91.76</td></tr><tr><td>Untreated</td><td></td><td>3.6910.2563.97</td><td>1.66 19.85 91.64</td><td></td><td>3.43 10.1066.03</td><td>1.6720.20 91.73</td><td></td><td>3.73 10.98 66.03</td><td></td><td>2.1025.37 91.73</td></tr><tr><td>Distillation</td><td>3.82</td><td>10.62 264.00</td><td>1.62</td><td>19.45 91.67</td><td>3.57 10.40 65.66</td><td>1.7020.4991.70</td><td>3.62</td><td>10.68 66.11</td><td>2.03 24.71</td><td>91.78</td></tr><tr><td>→Ensemble</td><td>2.51</td><td>7.14 64.81</td><td>1.02 12.9792.10</td><td>2.21</td><td>6.74 67.21</td><td>1.1114.3092.21</td><td>2.83</td><td>8.62 67.21</td><td>1.62 20.75</td><td>92.21</td></tr><tr><td>BCR</td><td>1.12</td><td>3.1464.36</td><td>0.84 10.21 91.78</td><td></td><td>1.05 3.12 66.45</td><td>0.84 10.3091.88</td><td>1.20</td><td>3.60 66.76</td><td>1.11 13.87</td><td>92.01</td></tr></table></body></html>",2
fc9ce0a4-4b19-4d3e-a44a-994be0efc0fa,"ref_ids: 454984230919739446, chunk_ids: 8, Score: 0.1973, Text: # Conclusion
In this paper, we tackle two challenges in incremental novel class discovery: how to leverage novel unlabeled data sets for effective training guidance and how to prevent catastrophic forgetting of previous knowledge. We propose Triple Comparison and Probability Regularization to control the probability discrepancy and diversity of categories for adaptive category assignment. In addition, we design a hybrid structure, Adaptive Model Merging, which preserves the previous knowledge by reducing the novel branch’s interference with the old classes. Extensive experiments on classiNCD demonstrate that our method can significantly outperform the existing methods without increasing the computational cost.

# Acknowledgements
The study was funded by the National Natural Science Foundation of China under contracts No. 62332002, No. 62027804, No. 62088102, No. 61825101, No. 62372010, and No. 62202010, and the major key project of the Peng Cheng Laboratory (PCL2021A13). Computing support was provided by Pengcheng Cloudbrain.",2
