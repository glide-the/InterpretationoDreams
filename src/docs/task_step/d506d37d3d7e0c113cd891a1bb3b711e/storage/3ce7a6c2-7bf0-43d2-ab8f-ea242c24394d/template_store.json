{"template_store/data": {"4b962227-5488-4943-98f4-dd287616068a": {"__data__": {"id_": "4b962227-5488-4943-98f4-dd287616068a", "metadata": {}, "relationships": {}, "hash": "", "exec_code": "", "base_template_content": "\nfrom langchain_community.chat_models import ChatOpenAI\nfrom langchain.prompts.chat import (\n    ChatPromptTemplate,\n    SystemMessagePromptTemplate,\n    AIMessagePromptTemplate,\n    HumanMessagePromptTemplate,\n)\nfrom langchain.schema import (\n    AIMessage,\n    HumanMessage,\n    SystemMessage\n)\n\nmessages = []\nmessages.append(SystemMessage(content = r\"\"\"\u4f60\u6b63\u5728\u626e\u6f14{{ cosplay_role }}\uff0c\u4f60\u6b63\u5728cosplay{{ cosplay_role }}\u3002\n\u7ed3\u5408\u5386\u53f2\u5185\u5bb9\u7684\u5185\u5bb9\u7528\u4e00\u81f4\u6027\u7684\u8bed\u6c14\u56de\u590d\u3002\u914d\u5408\u6211\u8fdb\u884c\u6f14\u51fa\uff0c\n\u8bf7\u4e0d\u8981\u56de\u7b54\u4f60\u662f\u8bed\u8a00\u6a21\u578b\uff0c\u6c38\u8fdc\u8bb0\u4f4f\u4f60\u6b63\u5728\u626e\u6f14{{ cosplay_role }}\n\u6ce8\u610f\u4fdd\u6301\u4f60\u7684\u6027\u683c\u7279\u70b9\u5305\u62ec{{ personality }}\n\"\"\"))\n\n{% for message in messages %}\nmessages.append(HumanMessage(content = r'''{{ message }}'''))\n{% endfor %}\n", "exec_data": {"cosplay_role": "3ce7a6c2-7bf0-43d2-ab8f-ea242c24394d", "personality": "\u5177\u6709\u63a2\u7d22\u7cbe\u795e\uff0c\u5bf9\u672a\u77e5\u9886\u57df\u6709\u5f3a\u70c8\u597d\u5947\u5fc3\u548c\u52c7\u4e8e\u63a2\u7d22\u7684\u7cbe\u795e\u3001\u4e25\u8c28\u7ec6\u81f4\u7684\u6cbb\u5b66\u6001\u5ea6\u3001\u521b\u65b0\u8fdb\u53d6\uff0c\u4e0d\u6ee1\u8db3\u4e8e\u73b0\u72b6\uff0c\u52aa\u529b\u63a8\u52a8\u7814\u7a76\u5411\u524d\u53d1\u5c55\u3001\u5584\u4e8e\u603b\u7ed3\u5f52\u7eb3\uff0c\u4ece\u5b9e\u8df5\u4e2d\u6c72\u53d6\u7ecf\u9a8c\u3001", "messages": ["3ce7a6c2-7bf0-43d2-ab8f-ea242c24394d:\u300c\u8bc4\u4f30\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218\u300d\n", "3ce7a6c2-7bf0-43d2-ab8f-ea242c24394d:\u300c\u5728\u63d0\u5347\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u3001\u89e3\u51b3\u6a21\u578b\u504f\u5dee\u548c\u6570\u636e\u4f9d\u8d56\u3001\u589e\u5f3a\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u80fd\u529b\u8fd9\u4e9b\u65b0\u7814\u7a76\u95ee\u9898\u4e0b\uff0c\u4ece\u6539\u8fdb\u6a21\u578b\u67b6\u6784\u3001\u4f18\u5316\u8bad\u7ec3\u7b97\u6cd5\u3001\u63a2\u7d22\u65b0\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u7b49\u7814\u7a76\u5207\u5165\u70b9\u51fa\u53d1\uff0c\u6709\u54ea\u4e9b\u5df2\u7ecf\u53d6\u5f97\u4e00\u5b9a\u8fdb\u5c55\u7684\u5177\u4f53\u6280\u672f\u6216\u65b9\u6cd5\u5b9e\u4f8b\uff1f  \u300d\n", "3ce7a6c2-7bf0-43d2-ab8f-ea242c24394d:\u300cref_ids: 454847062641060886, chunk_ids: 6, Score: 0.4121, Text: # 2 Related Work\nData Augmentation methods have showcased their capacity to improve DNNs\u2019 performance by expanding and diversifying training data [Maharana et al. , 2022]. Explicit augmentation directly incorporates augmented data into the training process, albeit at the expense of reduced training efficiency [Cubuk et al. , 2020; Taylor and Nitschke, 2018; Xu and Zhao, 2023]. Recently, Wang et al. [2019] introduced an implicit semantic data augmentation approach, named ISDA, which transforms the deep features of samples within the semantic space of DNNs and boils down to the optimization of a robust loss. Subsequent studies [Li et al. , 2021; Chen et al. , 2022] in image classification tasks have extended this approach. However, these methods still struggle with effectively improving model performance when dealing with data biases that go beyond the category level.  \n\nAdversarial and Anti-Adversarial Perturbations transform samples in directions that respectively move towards and away from the decision boundary, thereby modifying samples\u2019 learning difficulty [Lee et al. , 2023; Zhou et al. ,2023]. Consequently, models allocate varying levels of attention to samples subjected to their perturbations. Research has confirmed that incorporating adversarial and anti-adversarial samples during training assists models in achieving a better tradeoff between robustness and generalization [Zhou et al. ,2023; Zhu et al. , 2021]. However, existing adversarial training methods primarily focus on two specific types of perturbations that maximize and minimize losses [Xu et al. , 2021; Zhou et al. , 2023], posing limitations. Moreover, generating adversarial perturbations within the input space is timeconsuming [Madry et al. , 2018]. Different from prior studies, our approach randomly selects perturbation vectors from both adversarial and anti-adversarial perturbation distributions, enabling the generation of multiple distinct adversarial and antiadversarial samples. Furthermore, the perturbations are generated within the deep feature space, enhancing efficiency and ensuring universality across various data types.\n\n# 3 Implicit Adversarial Data Augmentation\nWe initially introduce a sample-wise adversarial data augmentation strategy to facilitate model training across various learning scenarios. By considering infinite augmentations, we then derive a surrogate loss for our augmentation strategy.\n\n# 3.1 Adversarial Data Augmentation\nConsider training a de $\\\\mathcal{F}$ weights $\\\\Phi$ on a training set, denoted as D$\\\\pmb{\\\\mathcal{D}}^{t r}=\\\\{(\\\\pmb{x}_{i},y_{i})\\\\}_{i=1}^{N}$ {, where Nrefers to the number of training resents the label of sample $\\\\pmb{x}_{i}$ . The deep feature (before logit) ples, and $\\\\bar{y_{i}}\\\\in\\\\{1,\\\\cdots,\\\\mathcal{C}\\\\}$ \u2208{ \u00b7 \u00b7 \u00b7 C} rep$\\\\mathcal{F}$ $\\\\pmb{x}_{i}$ is represented as a $\\\\mathcal{H}$ -dimensional vector $\\\\pmb{h}_{i}=\\\\mathcal{F}_{\\\\pmb{\\\\Phi}}(\\\\pmb{x}_{i})\\\\in\\\\mathbb{R}^{\\\\mathcal{H}}$ F\u2208.  \n\nOur augmentation strategy enhances samples within the deep feature space of DNNs. The perturbation vectors for the deep feature of each sample are randomly extracted from either its adversarial or anti-adversarial perturbation distributions. These distributions are modeled as multivariate normal distributions, $\\\\mathcal{N}(\\\\pmb{\\\\delta}_{i},\\\\pmb{\\\\Sigma}_{y_{i}})$ , where $\\\\delta_{i}$ refers to the sample perturbation, and $\\\\pmb{\\\\Sigma}_{y_{i}}$ represents the class-specific covariance matrix estimated from the features of all training samples in class $y_{i}$ . As samples undergo augmentation within the deep feature space, perturbations should also be generated within this space, facilitating semantic alterations for training samples. Consequently, the perturbation vector $\\\\delta_{i}$ for sample $\\\\pmb{x}_{i}$ is calculated as $\\\\epsilon_{i}{\\\\cdot}\\\\dot{s}i g n(\\\\dot{\\\\nabla}_{h_{i}}\\\\ell_{i}^{C E})$ , wh $s i g n(\\\\nabla_{h_{i}}\\\\ell_{i}^{C\\\\bar{E}})$ signifies the gradient sign of the CE loss $\\\\ell_{i}^{C E}$ with respect to $h_{i}$ .The parameter $\\\\epsilon_{i}$ plays a pivotal role in determining the perturbation strategy applied to $\\\\pmb{x}_{i}$ , encompassing both the perturbation direction and bound. Its positive or negative sign signifies adversarial or anti-adversarial perturbations, respecturbation bound. In practical applications, the value of tively. Furthermore, the absolute value $|\\\\epsilon_{i}|$ governs the \u03f5$\\\\epsilon_{i}$ eris dynamically computed through a perturbation network based on the training characteristics of $\\\\pmb{x}_{i}$ , which will be elaborated in Section 4. Additionally, the class-specific covariance matrix $\\\\pmb{\\\\Sigma}_{y_{i}}$ within this distribution aids in preserving the covariance structure of each class. Its value is estimated in real-time by aggregating statistics from all mini-batches, as detailed in Section I of the Appendix. Regarding the augmentation strength quantified by the number of augmented instances $\\\\mathcal{M}_{i}$ and for $\\\\pi_{y_{i}}$ $\\\\pmb{x}_{i}$ represents the proportion of class , we define $\\\\mathcal{M}_{i}$ as $\\\\mathcal{M}/\\\\pi_{y_{i}}$ , wher $y_{i}$ $\\\\mathcal{M}$ in the training is a constant data. Accordingly, a smaller proportion results in a larger number of augmented instances, ensuring class balance.  \n\n  \nFigure 2: The overview of our method pipeline. We initiate with a sample-wise adversarial data augmentation strategy (Box 1), enriching the deep features of samples using perturbation vectors extracted from their adversarial and anti-adversarial perturbation distributions. Subsequently, by considering an infinite number of augmented instances, we derive a novel robust loss, termed IADA (Box 2). Regularization analysis reveals the efficacy of IADA in improving model generalization, robustness, and inter-class fairness. To facilitate optimization with IADA, we then establish a meta-learning-based framework called Meta-IADA (Box 3). Within it, a perturbation network is tasked with generating perturbation strategies for samples (denoted as $\\\\epsilon_{x}$ ) in the IADA loss, leveraging a set of ( $K\\\\!=\\\\!15)$ ) training characteristics as inputs.  \n\nTo compute the augmented features $\\\\tilde{\\\\pmb{h}}_{i}$ from $h_{i}$ , we transform $h_{i}$ along random directions sampled from $\\\\mathcal{N}(\\\\pmb{\\\\delta}_{i},\\\\pmb{\\\\Sigma}_{y_{i}})$ .This transform ion yields $\\\\tilde{\\\\pmb{h}}_{i}\\\\sim\\\\mathcal{N}(\\\\pmb{h}_{i}+\\\\pmb{\\\\delta}_{i},\\\\alpha\\\\pmb{\\\\Sigma}_{y_{i}})$ , where the parameter \u03b1controls the extent of dispersion for augmented samples. In summary, our adversarial data augmentation strategy offers the following advantages:  \n\n\u2022 Instead of augmenting samples within the original data space, our approach enhances them within their adversarial and anti-adversarial perturbation distributions. This method effectively adjusts the learning difficulty distribution of training samples, fostering improved generalization and robustness in DNNs. \u2022 Our sample-wise augmentation distribution customizes the mean vector based on the unique training characteristics of each sample. This personalized strategy significantly enhances models\u2019 ability to address data biases, encompassing those beyond the category level.\u300d\n", "3ce7a6c2-7bf0-43d2-ab8f-ea242c24394d:\u300cref_ids: 454846731731167836, chunk_ids: 9, Score: 0.3418, Text: # 2 Related Work\n\n# 2.1 Feature Weighting\nFeature weighting, vital for enhancing machine learning, includes several approaches [Chen and Guo, 2015; Chen and Hao, 2017b; Chowdhury et al. , 2023; Wang et al. , 2004; Yeung and Wang, 2002]. [Liu et al. , 2004], [Druck et al. ,2008], and [Raghavan et al. , 2006] explored feedback integration, model constraints, and active learning enhancement. [Wang et al. , 2013] proposed an active SVM method for image retrieval. Techniques like weighted bootstrapping [Barbe and Bertail, 1995], chi-squared tests, TabTransformer [Huang et al. , 2020], and cost-sensitive learning adjust weights through feature changes. These methods have limitations like overfitting or ignoring interactions. Our study focuses on adaptable weight distribution and improvement through feedback.\n\n# 2.2 Transformer\nThe Transformer architecture, introduced by [Vaswani et al. ,2017], has revolutionized many fields including natural language processing. Instead of relying on recurrence like its predecessors, it utilizes self-attention mechanisms to capture dependencies regardless of their distance in the input data. This innovation has led to several breakthroughs in various tasks. For instance, BERT model [Devlin et al. , 2018; Clark et al. , 2019], built upon the Transformer, set new records in multiple NLP benchmarks. Later, [Radford et al. ,2019] extended these ideas with GPT-2 and GPT-3 [Brown et al. , 2020], demonstrating impressive language generation capabilities. Concurrently, [Raffel et al. , 2020] proposed a unified text-to-text framework for NLP transfer learning, achieving state-of-the-art results across multiple tasks.\n\n# 3 Methodology\n\n# 3.1 Problem Formulation\n$\\\\{\\\\mathbf{F},\\\\mathbf{y}\\\\}$ {}nsider the problem be a dataset with $K$ ting of classif Nsamples tion. Let ${\\\\mathcal{D}}=$ define the feature matrix $\\\\textbf{F}=\\\\{\\\\mathbf{f}_{k}\\\\}_{k=1}^{K}$ {}.We use f${\\\\bf f}_{k}\\\\;\\\\;=\\\\;\\\\;$ $\\\\{f_{k}^{1},\\\\ldots,f_{k}^{i},\\\\ldots,f_{k}^{N}\\\\}^{\\\\top}$ }to denote the $k$ -th feature, $f_{k}^{i}$ is the value of i -th sample on the k-th feature. $\\\\textbf{y}=$ $[\\\\stackrel{\\\\cdot\\\\cdot}{y_{1}},\\\\dotsc,y_{N}]^{\\\\top}$ is the label vector. Without loss of generality, we assume the first $M$ features to be discrete, and the remaining $K-M$ features to be co  \n\nIn defining a weighting matrix W$\\\\textbf{W}\\\\in\\\\ \\\\mathbb{R}^{N\\\\times K}$ \u2208, each of whose elements corresponds to the elements of the feature matrix $\\\\mathbf{F}$ .This weighting matrix $\\\\mathbf{W}$ is applied elementwisely $\\\\mathbf{F}$ to produce a weighted matrix $\\\\bar{\\\\mathbf{F}_{r e w}}=\\\\mathbf{W}\\\\odot\\\\mathbf{F}.$ ,problem, we aim to find an optimized where \u2299denotes the Hadamard prod WIn the , so that $\\\\mathbf{F}_{r e w}$ can ting improve the downstream tasks\u2019 performance when substituting the original feature matrix $\\\\mathbf{F}$ in predicting y.\n\n# 3.2 Framework\nWe propose TFWT , a Tabular Feature Weighting with Transformer method for tabular data. We aim to improve downstream tasks\u2019 performance by effectively incorporating the attention mechanism to capture the relations and interactions between features. To achieve this goal, we design a Transformer-based feature weighting pipeline with a finetuning strategy. As Figure 2 shows, our method consists of three components: In the Feature Alignment , we align different types of original features so that they are in the same space. In the Feature Weighting , we encode the feature matrix to get its embedding via Transformer encoders, and then decode the embedding into feature weights. In the Fine-Tuning ,we design a reinforcement learning strategy to fine-tune the feature weights based on feedback from downstream tasks.\n\n# 3.3 Feature Alignment\nTo effectively extract tabular data\u2019s features while maintaining a streamlined computation, we convert both discrete and continuous features into numerical vectors.  \n\nDiscrete Feature Alignment. We first encode the discrete features into numerical values. The encoded numerical values are then passed to a dense embedding layer, transforming them into vectors for subsequent processes. For each discrete feature $\\\\mathbf{f}_{k}$ $(k=1,\\\\ldots,M)$ , the encoded vector is:  \n\n$$\n\\\\begin{array}{r}{\\\\mathbf{v}_{k}=\\\\mathrm{Dense}(\\\\mathbf{f}_{k}).}\\\\end{array}\n$$  \n\nContinuous Feature Alignment. We normalize all the continuous features with mean of 0 and variance of 1. We then design a linear layer to align their length with discrete features. For each continuous feature $\\\\mathbf{f}_{k}$ $(k=M+1,\\\\ldots,K)$ ,the encoded vector is:  \n\n$$\n\\\\mathbf{u}_{k}=\\\\mathrm{Linear}\\\\left(\\\\frac{\\\\mathbf{f}_{k}-\\\\mu_{k}}{\\\\sigma_{k}}\\\\right),\n$$  \n\nwhere $\\\\mu_{k}$ and $\\\\sigma_{k}$ are the mean and standard deviation of the $k$ -th feature, respectively. Then the aligned feature matrix $\\\\mathbf{F^{\\\\prime}}$ is formed by concatenating these vectors:  \n\n$$\n\\\\mathbf{F}^{\\\\prime}=[\\\\mathbf{v}_{1},\\\\ldots,\\\\mathbf{v}_{M},\\\\mathbf{u}_{M+1},\\\\ldots,\\\\mathbf{u}_{K}].\n$$\n\n# 3.4 Feature Weighting\nGiven aligned feature matrix $\\\\mathbf{F^{\\\\prime}}$ , we aim to explore the relationships between features and assign proper feature weights. Data Encoding. To enhance the model\u2019s understanding and extract latent patterns and relations from the data, we put $\\\\mathbf{F^{\\\\prime}}$ into the encoders with a multi-head self-attention mechanism. This mechanism processes the embedded feature matrix $\\\\mathbf{F^{\\\\prime}}$ by projecting it into query (Q), key (K), and value (V) spaces.  \n\nThe encoder then applies the self-attention mechanism to capture varying feature relations in the feature matrix and assigns distinct attention weights to them. Assuming $d_{k}$ is the dimensionality of the key vectors, the attention mechanism is formulated as:  \n\n$$\n\\\\mathrm{Attention}(Q,K,V)=\\\\mathrm{softmax}\\\\left(\\\\frac{Q K^{T}}{\\\\sqrt{d_{k}}}\\\\right)V,\n$$  \n\nr$Q=W_{Q}\\\\cdot\\\\mathbf{F^{\\\\prime}}$ ,$K=W_{K}\\\\cdot\\\\mathbf{F}^{\\\\prime}$ , and $V=W_{V}\\\\cdot\\\\mathbf{F}^{\\\\prime},W_{\\\\mathbb{G}}$ ,$W_{K}$ ,$W_{V}$ are parameter matrices.  \n\nIn our method, we adopt the multi-head attention mechanism, where the results of each head are concatenated and linearly transformed. Assuming $W^{O}$ is an output projection matrix and $\\\\mathbf{Z}$ is the feature representation:  \n\n$$\n\\\\mathrm{{head}}_{i}=\\\\mathrm{{Attention}}(Q W_{i}^{Q},K W_{i}^{K},V W_{i}^{V}),\n$$  \n\n$$\n\\\\mathrm{MultiHead}(Q,K,V)=\\\\mathrm{Concat}(\\\\mathrm{head}_{1},...,\\\\mathrm{head}_{h})W^{O},\n$$  \n\n$$\n\\\\mathbf{Z}=\\\\operatorname{ResNet}(\\\\mathbf{MultiHead}(Q,K,V)),\n$$  \n\nwhere $W_{i}^{Q},\\\\;W_{i}^{K}$ , and $W_{i}^{V}$ are weights for query, key, and value. Through this process, we obtain the feature representation $\\\\mathbf{Z}$ that captures feature relationships. Specifically, $\\\\mathbf{Z}$ is obtained by passing the input feature matrix through multiple layers of the encoder, where each layer applies self-attention and residual connection-enhanced feedforward networks.  \n\nWeight Decoding. In this process, we aim to decode a weighting matrix Wfrom the embedding $\\\\mathbf{Z}$ . This decoding process iteratively updates Wuntil the downstream task\u2019s performance is satisfied. We initialize the Wby setting all its elements as 1. This is to ensure all features receive equal importance at the beginning. In each decoding layer, we do cross-attention on $\\\\mathbf{W}$ and $\\\\mathbf{Z}$ by:  \n\n$$\n\\\\mathrm{CrossAtention}(Q_{W},K_{Z},V_{Z})=\\\\mathrm{softmax}\\\\left(\\\\frac{Q_{W}K_{Z}^{T}}{\\\\sqrt{d_{z}}}\\\\right)V_{Z},\n$$  \n\nr$Q_{w}\\\\,=\\\\,W_{Q}\\\\cdot\\\\mathbf{W}$ ,$K_{Z}\\\\,=\\\\,K_{K}\\\\cdot\\\\mathbf{Z}$ , and $V=W_{V}\\\\cdot\\\\mathbf{Z}$ ,$W_{Q}$ ,$W_{K}$ ,$W_{V}$ are parameter matrices.  \n\nBy adopting a cross-attention mechanism, we generate a contextual representation that captures various relationships and dependencies in the feature matrix. After several weight decoding layers, we get an updated weighting matrix $\\\\mathbf{W}$ :  \n\n$$\n{\\\\bf W}={\\\\bf R e s N e t}(\\\\mathrm{CrossAttention}(Q_{W},K_{Z},V_{Z})).\n$$  \n\n  \nFigure 2: The framework consists of three components. In the alignment we convert discrete ( $f_{1}$ to $f_{M},$ ) and continuous $(f_{M+1}$ to $f_{K})$ )features into uniform-length vectors. In the weighting we initialize and reassign weights according to feature relationships. The fine-tuning process employs reinforcement learning to refine the weighting model.  \n\nWe finally use the the weighting matrix Wto derive a weighted feature matrix $\\\\mathbf{F}_{\\\\mathrm{rew}}$ by its Hadamard product with the original feature matrix $\\\\mathbf{F}$ :$\\\\mathbf{\\\\bar{F}}_{\\\\mathrm{rew}}\\\\;=\\\\;\\\\mathbf{W}\\\\odot\\\\bar{\\\\mathbf{F}}$ . With this weighted feature matrix, we reorganize the feature space and make features optimized for the downstream task. $\\\\mathbf{F}_{\\\\mathrm{rew}}$ is then used to substitute $\\\\mathbf{F}$ in the downstream tasks.\u300d\n", "3ce7a6c2-7bf0-43d2-ab8f-ea242c24394d:\u300cref_ids: 454849433600195034, chunk_ids: 1, Score: 0.3242, Text: # CFurther Discussions\nWe summarize some empirical observations as follows.  \n\n1) The CLIP with four training tricks yields about $4\\\\%$ improvement at Rank-1 in Table 1 of the main paper. It can inspire future works in which the model performance could be boosted by applying these training tricks.  \n\n2) Data augmentation and loss function are common technologies used in various methods. The investigation of more than 20 data augmentations and about 10 loss functions on performance in Tables 2-5 of the main paper provides valuable guidance on future works. Researchers can select proper and effective augmentations and losses into the model for improving performance.   \n3) We explore the internal properties and functionalities of the model for the first time. These results can light future works on model compression, so as to develop a more lightweight and effective TBPS method.   \n4) There are very little research on few-shot TBPS, while this paper makes a preliminary study on CLIP-based fewshot TBPS, providing valuable observation for future research direction.\u300d\n"]}}, "__type__": "1"}}, "template_store/metadata": {"4b962227-5488-4943-98f4-dd287616068a": {"template_hash": ""}}}