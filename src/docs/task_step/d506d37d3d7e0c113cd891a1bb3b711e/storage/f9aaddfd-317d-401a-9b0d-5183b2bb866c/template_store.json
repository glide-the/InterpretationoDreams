{"template_store/data": {"ef7631e4-008c-4d77-89c3-ffcef96a32f8": {"__data__": {"id_": "ef7631e4-008c-4d77-89c3-ffcef96a32f8", "metadata": {}, "relationships": {}, "hash": "", "exec_code": "", "base_template_content": "\nfrom langchain_community.chat_models import ChatOpenAI\nfrom langchain.prompts.chat import (\n    ChatPromptTemplate,\n    SystemMessagePromptTemplate,\n    AIMessagePromptTemplate,\n    HumanMessagePromptTemplate,\n)\nfrom langchain.schema import (\n    AIMessage,\n    HumanMessage,\n    SystemMessage\n)\n\nmessages = []\nmessages.append(SystemMessage(content = r\"\"\"\u4f60\u6b63\u5728\u626e\u6f14{{ cosplay_role }}\uff0c\u4f60\u6b63\u5728cosplay{{ cosplay_role }}\u3002\n\u7ed3\u5408\u5386\u53f2\u5185\u5bb9\u7684\u5185\u5bb9\u7528\u4e00\u81f4\u6027\u7684\u8bed\u6c14\u56de\u590d\u3002\u914d\u5408\u6211\u8fdb\u884c\u6f14\u51fa\uff0c\n\u8bf7\u4e0d\u8981\u56de\u7b54\u4f60\u662f\u8bed\u8a00\u6a21\u578b\uff0c\u6c38\u8fdc\u8bb0\u4f4f\u4f60\u6b63\u5728\u626e\u6f14{{ cosplay_role }}\n\u6ce8\u610f\u4fdd\u6301\u4f60\u7684\u6027\u683c\u7279\u70b9\u5305\u62ec{{ personality }}\n\"\"\"))\n\n{% for message in messages %}\nmessages.append(HumanMessage(content = r'''{{ message }}'''))\n{% endfor %}\n", "exec_data": {"cosplay_role": "f9aaddfd-317d-401a-9b0d-5183b2bb866c", "personality": "\u4e25\u8c28\u3001\u7ec6\u81f4\u3001\u4e13\u4e1a\u3001\u6ce8\u91cd\u903b\u8f91\u6027\u548c\u7cfb\u7edf\u6027\u3001\u7406\u6027\u548c\u51b7\u9759\u3001\u5f3a\u70c8\u7684\u6c42\u77e5\u6b32\u548c\u63a2\u7d22\u7cbe\u795e\u3001", "messages": ["f9aaddfd-317d-401a-9b0d-5183b2bb866c:\u300c\u63a2\u8ba8\u8ba1\u7b97\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\u300d\n", "f9aaddfd-317d-401a-9b0d-5183b2bb866c:\u300c### \u95ee\u9898\n\n\u5728\u63a2\u8ba8\u8ba1\u7b97\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\u65f6\uff0c\u5982\u4f55\u6709\u6548\u5229\u7528\u591a\u6a21\u6001\u5b66\u4e60\uff08\u5982CLIP\u7ed3\u5408\u56fe\u50cf\u548c\u6587\u672c\u8fdb\u884c\u8054\u5408\u5b66\u4e60\uff09\u548c\u8de8\u9886\u57df\u8fc1\u79fb\u5b66\u4e60\uff08\u5982\u5728\u6e90\u9886\u57df\u9884\u8bad\u7ec3\uff0c\u5728\u76ee\u6807\u9886\u57df\u5fae\u8c03\uff09\u6765\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff1f\u5177\u4f53\u6765\u8bf4\uff0c\u6709\u54ea\u4e9b\u65b9\u6cd5\u53ef\u4ee5\u786e\u4fdd\u6a21\u578b\u5728\u9762\u5bf9\u65b0\u6570\u636e\u548c\u573a\u666f\u65f6\u4fdd\u6301\u8f83\u9ad8\u7684\u9c81\u68d2\u6027\u548c\u7a33\u5b9a\u6027\uff1f\u6b64\u5916\uff0c\u5982\u4f55\u901a\u8fc7\u591a\u6a21\u6001\u878d\u5408\uff08\u5982\u5229\u7528Transformer\u8fdb\u884c\u591a\u6a21\u6001\u6570\u636e\u7684\u8054\u5408\u5efa\u6a21\uff09\u548c\u6cdb\u5316\u7406\u8bba\uff08\u5982\u7814\u7a76\u6a21\u578b\u6cdb\u5316\u7684\u7406\u8bba\u754c\u9650\u548c\u4f18\u5316\u65b9\u6cd5\uff09\u6765\u8fdb\u4e00\u6b65\u4f18\u5316\u6a21\u578b\u7684\u8de8\u9886\u57df\u5e94\u7528\u6027\u80fd\uff1f\u300d\n", "f9aaddfd-317d-401a-9b0d-5183b2bb866c:\u300cref_ids: 454846660891728744, chunk_ids: 0, Score: 0.4434, Text: # 2. Related Work\nEfficient learning for CLIP. One can improve learning efficiency through utilizing an enhanced training objective. Examples include image masking [ 17 ,36 ,54 ,70 ], unimodal self-supervision [ 34 ,42 ], fine-grained image-text alignment [ 71 ], contrastive learning in image-text-label space [ 68 ], and pairwise Sigmoid loss [ 76 ]. Recently, CLIPA [ 33 ] proposed training at multi-resolutions for costeffective CLIP training. These methods are complementary to our proposed method and can be exploited for further improvements.  \n\nCLIP training dataset is often comprising noisy imagetext pairs obtained at web-scale. Since the original CLIP model [ 46 ], several works have demonstrated improved results on large-scale and filtered datasets [ 16 ,18 ,50 ,51 ,76 ]. Complementary to data collection and filtering, recent works show that using visually enriched synthetic captions generated from a pretrained captioning model, along with real captions, can improve the quality of CLIP models [ 31 ,44 ,69 ]. Our proposed reinforced multi-modal dataset also benefits from synthetically generated captions, which we show are crucial for improved learning efficiency.  \n\nPrevious works explored extending unimodal knowledge distillation [ 25 ] to vision-language models. DIME-FM [ 55 ]proposes using in-domain unimodal data for distillation with a focus on zero-shot classification. TinyCLIP [ 67 ] trains compact CLIP models via cross-modal affinity mimicking and weight inheritance. Multi-modal distillation is also explored in setups where the student is a fused vision-language model for specific tasks [ 30 ,63 ,64 ]. Our proposed multimodal reinforced training also includes cross-modal affinity mimicking [ 67 ] toward targets that are added to our reinforced datasets. Further, we extend unimodal model ensembling [ 32 ,45 ] to multimodal setup, and store targets obtained from an ensemble of CLIP models.  \n\nOffline knowledge distillation methods [ 14 ,53 ,75 ] have been proposed recently to mitigate the training-time overhead cost due to running large teacher models. We extend the dataset reinforcement strategy [ 14 ] to the multi-modal setup of CLIP. Our proposed reinforced multi-modal datasets result in significant accuracy improvement without adding a training-time computational overhead.  \n\nEfficient architectures for CLIP. Recently there have been a wide range of architectures that have shown great promise for accomplishing vision tasks on resource constraint devices. These architectures can be broadly classified into purely convolutional [ 11 ,22 ,26 ,27 ,40 ,47 ,49 ,60 ], transformer based [ 12 ,39 ,58 ] and convolution-transformer hybrids like [ 21 ,35 ,37 ,43 ,52 ,61 ]. Similarly there are transformer based [ 62 ] and convolution-transformer hybrids like [ 19 ,66 ] for text encoding. There have been works like [ 67 ], that prune ViT architectures to obtain smaller and faster CLIP models or works like [ 3 ] that reduce image-text tokens for faster inference of vision-language models. These models can still be quite large and inefficient to be deployed on a mobile device. In our work, we introduce an improved convolution-transformer hybrid architecture for both vision and text modalities, that improve over recent state-of-the-art like [ 21 ,37 ,43 ,52 ]. The optimizations introduced in [ 3 ,67 ]can be used to further improve efficiency of our models.\n\n# 3. Multi-Modal Reinforced Training\nOur multi-modal reinforced training leverages knowledge transfer from an image captioning model and a strong ensemble of pretrained CLIP models for training the target model. It consists of two main components: i) leveraging the knowledge of an image captioning model via synthetic captions, and ii) knowledge distillation of image-text alignments from an ensemble of strong pre-trained CLIP models. We follow the dataset reinforcement strategy of [ 14 ] and store the additional knowledge (synthetic captions and teacher embeddings) in the dataset (see Fig. 3 ), thereby avoiding any additional training time computational overhead such as evaluating the captioning model or the ensemble teacher. The proposed training strategy results in significant improvement in learning efficiency, i.e., reaching to certain target performance with less training budget and fewer samples.\n\n# 3.1. Dataset Reinforcement\nSynthetic captions. Image-text datasets used to train CLIP models are mostly sourced from the web, which is inherently noisy. Recent efforts such as DataComp [ 18 ] and data filtering networks [ 16 ] improve the quality of web-sourced datasets by using extensive filtering mechanisms. While these filtered datasets have lower noise, the captions may still not be descriptive enough. In order to boost the visual descriptiveness of the captions we use the popular CoCa [ 73 ]model and generate multiple synthetic captions ${\\\\bf\\\\boldsymbol{x}}_{\\\\mathrm{syn}}^{(i,s)}$ for each image $\\\\pmb{x}_{\\\\mathrm{img}}^{(i)}$ (see Fig. 3 a). Ablations on the number of synthetic captions generated per image are provided in Sec. 5.1 . Figure 5 shows some examples of synthetic captions generated by the CoCa model. Real captions in comparison to synthetic captions are generally more specific but noisier. We show (Tab. 2a ) a combination of both real and synthetic captions is crucial to obtain best zero-shot retrieval and classification performance.  \n\nImage augmentations. For each image $\\\\pmb{x}_{\\\\mathrm{img}}^{(i)}$ , we generate multiple augmented images $\\\\hat{\\\\pmb{x}}_{\\\\mathrm{img}}^{(i,j)}$ using a parametrized  \n\naugmentation function $\\\\boldsymbol{\\\\mathcal{A}}$ :  \n\n$$\n\\\\hat{\\\\pmb{x}}_{\\\\mathrm{img}}^{(i,j)}=\\\\mathcal{A}(\\\\pmb{x}_{\\\\mathrm{img}}^{(i)};\\\\pmb{a}^{(i,j)})\\\\,,\n$$  \n\nwhere $\\\\pmb{a}^{(i,j)}$ are the augmentation parameters that are sufficient to reproduce $\\\\hat{\\\\pmb{x}}_{\\\\mathrm{img}}^{(i,\\\\bar{j})}$ from $\\\\pmb{x}_{\\\\mathrm{img}}^{(i)}$ (see Fig. 3 a). Ablations on the number and different kinds of augmentations used per image are provided in Tabs. 3a and 11 , respectively.  \n\nEnsemble teacher. Model ensembling is a widely used technique for creating a stronger model from a set of independently trained ones [ 32 ,45 ]. We extend this technique to multi-modal setup and use an ensemble of $K$ CLIP models as a strong teacher (see Sec. 5.1 for our teacher ablations). We compute the feature embeddings of these models for augmented images $\\\\hat{\\\\pmb{x}}_{\\\\mathrm{img}}^{(i,j)}$ and synthetic captions ${\\\\bf\\\\boldsymbol{x}}_{\\\\mathrm{syn}}^{(i,s)}$ obtaining $d_{k}$ -dimensional vectors $\\\\psi_{\\\\mathrm{img}}^{(i,j,k)}$ and $\\\\psi_{\\\\mathrm{syn}}^{(i,s,k)}$ for the $k$ -th teacher model. We also compute the teacher embeddings $\\\\psi_{\\\\mathrm{txt}}^{(i,k)}$ of the ground-truth captions $\\\\pmb{x}_{\\\\mathrm{txt}}^{(i)}$ (see Fig. 3 b). Reinforced dataset. We store the image augmentation parameters $\\\\pmb{a}^{(i,j)}$ , synthetic captions ${\\\\bf\\\\boldsymbol{x}}_{\\\\mathrm{syn}}^{(i,s)}$ , feature embeddings $\\\\psi_{\\\\mathrm{img}}^{(i,j,k)}$ ,$\\\\psi_{\\\\mathrm{syn}}^{(i,s,k)}$ and $\\\\psi_{\\\\mathrm{txt}}^{(i,k)}$ of the CLIP teachers as additional knowledge in the dataset along with the original image $\\\\pmb{x}_{\\\\mathrm{img}}^{(i)}$ and caption $\\\\pmb{x}_{\\\\mathrm{txt}}^{(i)}$ (see Fig. 3 c). Note that dataset reinforcement is a one-time cost that is amortized by several efficient model training and experimentation.\u300d\n", "f9aaddfd-317d-401a-9b0d-5183b2bb866c:\u300cref_ids: 454895302186370932, chunk_ids: 10, Score: 0.2324, Text: # Model Reprogramming: Resource-Efficient Cross-Domain Machine Learning\nPin-Yu Chen 1  \n\n1 IBM Research\n\n# Abstract\nIn data-rich domains such as vision, language, and speech, deep learning prevails to deliver high-performance taskspecific models and can even learn general task-agnostic representations for efficient finetuning to downstream tasks. However, deep learning in resource-limited domains still faces multiple challenges including (i) limited data, (ii) constrained model development cost, and (iii) lack of adequate pre-trained models for effective finetuning. This paper provides an overview of model reprogramming to bridge this gap. Model reprogramming enables resource-efficient crossdomain machine learning by repurposing and reusing a welldeveloped pre-trained model from a source domain to solve tasks in a target domain without model finetuning, where the source and target domains can be vastly different. In many applications, model reprogramming outperforms transfer learning and training from scratch. This paper elucidates the methodology of model reprogramming, summarizes existing use cases, provides a theoretical explanation of the success of model reprogramming, and concludes with a discussion on open-ended research questions and opportunities. A list of model reprogramming studies is actively maintained and updated at https://github.com/IBM/model-reprogramming .\u300d\n", "f9aaddfd-317d-401a-9b0d-5183b2bb866c:\u300cref_ids: 454984230906632244, chunk_ids: 7, Score: 0.2236, Text: # 1. Introduction\nOver the past few decades, Machine Learning (ML) has demonstrated remarkable achievements across diverse areas such as Computer Vision, Natural Language and Speech Processing, or Robotics [ 5 ]. In general, most ML models rely on an over-simplified assumption, i.e., the training and testing data are independent and identically distributed, which does not always reflect real-world practices. In practical scenarios where the distribution of testing data diverges from that of training data, the performance of ML models often drops catastrophically due to the domain shift issue [ 28 ]. Additionally, obtaining or identifying the testing data before model deployment can be challenging in numerous applications. For instance, in biomedical applications where data characteristics vary across different equipment and institutions, gathering data from all potential domains in advance is impractical. Therefore, it is essential to have a solution that can improve the generalization capability of such ML models to adapt effectively to unseen domains.  \n\nDomain Generalization (DG) has been proposed to address the challenge of training ML models using data from single or multiple source domains with the expectation that these models will perform well on unseen domains [41 ]. The majority of existing DG methods fall under the category of domain-invariant representation learning approach [ 13 ,20 ,25 ,29 ,33 ]. This approach relies on a broadly acknowledged assumption that each domain contains its own domain-specific features, which are biased towards spurious relations in the data, and that all domains share domain-invariant features, which are general and robust to any unseen domains. From this assumption, previous works propose methods that remove domain-specific features and distill domain-invariant features to achieve the generalization ability. Alternative approaches for DG encompass data augmentation [23 ,43 ,44 ,47 ], which involves exposing models to artificially generated domains, and meta-learning [1 ,6 ,12 ], an approach that emulates the domain shift during the meta-training phase. However, most methods require a centralized setting where all source domains are collected together. Consequently, these methods cannot be readily expanded to decentralized settings.  \n\nFederated Learning (FL) [ 21 ] is an emerging decentralized learning paradigm widely adopted in various applications to cope with the increasing privacy concerns of data centralization [ 40 ]. Specifically, the paradigm works in a way that each client learns from their data and only aggregates local models\u2019 parameters at a certain frequency at the central server to generate a global model. Notably, all data samples are kept within each client during the FL training process. Due to the nature of data decentralization, where each client owns a single source domain, as illustrated in Figure 1 , the FL paradigm poses further significant challenges for DG and limits the applicability of available DG methods. There have been some early attempts to address the DG problem in the FL scenario. For instance, Liu et al. [18 ] introduces a method that allows clients to share their image data in the frequency space with each other, thus relatively recovering the centralization process at each client. Similarly, Chen et al. [ 3 ] introduces another method that extracts and exchanges the style of local images among all clients. It is evident that these initial efforts employ a strategy that necessitates the sharing of partial client data, thereby compromising the data privacy constraints of FL to a certain extent. Although they show promising results, these methods can be overly complicated to implement in practice and lead to additional privacy risks during the FL training process.  \n\n  \nFigure 1. An overview of our proposed gPerXAN method for solving the FedDG problem.  \n\nTo address the aforementioned challenges, this paper introduces a novel architectural method for domain-invariant representation learning within the FL framework. The proposed method enhances the generalization ability while upholding the fundamental privacy principles of FL. Based on the effectiveness of discarding domain-specific information from learned features [ 25 ,30 ], we properly assemble Instance Normalization layers (IN) into Batch Normalization layers (BN) in well-known Convolutional Neural Networks (CNNs) using an explicit differential mixture as in Eqn ( 2 ). Moreover, thanks to the explicit property, the benefit of personalization in FL [ 27 ,35 ] can be incorporated into the normalization scheme using local BN sides. Specifically, during the FL training process, while IN sides are globally aggregated along with other model parameters, BN sides are updated locally without broadcasting. In addition, we argue that only relying on the ability to filter domain-specific features of IN while lacking guidance to distill domain-invariant representations directly might lead to suboptimal performance. Based on this observation, we introduce a simple yet highly effective regularization term to guide client models to directly capture domain-invariant representations that can be used by the global model\u2019s classifier, which is aggregated from client models\u2019 classifiers.  \n\nTo summarize, our main contributions in this paper are highlighted as follows:  \n\n\u2022 Different from existing methods for DG in the FL scenario, we propose a novel method that concentrates on a personalized normalization scheme, global IN while local BN, for filtering domain-specific features and fully respecting the privacy-preserving principles of FL. \u2022 Furthermore, we propose a simple yet effective regularization term to introduce clear guidance to client models for directly capturing domain-invariant representations, further improving performance on unseen domains. \u2022 Finally, we conduct extensive experiments on two benchmark datasets, i.e., PACS and Office-Home, and a realworld medical dataset, Camelyon17, where our proposed method outperforms existing relevant ones.\u300d\n"]}}, "__type__": "1"}}, "template_store/metadata": {"ef7631e4-008c-4d77-89c3-ffcef96a32f8": {"template_hash": ""}}}