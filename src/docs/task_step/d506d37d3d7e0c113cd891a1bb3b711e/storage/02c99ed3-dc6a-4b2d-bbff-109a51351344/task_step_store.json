{"task_step_store/data": {"8da2e2af-46ef-4ee5-8766-c77589af9874": {"__data__": {"id_": "8da2e2af-46ef-4ee5-8766-c77589af9874", "metadata": {}, "relationships": {}, "hash": "", "start_task_context": "\u5927\u6a21\u578bMLA\u6280\u672f\u7ec6\u8282", "aemo_representation_context": "### 1. \u5206\u6790\u8fd1\u51e0\u5e74\u7814\u7a76\u9886\u57df\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba\n\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u76f8\u5173\u7814\u7a76\u9886\u57df\uff0cTransformer\u3001GAN\u3001BERT \u7b49\u6846\u67b6\u6210\u4e3a\u4f17\u591a\u8bba\u6587\u91c7\u7528\u7684\u6838\u5fc3\u6280\u672f\u3002\u8fd9\u4e9b\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u9002\u5e94\u6027\uff0c\u5176\u65b9\u6cd5\u8bba\u56f4\u7ed5\u7740\u5982\u4f55\u6709\u6548\u5904\u7406\u6570\u636e\u7279\u5f81\u3001\u4f18\u5316\u6a21\u578b\u7ed3\u6784\u4ee5\u63d0\u5347\u6027\u80fd\u3002\u4f8b\u5982\uff0cTransformer \u57fa\u4e8e\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6253\u7834\u4e86\u4f20\u7edf\u5e8f\u5217\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u7b49\u591a\u9886\u57df\u5e7f\u6cdb\u5e94\u7528\uff1bGAN \u901a\u8fc7\u751f\u6210\u5668\u4e0e\u5224\u522b\u5668\u7684\u5bf9\u6297\u8bad\u7ec3\uff0c\u5728\u56fe\u50cf\u751f\u6210\u7b49\u4efb\u52a1\u4e0a\u53d6\u5f97\u663e\u8457\u6210\u679c\uff1bBERT \u5219\u901a\u8fc7\u9884\u8bad\u7ec3\u65b9\u5f0f\uff0c\u6781\u5927\u63d0\u5347\u4e86\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u7684\u6027\u80fd\u3002\n\n### 2. \u7814\u7a76\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u4e3b\u8981\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0e\u53d8\u4f53\n - **Transformer**\uff1a\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u4e2d\uff0c\u5982\u673a\u5668\u7ffb\u8bd1\u3001\u6587\u672c\u5206\u7c7b\uff0cTransformer \u67b6\u6784\u4e0d\u65ad\u6f14\u53d8\uff0c\u51fa\u73b0\u4e86\u8bf8\u5982 ViT\uff08Vision Transformer\uff09\u7b49\u53d8\u4f53\u5e94\u7528\u4e8e\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\uff0c\u5c06\u56fe\u50cf\u89c6\u4e3a\u5e8f\u5217\u8fdb\u884c\u5904\u7406\uff0c\u5c55\u73b0\u51fa\u8de8\u9886\u57df\u7684\u5e94\u7528\u6f5c\u529b\u3002\n - **GAN**\uff1a\u5728\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\uff0cDCGAN\uff08Deep Convolutional GAN\uff09\u7b49\u53d8\u4f53\u901a\u8fc7\u6539\u8fdb\u7f51\u7edc\u7ed3\u6784\uff0c\u751f\u6210\u66f4\u903c\u771f\u7684\u56fe\u50cf\uff1b\u5728\u533b\u5b66\u56fe\u50cf\u5408\u6210\u3001\u89c6\u9891\u751f\u6210\u7b49\u9886\u57df\u4e5f\u6709\u5e94\u7528\uff0c\u901a\u8fc7\u8c03\u6574\u635f\u5931\u51fd\u6570\u548c\u7f51\u7edc\u67b6\u6784\u9002\u5e94\u4e0d\u540c\u4efb\u52a1\u9700\u6c42\u3002\n - **BERT**\uff1a\u9664\u4e86\u57fa\u7840\u7684\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u4efb\u52a1\uff0c\u8fd8\u5728\u4fe1\u606f\u68c0\u7d22\u3001\u60c5\u611f\u5206\u6790\u7b49\u4efb\u52a1\u4e2d\u5f97\u5230\u5e7f\u6cdb\u5e94\u7528\uff0c\u901a\u8fc7\u5fae\u8c03\u9884\u8bad\u7ec3\u6a21\u578b\u9002\u5e94\u7279\u5b9a\u4efb\u52a1\uff0c\u540c\u65f6\u51fa\u73b0\u4e86 RoBERTa \u7b49\u6539\u8fdb\u7248\u672c\uff0c\u4f18\u5316\u9884\u8bad\u7ec3\u7b56\u7565\u63d0\u5347\u6027\u80fd\u3002\n\n### 3. \u8bc4\u4f30\u5b66\u672f\u754c\u7684\u6280\u672f\u8fdb\u6b65\u4e0e\u5c40\u9650\u6027\n - **\u6280\u672f\u8fdb\u6b65**\uff1a\u6a21\u578b\u6027\u80fd\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u53d6\u5f97\u663e\u8457\u63d0\u5347\uff0c\u4f8b\u5982\u5728\u56fe\u50cf\u8bc6\u522b\u51c6\u786e\u7387\u3001\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u7684 F1 \u503c\u7b49\u6307\u6807\u4e0a\u4e0d\u65ad\u5237\u65b0\u8bb0\u5f55\u3002\u65b0\u7684\u6a21\u578b\u67b6\u6784\u548c\u8bad\u7ec3\u65b9\u6cd5\u4e0d\u65ad\u6d8c\u73b0\uff0c\u63a8\u52a8\u4e86\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\u3002\n - **\u5c40\u9650\u6027**\uff1a\u4f9d\u7136\u5b58\u5728\u6a21\u578b\u504f\u5dee\u548c\u6570\u636e\u4f9d\u8d56\u95ee\u9898\u3002\u6a21\u578b\u5728\u67d0\u4e9b\u7279\u5b9a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u5176\u4ed6\u6570\u636e\u96c6\u6216\u73b0\u5b9e\u573a\u666f\u4e2d\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002\u90e8\u5206\u6a21\u578b\u5bf9\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u4f9d\u8d56\u4e25\u91cd\uff0c\u6570\u636e\u8d28\u91cf\u548c\u6570\u91cf\u5f71\u54cd\u6a21\u578b\u6027\u80fd\uff0c\u4e14\u6a21\u578b\u89e3\u91ca\u6027\u5dee\uff0c\u96be\u4ee5\u7406\u89e3\u5176\u51b3\u7b56\u8fc7\u7a0b\u3002\n\n### 4. \u63a2\u8ba8\u8ba1\u7b97\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\n - **\u9002\u7528\u6027**\uff1a\u4e0d\u540c\u6a21\u578b\u5728\u7279\u5b9a\u6570\u636e\u96c6\u548c\u5e94\u7528\u573a\u666f\u4e0b\u6709\u5404\u81ea\u4f18\u52bf\u3002\u4f8b\u5982\uff0c\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6a21\u578b\u5728\u5927\u89c4\u6a21\u3001\u9ad8\u7ef4\u6570\u636e\u7684\u56fe\u50cf\u548c\u8bed\u97f3\u8bc6\u522b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff1b\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u5c0f\u6570\u636e\u3001\u7b80\u5355\u4efb\u52a1\u4e2d\u4ecd\u6709\u5e94\u7528\u4ef7\u503c\u3002\n - **\u6cdb\u5316\u80fd\u529b**\uff1a\u5c3d\u7ba1\u90e8\u5206\u6a21\u578b\u5728\u7279\u5b9a\u9886\u57df\u53d6\u5f97\u6210\u529f\uff0c\u4f46\u5728\u591a\u9886\u57df\u3001\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u7684\u73b0\u5b9e\u5e94\u7528\u4e2d\uff0c\u6cdb\u5316\u80fd\u529b\u9762\u4e34\u6311\u6218\u3002\u591a\u6a21\u6001\u6570\u636e\u7684\u5f02\u8d28\u6027\u548c\u590d\u6742\u6027\u4f7f\u5f97\u6a21\u578b\u96be\u4ee5\u6709\u6548\u63d0\u53d6\u548c\u878d\u5408\u7279\u5f81\uff0c\u9650\u5236\u4e86\u5176\u5728\u590d\u6742\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002\n\n### 5. \u5206\u6790\u6700\u65b0\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027\n - **\u7a33\u5b9a\u6027\u4f18\u5316**\uff1a\u4e00\u4e9b\u7b97\u6cd5\u9488\u5bf9\u7a33\u5b9a\u6027\u8fdb\u884c\u4e86\u4f18\u5316\uff0c\u4f8b\u5982\u91c7\u7528\u6b63\u5219\u5316\u6280\u672f\u3001\u5bf9\u6297\u8bad\u7ec3\u7b49\u65b9\u6cd5\u63d0\u9ad8\u6a21\u578b\u5728\u590d\u6742\u73af\u5883\u4e0b\u7684\u7a33\u5b9a\u6027\u3002\u5728\u52a8\u6001\u73af\u5883\u4e2d\uff0c\u90e8\u5206\u7b97\u6cd5\u901a\u8fc7\u5728\u7ebf\u5b66\u4e60\u3001\u81ea\u9002\u5e94\u8c03\u6574\u53c2\u6570\u7b49\u65b9\u5f0f\u4fdd\u6301\u6027\u80fd\u7a33\u5b9a\u3002\n - **\u5927\u89c4\u6a21\u6570\u636e\u9002\u5e94\u6027**\uff1a\u5728\u5927\u89c4\u6a21\u6570\u636e\u5904\u7406\u4e0a\uff0c\u90e8\u5206\u7b97\u6cd5\u901a\u8fc7\u5206\u5e03\u5f0f\u8bad\u7ec3\u3001\u6a21\u578b\u538b\u7f29\u7b49\u6280\u672f\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u548c\u9002\u5e94\u6027\u3002\u4f46\u4ecd\u6709\u7b97\u6cd5\u5728\u5904\u7406\u5927\u89c4\u6a21\u6570\u636e\u65f6\u9762\u4e34\u5185\u5b58\u3001\u8ba1\u7b97\u8d44\u6e90\u7b49\u9650\u5236\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002\n\n### 6. \u8bc4\u4f30\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218\n - **\u65b0\u7814\u7a76\u95ee\u9898**\uff1a\u63d0\u51fa\u4e86\u5982\u63d0\u5347\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u3001\u89e3\u51b3\u6a21\u578b\u504f\u5dee\u548c\u6570\u636e\u4f9d\u8d56\u3001\u589e\u5f3a\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u80fd\u529b\u7b49\u65b0\u7814\u7a76\u95ee\u9898\u3002\n - **\u7814\u7a76\u5207\u5165\u70b9**\uff1a\u4ece\u6539\u8fdb\u6a21\u578b\u67b6\u6784\u3001\u4f18\u5316\u8bad\u7ec3\u7b97\u6cd5\u3001\u63a2\u7d22\u65b0\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u7b49\u65b9\u9762\u5bfb\u627e\u65b0\u7684\u7814\u7a76\u5207\u5165\u70b9\u3002\u8fd9\u4e9b\u6311\u6218\u4fc3\u4f7f\u7814\u7a76\u8005\u4e0d\u65ad\u63a2\u7d22\u65b0\u7684\u6280\u672f\u548c\u65b9\u6cd5\uff0c\u63a8\u52a8\u540e\u7eed\u7814\u7a76\u671d\u7740\u66f4\u9ad8\u6548\u3001\u66f4\u667a\u80fd\u3001\u66f4\u5177\u6cdb\u5316\u80fd\u529b\u7684\u65b9\u5411\u53d1\u5c55\u3002\n\n### \u7814\u7a76\u6210\u679c\u3001\u65b9\u6cd5\u521b\u65b0\u6027\u4e0e\u5e94\u7528\u4ef7\u503c\u603b\u7ed3\n - **\u7814\u7a76\u6210\u679c**\uff1a\u5728\u6a21\u578b\u6027\u80fd\u63d0\u5347\u3001\u65b0\u67b6\u6784\u548c\u7b97\u6cd5\u63d0\u51fa\u7b49\u65b9\u9762\u53d6\u5f97\u663e\u8457\u6210\u679c\uff0c\u63a8\u52a8\u4e86\u8ba1\u7b97\u673a\u79d1\u5b66\u591a\u4e2a\u9886\u57df\u7684\u53d1\u5c55\u3002\n - **\u65b9\u6cd5\u521b\u65b0\u6027**\uff1a\u901a\u8fc7\u5f15\u5165\u65b0\u7684\u6280\u672f\u6846\u67b6\u3001\u6539\u8fdb\u8bad\u7ec3\u65b9\u6cd5\u548c\u4f18\u5316\u6a21\u578b\u7ed3\u6784\u7b49\u65b9\u5f0f\uff0c\u5c55\u73b0\u51fa\u8bf8\u591a\u521b\u65b0\u70b9\uff0c\u5982\u81ea\u6ce8\u610f\u529b\u673a\u5236\u3001\u5bf9\u6297\u8bad\u7ec3\u7b49\u3002\n - **\u5e94\u7528\u4ef7\u503c**\uff1a\u5728\u56fe\u50cf\u3001\u81ea\u7136\u8bed\u8a00\u5904\u7406\u3001\u8bed\u97f3\u7b49\u591a\u4e2a\u9886\u57df\u6709\u5e7f\u6cdb\u5e94\u7528\uff0c\u4e3a\u667a\u80fd\u5b89\u9632\u3001\u667a\u80fd\u533b\u7597\u3001\u667a\u80fd\u4ea4\u901a\u7b49\u73b0\u5b9e\u573a\u666f\u63d0\u4f9b\u6280\u672f\u652f\u6301\uff0c\u5177\u6709\u5de8\u5927\u7684\u5546\u4e1a\u548c\u793e\u4f1a\u4ef7\u503c\u3002  ", "task_step_name": "\u5206\u6790\u8fd1\u51e0\u5e74\u7814\u7a76\u9886\u57df\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba", "task_step_description": "\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u76f8\u5173\u7814\u7a76\u9886\u57df\uff0cTransformer\u3001GAN\u3001BERT \u7b49\u6846\u67b6\u6210\u4e3a\u4f17\u591a\u8bba\u6587\u91c7\u7528\u7684\u6838\u5fc3\u6280\u672f\u3002\u8fd9\u4e9b\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u9002\u5e94\u6027\uff0c\u5176\u65b9\u6cd5\u8bba\u56f4\u7ed5\u7740\u5982\u4f55\u6709\u6548\u5904\u7406\u6570\u636e\u7279\u5f81\u3001\u4f18\u5316\u6a21\u578b\u7ed3\u6784\u4ee5\u63d0\u5347\u6027\u80fd\u3002\u4f8b\u5982\uff0cTransformer \u57fa\u4e8e\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6253\u7834\u4e86\u4f20\u7edf\u5e8f\u5217\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u7b49\u591a\u9886\u57df\u5e7f\u6cdb\u5e94\u7528\uff1bGAN \u901a\u8fc7\u751f\u6210\u5668\u4e0e\u5224\u522b\u5668\u7684\u5bf9\u6297\u8bad\u7ec3\uff0c\u5728\u56fe\u50cf\u751f\u6210\u7b49\u4efb\u52a1\u4e0a\u53d6\u5f97\u663e\u8457\u6210\u679c\uff1bBERT \u5219\u901a\u8fc7\u9884\u8bad\u7ec3\u65b9\u5f0f\uff0c\u6781\u5927\u63d0\u5347\u4e86\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u7684\u6027\u80fd\u3002", "task_step_level": "0", "task_step_question": "\u8fd1\u51e0\u5e74\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u76f8\u5173\u7814\u7a76\u9886\u57df\uff0cTransformer\u3001GAN\u3001BERT \u7b49\u6846\u67b6\u662f\u5982\u4f55\u5177\u4f53\u56f4\u7ed5\u6709\u6548\u5904\u7406\u6570\u636e\u7279\u5f81\u548c\u4f18\u5316\u6a21\u578b\u7ed3\u6784\u6765\u63d0\u5347\u6027\u80fd\u7684\uff1f ", "task_step_question_context": [{"ref_id": "454847723981641856", "chunk_id": "1", "score": 0.458984375, "text": "# 2 RELATED WORK\nPre-trained language models. Transformer (Vaswani et al., 2017) is a sequence-to-sequence model that heavily uses the concept of self-attention. Later on, numerous transformer-based models have been proposed and show overwhelming performance on natural language processing (NLP) and on computer vision (CV) tasks. Devlin et al. (2019) designed BERT that pre-train deep bidirectional representations from unlabeled text and reached powerful performance. Liu et al. (2019) found that BERT were terribly undertrained and proposed RoBERTa, an enhanced training recipe for BERT which can greatly boost the performance. He et al. (2020) proposed decoding-enhanced BERT with disentangled attention (DeBERTa) that incorporates the disentangled attention mechanism and an improved mask encoder to enhance BERT and RoBERTa. More variants like XL-Net, Albert, and Electra have also been proposed in recent years (Yang et al., 2019; Lan et al., 2019; Clark et al., 2019). The series of GPT models (Radford et al., 2019; Brown et al., 2020) are later developed based on transformers decoder blocks rather than encoder blocks like BERT, which again have shown superior performance on different tasks. These large models pretrained on a large amount of unlabelled texts would need to be further fine-tuned on downstream tasks for better performance.  \n\nOne of the accompanying disadvantages of these pre-training models with tremendous parameter counts (e.g., 175 B in GPT-3) is the unaffordable computational cost for further fine-tuning.  \n\nPruning and Low-rank decomposition. Pruning is a widely-used model compression technique. It can reduce the number of parameters inside models, which possibly brings training and inference efficiency. Along with weight pruning method (Han et al., 2015b) being one of the most effective methods (Gordon et al., 2020), various criterion have been proposed to select insignifi- cant weights for pruning, such as Taylor approximation (Molchanov et al., 2019), Hessian score approximation (Hassibi & Stork, 1993), and other saliency scores such as SNIP (Lee et al., 2018), GraSP (Wang et al., 2019) and SynFlow (Tanaka et al., 2020). Several pruning methods have been commonly adapted to compress language models (McCarley et al., 2019; Gordon et al., 2020; Sanh et al., 2020; Wang et al., 2020; Chen et al., 2021). Specifically, McCarley et al. (2019) proposed to prune attention heads that had less contribution to the model. Wang et al. (2020) pruned BERT models by involving low-rank factorization and $\\ell_{0}$ regularization. Sanh et al. (2020) invented an improved version of magnitude pruning ( i.e. , pruning based on the weight change) that can better suit the transfer learning. Chen et al. (2021) performed structured pruning on BERT via $\\ell_{1}$ sparse regularization, which reduced a large portion of parameters and decreased the training cost.  \n\nLow-rank approximation (Ye, 2005) is also vastly studied. One classical scenario is robust principle component analysis (Cand\\`es et al., 2011), which decomposes a matrix into a low-rank plus a sparse component. Existing literature shows that in deep learning, the learned over-parameterized models often naturally bear approximate low-rank weight structures (Oymak et al., 2019; Yu et al., 2017). Some (Jaderberg et al., 2014; Povey et al., 2018; Sainath et al., 2013; Zhang et al., 2014; Zhao et al., 2016) have explicitly imposed the low-rank constraint during training. Wang et al. (2020); Hu et al. (2021) utilized low-rank decomposition to shrink the model size and trim down the trainable parameters during fine-tuning. However, to our best knowledge, integrating sparsity and low-rank structures has never been studied before for efficient fine-tuning of pre-trained language models.  \n\nParameter-efficient adaptation. Parameter-efficient adaptation aims at reducing the number of trainable parameters when fine-tuning the models across different downstream domains. Unlike pruning, it generates sparse updates instead of building sparse models. Various approaches are invented to achieve the goal. Rebuffiet al. (2017); Houlsby et al. (2019) inserted and only trained adapters between existing layers, whose parameters are much less compared to the pretrained models. Guo et al. (2020) leveraged $\\ell_{0}$ regularization to limit the number of non-zero elements in the update vectors. Lester et al. (2021); Li & Liang (2021) introduced efficient prompt tuning which optimizes only a small continuous task-specific vector. Hu et al. (2021) proposed a low-rank decomposition-based method that can also significantly reduce the number of trainable parameters. However, fine-tuned models yielded by these methods work have the same amount of weights as the pre-trained starting point; hence they contribute no resource efficiency of the final model.\n\n# 3 METHODOLOGY\nIn this section, we begin by describing our notations and definitions of sparsity generation and parameter-efficient fine-tuning in Section 3.1. Then, we introduce the (dually) sparsity-embedded efficient fine-tuning algorithms in Sections 3.2 and 3.3.\n\n# 3.1 PRELIMINARIES\nSparsity generation and resource-efficient fine-tuning. We adopt both unstructured and structured pruning methods to produce sparsity. They can lead to resource-efficiency including memory and computation savings.  \n\n$\\boldsymbol{\\mathscr{W}}\\,\\in\\,\\mathbb{R}^{m\\times n}$ denote a weight matrix. The goal of ning is to fi a binary mas $s\\ \\in$ $\\{0,1\\}^{\\|\\mathcal{W}\\|_{0}}$ {results in a sparse weight structured pruning, it helps save computational cost since the sparse weights can be smaller in size }, where $\\|\\mathcal{W}\\|_{0}$ W \u2299S . For unstructured pruning, only memory cost is saved; but for umber of parameters in W. The mask Sis applied to Wand by wiping out all-zero columns or rows. However, the performance of networks after structured pruning is often shown to be inferior compared with the unstructured pruning counterpart.  \n\nParameter-efficient fine-tuning. stream rn task ific weight update To leverage $\\Delta{\\mathcal{W}}$ knowledge in pre-trained weights via fine-tuning and generate predictions with $\\mathcal{W}$ , downweights massive resources as the size of the pre-trained model increases. Parameter-efficient fine-tuning try W$\\mathcal{W}\\!+\\!\\Delta\\mathcal{W}$ W. Since $\\Delta{\\mathcal{W}}$ Whas the same size of W, learning the update matrices usually requires to solve this problem by using as few trainable parameters as possible to represent $\\Delta{\\mathcal{W}}$ , while maintaining competitive downstream fine-tuning performance. Previous literature reaches the goal via either sparsifying weigh a matrices $\\Delta{\\mathcal{W}}$ (Guo et al., 2020) or leveraging low-rank decomposed matrices to compute $\\Delta{\\mathcal{W}}$ W(Hu et al., 2021).  \n\n  \nFigure 1: The overview of our proposals. The sparse masks can have unstructured or structured patterns, hich leads to training and erence efficiency. During the fine-tuning, we only train decomposed matrices $\\mathcal{U}$ ,Vand non-zero elements in S$S_{2}$ 2 .  \n\n<html><body><table><tr><td>Algorithm 1: Sparsity-Embedded Low- Rank Decomposition</td></tr><tr><td>Input: Pretrained weights W, number of non-zero elements N Output: Sparse matrices S2 Initialize S2 to be an empty set. foreachself-attentionprojectionweights w inW do */</td></tr><tr><td>/+ Decomposition Perform matrix decomposition: w; \u2248 UV + S' by solving the optimization problem in Eqn.1. /* Identify important</td></tr><tr><td>elements to form S2 */ Perform thresholding on S': Keep N elements in S' with top magnitudes, and set the rest 0. Append S' into S2. end</td></tr></table></body></html>  \n\n<html><body><table><tr><td>Algorithm2:DSEE</td></tr><tr><td>Input: Pretrained weights W, number of non-zero elements N, desired sparsity s, loss function Output: Sparse mask S1, matrices U,V, S2 Decompose W into U,V and S2</td></tr><tr><td>(Re-)Initialization: U = 0, V ~ N(0,0.02),\u03a9 = indexes of non-zero elements in S2, S = 0 /\ufe62 I:train before pruning * Train U,V, S with respect to L under the</td></tr><tr><td>constraint of Poc (S) = 0. / II: pruning the model */</td></tr><tr><td>Prune (1-s%) parameters in W globally by sorting the magnitude of W + UV + S,</td></tr><tr><td>deriving the sparsity mask S1 /+ III:tuning after pruning * Tune U, V, S2 for E epochs for recovering the performance.</td></tr></table></body></html>"}, {"ref_id": "454847724073130116", "chunk_id": "3", "score": 0.40234375, "text": "# 2. Related work\n\n# 2.1. Transformer v.s. CNN\nTransformer [ 63 ] was introduced in 2017 for NLP tasks. Compared with LSTM, Transformer can not only train in parallel but also achieve better performance. Then many famous NLP models are built on Transformer, including GPT series [ 44 ,45 ,3 ,42 ], BERT [ 12 ], T5 [ 47 ], and OPT [80 ]. For the application of the Transformer in vision tasks, Vision Transformer (ViT) is definitely the seminal work, showing that Transformer can achieve impressive performance after large-scale supervised training. Followup works [ 61 ,76 ,65 ,66 ,18 ] like Swin [ 37 ] continually improve model performance, achieving new state-of-the-art on various vision tasks. These results seem to tell us \u201cAttention is all you need\u201d [ 63 ].  \n\nBut it is not that simple. ViT variants like DeiT usually adopt modern training procedures including various advanced techniques of data augmentation [ 10 ,9 ,79 ,77 ,83 ], regularization [ 57 ,25 ] and optimizers [ 28 ,39 ]. Wightman et al. find that with similar training procedures, the performance of ResNet can be largely improved. Besides, Yu et al. [74 ] argue that the general architecture instead of attention plays a key role in model performance. Han et al. [19 ] find by replacing attention in Swin with regular or dynamic depthwise convolution, the model can also obtain comparable performance. ConvNeXt [ 38 ], a remarkable work, modernizes ResNet into an advanced version with some designs from ViTs, and the resulting models consistently outperform Swin [ 37 ]. Other works like RepLKNet [13 ], VAN [ 17 ], FocalNets [ 72 ], HorNet [ 49 ], SLKNet [ 36 ], ConvFormer [ 75 ], Conv2Former [ 22 ], and InternImage [ 64 ]constantly improve performance of CNNs. Despite the high performance obtained, these models neglect efficiency, exhibiting lower speed than ConvNeXt. Actually, ConvNeXt is also not an efficient model compared with ResNet. We argue that CNN models should keep the original advantage of efficiency. Thus, in this paper, we aim to improve the model efficiency of CNNs while maintaining high performance.\n\n# 2.2. Convolution with large kernels.\nWell-known works, like AlexNet [ 30 ] and Inception v1 [56 ] already utilize large kernels up to $11\\times11$ and $7\\times7$ , respectively. To improve the efficiency of large kernels, VGG [53 ] proposes to heavily s $3\\times3$ convolutions e In$k\\times1$ $k\\times k$ tant for semantic segmentation and they decompose large \u00d7\u00d7on v3 [ staking sequentially. For depthwise convoluti [. Besides, Peng 60 ] splits kernels into several groups from 57 ] factorizes et al. $k\\times k$ find that large kernels are impor\u00d7convolution into $1\\times k$ $3\\times3$ \u00d7\u00d7and ixto kernels similar to Inception v3 [ 57 ]. Witnessing the success of Transformer in vision tasks [ 16 ,65 ,37 ], large-kernel convolution is more emphasized since it can offer a large receptive field to imitate attention [ 19 ,38 ]. For example, ConvNeXt adopts kernel size of $7\\times7$ for depthwise convolution by default. To employ larger kernels, RepLKNet [ 13 ]proposes to utilize structural re-parameterization techniques [78 ,14 ] to scale up kernel size to $31\\times31$ ; VAN [ 17 ] sequentially stacks large-kernel depth-wise convolution (DWConv) and depth-wise dilation convolution to obtain $21\\!\\times\\!21$ receptive filed; FocalNets employs a gating mechanism to fuse multi-level features from stacking depthwise convolutions; Recently, SLaK [ 36 ] fac es la ernel $k\\times k$ two small non-square kernels ( Different from these works, we do not aim to scale up larger $k\\!\\times\\!s$ \u00d7and $s\\!\\times\\!k$ \u00d7, where $s<k$ ). kernels. Instead, we target efficiency and decompose large kernels in a simple and speed-friendly way while keeping comparable performance.\n\n# 3. Method\n\n# 3.1. MetaNeXt\nFormulation of MetaNeXt Block. ConvNeXt [ 38 ] is a modern CNN model with simple architecture. For each ConvNeXt block, the input $X$ is first processed by a depthwise convolutioin to propagate information along spatial dimensions. We follow MetaFormer [ 74 ] to abstract the depthwise convolution as a token mixer which is responsible for spatial information interaction. Accordingly, as shown in the second subfigure in Figure 2 , the ConvNeXt is abstracted as MetaNeXt block. Formally, in a MetaNeXt block, its input $X$ is firstly processed as  \n\n$$\nX^{\\prime}=\\operatorname{TokenMixer}(X)\n$$  \n\nimport torch.nn as nn  \n\nAlgorithm 1 Inception Depthwise Convolution (PyTorchlike Code)   \n\n\n<html><body><table><tr><td>def</td><td>class InceptionDwConv2d(nn.Module): init__(self\uff0c in_channels,</td></tr><tr><td></td><td>square_kernel_size=3, band_kernel_size=11, branch_ratio=1/8): super().\u2014init()</td></tr><tr><td></td><td>gc = int(in_channels * branch_ratio) # channel number of a convolutionbranch</td></tr><tr><td></td><td>self.dwconv_hw = nn.Conv2d(gc, gc, square_kernel_size, padding= square_kernel_size//2, groups=gc)</td></tr><tr><td></td><td>self.dwconv_w = nn.Conv2d(gc, gc, kernel_size =(l, band_kernel_size), padding=(0,</td></tr><tr><td></td><td>band_kernel_size//2), groups=gc) self.dwconv_h = nn.Conv2d(gc, gc,. kernel_size =(band_kernel_size, l), padding=(</td></tr><tr><td></td><td>band_kernel_size//2, 0)\uff0c groups=gc) self.split_indexes = (gc, gc, gc, in_channels -\"3*gc)</td></tr><tr><td></td><td>def forward(self\uff0cx): # B\uff0c C\uff0c H,\uff0c W = x.shape</td></tr><tr><td></td><td>x_hw\uff0c x_w\uff0c x_h, x_id = torch.split(x\uff0c self.</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td>split_indexes, dim=l)</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td>return torch.cat(</td></tr><tr><td></td><td></td></tr><tr><td></td><td>(self.dwconv_hw(x_hw\uff09,</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td>self.dwconv_w(x_w),</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td>x_id),</td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td>self.dwconv_h(x_h),</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td>dim=1)</td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr></table></body></html>  \n\nwhere $X,X^{\\prime}\\in\\mathbb{R}^{B\\times C\\times H\\times W}$ with $B,C,H$ and $W$ denoting batch size, channel number, height and width, respectively. Then the output from the token mixer is normalized,  \n\n$$\nY=\\operatorname{Norm}(X^{\\prime})\n$$  \n\nAfter normalization [ 27 ,1 ], the resulting features are inputted into an MLP module consisting of two fullyconnected layers with an activation function sandwiched between them, the same as feed-forward network in Transformer [ 63 ]. The two fully-connected layers can also be implemented by $1\\times1$ convolutions. Also, shortcut connection [ 20 ,54 ] is adopted. This process can be expressed by  \n\n$$\nY=\\mathrm{Conv}_{1\\times1}^{r C\\to C}\\{\\sigma[\\mathrm{Conv}_{1\\times1}^{C\\to r C}(Y)]\\}+X,\n$$  \n\nwhere $\\mathrm{Conv}_{k\\times k}^{C_{i}\\rightarrow C_{o}}$ means convolution with kernel size of $k\\times k$ , input channels o \u00d7$C_{i}$ and output channels of $C_{o};r$ is the expansion ratio and \u03c3denotes activation function.  \n\nComparison to MetaFormer block. As shown in Figure 2 , it can be found that MetaNeXt block shares similar modules with MetaFormer block [ 74 ], e.g. token mixer and MLP. Nevertheless, a critical differentiation between the two models lies in the number of shortcut connections [20 ,54 ]. MetaNeXt block implements a single shortcut connection, whereas the MetaFormer block incorporates two, one for the token mixer and the other for the MLP. From this aspect, MetaNeXt block can be regarded as a result of merging two residual sub-blocks from MetaFormer, thereby simplifying the overall architecture. As a result, the MetaNeXt architecture exhibits a higher speed compared to MetaFormer. However, this simpler design comes with a limitation: the token mixer component in MetaNeXt cannot be complicated ( e.g. , Attention) as shown in our experiments (Table 3 ).  \n\nInstantiation to ConvNeXt. As shown in Figure 2 , in ConvNeXt, the token mixer is simply implemented by a depthwise convolution,  \n\n$$\nX^{\\prime}=\\operatorname{TokenMixer}(X)=\\operatorname{DWConv}_{k\\times k}^{C\\to C}(X)\n$$  \n\nkernel size of where $\\mathrm{DWConv}_{k\\times k}^{C\\rightarrow C}$ $k\\times k$ \u00d7. In ConvNeXt, denotes depthwise convolution with $k$ is set as 7 by default."}, {"ref_id": "454845616393828768", "chunk_id": "2", "score": 0.39453125, "text": "# 2 Related Work\n\n# 2.1 Transformers for Graph\nRecently, Transformer [ 33 ] has shown its superiority in an increasing number of domains [ 7 ,8 ,40 ], e.g. Bert [ 7 ] in NLP and ViT [ 8 ] in CV. Existing works attempting to generalize Transformer to graph data mainly focus on two problems: (1) How to design dedicated positional encoding for the nodes; (2) How to alleviate the quadratic computational complexity of the vanilla Transformer and scale the Graph Transformer to large graphs. As for the positional encoding, GT [ 9 ] firstly uses Laplacian eigenvectors to enhance node features. Graph-Bert [ 41 ] studies employing Weisfeiler-Lehman code to encode structural information. Graphormer [ 38 ] utilizes centrality encoding to enhance node features while incorporating edge information with spatial (SPD-indexed attention bias) and edge encoding. SAN [ 21 ] further replaces the static Laplacian eigenvectors with learnable positional encodings and designs an attention mechanism that distinguishes local connectivity. For the scalability issue, one immediate idea is to restrict the number of attending nodes. For example, GAT [ 34 ] and GT-Sparse [9 ] only consider the 1-hop neighboring nodes; Gophormer [ 46 ] uses GraphSAGE [ 11 ] sampling to uniformly sample ego-graphs with pre-defined maximum depth; Graph-Bert [ 41 ] restricts the receptive field of each node to the nodes with top$\\cdot\\mathbf{k}$ intimacy scores (e.g., Katz and PPR). However, these fixed node sampling strategies sacrifice the advantage of the Transformer architecture. SAC [22 ] tries to use an LSTM edge predictor to predict edges for self-attention operations. However, the fact that LSTM can hardly be parallelized reduces the computational efficiency of the Transformer.\n\n# 2.2 Sparse Transformers\nIn parallel, many efforts have been devoted to reducing the computational complexity of the Transformer in the field of NLP [ 23 ] and CV [ 32 ]. In the domain of NLP, Longformer [ 2 ] applies block-wise or strode patterns while only fixing on fixed neighbors. Reformer [ 19 ] replaces dot-product attention by using approximate attention computation based on locality-sensitive hashing. Routing Transformer [30] employs online $\\mathbf{k}$ -means clustering on the tokens. Linformer [35] demonstrates that the self-attention mechanism can be approximated by a low-rank matrix and reduces the complexity from ${\\mathcal{O}}(n^{2})$ scheme which brings greater efficiency by limiting self-attention computation to non-overlapping to $\\mathcal{O}(n)$ . As for vision transformers, Swin Transformer [ 24 ] proposes the shifted windowing local windows while also allowing for cross-window connection. Focal Transformer [ 37 ] presents a new mechanism incorporating both fine-grained local and coarse-grained global attention to capture short- and long-range visual dependencies efficiently. However, these sparse transformers do not take the unique graph properties into consideration.\n\n# 2.3 Graph Neural Networks and Node Sampling\nGraph neural networks (GNNs) [ 18 ,11 ,12 ,44 ,43 ,31 ,45 ,42 ] follow a message-passing schema that iteratively updates the representation of a node by aggregating representations from neighboring nodes. When generalizing to large graphs, Graph Neural Networks face a similar scalability issue. This is mainly due to the uncontrollable neighborhood expansion in the aggregation stage of GNN. Several node sampling algorithms have been proposed to limit the neighborhood expansion, which mainly falls into node-wise sampling methods and layer-wise sampling methods. In node-wise sampling, each node samples $k$ neighbors from its sampling distribution, then the total number of nodes in the $l$ -th layer becomes $\\mathcal{O}(\\breve{k}^{l})$ . GraphSage [ 11 ] is one of the most well-known node-wise sampling methods with the uniform sampling distribution. GCN-BS [ 25 ] introduces a variance reduced sampler based on multi-armed bandits. To alleviate the exponential neighbor expansion $\\mathcal{O}(k^{l})$ of the node-wise samplers, layer-wise samplers define the sampling distribution as a probability of sampling nodes given a set of nodes in the upper layer [ 4 ,16 ,49 ]. From another perspective, these sampling methods can also be categorized into fixed sampling strategies [ 11 ,4 ,49 ] and adaptive strategies [ 25 ,39 ]. However, none of the above sampling methods in GNNs can be directly applied in Graph Transformer as Graph Transformer does not follow the message passing schema.\n\n# 3 Preliminaries\n\n# 3.1 Problem Definition\nLet $G=(A,X)$ denote t ted graph where $A\\in\\mathbb{R}^{n\\times n}$ represents the symmetric adjac matrix with nnodes, and $X\\in\\mathbb{R}^{n\\times p}$ \u2208is the attribute matrix of pattributes r node. The element $A_{i j}$ in the adjacency matrix equals to 1 if there exists an edge between node $v_{i}$ and node $v_{j}$ , otherwise $A_{i j}~=~0$ . The label of node $v_{i}$ is $y_{i}$ . In the node classification problem, the classifier has the knowledge of the labels of a subset of nodes $V_{L}$ . The goal of semi-supervised node classification is to infer the labels of nodes in $V\\backslash V_{L}$ by learning a classification function.\n\n# 3.2 Transformer Architecture\nThe Transformer architecture consists of a series of Transformer layers [ 33 ]. Each Transformer layer has two parts: a multi-head self-attention (MHA) module and a position-wise feed-forward network (FFN). Let $\\mathbf{H}=\\left[h_{1},\\cdot\\cdot\\cdot\\mathbf{\\Phi},h_{m}\\right]^{\\top}\\in\\mathbb{R}^{m\\times d}$ denote the input to the self-attention module where $d$ is  \n\nthe hidden dimension, $h_{i}\\in\\mathbb{R}^{d\\times1}$ is the hidden represe ation at position $i$ , and $m$ is the number of positions. The MHA module firstly projects the input Hto query-, key-, value-spaces, denoted as $\\mathbf{Q},\\mathbf{K},\\mathbf{V}$ , using three matrices $\\mathbf{W}_{Q}\\in\\mathbb{R}^{d\\times d_{K}}$ ,$\\mathbf{W}_{K}\\in\\mathbb{R}^{d\\times d_{K}^{\\textbf{\\texttt{L}}}}$ and $\\dot{\\mathbf{W}_{V}}\\in\\mathbb{R}^{d\\times d_{V}}$ :  \n\n$$\n\\mathbf{Q}=\\mathbf{H}\\mathbf{W}_{Q},\\quad\\mathbf{K}=\\mathbf{H}\\mathbf{W}_{K},\\quad\\mathbf{V}=\\mathbf{H}\\mathbf{W}_{V}.\n$$  \n\nThen, in each head $i\\,\\in\\,\\{1,2,\\dots,B\\}$ ($B$ is the to heads), the scaled dot-product attention mechanism is applied to the corresponding {$\\{\\mathbf{Q}_{i},\\mathbf{K}_{i},\\mathbf{V}_{i}\\}$ }:  \n\n$$\n\\mathrm{head}_{i}=\\mathrm{Softmax}\\left(\\frac{\\mathbf{Q}_{i}\\mathbf{K}_{i}^{T}}{\\sqrt{d_{K}}}\\right)\\mathbf{V}_{i}.\n$$  \n\nFinally, the outputs from different heads are further concatenated and transformed to obtain the final output of MHA:  \n\n$$\n\\mathrm{MHA}(\\mathbf{H})=\\mathrm{Concat}\\left(\\mathbf{\\Pi}_{\\mathrm{head}_{1},\\mathbf{\\Pi}_{\\cdot}\\mathbf{\\Pi}_{\\cdot},\\mathrm{...},\\mathrm{head}_{B}}\\right)\\mathbf{W}_{O},\n$$  \n\nwhere $\\mathbf{W}_{O}\\in\\mathbb{R}^{d\\times d}$ . In this work, we employ $d_{K}=d_{V}=d/B$ ."}], "task_step_question_answer": "\u8fd9\u4e9b\u6280\u672f\u6846\u67b6\u7684\u6301\u7eed\u53d1\u5c55\u548c\u521b\u65b0\uff0c\u4e0d\u4ec5\u63a8\u52a8\u4e86\u8ba1\u7b97\u673a\u79d1\u5b66\u9886\u57df\u7684\u8fdb\u6b65\uff0c\u4e5f\u4e3a\u5176\u4ed6\u76f8\u5173\u5b66\u79d1\u5e26\u6765\u4e86\u65b0\u7684\u7814\u7a76\u601d\u8def\u548c\u65b9\u6cd5\u3002\u5728\u8de8\u5b66\u79d1\u7814\u7a76\u4e2d\uff0c\u8fd9\u4e9b\u6280\u672f\u6846\u67b6\u53d1\u6325\u7740\u8d8a\u6765\u8d8a\u91cd\u8981\u7684\u4f5c\u7528\u3002\u4f8b\u5982\u5728\u73af\u5883\u79d1\u5b66\u9886\u57df\uff0c\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u6846\u67b6\u53ef\u4ee5\u5bf9\u5927\u91cf\u7684\u6c14\u8c61\u6570\u636e\u3001\u5730\u7406\u4fe1\u606f\u6570\u636e\u7b49\u8fdb\u884c\u5206\u6790\u548c\u5904\u7406\uff0c\u9884\u6d4b\u6c14\u5019\u53d8\u5316\u8d8b\u52bf\uff0c\u4e3a\u73af\u5883\u4fdd\u62a4\u548c\u8d44\u6e90\u7ba1\u7406\u63d0\u4f9b\u79d1\u5b66\u4f9d\u636e\u3002\u5728\u793e\u4f1a\u79d1\u5b66\u9886\u57df\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6280\u672f\u6846\u67b6\u5bf9\u793e\u4ea4\u5a92\u4f53\u6570\u636e\u3001\u65b0\u95fb\u6587\u672c\u7b49\u8fdb\u884c\u6316\u6398\u548c\u5206\u6790\uff0c\u53ef\u4ee5\u4e86\u89e3\u516c\u4f17\u7684\u60c5\u7eea\u548c\u89c2\u70b9\uff0c\u4e3a\u653f\u7b56\u5236\u5b9a\u548c\u793e\u4f1a\u7814\u7a76\u63d0\u4f9b\u652f\u6301\u3002\u968f\u7740\u6280\u672f\u7684\u4e0d\u65ad\u8fdb\u6b65\uff0c\u8fd9\u4e9b\u6280\u672f\u6846\u67b6\u8fd8\u5c06\u4e0d\u65ad\u62d3\u5c55\u5176\u5e94\u7528\u9886\u57df\uff0c\u4e3a\u89e3\u51b3\u5404\u79cd\u590d\u6742\u7684\u5b9e\u9645\u95ee\u9898\u63d0\u4f9b\u66f4\u6709\u6548\u7684\u624b\u6bb5\u3002", "ref_task_step_id": ""}, "__type__": "task_step_node"}, "0b08a81b-65f3-4cf0-9626-683e37b4bdaa": {"__data__": {"id_": "0b08a81b-65f3-4cf0-9626-683e37b4bdaa", "metadata": {}, "relationships": {}, "hash": "", "start_task_context": "\u5927\u6a21\u578bMLA\u6280\u672f\u7ec6\u8282", "aemo_representation_context": "### 1. \u5206\u6790\u8fd1\u51e0\u5e74\u7814\u7a76\u9886\u57df\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba\n\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u76f8\u5173\u7814\u7a76\u9886\u57df\uff0cTransformer\u3001GAN\u3001BERT \u7b49\u6846\u67b6\u6210\u4e3a\u4f17\u591a\u8bba\u6587\u91c7\u7528\u7684\u6838\u5fc3\u6280\u672f\u3002\u8fd9\u4e9b\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u9002\u5e94\u6027\uff0c\u5176\u65b9\u6cd5\u8bba\u56f4\u7ed5\u7740\u5982\u4f55\u6709\u6548\u5904\u7406\u6570\u636e\u7279\u5f81\u3001\u4f18\u5316\u6a21\u578b\u7ed3\u6784\u4ee5\u63d0\u5347\u6027\u80fd\u3002\u4f8b\u5982\uff0cTransformer \u57fa\u4e8e\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6253\u7834\u4e86\u4f20\u7edf\u5e8f\u5217\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u7b49\u591a\u9886\u57df\u5e7f\u6cdb\u5e94\u7528\uff1bGAN \u901a\u8fc7\u751f\u6210\u5668\u4e0e\u5224\u522b\u5668\u7684\u5bf9\u6297\u8bad\u7ec3\uff0c\u5728\u56fe\u50cf\u751f\u6210\u7b49\u4efb\u52a1\u4e0a\u53d6\u5f97\u663e\u8457\u6210\u679c\uff1bBERT \u5219\u901a\u8fc7\u9884\u8bad\u7ec3\u65b9\u5f0f\uff0c\u6781\u5927\u63d0\u5347\u4e86\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u7684\u6027\u80fd\u3002\n\n### 2. \u7814\u7a76\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u4e3b\u8981\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0e\u53d8\u4f53\n - **Transformer**\uff1a\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u4e2d\uff0c\u5982\u673a\u5668\u7ffb\u8bd1\u3001\u6587\u672c\u5206\u7c7b\uff0cTransformer \u67b6\u6784\u4e0d\u65ad\u6f14\u53d8\uff0c\u51fa\u73b0\u4e86\u8bf8\u5982 ViT\uff08Vision Transformer\uff09\u7b49\u53d8\u4f53\u5e94\u7528\u4e8e\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\uff0c\u5c06\u56fe\u50cf\u89c6\u4e3a\u5e8f\u5217\u8fdb\u884c\u5904\u7406\uff0c\u5c55\u73b0\u51fa\u8de8\u9886\u57df\u7684\u5e94\u7528\u6f5c\u529b\u3002\n - **GAN**\uff1a\u5728\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\uff0cDCGAN\uff08Deep Convolutional GAN\uff09\u7b49\u53d8\u4f53\u901a\u8fc7\u6539\u8fdb\u7f51\u7edc\u7ed3\u6784\uff0c\u751f\u6210\u66f4\u903c\u771f\u7684\u56fe\u50cf\uff1b\u5728\u533b\u5b66\u56fe\u50cf\u5408\u6210\u3001\u89c6\u9891\u751f\u6210\u7b49\u9886\u57df\u4e5f\u6709\u5e94\u7528\uff0c\u901a\u8fc7\u8c03\u6574\u635f\u5931\u51fd\u6570\u548c\u7f51\u7edc\u67b6\u6784\u9002\u5e94\u4e0d\u540c\u4efb\u52a1\u9700\u6c42\u3002\n - **BERT**\uff1a\u9664\u4e86\u57fa\u7840\u7684\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u4efb\u52a1\uff0c\u8fd8\u5728\u4fe1\u606f\u68c0\u7d22\u3001\u60c5\u611f\u5206\u6790\u7b49\u4efb\u52a1\u4e2d\u5f97\u5230\u5e7f\u6cdb\u5e94\u7528\uff0c\u901a\u8fc7\u5fae\u8c03\u9884\u8bad\u7ec3\u6a21\u578b\u9002\u5e94\u7279\u5b9a\u4efb\u52a1\uff0c\u540c\u65f6\u51fa\u73b0\u4e86 RoBERTa \u7b49\u6539\u8fdb\u7248\u672c\uff0c\u4f18\u5316\u9884\u8bad\u7ec3\u7b56\u7565\u63d0\u5347\u6027\u80fd\u3002\n\n### 3. \u8bc4\u4f30\u5b66\u672f\u754c\u7684\u6280\u672f\u8fdb\u6b65\u4e0e\u5c40\u9650\u6027\n - **\u6280\u672f\u8fdb\u6b65**\uff1a\u6a21\u578b\u6027\u80fd\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u53d6\u5f97\u663e\u8457\u63d0\u5347\uff0c\u4f8b\u5982\u5728\u56fe\u50cf\u8bc6\u522b\u51c6\u786e\u7387\u3001\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u7684 F1 \u503c\u7b49\u6307\u6807\u4e0a\u4e0d\u65ad\u5237\u65b0\u8bb0\u5f55\u3002\u65b0\u7684\u6a21\u578b\u67b6\u6784\u548c\u8bad\u7ec3\u65b9\u6cd5\u4e0d\u65ad\u6d8c\u73b0\uff0c\u63a8\u52a8\u4e86\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\u3002\n - **\u5c40\u9650\u6027**\uff1a\u4f9d\u7136\u5b58\u5728\u6a21\u578b\u504f\u5dee\u548c\u6570\u636e\u4f9d\u8d56\u95ee\u9898\u3002\u6a21\u578b\u5728\u67d0\u4e9b\u7279\u5b9a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u5176\u4ed6\u6570\u636e\u96c6\u6216\u73b0\u5b9e\u573a\u666f\u4e2d\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002\u90e8\u5206\u6a21\u578b\u5bf9\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u4f9d\u8d56\u4e25\u91cd\uff0c\u6570\u636e\u8d28\u91cf\u548c\u6570\u91cf\u5f71\u54cd\u6a21\u578b\u6027\u80fd\uff0c\u4e14\u6a21\u578b\u89e3\u91ca\u6027\u5dee\uff0c\u96be\u4ee5\u7406\u89e3\u5176\u51b3\u7b56\u8fc7\u7a0b\u3002\n\n### 4. \u63a2\u8ba8\u8ba1\u7b97\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\n - **\u9002\u7528\u6027**\uff1a\u4e0d\u540c\u6a21\u578b\u5728\u7279\u5b9a\u6570\u636e\u96c6\u548c\u5e94\u7528\u573a\u666f\u4e0b\u6709\u5404\u81ea\u4f18\u52bf\u3002\u4f8b\u5982\uff0c\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6a21\u578b\u5728\u5927\u89c4\u6a21\u3001\u9ad8\u7ef4\u6570\u636e\u7684\u56fe\u50cf\u548c\u8bed\u97f3\u8bc6\u522b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff1b\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u5c0f\u6570\u636e\u3001\u7b80\u5355\u4efb\u52a1\u4e2d\u4ecd\u6709\u5e94\u7528\u4ef7\u503c\u3002\n - **\u6cdb\u5316\u80fd\u529b**\uff1a\u5c3d\u7ba1\u90e8\u5206\u6a21\u578b\u5728\u7279\u5b9a\u9886\u57df\u53d6\u5f97\u6210\u529f\uff0c\u4f46\u5728\u591a\u9886\u57df\u3001\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u7684\u73b0\u5b9e\u5e94\u7528\u4e2d\uff0c\u6cdb\u5316\u80fd\u529b\u9762\u4e34\u6311\u6218\u3002\u591a\u6a21\u6001\u6570\u636e\u7684\u5f02\u8d28\u6027\u548c\u590d\u6742\u6027\u4f7f\u5f97\u6a21\u578b\u96be\u4ee5\u6709\u6548\u63d0\u53d6\u548c\u878d\u5408\u7279\u5f81\uff0c\u9650\u5236\u4e86\u5176\u5728\u590d\u6742\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002\n\n### 5. \u5206\u6790\u6700\u65b0\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027\n - **\u7a33\u5b9a\u6027\u4f18\u5316**\uff1a\u4e00\u4e9b\u7b97\u6cd5\u9488\u5bf9\u7a33\u5b9a\u6027\u8fdb\u884c\u4e86\u4f18\u5316\uff0c\u4f8b\u5982\u91c7\u7528\u6b63\u5219\u5316\u6280\u672f\u3001\u5bf9\u6297\u8bad\u7ec3\u7b49\u65b9\u6cd5\u63d0\u9ad8\u6a21\u578b\u5728\u590d\u6742\u73af\u5883\u4e0b\u7684\u7a33\u5b9a\u6027\u3002\u5728\u52a8\u6001\u73af\u5883\u4e2d\uff0c\u90e8\u5206\u7b97\u6cd5\u901a\u8fc7\u5728\u7ebf\u5b66\u4e60\u3001\u81ea\u9002\u5e94\u8c03\u6574\u53c2\u6570\u7b49\u65b9\u5f0f\u4fdd\u6301\u6027\u80fd\u7a33\u5b9a\u3002\n - **\u5927\u89c4\u6a21\u6570\u636e\u9002\u5e94\u6027**\uff1a\u5728\u5927\u89c4\u6a21\u6570\u636e\u5904\u7406\u4e0a\uff0c\u90e8\u5206\u7b97\u6cd5\u901a\u8fc7\u5206\u5e03\u5f0f\u8bad\u7ec3\u3001\u6a21\u578b\u538b\u7f29\u7b49\u6280\u672f\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u548c\u9002\u5e94\u6027\u3002\u4f46\u4ecd\u6709\u7b97\u6cd5\u5728\u5904\u7406\u5927\u89c4\u6a21\u6570\u636e\u65f6\u9762\u4e34\u5185\u5b58\u3001\u8ba1\u7b97\u8d44\u6e90\u7b49\u9650\u5236\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002\n\n### 6. \u8bc4\u4f30\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218\n - **\u65b0\u7814\u7a76\u95ee\u9898**\uff1a\u63d0\u51fa\u4e86\u5982\u63d0\u5347\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u3001\u89e3\u51b3\u6a21\u578b\u504f\u5dee\u548c\u6570\u636e\u4f9d\u8d56\u3001\u589e\u5f3a\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u80fd\u529b\u7b49\u65b0\u7814\u7a76\u95ee\u9898\u3002\n - **\u7814\u7a76\u5207\u5165\u70b9**\uff1a\u4ece\u6539\u8fdb\u6a21\u578b\u67b6\u6784\u3001\u4f18\u5316\u8bad\u7ec3\u7b97\u6cd5\u3001\u63a2\u7d22\u65b0\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u7b49\u65b9\u9762\u5bfb\u627e\u65b0\u7684\u7814\u7a76\u5207\u5165\u70b9\u3002\u8fd9\u4e9b\u6311\u6218\u4fc3\u4f7f\u7814\u7a76\u8005\u4e0d\u65ad\u63a2\u7d22\u65b0\u7684\u6280\u672f\u548c\u65b9\u6cd5\uff0c\u63a8\u52a8\u540e\u7eed\u7814\u7a76\u671d\u7740\u66f4\u9ad8\u6548\u3001\u66f4\u667a\u80fd\u3001\u66f4\u5177\u6cdb\u5316\u80fd\u529b\u7684\u65b9\u5411\u53d1\u5c55\u3002\n\n### \u7814\u7a76\u6210\u679c\u3001\u65b9\u6cd5\u521b\u65b0\u6027\u4e0e\u5e94\u7528\u4ef7\u503c\u603b\u7ed3\n - **\u7814\u7a76\u6210\u679c**\uff1a\u5728\u6a21\u578b\u6027\u80fd\u63d0\u5347\u3001\u65b0\u67b6\u6784\u548c\u7b97\u6cd5\u63d0\u51fa\u7b49\u65b9\u9762\u53d6\u5f97\u663e\u8457\u6210\u679c\uff0c\u63a8\u52a8\u4e86\u8ba1\u7b97\u673a\u79d1\u5b66\u591a\u4e2a\u9886\u57df\u7684\u53d1\u5c55\u3002\n - **\u65b9\u6cd5\u521b\u65b0\u6027**\uff1a\u901a\u8fc7\u5f15\u5165\u65b0\u7684\u6280\u672f\u6846\u67b6\u3001\u6539\u8fdb\u8bad\u7ec3\u65b9\u6cd5\u548c\u4f18\u5316\u6a21\u578b\u7ed3\u6784\u7b49\u65b9\u5f0f\uff0c\u5c55\u73b0\u51fa\u8bf8\u591a\u521b\u65b0\u70b9\uff0c\u5982\u81ea\u6ce8\u610f\u529b\u673a\u5236\u3001\u5bf9\u6297\u8bad\u7ec3\u7b49\u3002\n - **\u5e94\u7528\u4ef7\u503c**\uff1a\u5728\u56fe\u50cf\u3001\u81ea\u7136\u8bed\u8a00\u5904\u7406\u3001\u8bed\u97f3\u7b49\u591a\u4e2a\u9886\u57df\u6709\u5e7f\u6cdb\u5e94\u7528\uff0c\u4e3a\u667a\u80fd\u5b89\u9632\u3001\u667a\u80fd\u533b\u7597\u3001\u667a\u80fd\u4ea4\u901a\u7b49\u73b0\u5b9e\u573a\u666f\u63d0\u4f9b\u6280\u672f\u652f\u6301\uff0c\u5177\u6709\u5de8\u5927\u7684\u5546\u4e1a\u548c\u793e\u4f1a\u4ef7\u503c\u3002  ", "task_step_name": "\u7814\u7a76\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u4e3b\u8981\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0e\u53d8\u4f53", "task_step_description": "- **Transformer**\uff1a\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u4e2d\uff0c\u5982\u673a\u5668\u7ffb\u8bd1\u3001\u6587\u672c\u5206\u7c7b\uff0cTransformer \u67b6\u6784\u4e0d\u65ad\u6f14\u53d8\uff0c\u51fa\u73b0\u4e86\u8bf8\u5982 ViT\uff08Vision Transformer\uff09\u7b49\u53d8\u4f53\u5e94\u7528\u4e8e\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\uff0c\u5c06\u56fe\u50cf\u89c6\u4e3a\u5e8f\u5217\u8fdb\u884c\u5904\u7406\uff0c\u5c55\u73b0\u51fa\u8de8\u9886\u57df\u7684\u5e94\u7528\u6f5c\u529b\u3002 - **GAN**\uff1a\u5728\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\uff0cDCGAN\uff08Deep Convolutional GAN\uff09\u7b49\u53d8\u4f53\u901a\u8fc7\u6539\u8fdb\u7f51\u7edc\u7ed3\u6784\uff0c\u751f\u6210\u66f4\u903c\u771f\u7684\u56fe\u50cf\uff1b\u5728\u533b\u5b66\u56fe\u50cf\u5408\u6210\u3001\u89c6\u9891\u751f\u6210\u7b49\u9886\u57df\u4e5f\u6709\u5e94\u7528\uff0c\u901a\u8fc7\u8c03\u6574\u635f\u5931\u51fd\u6570\u548c\u7f51\u7edc\u67b6\u6784\u9002\u5e94\u4e0d\u540c\u4efb\u52a1\u9700\u6c42\u3002 - **BERT**\uff1a\u9664\u4e86\u57fa\u7840\u7684\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u4efb\u52a1\uff0c\u8fd8\u5728\u4fe1\u606f\u68c0\u7d22\u3001\u60c5\u611f\u5206\u6790\u7b49\u4efb\u52a1\u4e2d\u5f97\u5230\u5e7f\u6cdb\u5e94\u7528\uff0c\u901a\u8fc7\u5fae\u8c03\u9884\u8bad\u7ec3\u6a21\u578b\u9002\u5e94\u7279\u5b9a\u4efb\u52a1\uff0c\u540c\u65f6\u51fa\u73b0\u4e86 RoBERTa \u7b49\u6539\u8fdb\u7248\u672c\uff0c\u4f18\u5316\u9884\u8bad\u7ec3\u7b56\u7565\u63d0\u5347\u6027\u80fd\u3002", "task_step_level": "1", "task_step_question": "Transformer\u3001GAN\u3001BERT\u8fd9\u4e09\u79cd\u4e3b\u8981\u6846\u67b6\u5728\u5404\u81ea\u4e0d\u540c\u4efb\u52a1\u7684\u5e94\u7528\u548c\u53d8\u4f53\u4e2d\uff0c\u54ea\u79cd\u53d8\u4f53\u5728\u8de8\u9886\u57df\u5e94\u7528\u6216\u6027\u80fd\u63d0\u5347\u65b9\u9762\u53d6\u5f97\u7684\u6548\u679c\u6700\u4e3a\u663e\u8457\uff0c\u4f9d\u636e\u662f\u4ec0\u4e48\uff1f ", "task_step_question_context": [{"ref_id": "454847723981641856", "chunk_id": "1", "score": 0.32421875, "text": "# 2 RELATED WORK\nPre-trained language models. Transformer (Vaswani et al., 2017) is a sequence-to-sequence model that heavily uses the concept of self-attention. Later on, numerous transformer-based models have been proposed and show overwhelming performance on natural language processing (NLP) and on computer vision (CV) tasks. Devlin et al. (2019) designed BERT that pre-train deep bidirectional representations from unlabeled text and reached powerful performance. Liu et al. (2019) found that BERT were terribly undertrained and proposed RoBERTa, an enhanced training recipe for BERT which can greatly boost the performance. He et al. (2020) proposed decoding-enhanced BERT with disentangled attention (DeBERTa) that incorporates the disentangled attention mechanism and an improved mask encoder to enhance BERT and RoBERTa. More variants like XL-Net, Albert, and Electra have also been proposed in recent years (Yang et al., 2019; Lan et al., 2019; Clark et al., 2019). The series of GPT models (Radford et al., 2019; Brown et al., 2020) are later developed based on transformers decoder blocks rather than encoder blocks like BERT, which again have shown superior performance on different tasks. These large models pretrained on a large amount of unlabelled texts would need to be further fine-tuned on downstream tasks for better performance.  \n\nOne of the accompanying disadvantages of these pre-training models with tremendous parameter counts (e.g., 175 B in GPT-3) is the unaffordable computational cost for further fine-tuning.  \n\nPruning and Low-rank decomposition. Pruning is a widely-used model compression technique. It can reduce the number of parameters inside models, which possibly brings training and inference efficiency. Along with weight pruning method (Han et al., 2015b) being one of the most effective methods (Gordon et al., 2020), various criterion have been proposed to select insignifi- cant weights for pruning, such as Taylor approximation (Molchanov et al., 2019), Hessian score approximation (Hassibi & Stork, 1993), and other saliency scores such as SNIP (Lee et al., 2018), GraSP (Wang et al., 2019) and SynFlow (Tanaka et al., 2020). Several pruning methods have been commonly adapted to compress language models (McCarley et al., 2019; Gordon et al., 2020; Sanh et al., 2020; Wang et al., 2020; Chen et al., 2021). Specifically, McCarley et al. (2019) proposed to prune attention heads that had less contribution to the model. Wang et al. (2020) pruned BERT models by involving low-rank factorization and $\\ell_{0}$ regularization. Sanh et al. (2020) invented an improved version of magnitude pruning ( i.e. , pruning based on the weight change) that can better suit the transfer learning. Chen et al. (2021) performed structured pruning on BERT via $\\ell_{1}$ sparse regularization, which reduced a large portion of parameters and decreased the training cost.  \n\nLow-rank approximation (Ye, 2005) is also vastly studied. One classical scenario is robust principle component analysis (Cand\\`es et al., 2011), which decomposes a matrix into a low-rank plus a sparse component. Existing literature shows that in deep learning, the learned over-parameterized models often naturally bear approximate low-rank weight structures (Oymak et al., 2019; Yu et al., 2017). Some (Jaderberg et al., 2014; Povey et al., 2018; Sainath et al., 2013; Zhang et al., 2014; Zhao et al., 2016) have explicitly imposed the low-rank constraint during training. Wang et al. (2020); Hu et al. (2021) utilized low-rank decomposition to shrink the model size and trim down the trainable parameters during fine-tuning. However, to our best knowledge, integrating sparsity and low-rank structures has never been studied before for efficient fine-tuning of pre-trained language models.  \n\nParameter-efficient adaptation. Parameter-efficient adaptation aims at reducing the number of trainable parameters when fine-tuning the models across different downstream domains. Unlike pruning, it generates sparse updates instead of building sparse models. Various approaches are invented to achieve the goal. Rebuffiet al. (2017); Houlsby et al. (2019) inserted and only trained adapters between existing layers, whose parameters are much less compared to the pretrained models. Guo et al. (2020) leveraged $\\ell_{0}$ regularization to limit the number of non-zero elements in the update vectors. Lester et al. (2021); Li & Liang (2021) introduced efficient prompt tuning which optimizes only a small continuous task-specific vector. Hu et al. (2021) proposed a low-rank decomposition-based method that can also significantly reduce the number of trainable parameters. However, fine-tuned models yielded by these methods work have the same amount of weights as the pre-trained starting point; hence they contribute no resource efficiency of the final model.\n\n# 3 METHODOLOGY\nIn this section, we begin by describing our notations and definitions of sparsity generation and parameter-efficient fine-tuning in Section 3.1. Then, we introduce the (dually) sparsity-embedded efficient fine-tuning algorithms in Sections 3.2 and 3.3.\n\n# 3.1 PRELIMINARIES\nSparsity generation and resource-efficient fine-tuning. We adopt both unstructured and structured pruning methods to produce sparsity. They can lead to resource-efficiency including memory and computation savings.  \n\n$\\boldsymbol{\\mathscr{W}}\\,\\in\\,\\mathbb{R}^{m\\times n}$ denote a weight matrix. The goal of ning is to fi a binary mas $s\\ \\in$ $\\{0,1\\}^{\\|\\mathcal{W}\\|_{0}}$ {results in a sparse weight structured pruning, it helps save computational cost since the sparse weights can be smaller in size }, where $\\|\\mathcal{W}\\|_{0}$ W \u2299S . For unstructured pruning, only memory cost is saved; but for umber of parameters in W. The mask Sis applied to Wand by wiping out all-zero columns or rows. However, the performance of networks after structured pruning is often shown to be inferior compared with the unstructured pruning counterpart.  \n\nParameter-efficient fine-tuning. stream rn task ific weight update To leverage $\\Delta{\\mathcal{W}}$ knowledge in pre-trained weights via fine-tuning and generate predictions with $\\mathcal{W}$ , downweights massive resources as the size of the pre-trained model increases. Parameter-efficient fine-tuning try W$\\mathcal{W}\\!+\\!\\Delta\\mathcal{W}$ W. Since $\\Delta{\\mathcal{W}}$ Whas the same size of W, learning the update matrices usually requires to solve this problem by using as few trainable parameters as possible to represent $\\Delta{\\mathcal{W}}$ , while maintaining competitive downstream fine-tuning performance. Previous literature reaches the goal via either sparsifying weigh a matrices $\\Delta{\\mathcal{W}}$ (Guo et al., 2020) or leveraging low-rank decomposed matrices to compute $\\Delta{\\mathcal{W}}$ W(Hu et al., 2021).  \n\n  \nFigure 1: The overview of our proposals. The sparse masks can have unstructured or structured patterns, hich leads to training and erence efficiency. During the fine-tuning, we only train decomposed matrices $\\mathcal{U}$ ,Vand non-zero elements in S$S_{2}$ 2 .  \n\n<html><body><table><tr><td>Algorithm 1: Sparsity-Embedded Low- Rank Decomposition</td></tr><tr><td>Input: Pretrained weights W, number of non-zero elements N Output: Sparse matrices S2 Initialize S2 to be an empty set. foreachself-attentionprojectionweights w inW do */</td></tr><tr><td>/+ Decomposition Perform matrix decomposition: w; \u2248 UV + S' by solving the optimization problem in Eqn.1. /* Identify important</td></tr><tr><td>elements to form S2 */ Perform thresholding on S': Keep N elements in S' with top magnitudes, and set the rest 0. Append S' into S2. end</td></tr></table></body></html>  \n\n<html><body><table><tr><td>Algorithm2:DSEE</td></tr><tr><td>Input: Pretrained weights W, number of non-zero elements N, desired sparsity s, loss function Output: Sparse mask S1, matrices U,V, S2 Decompose W into U,V and S2</td></tr><tr><td>(Re-)Initialization: U = 0, V ~ N(0,0.02),\u03a9 = indexes of non-zero elements in S2, S = 0 /\ufe62 I:train before pruning * Train U,V, S with respect to L under the</td></tr><tr><td>constraint of Poc (S) = 0. / II: pruning the model */</td></tr><tr><td>Prune (1-s%) parameters in W globally by sorting the magnitude of W + UV + S,</td></tr><tr><td>deriving the sparsity mask S1 /+ III:tuning after pruning * Tune U, V, S2 for E epochs for recovering the performance.</td></tr></table></body></html>"}, {"ref_id": "454984242683452570", "chunk_id": "1", "score": 0.296875, "text": "# 3. BERT Revisited\n\n# 3.1. BERT in Vision Problems\nTransformers and deep learning have significantly improved results for many tasks in computer vision [ 1 ,7 ,9 ,22 ,23 ,26 ,27 ,30 ,40 ,41 ]. Worth mentioning is Vision Transformer (ViT) [ 7 ], one of the first research efforts at the intersection of Transformers and computer vision. Unlike the traditional CNN network, ViT splits an image into a sequence of patches and applies the Transformers-based framework directly. Inspired by the success of BERT in Natural Language Processing (NLP), Bidirectional Encoder representation from Image Transformers (BEiT) [ 1 ] is presented as a self-supervised learning framework in computer vision. In particular, image patches are tokenized using DALL-E [ 32 ] to the visual tokens. These tokens are then randomly masked before feeding into the transformer backbone. The training objective is to recover the original visual tokens from the corrupted patches. These methods [ 1 ,38 ]have marked a remarkable improvement compared to supervised learning methods by leveraging large-scale unlabelled datasets, e.g., ImageNet-1K, ImagNet-21K [ 33 ], to discover semantic information.\n\n# 3.2. Limitations of BERT in Vision Problems\nOne limitation of using BERT in vision problems is the tokenization step. In the NLP field, a token has precisely one word mapped into it. In vision problems, however, many possible images or patches can share the same token as long as they have the same content. Therefore, designing a BERT model to mask a token and train a prediction model in the missing contexts in computer vision is more challenging than NLP. In addition, the tokenizer , i.e., DALLE [ 32 ], is not robust enough to map similar contexts to a token. It yields noise in the tokenization process and affects the overall training performance. He et al., [ 9 ] presented a Masked Auto Encoder (MAE) that utilizes the BERT framework. Instead of tokenizing images, it eliminates patches of an image via a random masking strategy and reconstructs the context of these masked patches to the original content. Although this method can avoid using the tokenizer, it only considers the context inside an image. Thus, it does not apply to micro-expression, which requires understanding semantic information from consecutive video frames. In this paper, $\\mu$ -BERT is presented to address these limitations.\n\n# 4. The Proposed $\\mu$ -BERT Approach\n$\\mu$ -BERT is designed to model micro-changes of facial texture across temporal dimensions, which is hard to observe by unaided human eyes via a reconstruction process. The proposed $\\mu$ -BERT architecture, shown in Figure 2 , consists of five main blocks: a $\\mu$ -Encoder ,Patch of Interest $(P o I)$ ,Blockwise Swapping ,Diagonal Micro Attention (DMA) , and a $\\mu$ -Decoder . Given input images $I_{t}$ and $I_{t+\\delta}$ , the role of the $\\mu$ -Encoder is to represent $I_{t}$ and $I_{t+\\delta}$ into latent vectors. Then, Patch of Interest (PoI) constrains $\\mu$ -BERT to look into facial regions containing microexpressions rather than unrelated regions such as the background. Blockwise Swapping and Diagonal Micro Attention (DMA) allow the model to focus on facial regions that primarily consist of micro differences between frames. Finally, $\\mu$ -Decoder reconstructs the output signal back to the determined one. Compared to prior works, $\\mu$ -BERT can adaptively focus on changes in facial regions while ignoring the ones in the background and effectively recognizes micro-expressions even when face movements occur. Moreover, $\\mu$ -BERT can also alleviate the dependency on the accuracy of alignment approaches in pre-processing step.  \n\n  \nFigure 2. An overview of the proposed $\\mu$ -BERT approach to facial micro-expression recognition.\n\n# 4.1. Non-overlapping Patches Representation\nIn $\\mu$ -BERT, an input frame $I_{t}\\,\\,\\in\\,\\,\\mathbb{R}^{H\\times W\\times C}$ is divided into a set of several non-overlapping patches Pas Eqn. ( 1 ).  \n\n$$\n\\mathcal{P}_{t}=\\{p_{t}^{i}\\}_{i=0}^{N_{p}-1}\\qquad|\\mathcal{P}_{t}|=H W/({p s}^{2})\n$$  \n\nwhere $H,W,C$ are the height, width, and number of channels, respectively. Each patch $p_{t}^{i}$ sution of $p s\\times p s$ .In our experiments, $H=W=224$ ,$C=3$ , and $p s=8$ .\n\n# 4.2. $\\mu$ -Encoder\nEach patch $p_{i}~\\in~\\mathcal{P}_{t}$ is linear d into a latent vector of dimension fixed positional encoding [ $d$ denoted as 42 ]. Then, an image $\\mathbf{z}_{t}^{i}\\in\\mathbb{R}^{1\\times d}$ \u2208, wit $I_{t}$ additive can be represented as in Eqn. ( 2 ).  \n\n$$\n\\begin{array}{r l}&{\\mathbf{Z}_{t}=c o n c a t\\left[\\mathbf{z}_{t}^{0},\\mathbf{z}_{t}^{1},...\\mathbf{z}_{t}^{N_{p}-1}\\right]\\in\\mathbb{R}^{N_{p}\\times d}}\\\\ &{\\mathbf{z}_{t}^{i}=\\alpha(p_{t}^{i})+\\mathbf{e}(i)}\\end{array}\n$$  \n\nwhere $\\alpha$ and $\\mathbf{e}$ are the projection embedding network and positional embedding, respectively. Let $\\mu$ -Encoder, denoted as $\\mathcal{E}$ , be a stack of continuous blocks. Each block consists of alternating layers of Multi Head Attention (MHA) and Multi-Layer Perceptron (MLP), as illustrated in Figure 3 .The Layer Norm (LN) is employed to the input signal before feeding to MHA and MLP layers, as in Eqn. ( 3 ).  \n\n  \nFigure 3. Builing block of Encoder and Decoder. Each block includes Multi-Head Attention (MHA) and Layer Normalization.  \n\n$$\n\\begin{array}{r l}&{\\mathbf{x}_{\\iota}^{\\prime}\\iota=\\mathbf{x}_{\\iota-1}+\\mathrm{MHA}(\\mathrm{LN}(\\mathbf{x}_{\\iota-1}))}\\\\ &{\\mathbf{x}_{\\iota}=\\mathbf{x}_{\\iota}^{\\prime}+\\mathrm{MLP}(\\mathrm{LN}(\\mathbf{x}_{\\iota}^{\\prime}))}\\\\ &{\\mathbf{x}_{0}=\\mathbf{Z}_{\\iota},\\,1\\leq\\iota\\leq L_{e}}\\end{array}\n$$  \n\nwhere $L_{e}$ is the number of blocks in $\\mathcal{E}$ . Given $\\mathbf{Z}_{t}$ , The output latent vector $\\mathbf{P_{t}}$ is represented as in Eqn. ( 4 ).  \n\n$$\n\\mathbf{P}_{t}=\\mathcal{E}(\\mathbf{Z}_{t})\\qquad\\mathbf{P}_{t}\\in\\mathbb{R}^{N_{p}\\times d}\n$$"}, {"ref_id": "454895298648999436", "chunk_id": "0", "score": 0.263671875, "text": "# 2. Related work\n\n# 2.1. Transformer v.s. CNN\nTransformer [ 63 ] was introduced in 2017 for NLP tasks. Compared with LSTM, Transformer can not only train in parallel but also achieve better performance. Then many famous NLP models are built on Transformer, including GPT series [ 44 ,45 ,3 ,42 ], BERT [ 12 ], T5 [ 47 ], and OPT [80 ]. For the application of the Transformer in vision tasks, Vision Transformer (ViT) is definitely the seminal work, showing that Transformer can achieve impressive performance after large-scale supervised training. Followup works [ 61 ,76 ,65 ,66 ,18 ] like Swin [ 37 ] continually improve model performance, achieving new state-of-the-art on various vision tasks. These results seem to tell us \u201cAttention is all you need\u201d [ 63 ].  \n\nBut it is not that simple. ViT variants like DeiT usually adopt modern training procedures including various advanced techniques of data augmentation [ 10 ,9 ,79 ,77 ,83 ], regularization [ 57 ,25 ] and optimizers [ 28 ,39 ]. Wightman et al. find that with similar training procedures, the performance of ResNet can be largely improved. Besides, Yu et al. [74 ] argue that the general architecture instead of attention plays a key role in model performance. Han et al. [19 ] find by replacing attention in Swin with regular or dynamic depthwise convolution, the model can also obtain comparable performance. ConvNeXt [ 38 ], a remarkable work, modernizes ResNet into an advanced version with some designs from ViTs, and the resulting models consistently outperform Swin [ 37 ]. Other works like RepLKNet [13 ], VAN [ 17 ], FocalNets [ 72 ], HorNet [ 49 ], SLKNet [ 36 ], ConvFormer [ 75 ], Conv2Former [ 22 ], and InternImage [ 64 ]constantly improve performance of CNNs. Despite the high performance obtained, these models neglect efficiency, exhibiting lower speed than ConvNeXt. Actually, ConvNeXt is also not an efficient model compared with ResNet. We argue that CNN models should keep the original advantage of efficiency. Thus, in this paper, we aim to improve the model efficiency of CNNs while maintaining high performance.\n\n# 2.2. Convolution with large kernels.\nWell-known works, like AlexNet [ 30 ] and Inception v1 [56 ] already utilize large kernels up to $11\\times11$ and $7\\times7$ , respectively. To improve the efficiency of large kernels, VGG [53 ] proposes to heavily s $3\\times3$ convolutions e In$k\\times1$ $k\\times k$ tant for semantic segmentation and they decompose large \u00d7\u00d7on v3 [ staking sequentially. For depthwise convoluti [. Besides, Peng 60 ] splits kernels into several groups from 57 ] factorizes et al. $k\\times k$ find that large kernels are impor\u00d7convolution into $1\\times k$ $3\\times3$ \u00d7\u00d7and ixto kernels similar to Inception v3 [ 57 ]. Witnessing the success of Transformer in vision tasks [ 16 ,65 ,37 ], large-kernel convolution is more emphasized since it can offer a large receptive field to imitate attention [ 19 ,38 ]. For example, ConvNeXt adopts kernel size of $7\\times7$ for depthwise convolution by default. To employ larger kernels, RepLKNet [ 13 ]proposes to utilize structural re-parameterization techniques [78 ,14 ] to scale up kernel size to $31\\times31$ ; VAN [ 17 ] sequentially stacks large-kernel depth-wise convolution (DWConv) and depth-wise dilation convolution to obtain $21\\!\\times\\!21$ receptive filed; FocalNets employs a gating mechanism to fuse multi-level features from stacking depthwise convolutions; Recently, SLaK [ 36 ] fac es la ernel $k\\times k$ two small non-square kernels ( Different from these works, we do not aim to scale up larger $k\\!\\times\\!s$ \u00d7and $s\\!\\times\\!k$ \u00d7, where $s<k$ ). kernels. Instead, we target efficiency and decompose large kernels in a simple and speed-friendly way while keeping comparable performance.\n\n# 3. Method\n\n# 3.1. MetaNeXt\nFormulation of MetaNeXt Block. ConvNeXt [ 38 ] is a modern CNN model with simple architecture. For each ConvNeXt block, the input $X$ is first processed by a depthwise convolutioin to propagate information along spatial dimensions. We follow MetaFormer [ 74 ] to abstract the depthwise convolution as a token mixer which is responsible for spatial information interaction. Accordingly, as shown in the second subfigure in Figure 2 , the ConvNeXt is abstracted as MetaNeXt block. Formally, in a MetaNeXt block, its input $X$ is firstly processed as  \n\n$$\nX^{\\prime}=\\operatorname{TokenMixer}(X)\n$$  \n\nimport torch.nn as nn  \n\nAlgorithm 1 Inception Depthwise Convolution (PyTorchlike Code)   \n\n\n<html><body><table><tr><td>def</td><td>class InceptionDwConv2d(nn.Module): init__(self\uff0c in_channels,</td></tr><tr><td></td><td>square_kernel_size=3, band_kernel_size=11, branch_ratio=1/8): super().\u2014init()</td></tr><tr><td></td><td>gc = int(in_channels * branch_ratio) # channel number of a convolutionbranch</td></tr><tr><td></td><td>self.dwconv_hw = nn.Conv2d(gc, gc, square_kernel_size, padding= square_kernel_size//2, groups=gc)</td></tr><tr><td></td><td>self.dwconv_w = nn.Conv2d(gc, gc, kernel_size =(l, band_kernel_size), padding=(0,</td></tr><tr><td></td><td>band_kernel_size//2), groups=gc) self.dwconv_h = nn.Conv2d(gc, gc,. kernel_size =(band_kernel_size, l), padding=(</td></tr><tr><td></td><td>band_kernel_size//2, 0)\uff0c groups=gc) self.split_indexes = (gc, gc, gc, in_channels -\"3*gc)</td></tr><tr><td></td><td>def forward(self\uff0cx): # B\uff0c C\uff0c H,\uff0c W = x.shape</td></tr><tr><td></td><td>x_hw\uff0c x_w\uff0c x_h, x_id = torch.split(x\uff0c self.</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td>split_indexes, dim=l)</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td>return torch.cat(</td></tr><tr><td></td><td></td></tr><tr><td></td><td>(self.dwconv_hw(x_hw\uff09,</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td>self.dwconv_w(x_w),</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td>x_id),</td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td>self.dwconv_h(x_h),</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td>dim=1)</td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr></table></body></html>  \n\nwhere $X,X^{\\prime}\\in\\mathbb{R}^{B\\times C\\times H\\times W}$ with $B,C,H$ and $W$ denoting batch size, channel number, height and width, respectively. Then the output from the token mixer is normalized,  \n\n$$\nY=\\operatorname{Norm}(X^{\\prime})\n$$  \n\nAfter normalization [ 27 ,1 ], the resulting features are inputted into an MLP module consisting of two fullyconnected layers with an activation function sandwiched between them, the same as feed-forward network in Transformer [ 63 ]. The two fully-connected layers can also be implemented by $1\\times1$ convolutions. Also, shortcut connection [ 20 ,54 ] is adopted. This process can be expressed by  \n\n$$\nY=\\mathrm{Conv}_{1\\times1}^{r C\\to C}\\{\\sigma[\\mathrm{Conv}_{1\\times1}^{C\\to r C}(Y)]\\}+X,\n$$  \n\nwhere $\\mathrm{Conv}_{k\\times k}^{C_{i}\\rightarrow C_{o}}$ means convolution with kernel size of $k\\times k$ , input channels o \u00d7$C_{i}$ and output channels of $C_{o};r$ is the expansion ratio and \u03c3denotes activation function.  \n\nComparison to MetaFormer block. As shown in Figure 2 , it can be found that MetaNeXt block shares similar modules with MetaFormer block [ 74 ], e.g. token mixer and MLP. Nevertheless, a critical differentiation between the two models lies in the number of shortcut connections [20 ,54 ]. MetaNeXt block implements a single shortcut connection, whereas the MetaFormer block incorporates two, one for the token mixer and the other for the MLP. From this aspect, MetaNeXt block can be regarded as a result of merging two residual sub-blocks from MetaFormer, thereby simplifying the overall architecture. As a result, the MetaNeXt architecture exhibits a higher speed compared to MetaFormer. However, this simpler design comes with a limitation: the token mixer component in MetaNeXt cannot be complicated ( e.g. , Attention) as shown in our experiments (Table 3 ).  \n\nInstantiation to ConvNeXt. As shown in Figure 2 , in ConvNeXt, the token mixer is simply implemented by a depthwise convolution,  \n\n$$\nX^{\\prime}=\\operatorname{TokenMixer}(X)=\\operatorname{DWConv}_{k\\times k}^{C\\to C}(X)\n$$  \n\nkernel size of where $\\mathrm{DWConv}_{k\\times k}^{C\\rightarrow C}$ $k\\times k$ \u00d7. In ConvNeXt, denotes depthwise convolution with $k$ is set as 7 by default."}], "task_step_question_answer": "\u5728\u672a\u6765\u7684\u7814\u7a76\u4e2d\uff0cTransformer \u7684\u53d8\u4f53\u53ef\u80fd\u4f1a\u8fdb\u4e00\u6b65\u6269\u5c55\u5230\u66f4\u591a\u975e\u5e8f\u5217\u6570\u636e\u7684\u5904\u7406\u4efb\u52a1\u4e2d\uff0c\u4f8b\u5982\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u548c\u56fe\u6570\u636e\u5904\u7406\u3002\u6700\u8fd1\u7684\u7814\u7a76\u8868\u660e\uff0cTransformer \u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f8b\u5982 Temporal Fusion Transformer (TFT) \u901a\u8fc7\u5f15\u5165\u65f6\u95f4\u6ce8\u610f\u529b\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002\u5728\u56fe\u6570\u636e\u5904\u7406\u65b9\u9762\uff0cGraph Transformer \u901a\u8fc7\u7ed3\u5408\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u548c\u56fe\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5df2\u7ecf\u5728\u793e\u4ea4\u7f51\u7edc\u5206\u6790\u548c\u63a8\u8350\u7cfb\u7edf\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6210\u679c\u3002\u4f8b\u5982\uff0cGraph Transformer \u5728\u793e\u4ea4\u7f51\u7edc\u4e2d\u7684\u8282\u70b9\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0c\u901a\u8fc7\u6355\u6349\u8282\u70b9\u4e4b\u95f4\u7684\u590d\u6742\u5173\u7cfb\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u7cbe\u5ea6\u3002\u6b64\u5916\uff0cTransformer \u5728\u97f3\u9891\u4fe1\u53f7\u5904\u7406\u4e2d\u7684\u5e94\u7528\u4e5f\u53d6\u5f97\u4e86\u91cd\u8981\u8fdb\u5c55\uff0c\u5982 Conformer \u6a21\u578b\u5728\u8bed\u97f3\u8bc6\u522b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u901a\u8fc7\u7ed3\u5408\u5377\u79ef\u548c\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bed\u97f3\u8bc6\u522b\u7684\u51c6\u786e\u6027\u3002\u5728\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u65b9\u9762\uff0cCLIP \u6a21\u578b\u901a\u8fc7\u8054\u5408\u8bad\u7ec3\u56fe\u50cf\u548c\u6587\u672c\u7f16\u7801\u5668\uff0c\u5b9e\u73b0\u4e86\u8de8\u6a21\u6001\u7684\u8bed\u4e49\u7406\u89e3\uff0c\u4e3a\u56fe\u50cf\u68c0\u7d22\u548c\u6587\u672c\u751f\u6210\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002Perceiver \u548c Flamingo \u7b49\u53d8\u4f53\u8fdb\u4e00\u6b65\u6269\u5c55\u4e86 Transformer \u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\uff0c\u901a\u8fc7\u5904\u7406\u590d\u6742\u7684\u591a\u6a21\u6001\u6570\u636e\uff08\u5982\u6587\u672c-\u56fe\u50cf-\u97f3\u9891\uff09\u6765\u63d0\u5347\u8de8\u6a21\u6001\u7406\u89e3\u7684\u80fd\u529b\u3002\n\nGAN \u7684\u53d8\u4f53\u53ef\u80fd\u4f1a\u901a\u8fc7\u5f15\u5165\u66f4\u590d\u6742\u7684\u751f\u6210\u5668\u548c\u5224\u522b\u5668\u7ed3\u6784\uff0c\u8fdb\u4e00\u6b65\u63d0\u5347\u751f\u6210\u56fe\u50cf\u7684\u8d28\u91cf\u548c\u591a\u6837\u6027\u3002StyleGAN \u901a\u8fc7\u5f15\u5165\u98ce\u683c\u8fc1\u79fb\u673a\u5236\uff0c\u80fd\u591f\u751f\u6210\u66f4\u52a0\u903c\u771f\u548c\u591a\u6837\u5316\u7684\u56fe\u50cf\uff0c\u4e3a\u827a\u672f\u521b\u4f5c\u548c\u56fe\u50cf\u7f16\u8f91\u63d0\u4f9b\u4e86\u65b0\u7684\u5de5\u5177\u3002BigGAN \u5219\u901a\u8fc7\u589e\u52a0\u6a21\u578b\u5bb9\u91cf\u548c\u4f18\u5316\u8bad\u7ec3\u7b56\u7565\uff0c\u751f\u6210\u4e86\u66f4\u9ad8\u5206\u8fa8\u7387\u548c\u66f4\u903c\u771f\u7684\u56fe\u50cf\u3002\u8fd9\u4e9b\u6539\u8fdb\u4e0d\u4ec5\u63d0\u5347\u4e86\u56fe\u50cf\u751f\u6210\u7684\u8d28\u91cf\uff0c\u8fd8\u4e3a\u533b\u5b66\u56fe\u50cf\u5408\u6210\u548c\u89c6\u9891\u751f\u6210\u7b49\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002\u6b64\u5916\uff0cGAN \u5728\u89c6\u9891\u751f\u6210\u548c 3D \u6a21\u578b\u751f\u6210\u9886\u57df\u4e5f\u6709\u5e7f\u6cdb\u5e94\u7528\u3002\u4f8b\u5982\uff0cVideoGAN \u901a\u8fc7\u5f15\u5165\u65f6\u95f4\u4e00\u81f4\u6027\u7ea6\u675f\uff0c\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u89c6\u9891\u5e8f\u5217\uff0c\u4e3a\u89c6\u9891\u7f16\u8f91\u548c\u865a\u62df\u73b0\u5b9e\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u7684\u5de5\u5177\u30023D-GAN \u5219\u901a\u8fc7\u7ed3\u5408\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u548c 3D \u5377\u79ef\uff0c\u80fd\u591f\u751f\u6210\u903c\u771f\u7684 3D \u6a21\u578b\uff0c\u4e3a\u6e38\u620f\u5f00\u53d1\u548c\u865a\u62df\u73b0\u5b9e\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002CycleGAN \u548c StarGAN \u7b49\u53d8\u4f53\u5728\u98ce\u683c\u8fc1\u79fb\u548c\u56fe\u50cf\u4fee\u590d\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u901a\u8fc7\u6539\u8fdb\u751f\u6210\u5668\u548c\u5224\u522b\u5668\u7ed3\u6784\u6765\u63d0\u5347\u751f\u6210\u56fe\u50cf\u7684\u8d28\u91cf\u548c\u591a\u6837\u6027\u3002\n\nBERT \u7684\u53d8\u4f53\u53ef\u80fd\u4f1a\u5728\u66f4\u591a\u8de8\u9886\u57df\u4efb\u52a1\u4e2d\u5f97\u5230\u5e94\u7528\uff0c\u4f8b\u5982\u5728\u751f\u7269\u4fe1\u606f\u5b66\u4e2d\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u6a21\u578b\u6355\u6349\u86cb\u767d\u8d28\u5e8f\u5217\u4e2d\u7684\u8bed\u4e49\u4fe1\u606f\uff0c\u8f85\u52a9\u86cb\u767d\u8d28\u529f\u80fd\u9884\u6d4b\u548c\u7ed3\u6784\u5206\u6790\u3002\u6700\u8fd1\u7684\u7814\u7a76\u8868\u660e\uff0cBERT \u5728\u86cb\u767d\u8d28\u5e8f\u5217\u5206\u6790\u4e2d\u7684\u5e94\u7528\u5df2\u7ecf\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f8b\u5982 ProtBERT \u901a\u8fc7\u9884\u8bad\u7ec3\u86cb\u767d\u8d28\u5e8f\u5217\uff0c\u663e\u8457\u63d0\u5347\u4e86\u86cb\u767d\u8d28\u529f\u80fd\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002\u5728\u91d1\u878d\u9886\u57df\uff0cBERT \u53ef\u4ee5\u7528\u4e8e\u60c5\u611f\u5206\u6790\u548c\u5e02\u573a\u9884\u6d4b\uff0c\u901a\u8fc7\u5206\u6790\u65b0\u95fb\u548c\u793e\u4ea4\u5a92\u4f53\u6587\u672c\uff0c\u9884\u6d4b\u80a1\u7968\u5e02\u573a\u7684\u6ce2\u52a8\u3002\u4f8b\u5982\uff0cFinBERT \u901a\u8fc7\u5fae\u8c03\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u91d1\u878d\u6587\u672c\u60c5\u611f\u5206\u6790\u7684\u51c6\u786e\u6027\uff0c\u4e3a\u5e02\u573a\u9884\u6d4b\u63d0\u4f9b\u4e86\u6709\u529b\u652f\u6301\u3002\u6b64\u5916\uff0cBERT \u5728\u6cd5\u5f8b\u6587\u672c\u5206\u6790\u548c\u6559\u80b2\u9886\u57df\u7684\u81ea\u52a8\u8bc4\u5206\u7cfb\u7edf\u4e2d\u4e5f\u6709\u5e7f\u6cdb\u5e94\u7528\u3002\u4f8b\u5982\uff0cLegal-BERT \u901a\u8fc7\u9884\u8bad\u7ec3\u6cd5\u5f8b\u6587\u672c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6cd5\u5f8b\u6587\u672c\u5206\u7c7b\u548c\u68c0\u7d22\u7684\u51c6\u786e\u6027\u3002\u5728\u6559\u80b2\u9886\u57df\uff0cBERT-based essay scoring \u7cfb\u7edf\u901a\u8fc7\u5206\u6790\u5b66\u751f\u4f5c\u6587\u7684\u8bed\u4e49\u548c\u7ed3\u6784\uff0c\u80fd\u591f\u81ea\u52a8\u8bc4\u4f30\u4f5c\u6587\u8d28\u91cf\uff0c\u4e3a\u6559\u80b2\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u7684\u5de5\u5177\u3002BioBERT \u548c ClinicalBERT \u7b49\u53d8\u4f53\u5728\u533b\u7597\u6587\u672c\u5206\u6790\u4e2d\u7684\u5e94\u7528\u4e5f\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u901a\u8fc7\u63d0\u5347\u75be\u75c5\u8bca\u65ad\u548c\u836f\u7269\u53d1\u73b0\u7684\u6027\u80fd\uff0c\u4e3a\u533b\u7597\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u652f\u6301\u3002", "ref_task_step_id": ""}, "__type__": "task_step_node"}, "fc9ce0a4-4b19-4d3e-a44a-994be0efc0fa": {"__data__": {"id_": "fc9ce0a4-4b19-4d3e-a44a-994be0efc0fa", "metadata": {}, "relationships": {}, "hash": "", "start_task_context": "\u5927\u6a21\u578bMLA\u6280\u672f\u7ec6\u8282", "aemo_representation_context": "### 1. \u5206\u6790\u8fd1\u51e0\u5e74\u7814\u7a76\u9886\u57df\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba\n\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u76f8\u5173\u7814\u7a76\u9886\u57df\uff0cTransformer\u3001GAN\u3001BERT \u7b49\u6846\u67b6\u6210\u4e3a\u4f17\u591a\u8bba\u6587\u91c7\u7528\u7684\u6838\u5fc3\u6280\u672f\u3002\u8fd9\u4e9b\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u9002\u5e94\u6027\uff0c\u5176\u65b9\u6cd5\u8bba\u56f4\u7ed5\u7740\u5982\u4f55\u6709\u6548\u5904\u7406\u6570\u636e\u7279\u5f81\u3001\u4f18\u5316\u6a21\u578b\u7ed3\u6784\u4ee5\u63d0\u5347\u6027\u80fd\u3002\u4f8b\u5982\uff0cTransformer \u57fa\u4e8e\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6253\u7834\u4e86\u4f20\u7edf\u5e8f\u5217\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u7b49\u591a\u9886\u57df\u5e7f\u6cdb\u5e94\u7528\uff1bGAN \u901a\u8fc7\u751f\u6210\u5668\u4e0e\u5224\u522b\u5668\u7684\u5bf9\u6297\u8bad\u7ec3\uff0c\u5728\u56fe\u50cf\u751f\u6210\u7b49\u4efb\u52a1\u4e0a\u53d6\u5f97\u663e\u8457\u6210\u679c\uff1bBERT \u5219\u901a\u8fc7\u9884\u8bad\u7ec3\u65b9\u5f0f\uff0c\u6781\u5927\u63d0\u5347\u4e86\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u7684\u6027\u80fd\u3002\n\n### 2. \u7814\u7a76\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u4e3b\u8981\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0e\u53d8\u4f53\n - **Transformer**\uff1a\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u4e2d\uff0c\u5982\u673a\u5668\u7ffb\u8bd1\u3001\u6587\u672c\u5206\u7c7b\uff0cTransformer \u67b6\u6784\u4e0d\u65ad\u6f14\u53d8\uff0c\u51fa\u73b0\u4e86\u8bf8\u5982 ViT\uff08Vision Transformer\uff09\u7b49\u53d8\u4f53\u5e94\u7528\u4e8e\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\uff0c\u5c06\u56fe\u50cf\u89c6\u4e3a\u5e8f\u5217\u8fdb\u884c\u5904\u7406\uff0c\u5c55\u73b0\u51fa\u8de8\u9886\u57df\u7684\u5e94\u7528\u6f5c\u529b\u3002\n - **GAN**\uff1a\u5728\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\uff0cDCGAN\uff08Deep Convolutional GAN\uff09\u7b49\u53d8\u4f53\u901a\u8fc7\u6539\u8fdb\u7f51\u7edc\u7ed3\u6784\uff0c\u751f\u6210\u66f4\u903c\u771f\u7684\u56fe\u50cf\uff1b\u5728\u533b\u5b66\u56fe\u50cf\u5408\u6210\u3001\u89c6\u9891\u751f\u6210\u7b49\u9886\u57df\u4e5f\u6709\u5e94\u7528\uff0c\u901a\u8fc7\u8c03\u6574\u635f\u5931\u51fd\u6570\u548c\u7f51\u7edc\u67b6\u6784\u9002\u5e94\u4e0d\u540c\u4efb\u52a1\u9700\u6c42\u3002\n - **BERT**\uff1a\u9664\u4e86\u57fa\u7840\u7684\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u4efb\u52a1\uff0c\u8fd8\u5728\u4fe1\u606f\u68c0\u7d22\u3001\u60c5\u611f\u5206\u6790\u7b49\u4efb\u52a1\u4e2d\u5f97\u5230\u5e7f\u6cdb\u5e94\u7528\uff0c\u901a\u8fc7\u5fae\u8c03\u9884\u8bad\u7ec3\u6a21\u578b\u9002\u5e94\u7279\u5b9a\u4efb\u52a1\uff0c\u540c\u65f6\u51fa\u73b0\u4e86 RoBERTa \u7b49\u6539\u8fdb\u7248\u672c\uff0c\u4f18\u5316\u9884\u8bad\u7ec3\u7b56\u7565\u63d0\u5347\u6027\u80fd\u3002\n\n### 3. \u8bc4\u4f30\u5b66\u672f\u754c\u7684\u6280\u672f\u8fdb\u6b65\u4e0e\u5c40\u9650\u6027\n - **\u6280\u672f\u8fdb\u6b65**\uff1a\u6a21\u578b\u6027\u80fd\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u53d6\u5f97\u663e\u8457\u63d0\u5347\uff0c\u4f8b\u5982\u5728\u56fe\u50cf\u8bc6\u522b\u51c6\u786e\u7387\u3001\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u7684 F1 \u503c\u7b49\u6307\u6807\u4e0a\u4e0d\u65ad\u5237\u65b0\u8bb0\u5f55\u3002\u65b0\u7684\u6a21\u578b\u67b6\u6784\u548c\u8bad\u7ec3\u65b9\u6cd5\u4e0d\u65ad\u6d8c\u73b0\uff0c\u63a8\u52a8\u4e86\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\u3002\n - **\u5c40\u9650\u6027**\uff1a\u4f9d\u7136\u5b58\u5728\u6a21\u578b\u504f\u5dee\u548c\u6570\u636e\u4f9d\u8d56\u95ee\u9898\u3002\u6a21\u578b\u5728\u67d0\u4e9b\u7279\u5b9a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u5176\u4ed6\u6570\u636e\u96c6\u6216\u73b0\u5b9e\u573a\u666f\u4e2d\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002\u90e8\u5206\u6a21\u578b\u5bf9\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u4f9d\u8d56\u4e25\u91cd\uff0c\u6570\u636e\u8d28\u91cf\u548c\u6570\u91cf\u5f71\u54cd\u6a21\u578b\u6027\u80fd\uff0c\u4e14\u6a21\u578b\u89e3\u91ca\u6027\u5dee\uff0c\u96be\u4ee5\u7406\u89e3\u5176\u51b3\u7b56\u8fc7\u7a0b\u3002\n\n### 4. \u63a2\u8ba8\u8ba1\u7b97\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\n - **\u9002\u7528\u6027**\uff1a\u4e0d\u540c\u6a21\u578b\u5728\u7279\u5b9a\u6570\u636e\u96c6\u548c\u5e94\u7528\u573a\u666f\u4e0b\u6709\u5404\u81ea\u4f18\u52bf\u3002\u4f8b\u5982\uff0c\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6a21\u578b\u5728\u5927\u89c4\u6a21\u3001\u9ad8\u7ef4\u6570\u636e\u7684\u56fe\u50cf\u548c\u8bed\u97f3\u8bc6\u522b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff1b\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u5c0f\u6570\u636e\u3001\u7b80\u5355\u4efb\u52a1\u4e2d\u4ecd\u6709\u5e94\u7528\u4ef7\u503c\u3002\n - **\u6cdb\u5316\u80fd\u529b**\uff1a\u5c3d\u7ba1\u90e8\u5206\u6a21\u578b\u5728\u7279\u5b9a\u9886\u57df\u53d6\u5f97\u6210\u529f\uff0c\u4f46\u5728\u591a\u9886\u57df\u3001\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u7684\u73b0\u5b9e\u5e94\u7528\u4e2d\uff0c\u6cdb\u5316\u80fd\u529b\u9762\u4e34\u6311\u6218\u3002\u591a\u6a21\u6001\u6570\u636e\u7684\u5f02\u8d28\u6027\u548c\u590d\u6742\u6027\u4f7f\u5f97\u6a21\u578b\u96be\u4ee5\u6709\u6548\u63d0\u53d6\u548c\u878d\u5408\u7279\u5f81\uff0c\u9650\u5236\u4e86\u5176\u5728\u590d\u6742\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002\n\n### 5. \u5206\u6790\u6700\u65b0\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027\n - **\u7a33\u5b9a\u6027\u4f18\u5316**\uff1a\u4e00\u4e9b\u7b97\u6cd5\u9488\u5bf9\u7a33\u5b9a\u6027\u8fdb\u884c\u4e86\u4f18\u5316\uff0c\u4f8b\u5982\u91c7\u7528\u6b63\u5219\u5316\u6280\u672f\u3001\u5bf9\u6297\u8bad\u7ec3\u7b49\u65b9\u6cd5\u63d0\u9ad8\u6a21\u578b\u5728\u590d\u6742\u73af\u5883\u4e0b\u7684\u7a33\u5b9a\u6027\u3002\u5728\u52a8\u6001\u73af\u5883\u4e2d\uff0c\u90e8\u5206\u7b97\u6cd5\u901a\u8fc7\u5728\u7ebf\u5b66\u4e60\u3001\u81ea\u9002\u5e94\u8c03\u6574\u53c2\u6570\u7b49\u65b9\u5f0f\u4fdd\u6301\u6027\u80fd\u7a33\u5b9a\u3002\n - **\u5927\u89c4\u6a21\u6570\u636e\u9002\u5e94\u6027**\uff1a\u5728\u5927\u89c4\u6a21\u6570\u636e\u5904\u7406\u4e0a\uff0c\u90e8\u5206\u7b97\u6cd5\u901a\u8fc7\u5206\u5e03\u5f0f\u8bad\u7ec3\u3001\u6a21\u578b\u538b\u7f29\u7b49\u6280\u672f\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u548c\u9002\u5e94\u6027\u3002\u4f46\u4ecd\u6709\u7b97\u6cd5\u5728\u5904\u7406\u5927\u89c4\u6a21\u6570\u636e\u65f6\u9762\u4e34\u5185\u5b58\u3001\u8ba1\u7b97\u8d44\u6e90\u7b49\u9650\u5236\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002\n\n### 6. \u8bc4\u4f30\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218\n - **\u65b0\u7814\u7a76\u95ee\u9898**\uff1a\u63d0\u51fa\u4e86\u5982\u63d0\u5347\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u3001\u89e3\u51b3\u6a21\u578b\u504f\u5dee\u548c\u6570\u636e\u4f9d\u8d56\u3001\u589e\u5f3a\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u80fd\u529b\u7b49\u65b0\u7814\u7a76\u95ee\u9898\u3002\n - **\u7814\u7a76\u5207\u5165\u70b9**\uff1a\u4ece\u6539\u8fdb\u6a21\u578b\u67b6\u6784\u3001\u4f18\u5316\u8bad\u7ec3\u7b97\u6cd5\u3001\u63a2\u7d22\u65b0\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u7b49\u65b9\u9762\u5bfb\u627e\u65b0\u7684\u7814\u7a76\u5207\u5165\u70b9\u3002\u8fd9\u4e9b\u6311\u6218\u4fc3\u4f7f\u7814\u7a76\u8005\u4e0d\u65ad\u63a2\u7d22\u65b0\u7684\u6280\u672f\u548c\u65b9\u6cd5\uff0c\u63a8\u52a8\u540e\u7eed\u7814\u7a76\u671d\u7740\u66f4\u9ad8\u6548\u3001\u66f4\u667a\u80fd\u3001\u66f4\u5177\u6cdb\u5316\u80fd\u529b\u7684\u65b9\u5411\u53d1\u5c55\u3002\n\n### \u7814\u7a76\u6210\u679c\u3001\u65b9\u6cd5\u521b\u65b0\u6027\u4e0e\u5e94\u7528\u4ef7\u503c\u603b\u7ed3\n - **\u7814\u7a76\u6210\u679c**\uff1a\u5728\u6a21\u578b\u6027\u80fd\u63d0\u5347\u3001\u65b0\u67b6\u6784\u548c\u7b97\u6cd5\u63d0\u51fa\u7b49\u65b9\u9762\u53d6\u5f97\u663e\u8457\u6210\u679c\uff0c\u63a8\u52a8\u4e86\u8ba1\u7b97\u673a\u79d1\u5b66\u591a\u4e2a\u9886\u57df\u7684\u53d1\u5c55\u3002\n - **\u65b9\u6cd5\u521b\u65b0\u6027**\uff1a\u901a\u8fc7\u5f15\u5165\u65b0\u7684\u6280\u672f\u6846\u67b6\u3001\u6539\u8fdb\u8bad\u7ec3\u65b9\u6cd5\u548c\u4f18\u5316\u6a21\u578b\u7ed3\u6784\u7b49\u65b9\u5f0f\uff0c\u5c55\u73b0\u51fa\u8bf8\u591a\u521b\u65b0\u70b9\uff0c\u5982\u81ea\u6ce8\u610f\u529b\u673a\u5236\u3001\u5bf9\u6297\u8bad\u7ec3\u7b49\u3002\n - **\u5e94\u7528\u4ef7\u503c**\uff1a\u5728\u56fe\u50cf\u3001\u81ea\u7136\u8bed\u8a00\u5904\u7406\u3001\u8bed\u97f3\u7b49\u591a\u4e2a\u9886\u57df\u6709\u5e7f\u6cdb\u5e94\u7528\uff0c\u4e3a\u667a\u80fd\u5b89\u9632\u3001\u667a\u80fd\u533b\u7597\u3001\u667a\u80fd\u4ea4\u901a\u7b49\u73b0\u5b9e\u573a\u666f\u63d0\u4f9b\u6280\u672f\u652f\u6301\uff0c\u5177\u6709\u5de8\u5927\u7684\u5546\u4e1a\u548c\u793e\u4f1a\u4ef7\u503c\u3002  ", "task_step_name": "\u8bc4\u4f30\u5b66\u672f\u754c\u7684\u6280\u672f\u8fdb\u6b65\u4e0e\u5c40\u9650\u6027", "task_step_description": "- **\u6280\u672f\u8fdb\u6b65**\uff1a\u6a21\u578b\u6027\u80fd\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u53d6\u5f97\u663e\u8457\u63d0\u5347\uff0c\u4f8b\u5982\u5728\u56fe\u50cf\u8bc6\u522b\u51c6\u786e\u7387\u3001\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u7684 F1 \u503c\u7b49\u6307\u6807\u4e0a\u4e0d\u65ad\u5237\u65b0\u8bb0\u5f55\u3002\u65b0\u7684\u6a21\u578b\u67b6\u6784\u548c\u8bad\u7ec3\u65b9\u6cd5\u4e0d\u65ad\u6d8c\u73b0\uff0c\u63a8\u52a8\u4e86\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\u3002 - **\u5c40\u9650\u6027**\uff1a\u4f9d\u7136\u5b58\u5728\u6a21\u578b\u504f\u5dee\u548c\u6570\u636e\u4f9d\u8d56\u95ee\u9898\u3002\u6a21\u578b\u5728\u67d0\u4e9b\u7279\u5b9a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u5176\u4ed6\u6570\u636e\u96c6\u6216\u73b0\u5b9e\u573a\u666f\u4e2d\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002\u90e8\u5206\u6a21\u578b\u5bf9\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u4f9d\u8d56\u4e25\u91cd\uff0c\u6570\u636e\u8d28\u91cf\u548c\u6570\u91cf\u5f71\u54cd\u6a21\u578b\u6027\u80fd\uff0c\u4e14\u6a21\u578b\u89e3\u91ca\u6027\u5dee\uff0c\u96be\u4ee5\u7406\u89e3\u5176\u51b3\u7b56\u8fc7\u7a0b\u3002", "task_step_level": "2", "task_step_question": "\u5b66\u672f\u754c\u5728\u6a21\u578b\u6027\u80fd\u63d0\u5347\u548c\u65b0\u67b6\u6784\u3001\u8bad\u7ec3\u65b9\u6cd5\u6d8c\u73b0\u65b9\u9762\u53d6\u5f97\u4e86\u6280\u672f\u8fdb\u6b65\uff0c\u4f46\u5b58\u5728\u6a21\u578b\u504f\u5dee\u3001\u6570\u636e\u4f9d\u8d56\u3001\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u4ee5\u53ca\u6a21\u578b\u89e3\u91ca\u6027\u5dee\u7b49\u5c40\u9650\u6027\uff0c\u90a3\u4e48\u5982\u4f55\u5728\u5229\u7528\u65b0\u67b6\u6784\u548c\u8bad\u7ec3\u65b9\u6cd5\u63d0\u5347\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\uff0c\u6709\u6548\u89e3\u51b3\u6a21\u578b\u504f\u5dee\u548c\u6570\u636e\u4f9d\u8d56\u95ee\u9898\uff0c\u63d0\u9ad8\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u89e3\u91ca\u6027\uff1f ", "task_step_question_context": [{"ref_id": "454847062641060886", "chunk_id": "6", "score": 0.302734375, "text": "# CFurther Discussions\nWe summarize some empirical observations as follows.  \n\n1) The CLIP with four training tricks yields about $4\\%$ improvement at Rank-1 in Table 1 of the main paper. It can inspire future works in which the model performance could be boosted by applying these training tricks.  \n\n2) Data augmentation and loss function are common technologies used in various methods. The investigation of more than 20 data augmentations and about 10 loss functions on performance in Tables 2-5 of the main paper provides valuable guidance on future works. Researchers can select proper and effective augmentations and losses into the model for improving performance.   \n3) We explore the internal properties and functionalities of the model for the first time. These results can light future works on model compression, so as to develop a more lightweight and effective TBPS method.   \n4) There are very little research on few-shot TBPS, while this paper makes a preliminary study on CLIP-based fewshot TBPS, providing valuable observation for future research direction."}, {"ref_id": "454846731731167836", "chunk_id": "9", "score": 0.2216796875, "text": "# 4.3. Backward-Congruent Re-ranking\nUnlike model ensemble, knowledge distillation attempts to explicitly align the behaviors of the new model to the old model during training . Alternatively, we propose BackwardCongruent Re-ranking (BCR), which does not impose any constraint to the training of the new model and only takes effect during inference .  \n\nRe-ranking is a popular approach in structured prediction to combine the strengths of two different models ( Collins & Koo ,2005 ;Socher et al. ,2013 ;Le & Zuidema ,2014 ;Do & Rehbein ,2020 ). It suggests to use one model as the candidate generator for creating a candidate pool and use the other model as the re-ranker for picking the best candidate. While previous work has been focused on developing powerful re-rankers for overall performance improvement, the purpose of BCR is to reduce model update regression. BCR treats the new model as the candidate generator and the old model as the re-ranker. Our motivations are two-fold. First, structured prediction models can produce a number of possible predictions. Second, different predictions may achieve similar error rates but differ by the mistakes they made. Among them, the most likely one according to the old model should have the least prediction flips. These make re-ranking particularly useful for structured prediction.  \n\nFormally, we have the old model and the new model parameterized by $\\phi_{\\mathrm{new}}$ and $\\phi_{\\mathrm{old}}$ respectively. For a given input $x$ ,the new model first generates a set of candidates $\\mathrm{GEN}_{\\phi_{\\mathrm{new}}}(x)$ .Then we choose the prediction $y^{\\ast}$ with the highest score computed by the old model.  \n\n$$\ny^{*}=\\operatorname*{arg\\,max}_{y\\in\\mathrm{GEN}_{\\phi_{\\mathrm{new}}}(x)}p_{\\phi_{\\mathrm{old}}}(y|x),\n$$  \n\nwhere $p_{\\phi_{\\mathrm{old}}}(y|x)$ is the old odel\u2019s generation probability (score) of $y$ given the input xand GEN can be implemented by various decoding methods :  \n\n\u2022Maximization-based Search The de facto decoding objective, particular for structured prediction tasks is, maximization-based search. This strategy is based on the assumption that the model assigns higher scores to better output. When finding the exact best $\\cdot k$ output is intractable, the common practice is to use beam search for approximation instead. Hence, we use the $k$ -best MST algorithm ( Zmigrod et al. ,2020 ;2021 ) for the biaffine parser and use beam search for other parsers.  \n\n\u2022Top$k$ Sampling Truncated stochastic sampling such as top-k sampling ( Fan et al. ,2018 ;Radford et al. ,2019 ) is a popular alternative for generating multiple outputs in open-ended text generation tasks. At each decoding step, the top $k$ possible local predictions are selected and the sampling is based on their relative probabilities.  \n\n\u2022Top $\\cdot p$ Sampling Similar to top$k$ sampling, top$\\boldsymbol{p}$ sampling ( Holtzman et al. ,2019 ) is another sampling-based decoding method. The only difference is that top$\\boldsymbol{p}$ sampling samples from the smallest set of top local predictions whose cumulative probability mass exceeds $p$ .  \n\n\u2022Dropout$\\boldsymbol{p}$ Sampling We explore a special sampling method, dropout$\\cdot p$ sampling, that has attracted little attention in the literature. Dropout ( Srivastava et al. ,2014 ) is a regularization technique used in almost all modern neural network-based models. The key idea is to randomly drop some neurons from the neural network during training. Normally, dropout is turned off during inference. However, in dropout$\\cdot p$ sampling, we keep using dropout with dropout rate $p$ . Compared to other methods, dropout$\\boldsymbol{p}$ sampling is unique in that (1) it changes the scoring function instead of the decoding objective; (2) unlike traditional truncated sampling that conducts local sampling at each decoding step, it can be regarded as a global sampling. Dropout$\\boldsymbol{p}$ sampling also has a broader applicable scope than top$k$ and top$p$ sampling as the latter two are designed for sequence generation models.  \n\nDiscussion Following previous work ( Shen et al. ,2004 ;Yan et al. ,2021 ;Xie et al. ,2021 ), our discussion has been focused on handling one model update. However, BCR can be extended to handle multiple turns of model updates. To do so, one can keep the most recent $k$ models and use a weighted combination of their scores as the re-ranking metric. In practice, $k$ can be set to trade-off between performance and runtime cost.  \n\nOne downside of BCR is that we must maintain and deploy both the old and new models. Because the re-ranking step has less time complexity compared to the decoding algorithms and the computation is fully parallelizable, this does not create much additional inference latency. However, the increase in memory footprint does entail an increase in inference hosting cost. One remedy could be to use knowldge distillation to distill the old model(s) into a smaller one, which we leave for future work.  \n\nMeasuring and Reducing Model Update Regression in Structured Prediction for NLP   \n\n\n<html><body><table><tr><td rowspan=\"3\"></td><td colspan=\"4\">deepbiafdeepbiaf</td><td colspan=\"3\">stackptrstackptr</td><td colspan=\"3\">deepbiaf=stackptr</td></tr><tr><td colspan=\"2\">UCM</td><td>UAS</td><td></td><td>UCM</td><td>UAS</td><td></td><td>UCM</td><td>UAS</td><td></td></tr><tr><td>NFR</td><td>NFIA ACC NFR</td><td></td><td>NFIACCNFR</td><td>NFIACC NFR</td><td>NFIACC</td><td>NFR</td><td></td><td>NFI ACC NFR</td><td>NFI ACC</td></tr><tr><td>PIO</td><td></td><td>-63.88</td><td></td><td>-91.76</td><td>-65.83</td><td>-91.81</td><td></td><td>-63.88</td><td></td><td>-91.76</td></tr><tr><td>Untreated</td><td></td><td>3.6910.2563.97</td><td>1.66 19.85 91.64</td><td></td><td>3.43 10.1066.03</td><td>1.6720.20 91.73</td><td></td><td>3.73 10.98 66.03</td><td></td><td>2.1025.37 91.73</td></tr><tr><td>Distillation</td><td>3.82</td><td>10.62 264.00</td><td>1.62</td><td>19.45 91.67</td><td>3.57 10.40 65.66</td><td>1.7020.4991.70</td><td>3.62</td><td>10.68 66.11</td><td>2.03 24.71</td><td>91.78</td></tr><tr><td>\u2192Ensemble</td><td>2.51</td><td>7.14 64.81</td><td>1.02 12.9792.10</td><td>2.21</td><td>6.74 67.21</td><td>1.1114.3092.21</td><td>2.83</td><td>8.62 67.21</td><td>1.62 20.75</td><td>92.21</td></tr><tr><td>BCR</td><td>1.12</td><td>3.1464.36</td><td>0.84 10.21 91.78</td><td></td><td>1.05 3.12 66.45</td><td>0.84 10.3091.88</td><td>1.20</td><td>3.60 66.76</td><td>1.11 13.87</td><td>92.01</td></tr></table></body></html>"}, {"ref_id": "454984230919739446", "chunk_id": "8", "score": 0.197265625, "text": "# Conclusion\nIn this paper, we tackle two challenges in incremental novel class discovery: how to leverage novel unlabeled data sets for effective training guidance and how to prevent catastrophic forgetting of previous knowledge. We propose Triple Comparison and Probability Regularization to control the probability discrepancy and diversity of categories for adaptive category assignment. In addition, we design a hybrid structure, Adaptive Model Merging, which preserves the previous knowledge by reducing the novel branch\u2019s interference with the old classes. Extensive experiments on classiNCD demonstrate that our method can significantly outperform the existing methods without increasing the computational cost.\n\n# Acknowledgements\nThe study was funded by the National Natural Science Foundation of China under contracts No. 62332002, No. 62027804, No. 62088102, No. 61825101, No. 62372010, and No. 62202010, and the major key project of the Peng Cheng Laboratory (PCL2021A13). Computing support was provided by Pengcheng Cloudbrain."}], "task_step_question_answer": "\u5728\u6df1\u5165\u5206\u6790\u6280\u672f\u8fdb\u6b65\u4e0e\u5c40\u9650\u6027\u7684\u57fa\u7840\u4e0a\uff0c\u8fdb\u4e00\u6b65\u63a2\u8ba8\u8fd9\u4e9b\u7814\u7a76\u6210\u679c\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u8f6c\u5316\u60c5\u51b5\u3002\u4f8b\u5982\uff0cTransformer\u67b6\u6784\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u9886\u57df\u7684\u5e94\u7528\u5df2\u5e7f\u6cdb\u6e17\u900f\u5230\u667a\u80fd\u5ba2\u670d\u3001\u673a\u5668\u7ffb\u8bd1\u7b49\u5b9e\u9645\u573a\u666f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7528\u6237\u4f53\u9a8c\u548c\u670d\u52a1\u6548\u7387\u3002\u7136\u800c\uff0c\u5176\u5728\u5904\u7406\u590d\u6742\u8bed\u5883\u548c\u591a\u4e49\u8bcd\u65f6\u7684\u8868\u73b0\u4ecd\u4e0d\u5c3d\u5982\u4eba\u610f\uff0c\u66b4\u9732\u51fa\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u8bed\u4e49\u7406\u89e3\u4e0a\u7684\u4e0d\u8db3\u3002GAN\u6280\u672f\u5728\u56fe\u50cf\u751f\u6210\u9886\u57df\u7684\u5e94\u7528\u867d\u53d6\u5f97\u4e86\u663e\u8457\u6210\u679c\uff0c\u4f46\u5728\u533b\u5b66\u56fe\u50cf\u5408\u6210\u7b49\u9ad8\u7cbe\u5ea6\u8981\u6c42\u573a\u666f\u4e2d\uff0c\u751f\u6210\u56fe\u50cf\u7684\u7ec6\u8282\u8fd8\u539f\u5ea6\u548c\u771f\u5b9e\u6027\u4ecd\u6709\u5f85\u63d0\u5347\u3002BERT\u6a21\u578b\u5728\u4fe1\u606f\u68c0\u7d22\u548c\u60c5\u611f\u5206\u6790\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u5728\u5904\u7406\u957f\u6587\u672c\u548c\u4f4e\u8d44\u6e90\u8bed\u8a00\u65f6\uff0c\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u9650\u5236\u4e86\u5176\u5e94\u7528\u8303\u56f4\u3002\n\n\u6b64\u5916\uff0c\u5b66\u672f\u754c\u4e0e\u5de5\u4e1a\u754c\u7684\u5408\u4f5c\u65e5\u76ca\u7d27\u5bc6\uff0c\u8bb8\u591a\u7814\u7a76\u6210\u679c\u901a\u8fc7\u5f00\u6e90\u9879\u76ee\u548c\u5546\u4e1a\u5e94\u7528\u5feb\u901f\u8f6c\u5316\u4e3a\u5b9e\u9645\u751f\u4ea7\u529b\u3002\u7136\u800c\uff0c\u8f6c\u5316\u8fc7\u7a0b\u4e2d\u4e5f\u9762\u4e34\u8bf8\u591a\u6311\u6218\uff0c\u5982\u6a21\u578b\u5728\u5b9e\u9645\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\u3001\u6570\u636e\u9690\u79c1\u4fdd\u62a4\u7b49\u95ee\u9898\u3002\u672a\u6765\u7814\u7a76\u9700\u66f4\u52a0\u6ce8\u91cd\u6a21\u578b\u7684\u5b9e\u7528\u6027\u548c\u666e\u9002\u6027\uff0c\u63a2\u7d22\u5728\u6709\u9650\u8d44\u6e90\u548c\u590d\u6742\u73af\u5883\u4e0b\u7684\u9ad8\u6548\u7b97\u6cd5\u3002\u540c\u65f6\uff0c\u8de8\u5b66\u79d1\u5408\u4f5c\u5982\u7ed3\u5408\u8ba4\u77e5\u79d1\u5b66\u3001\u5fc3\u7406\u5b66\u7b49\u9886\u57df\u7684\u7814\u7a76\u6210\u679c\uff0c\u53ef\u80fd\u4e3a\u89e3\u51b3\u6a21\u578b\u89e3\u91ca\u6027\u548c\u6cdb\u5316\u80fd\u529b\u95ee\u9898\u63d0\u4f9b\u65b0\u7684\u601d\u8def\u3002\u4f8b\u5982\uff0c\u901a\u8fc7\u5f15\u5165\u8ba4\u77e5\u79d1\u5b66\u7684\u7406\u8bba\uff0c\u53ef\u4ee5\u8bbe\u8ba1\u51fa\u66f4\u7b26\u5408\u4eba\u7c7b\u8ba4\u77e5\u89c4\u5f8b\u7684\u6a21\u578b\u67b6\u6784\uff0c\u4ece\u800c\u63d0\u5347\u6a21\u578b\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002\u6b64\u5916\uff0c\u5fc3\u7406\u5b66\u7684\u7814\u7a76\u53ef\u4ee5\u5e2e\u52a9\u6211\u4eec\u66f4\u597d\u5730\u7406\u89e3\u7528\u6237\u9700\u6c42\uff0c\u8bbe\u8ba1\u51fa\u66f4\u4eba\u6027\u5316\u7684\u667a\u80fd\u7cfb\u7edf\u3002", "ref_task_step_id": ""}, "__type__": "task_step_node"}, "02c99ed3-dc6a-4b2d-bbff-109a51351344": {"__data__": {"id_": "02c99ed3-dc6a-4b2d-bbff-109a51351344", "metadata": {}, "relationships": {}, "hash": "", "start_task_context": "\u5927\u6a21\u578bMLA\u6280\u672f\u7ec6\u8282", "aemo_representation_context": "### 1. \u5206\u6790\u8fd1\u51e0\u5e74\u7814\u7a76\u9886\u57df\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba\n\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u76f8\u5173\u7814\u7a76\u9886\u57df\uff0cTransformer\u3001GAN\u3001BERT \u7b49\u6846\u67b6\u6210\u4e3a\u4f17\u591a\u8bba\u6587\u91c7\u7528\u7684\u6838\u5fc3\u6280\u672f\u3002\u8fd9\u4e9b\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u9002\u5e94\u6027\uff0c\u5176\u65b9\u6cd5\u8bba\u56f4\u7ed5\u7740\u5982\u4f55\u6709\u6548\u5904\u7406\u6570\u636e\u7279\u5f81\u3001\u4f18\u5316\u6a21\u578b\u7ed3\u6784\u4ee5\u63d0\u5347\u6027\u80fd\u3002\u4f8b\u5982\uff0cTransformer \u57fa\u4e8e\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6253\u7834\u4e86\u4f20\u7edf\u5e8f\u5217\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u7b49\u591a\u9886\u57df\u5e7f\u6cdb\u5e94\u7528\uff1bGAN \u901a\u8fc7\u751f\u6210\u5668\u4e0e\u5224\u522b\u5668\u7684\u5bf9\u6297\u8bad\u7ec3\uff0c\u5728\u56fe\u50cf\u751f\u6210\u7b49\u4efb\u52a1\u4e0a\u53d6\u5f97\u663e\u8457\u6210\u679c\uff1bBERT \u5219\u901a\u8fc7\u9884\u8bad\u7ec3\u65b9\u5f0f\uff0c\u6781\u5927\u63d0\u5347\u4e86\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u7684\u6027\u80fd\u3002\n\n### 2. \u7814\u7a76\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u4e3b\u8981\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0e\u53d8\u4f53\n - **Transformer**\uff1a\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u4e2d\uff0c\u5982\u673a\u5668\u7ffb\u8bd1\u3001\u6587\u672c\u5206\u7c7b\uff0cTransformer \u67b6\u6784\u4e0d\u65ad\u6f14\u53d8\uff0c\u51fa\u73b0\u4e86\u8bf8\u5982 ViT\uff08Vision Transformer\uff09\u7b49\u53d8\u4f53\u5e94\u7528\u4e8e\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\uff0c\u5c06\u56fe\u50cf\u89c6\u4e3a\u5e8f\u5217\u8fdb\u884c\u5904\u7406\uff0c\u5c55\u73b0\u51fa\u8de8\u9886\u57df\u7684\u5e94\u7528\u6f5c\u529b\u3002\n - **GAN**\uff1a\u5728\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\uff0cDCGAN\uff08Deep Convolutional GAN\uff09\u7b49\u53d8\u4f53\u901a\u8fc7\u6539\u8fdb\u7f51\u7edc\u7ed3\u6784\uff0c\u751f\u6210\u66f4\u903c\u771f\u7684\u56fe\u50cf\uff1b\u5728\u533b\u5b66\u56fe\u50cf\u5408\u6210\u3001\u89c6\u9891\u751f\u6210\u7b49\u9886\u57df\u4e5f\u6709\u5e94\u7528\uff0c\u901a\u8fc7\u8c03\u6574\u635f\u5931\u51fd\u6570\u548c\u7f51\u7edc\u67b6\u6784\u9002\u5e94\u4e0d\u540c\u4efb\u52a1\u9700\u6c42\u3002\n - **BERT**\uff1a\u9664\u4e86\u57fa\u7840\u7684\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u4efb\u52a1\uff0c\u8fd8\u5728\u4fe1\u606f\u68c0\u7d22\u3001\u60c5\u611f\u5206\u6790\u7b49\u4efb\u52a1\u4e2d\u5f97\u5230\u5e7f\u6cdb\u5e94\u7528\uff0c\u901a\u8fc7\u5fae\u8c03\u9884\u8bad\u7ec3\u6a21\u578b\u9002\u5e94\u7279\u5b9a\u4efb\u52a1\uff0c\u540c\u65f6\u51fa\u73b0\u4e86 RoBERTa \u7b49\u6539\u8fdb\u7248\u672c\uff0c\u4f18\u5316\u9884\u8bad\u7ec3\u7b56\u7565\u63d0\u5347\u6027\u80fd\u3002\n\n### 3. \u8bc4\u4f30\u5b66\u672f\u754c\u7684\u6280\u672f\u8fdb\u6b65\u4e0e\u5c40\u9650\u6027\n - **\u6280\u672f\u8fdb\u6b65**\uff1a\u6a21\u578b\u6027\u80fd\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u53d6\u5f97\u663e\u8457\u63d0\u5347\uff0c\u4f8b\u5982\u5728\u56fe\u50cf\u8bc6\u522b\u51c6\u786e\u7387\u3001\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u7684 F1 \u503c\u7b49\u6307\u6807\u4e0a\u4e0d\u65ad\u5237\u65b0\u8bb0\u5f55\u3002\u65b0\u7684\u6a21\u578b\u67b6\u6784\u548c\u8bad\u7ec3\u65b9\u6cd5\u4e0d\u65ad\u6d8c\u73b0\uff0c\u63a8\u52a8\u4e86\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\u3002\n - **\u5c40\u9650\u6027**\uff1a\u4f9d\u7136\u5b58\u5728\u6a21\u578b\u504f\u5dee\u548c\u6570\u636e\u4f9d\u8d56\u95ee\u9898\u3002\u6a21\u578b\u5728\u67d0\u4e9b\u7279\u5b9a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u5176\u4ed6\u6570\u636e\u96c6\u6216\u73b0\u5b9e\u573a\u666f\u4e2d\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002\u90e8\u5206\u6a21\u578b\u5bf9\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u4f9d\u8d56\u4e25\u91cd\uff0c\u6570\u636e\u8d28\u91cf\u548c\u6570\u91cf\u5f71\u54cd\u6a21\u578b\u6027\u80fd\uff0c\u4e14\u6a21\u578b\u89e3\u91ca\u6027\u5dee\uff0c\u96be\u4ee5\u7406\u89e3\u5176\u51b3\u7b56\u8fc7\u7a0b\u3002\n\n### 4. \u63a2\u8ba8\u8ba1\u7b97\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\n - **\u9002\u7528\u6027**\uff1a\u4e0d\u540c\u6a21\u578b\u5728\u7279\u5b9a\u6570\u636e\u96c6\u548c\u5e94\u7528\u573a\u666f\u4e0b\u6709\u5404\u81ea\u4f18\u52bf\u3002\u4f8b\u5982\uff0c\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6a21\u578b\u5728\u5927\u89c4\u6a21\u3001\u9ad8\u7ef4\u6570\u636e\u7684\u56fe\u50cf\u548c\u8bed\u97f3\u8bc6\u522b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff1b\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u5c0f\u6570\u636e\u3001\u7b80\u5355\u4efb\u52a1\u4e2d\u4ecd\u6709\u5e94\u7528\u4ef7\u503c\u3002\n - **\u6cdb\u5316\u80fd\u529b**\uff1a\u5c3d\u7ba1\u90e8\u5206\u6a21\u578b\u5728\u7279\u5b9a\u9886\u57df\u53d6\u5f97\u6210\u529f\uff0c\u4f46\u5728\u591a\u9886\u57df\u3001\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u7684\u73b0\u5b9e\u5e94\u7528\u4e2d\uff0c\u6cdb\u5316\u80fd\u529b\u9762\u4e34\u6311\u6218\u3002\u591a\u6a21\u6001\u6570\u636e\u7684\u5f02\u8d28\u6027\u548c\u590d\u6742\u6027\u4f7f\u5f97\u6a21\u578b\u96be\u4ee5\u6709\u6548\u63d0\u53d6\u548c\u878d\u5408\u7279\u5f81\uff0c\u9650\u5236\u4e86\u5176\u5728\u590d\u6742\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002\n\n### 5. \u5206\u6790\u6700\u65b0\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027\n - **\u7a33\u5b9a\u6027\u4f18\u5316**\uff1a\u4e00\u4e9b\u7b97\u6cd5\u9488\u5bf9\u7a33\u5b9a\u6027\u8fdb\u884c\u4e86\u4f18\u5316\uff0c\u4f8b\u5982\u91c7\u7528\u6b63\u5219\u5316\u6280\u672f\u3001\u5bf9\u6297\u8bad\u7ec3\u7b49\u65b9\u6cd5\u63d0\u9ad8\u6a21\u578b\u5728\u590d\u6742\u73af\u5883\u4e0b\u7684\u7a33\u5b9a\u6027\u3002\u5728\u52a8\u6001\u73af\u5883\u4e2d\uff0c\u90e8\u5206\u7b97\u6cd5\u901a\u8fc7\u5728\u7ebf\u5b66\u4e60\u3001\u81ea\u9002\u5e94\u8c03\u6574\u53c2\u6570\u7b49\u65b9\u5f0f\u4fdd\u6301\u6027\u80fd\u7a33\u5b9a\u3002\n - **\u5927\u89c4\u6a21\u6570\u636e\u9002\u5e94\u6027**\uff1a\u5728\u5927\u89c4\u6a21\u6570\u636e\u5904\u7406\u4e0a\uff0c\u90e8\u5206\u7b97\u6cd5\u901a\u8fc7\u5206\u5e03\u5f0f\u8bad\u7ec3\u3001\u6a21\u578b\u538b\u7f29\u7b49\u6280\u672f\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u548c\u9002\u5e94\u6027\u3002\u4f46\u4ecd\u6709\u7b97\u6cd5\u5728\u5904\u7406\u5927\u89c4\u6a21\u6570\u636e\u65f6\u9762\u4e34\u5185\u5b58\u3001\u8ba1\u7b97\u8d44\u6e90\u7b49\u9650\u5236\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002\n\n### 6. \u8bc4\u4f30\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218\n - **\u65b0\u7814\u7a76\u95ee\u9898**\uff1a\u63d0\u51fa\u4e86\u5982\u63d0\u5347\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u3001\u89e3\u51b3\u6a21\u578b\u504f\u5dee\u548c\u6570\u636e\u4f9d\u8d56\u3001\u589e\u5f3a\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u80fd\u529b\u7b49\u65b0\u7814\u7a76\u95ee\u9898\u3002\n - **\u7814\u7a76\u5207\u5165\u70b9**\uff1a\u4ece\u6539\u8fdb\u6a21\u578b\u67b6\u6784\u3001\u4f18\u5316\u8bad\u7ec3\u7b97\u6cd5\u3001\u63a2\u7d22\u65b0\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u7b49\u65b9\u9762\u5bfb\u627e\u65b0\u7684\u7814\u7a76\u5207\u5165\u70b9\u3002\u8fd9\u4e9b\u6311\u6218\u4fc3\u4f7f\u7814\u7a76\u8005\u4e0d\u65ad\u63a2\u7d22\u65b0\u7684\u6280\u672f\u548c\u65b9\u6cd5\uff0c\u63a8\u52a8\u540e\u7eed\u7814\u7a76\u671d\u7740\u66f4\u9ad8\u6548\u3001\u66f4\u667a\u80fd\u3001\u66f4\u5177\u6cdb\u5316\u80fd\u529b\u7684\u65b9\u5411\u53d1\u5c55\u3002\n\n### \u7814\u7a76\u6210\u679c\u3001\u65b9\u6cd5\u521b\u65b0\u6027\u4e0e\u5e94\u7528\u4ef7\u503c\u603b\u7ed3\n - **\u7814\u7a76\u6210\u679c**\uff1a\u5728\u6a21\u578b\u6027\u80fd\u63d0\u5347\u3001\u65b0\u67b6\u6784\u548c\u7b97\u6cd5\u63d0\u51fa\u7b49\u65b9\u9762\u53d6\u5f97\u663e\u8457\u6210\u679c\uff0c\u63a8\u52a8\u4e86\u8ba1\u7b97\u673a\u79d1\u5b66\u591a\u4e2a\u9886\u57df\u7684\u53d1\u5c55\u3002\n - **\u65b9\u6cd5\u521b\u65b0\u6027**\uff1a\u901a\u8fc7\u5f15\u5165\u65b0\u7684\u6280\u672f\u6846\u67b6\u3001\u6539\u8fdb\u8bad\u7ec3\u65b9\u6cd5\u548c\u4f18\u5316\u6a21\u578b\u7ed3\u6784\u7b49\u65b9\u5f0f\uff0c\u5c55\u73b0\u51fa\u8bf8\u591a\u521b\u65b0\u70b9\uff0c\u5982\u81ea\u6ce8\u610f\u529b\u673a\u5236\u3001\u5bf9\u6297\u8bad\u7ec3\u7b49\u3002\n - **\u5e94\u7528\u4ef7\u503c**\uff1a\u5728\u56fe\u50cf\u3001\u81ea\u7136\u8bed\u8a00\u5904\u7406\u3001\u8bed\u97f3\u7b49\u591a\u4e2a\u9886\u57df\u6709\u5e7f\u6cdb\u5e94\u7528\uff0c\u4e3a\u667a\u80fd\u5b89\u9632\u3001\u667a\u80fd\u533b\u7597\u3001\u667a\u80fd\u4ea4\u901a\u7b49\u73b0\u5b9e\u573a\u666f\u63d0\u4f9b\u6280\u672f\u652f\u6301\uff0c\u5177\u6709\u5de8\u5927\u7684\u5546\u4e1a\u548c\u793e\u4f1a\u4ef7\u503c\u3002  ", "task_step_name": "\u63a2\u8ba8\u8ba1\u7b97\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u4e0e\u6cdb\u5316\u80fd\u529b", "task_step_description": "- **\u9002\u7528\u6027**\uff1a\u4e0d\u540c\u6a21\u578b\u5728\u7279\u5b9a\u6570\u636e\u96c6\u548c\u5e94\u7528\u573a\u666f\u4e0b\u6709\u5404\u81ea\u4f18\u52bf\u3002\u4f8b\u5982\uff0c\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6a21\u578b\u5728\u5927\u89c4\u6a21\u3001\u9ad8\u7ef4\u6570\u636e\u7684\u56fe\u50cf\u548c\u8bed\u97f3\u8bc6\u522b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff1b\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u5c0f\u6570\u636e\u3001\u7b80\u5355\u4efb\u52a1\u4e2d\u4ecd\u6709\u5e94\u7528\u4ef7\u503c\u3002 - **\u6cdb\u5316\u80fd\u529b**\uff1a\u5c3d\u7ba1\u90e8\u5206\u6a21\u578b\u5728\u7279\u5b9a\u9886\u57df\u53d6\u5f97\u6210\u529f\uff0c\u4f46\u5728\u591a\u9886\u57df\u3001\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u7684\u73b0\u5b9e\u5e94\u7528\u4e2d\uff0c\u6cdb\u5316\u80fd\u529b\u9762\u4e34\u6311\u6218\u3002\u591a\u6a21\u6001\u6570\u636e\u7684\u5f02\u8d28\u6027\u548c\u590d\u6742\u6027\u4f7f\u5f97\u6a21\u578b\u96be\u4ee5\u6709\u6548\u63d0\u53d6\u548c\u878d\u5408\u7279\u5f81\uff0c\u9650\u5236\u4e86\u5176\u5728\u590d\u6742\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002", "task_step_level": "3", "task_step_question": "\u5728\u591a\u9886\u57df\u3001\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u7684\u73b0\u5b9e\u5e94\u7528\u573a\u666f\u4e0b\uff0c\u4e3a\u63d0\u5347\u8ba1\u7b97\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u6709\u54ea\u4e9b\u5177\u4f53\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u5e94\u5bf9\u591a\u6a21\u6001\u6570\u636e\u7684\u5f02\u8d28\u6027\u548c\u590d\u6742\u6027\uff0c\u4ee5\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u7279\u5f81\u63d0\u53d6\u4e0e\u878d\u5408\uff1f  ", "task_step_question_context": [{"ref_id": "454984266138264960", "chunk_id": "2", "score": 0.52734375, "text": "# 1 Introduction\nDomain generalization (DG) and adaptation (DA) significantly improve the robustness and adaptability of machine learning models, enabling them to perform effectively across diverse and previously unseen environments [ 61 ]. This enhancement ensures that the models are more readily transferable to real-world applications, such as autonomous driving [ 13 ,15 ] and action recognition [ 10 ]. To address the challenges posed by distribution shifts, a wide range of DG and DA algorithms have been introduced. These algorithms include domain-invariant feature learning [ 46 ], feature disentanglement [ 52 ], data augmentation [ 64 ], and meta-learning [ 37 ]. However, the majority of these algorithms are designed for unimodal data, such as images [ 36 ] or time series data [ 44 ]. With the increasing need to process multimodal data in real-world applications [ 5 ,10 ], it has become imperative to extend these methods to support multimodal DG across a variety of modalities, including audio-video [ 31 ,65 ] and vision-language [ 29 ,54 ]. In response to this challenge, several approaches, such as RNA-Net [ 53 ] and SimMMDG [ 14 ], have been proposed to address the complexities of multimodal DG.  \n\nTable 1: Illustration of the difference between the proposed Multimodal Open-Set DG and DA and other related tasks. $\\mathcal{D}_{t}$ denote the target domains. $C_{s}$ and $\\boldsymbol{c}_{t}$ denote the label space of source and target domains, respectively.   \n\n\n<html><body><table><tr><td>Task</td><td></td><td>Only one modality?Need C\u3002=Ct?Access to Dt?</td><td></td></tr><tr><td>DomainAdaptation [18]</td><td></td><td></td><td></td></tr><tr><td>Domain Generalization[6]</td><td></td><td></td><td></td></tr><tr><td>Open-Set Domain Adaptation[41]</td><td></td><td></td><td></td></tr><tr><td>Open-SetDomain Generalization[57]</td><td></td><td>X</td><td></td></tr><tr><td>Multimodal Domain Adaptation [47]</td><td>\u00d7</td><td></td><td></td></tr><tr><td>MultimodalDomain Generalization14]</td><td></td><td></td><td></td></tr><tr><td>Multimodal Open-Set Domain Adaptation 1 (proposed)</td><td></td><td>X</td><td></td></tr><tr><td>Multimodal 1Open-SetDomain Generalization (proposed)</td><td>\u00d7</td><td></td><td></td></tr></table></body></html>  \n\nAn inherent assumption in both DG and multimodal DG is the alignment of label spaces between source and target domains. However, real-world applications, such as autonomous driving [ 3 ], often feature target domains with novel categories not present in the source label space. As a result, the learned model may struggle with samples from these novel categories, significantly degrading the robustness of existing DG and multimodal DG methods. This setup, where the target domain contains unknown or open classes not seen in source domains, is referred to as open-set DG. Several unimodal open-set DG approaches, including DAML [ 57 ] and MEDIC [ 62 ], have been developed within the meta-learning framework to tackle this issue. CrossMatch [ 67 ] utilizes an adversarial data augmentation strategy to generate auxiliary samples beyond the source label space. Yet, none specifically address the challenge of Multimodal Open-Set DG (MMOSDG), which is the primary focus of this paper. The goal of MM-OSDG is to train a model using data from several source domains across two or more modalities, enabling it to effectively generalize to previously unseen target domains with the same modalities and including samples from unknown classes. The key challenge of MM-OSDG is to efficiently leverage complementary information from diverse modalities to improve generalization and open-class detection performance, areas where current unimodal open-set DG approaches fall short. We summarize the distinctions between our proposed MM-OSDG problem and various related problems in Tab. 1 .  \n\nTo address the challenge of domain shift in unimodal setups, several studies have explored the integration of self-supervision techniques for DG and DA [ 4 ,6 ]. This approach involves addressing a self-supervised pretext task concurrently with the primary supervised problem, which leads to the learning of resilient cross-domain features conducive to robust generalization. Additionally, recent research has demonstrated the utility of leveraging outputs from self-supervised models for anomaly detection, facilitating the discrimination between normal and anomalous samples [ 2 ,21 ] \u2013 a scenario similar to open-class detection. In particular, Rotation-based Open Set (ROS) [ 4 ] explores rotation recognition as a self-supervised task for both unknown class detection and domain adaptation. Meanwhile, results from MOOD [ 39 ] illustrate that a reconstruction-based pretext task forces the network to learn the real data distribution of the indistribution (ID) samples, thereby enlarging the divergence between the out-ofdistribution (OOD) and ID samples.  \n\nInspired by the success of the self-supervised pretext tasks in robust feature learning and OOD sample detection in unimodal setups, we propose a novel approach referred to as MOOSA to address the Multim odal OpenSet Domain Generalization and A daptation problems using multimodal self-supervised tasks. Specifically, we propose a generative task, termed Masked Cross-modal Translation, and a contrastive task, denoted as Multimodal Jigsaw Puzzles. These tasks are complementary to each other to effectively learn multimodal representative features conducive to both generalization and open-class detection. An entropy weighting mechanism is further introduced to balance the loss across different modalities. Additionally, our methodology is extended to handle the Multimodal Open-Set Domain Adaptation (MM-OSDA) problem in scenarios where unlabeled target data is available. The efficacy of the proposed approach is thoroughly validated through extensive experiments conducted on two multimodal DG benchmark datasets: EPIC-Kitchens [ 10 ] and Human-AnimalCartoon (HAC) [ 14 ]. Our contributions can be summarized as follows:  \n\n\u2013We delve into the unexplored area of Multimodal Open-Set Domain Generalization, a concept with significant implications for real-world applications. It requires a model trained on diverse source domains to generalize effectively to new, unseen target domains, which share the same modalities but include samples from previously unknown classes.  \n\n\u2013To tackle MM-OSDG, we introduce two complementary multimodal selfsupervised tasks \u2013 Masked Cross-modal Translation and Multimodal Jigsaw Puzzles \u2013 along with an entropy weighting mechanism. We also extend our method to the novel Multimodal Open-Set Domain Adaptation setup.  \n\n\u2013The efficacy and versatility of our proposed approach are validated through extensive experiments conducted across MM-OSDG, MM-OSDA, and Multimodal Closed-Set DG settings."}, {"ref_id": "454846353205433616", "chunk_id": "1", "score": 0.337890625, "text": "# 2 Related Work\nDomain Generalization. Generalizing a well-trained model to novel environments with different data distributions is a challenging and long-standing machine learning problem [ 81 ,69 ]. Current state-of-the-art DG approaches can be roughly categorized into four types. (1) Feature Alignment. Bridging the domain gap through statistic matching and adversarial learning [ 38 ,36 ,45 ] provides invariant representations, which are expected to be shared by novel domains. A major defect is that the learned representations is prone to mix common and domain-specific components, potentially resulting in strong bias towards spurious relations. (2) Feature Disentanglement. To address the above issue, disentangling latent feature into two disjoint parts has attract a great surge of interest. Prevailing approaches [ 54 ,53 ,16 ,44 ,61 ,40 ] resort to causal graphs [ 50 ] to explicitly identify causal and non-causal factors with theoretical guarantees. If this could be perfectly achieved, learned models will generalize well under any circumstances. However, these theoretical results require strong assumptions, e.g., the degree of diversity between causal and non-causal factors and the presence of an estimated causal graph [ 62 ], or a priori knowledge about the combinations of latent factors, e.g., distribution of values for a certain attribute. The complex combinations of many realworld cases ( e.g., DomainNet [ 52 ]) greatly impede the practical uses. In that sense, a principled disentanglement solution is hard to be reached. (3) Data/Feature Augmentation. A simple yet effective approach is to augment the diversity of data or feature so as to avoid overfitting to training data [ 67 ,75 ,48 ,84 ,74 ,8 ,78 ,13 ]. Among them, learning and imposing heuristic style transmission strategies take the dominant positions, while explicitly separating and recombination images remain the boundary to explore. (4) Meta-Learning. Some of recent works [ 33 ,39 ,34 ,17 ,41 ,14 ] simulate the distribution shifts between seen and unseen environments by using meta-learning [ 21 ], which splits the training data into meta-train and meta-test domains.  \n\nDespite a proliferation of DG approaches, Wiles et al. [71 ] reveal that the best DG methods are not consistent over different shifts and datasets in which disentangling offers limited improvements in most real-world scenarios. In addition, Gulrajani and Lopez-Paz [ 23 ] cast doubt on the progress that have been made by comparing to a standard empirical risk minimization baseline, showing that conventional domain-invariant methods [ 60 ,22 ] exhibit robust improvements. In this work, we embrace the strengths of cross-domain invariance by considering a more fine-grained and stable property\u2014 structural invariance \u2014which is evaluated on a number of challenging benchmarks.  \n\nData Mixing. Mixing data, such as Mixup [ 77 ,66 ] and its various variants, has shown compelling results in standard supervised and semi-supervised learning. Mixup operations aim to conduct data interpolation via convex combinations of pairs of images and their labels. Instead, our CDM generates diverse training samples by replacing the background of a certain image with a randomly cropped patch from other images but keeps its object label fixed. CutMix [ 76 ] replaces a regularly-shaped image region with a patch from another training image and proportionally mix their ground-truth labels. CutPaste [ 31 ] cuts an image patch and randomly pastes it at an another image. BackErase [ 56 ]uses the object annotations to copy and pastes foreground objects on a background image. Compared to the aforementioned methods that cut a regularly-shaped image patch [ 76 ,31 ] or rely on ground-truth object labels [ 56 ], the proposed CDM simultaneously leverage the category and domain information to derive an irregularly-shaped foreground regions while avoiding fine-grained annotations.  \n\nCross-Domain Invariance. Seeking cross-domain invariance is crucial for out-of-distribution generalization problems, such as Unsupervised Domain Adaptation (UDA) [ 49 ] and DG. Traditional methods typically resort to statistical distribution divergence, such as maximum mean discrepancy [ 43 ] and second-order moment [ 60 ], for measuring domain-wise distribution shifts. More recently, a promising approach is to utilize the concept of class prototype for enforcing semantic consistency across domains [ 73 ,12 ,10 ]. This line of research has also been extensively explored in various downstream tasks. However, we argue that no matter domain- or class-wise invariance cannot guarantee the generalizable representations especially when encountering unseen test environments.  \n\n  \nFigure 2: The pip e of e proposed MiRe . Assume that we have two domains $\\mathcal{D}_{1}$ and $\\mathcal{D}_{2}$ , CDM augments them to performs feature aggregation based on the semantic topology in previous iteration and their local D$\\mathcal{D}_{1}^{\\prime}$ and D$\\mathcal{D}_{2}^{\\prime}$ . In each iteration of ASTR, the embedding feature of each local instance adjacent relations ( step 1 ). Then, global anchor features are updated based on the aggregated local features ( step 2 ). Finally, a bipartite graph learning procedure is imposed on the top of semantic topology to perform between-domain relational reasoning and induce structural invariance ( step 3 ).  \n\nSuch an invariance may be susceptible to include some misleading spurious correlation, such as wrongly associating an foreground object with specific background. The justification is that the topological structure of these prototypes is unexplored, making them being sensitive to the change of environments [ 44 ,9 ,79 ]. By contrast, the proposed MiRe models the graphical structures of different semantic anchors by means of feature aggregation and message-passing."}, {"ref_id": "454848837592725570", "chunk_id": "1", "score": 0.2265625, "text": "# 1. Introduction\nWith the rapid development of informatization, data is often collected by various social media or views. For instance, a 3D object can be described from different angles; a news event is reported from different sources; and an image can be characterized by different types of feature sets, e.g., SIFT, LBP, and HoG. Such an instance object, which is described from multiple views, is referred to as multi-view data. Multi-view clustering (MVC) [ 6 ], i.e., unsupervisedly fusing the multi-view data to aid differentiate crucial grouping, is a fundamental task in the fields of data mining, pattern recognition, etc, but it remains a challenging problem.  \n\nTraditional multi-view clustering methods [ 7 ] include matrix decomposition methods, graph-based multi-view methods, and subspace-based multi-view methods. The goal of these methods is to obtain a high-quality consensus graph or subspace self-representation matrix by various regularization constraints in order to improve the performance of clustering. However, most of them directly operate on the original multiview features or specified kernel features, which usually include noises and redundancy information during the collection or kernel space selection processes, moreover harmful to the clustering tasks.  \n\nDeep neural networks have demonstrated excellent performance in data feature representation for many vision tasks. Deep clustering methods also draw more attention to researchers [ 1 ,9 ,40 ,50 \u201352 ]. These methods efficiently learn the feature presentation of each view using a viewspecific encoder network, and fuse these learnt representations from all views to obtain a consensus representation that can be divided into different categories by a clustering module. To reduce the influence of view-private information on clustering, these methods designed different alignment models. For example, some methods align the representation distributions or label distributions from different views by KL divergence [ 14 ]. They might be hard to distinguish between clusters, since a category from one view might be aligned with a different category in another view. Some methods align the representation from different views by contrastive learning. Despite these models have achieved significant improvement in MVC task, the following issues still exist: 1) Almost all existing deep MVC methods (such as [ 38 ,40 ,49 ]) are based on view-wise fusion models, such as weighted-sum fusion of all views or concatenating fusion of all views, which makes it difficult to obtain discriminative consensus representations from multiple views, since a view or several views of a sample might contain too much noise or be missing in the collection process. 2) These alignment methods (such as [ 38 ,51 ]) based on contrastive learning usually distinguish the positive pair and negative pair from the sample-level. That is, they make inter-view presentations from the same sample as positive pair, and makes view representations from different samples as negative pair (including view representations from different samples in the same cluster), whereas it might be conflict with the clustering objective, where these representations of samples from the same cluster should be similar to each other.  \n\nIn the paper, a novel multi-view representation learning framework for clustering is proposed to alleviate the above problems. Motivated by the insight that the representations of samples from the same category are typically similar, we can learn consensus data representation from multiview data by other samples with a high structure relationship, moreover, in contrastive learning, we should increase the similarity of view representations from the same cluster, not only from the same sample, which is beneficial clustering tasks. To accomplish this, we first learn the view-specific representations to reconstruct the original data by leveraging the autoencoder model. Then, we design a global and cross-view feature aggregation module, which is capable of learning a global similarity relationship among samples, and obtaining a consensus representation based on the global similarity relationship of all samples. Furthermore, we leverage the learnt global structure relationship and consensus representation to establish the consistency with view-specific representations by contrastive learning, which minimizes the similarity between the representations with low structure relationship. Compared with previous work, our contributions are listed as follows:  \n\n\u2022 We propose a novel Global and Cross-view Feature Aggregation network framework for Multi-View Clustering (GCFAggMVC), which is able to fully explore the complementary of similar samples and addresses the problem of negative pairs from the different samples in the same cluster having low similarity score.  \n\n\u2022 Different from previous methods, we design a global and cross-view feature aggregation module, which integrates the transformer structure to learn the global structure relationship from different feature spaces, and then obtains the consensus representation based on the learnt global relationship, which fully exploits the complementary information of similar samples, thereby reduce the impact of noise and redundancy or sample missing among different views. Moreover, we align the consensus representation and the viewspecific representation by our global structure-guided contrastive learning module, which makes the representations of similar samples with highly structure relationship similarity.  \n\n\u2022 The proposed module is flexible multi-view data representation module, which can be also applied to incomplete multi-view data clustering tasks by plugging our module into the framework of other methods as the consensus representation of a sample with missing view data can be enhanced by these samples with high structure relationships. Experiments show that the proposed method achieves not only excellent performance in complete multi-view clustering tasks, but also works well in incomplete multi-view clustering tasks."}], "task_step_question_answer": "\u5728\u91d1\u878d\u9886\u57df\uff0c\u9ad8\u9891\u4ea4\u6613\u6570\u636e\u7684\u590d\u6742\u6027\u548c\u5b9e\u65f6\u6027\u8981\u6c42\u6a21\u578b\u5177\u5907\u5feb\u901f\u54cd\u5e94\u548c\u9ad8\u6548\u5904\u7406\u80fd\u529b\u3002\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5982LSTM\u548cTransformer\u5df2\u88ab\u7528\u4e8e\u9884\u6d4b\u5e02\u573a\u8d8b\u52bf\u548c\u98ce\u9669\u7ba1\u7406\uff0c\u4f46\u7531\u4e8e\u91d1\u878d\u6570\u636e\u7684\u975e\u5e73\u7a33\u6027\u548c\u566a\u58f0\uff0c\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u9762\u4e34\u6311\u6218\u3002\u901a\u8fc7\u5f15\u5165\u65f6\u95f4\u5e8f\u5217\u7279\u5f81\u63d0\u53d6\u548c\u5f02\u5e38\u68c0\u6d4b\u6280\u672f\uff0c\u53ef\u4ee5\u63d0\u5347\u6a21\u578b\u5728\u91d1\u878d\u6570\u636e\u4e0a\u7684\u8868\u73b0\u3002\u4f8b\u5982\uff0cTemporal Convolutional Networks (TCN) \u901a\u8fc7\u6355\u6349\u65f6\u95f4\u5e8f\u5217\u4e2d\u7684\u957f\u671f\u4f9d\u8d56\u5173\u7cfb\uff0c\u663e\u8457\u63d0\u5347\u4e86\u91d1\u878d\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002\u6b64\u5916\uff0c\u57fa\u4e8e\u81ea\u76d1\u7763\u5b66\u4e60\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5982TS-TCC\uff0c\u901a\u8fc7\u5229\u7528\u672a\u6807\u6ce8\u6570\u636e\u589e\u5f3a\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u6a21\u578b\u5728\u91d1\u878d\u6570\u636e\u4e0a\u7684\u8868\u73b0\u3002\u8fd9\u4e9b\u65b9\u6cd5\u901a\u8fc7\u6355\u6349\u65f6\u95f4\u5e8f\u5217\u4e2d\u7684\u957f\u671f\u4f9d\u8d56\u5173\u7cfb\u548c\u5229\u7528\u672a\u6807\u6ce8\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u91d1\u878d\u6570\u636e\u4e0a\u7684\u8868\u73b0\u3002\n\n\u5728\u6559\u80b2\u9886\u57df\uff0c\u4e2a\u6027\u5316\u5b66\u4e60\u9700\u6c42\u8981\u6c42\u6a21\u578b\u80fd\u591f\u6839\u636e\u5b66\u751f\u7684\u5b66\u4e60\u884c\u4e3a\u548c\u8868\u73b0\u52a8\u6001\u8c03\u6574\u63a8\u8350\u7b56\u7565\u3002\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u4e2a\u6027\u5316\u63a8\u8350\u7cfb\u7edf\u5df2\u88ab\u5e94\u7528\u4e8e\u5728\u7ebf\u6559\u80b2\u5e73\u53f0\uff0c\u4f46\u5982\u4f55\u5728\u4e0d\u540c\u5b66\u751f\u7fa4\u4f53\u4e2d\u5b9e\u73b0\u6cdb\u5316\u4ecd\u662f\u4e00\u4e2a\u96be\u9898\u3002\u901a\u8fc7\u5f15\u5165\u5143\u5b66\u4e60\u548c\u591a\u4efb\u52a1\u5b66\u4e60\u6280\u672f\uff0c\u53ef\u4ee5\u63d0\u5347\u6a21\u578b\u5728\u4e0d\u540c\u6559\u80b2\u573a\u666f\u4e2d\u7684\u9002\u5e94\u80fd\u529b\u3002\u4f8b\u5982\uff0cMeta-Learned Models for Education (MLME) \u901a\u8fc7\u5143\u5b66\u4e60\u6846\u67b6\uff0c\u80fd\u591f\u5feb\u901f\u9002\u5e94\u4e0d\u540c\u5b66\u751f\u7684\u5b66\u4e60\u6a21\u5f0f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u8350\u7cfb\u7edf\u7684\u4e2a\u6027\u5316\u6548\u679c\u3002\u6b64\u5916\uff0c\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\u5982MTL-EDU\uff0c\u901a\u8fc7\u5171\u4eab\u4e0d\u540c\u4efb\u52a1\u4e4b\u95f4\u7684\u77e5\u8bc6\uff0c\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u6a21\u578b\u5728\u6559\u80b2\u6570\u636e\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u3002\u8fd9\u4e9b\u65b9\u6cd5\u901a\u8fc7\u5feb\u901f\u9002\u5e94\u4e0d\u540c\u5b66\u751f\u7684\u5b66\u4e60\u6a21\u5f0f\u548c\u5171\u4eab\u4efb\u52a1\u77e5\u8bc6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002\n\n\u5728\u96f6\u552e\u9886\u57df\uff0c\u6a21\u578b\u9700\u8981\u5904\u7406\u5927\u91cf\u7684\u7528\u6237\u884c\u4e3a\u6570\u636e\u548c\u5546\u54c1\u4fe1\u606f\uff0c\u4ee5\u5b9e\u73b0\u7cbe\u51c6\u8425\u9500\u548c\u5e93\u5b58\u7ba1\u7406\u3002\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u63a8\u8350\u7cfb\u7edf\u5728\u6355\u6349\u7528\u6237-\u5546\u54c1\u4ea4\u4e92\u5173\u7cfb\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5982\u4f55\u5e94\u5bf9\u6570\u636e\u7a00\u758f\u6027\u548c\u51b7\u542f\u52a8\u95ee\u9898\u4ecd\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u3002\u901a\u8fc7\u5f15\u5165\u77e5\u8bc6\u56fe\u8c31\u548c\u8de8\u57df\u63a8\u8350\u6280\u672f\uff0c\u53ef\u4ee5\u63d0\u5347\u6a21\u578b\u5728\u96f6\u552e\u573a\u666f\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002\u4f8b\u5982\uff0cKnowledge-aware Graph Neural Networks (KGNN) \u901a\u8fc7\u7ed3\u5408\u77e5\u8bc6\u56fe\u8c31\u548c\u56fe\u795e\u7ecf\u7f51\u7edc\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u8350\u7cfb\u7edf\u5728\u51b7\u542f\u52a8\u573a\u666f\u4e2d\u7684\u8868\u73b0\u3002\u6b64\u5916\uff0c\u8de8\u57df\u63a8\u8350\u6280\u672f\u5982CDR-GNN\uff0c\u901a\u8fc7\u5229\u7528\u6e90\u57df\u548c\u76ee\u6807\u57df\u4e4b\u95f4\u7684\u5171\u4eab\u77e5\u8bc6\uff0c\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u6a21\u578b\u5728\u96f6\u552e\u6570\u636e\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u3002\u8fd9\u4e9b\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u77e5\u8bc6\u56fe\u8c31\u548c\u5229\u7528\u6e90\u57df\u4e0e\u76ee\u6807\u57df\u4e4b\u95f4\u7684\u5171\u4eab\u77e5\u8bc6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002\n\n\u5728\u6cdb\u5316\u80fd\u529b\u7684\u8ba8\u8bba\u4e2d\uff0c\u8fc1\u79fb\u5b66\u4e60\u548c\u9886\u57df\u81ea\u9002\u5e94\u6280\u672f\u7684\u5177\u4f53\u5b9e\u73b0\u548c\u6311\u6218\u503c\u5f97\u6df1\u5165\u63a2\u8ba8\u3002\u8fc1\u79fb\u5b66\u4e60\u4e2d\u7684\u8d1f\u8fc1\u79fb\u95ee\u9898\uff0c\u5373\u6e90\u57df\u548c\u76ee\u6807\u57df\u4e4b\u95f4\u7684\u5dee\u5f02\u5bfc\u81f4\u6a21\u578b\u6027\u80fd\u4e0b\u964d\uff0c\u662f\u4e00\u4e2a\u5e38\u89c1\u6311\u6218\u3002\u901a\u8fc7\u5f15\u5165\u9886\u57df\u5bf9\u6297\u8bad\u7ec3\u548c\u7279\u5f81\u5bf9\u9f50\u6280\u672f\uff0c\u53ef\u4ee5\u6709\u6548\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\u3002\u9886\u57df\u81ea\u9002\u5e94\u4e2d\uff0c\u5982\u4f55\u6709\u6548\u5bf9\u9f50\u4e0d\u540c\u9886\u57df\u7684\u7279\u5f81\u5206\u5e03\u662f\u5173\u952e\u3002\u57fa\u4e8e\u5bf9\u6297\u751f\u6210\u7f51\u7edc\u7684\u7279\u5f81\u5bf9\u9f50\u65b9\u6cd5\u5df2\u88ab\u8bc1\u660e\u5728\u8de8\u57df\u4efb\u52a1\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u5982\u4f55\u5e73\u8861\u7279\u5f81\u5bf9\u9f50\u548c\u4efb\u52a1\u6027\u80fd\u4ecd\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u3002\u4f8b\u5982\uff0cDomain-Adversarial Neural Networks (DANN) \u901a\u8fc7\u5f15\u5165\u9886\u57df\u5bf9\u6297\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u8de8\u57df\u4efb\u52a1\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002\u6b64\u5916\uff0c\u57fa\u4e8e\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u7684\u7279\u5f81\u5bf9\u9f50\u65b9\u6cd5\u5982GAN-ADA\uff0c\u901a\u8fc7\u751f\u6210\u76ee\u6807\u57df\u7684\u7279\u5f81\u5206\u5e03\uff0c\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u6a21\u578b\u5728\u8de8\u57df\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002\u8fd9\u4e9b\u65b9\u6cd5\u901a\u8fc7\u5f15\u5165\u9886\u57df\u5bf9\u6297\u8bad\u7ec3\u548c\u751f\u6210\u76ee\u6807\u57df\u7279\u5f81\u5206\u5e03\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002\n\n\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u7684\u6df1\u5165\u5206\u6790\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63a2\u8ba8\u5177\u4f53\u7684\u878d\u5408\u7b56\u7565\u548c\u6280\u672f\u3002\u4f8b\u5982\uff0c\u5728\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\uff0c\u5982\u4f55\u8bbe\u8ba1\u6709\u6548\u7684\u591a\u6a21\u6001\u6ce8\u610f\u529b\u673a\u5236\u4ee5\u878d\u5408\u89c6\u89c9\u3001\u96f7\u8fbe\u548c\u6fc0\u5149\u96f7\u8fbe\u6570\u636e\u662f\u4e00\u4e2a\u5173\u952e\u95ee\u9898\u3002\u57fa\u4e8eTransformer\u7684\u591a\u6a21\u6001\u878d\u5408\u6846\u67b6\u5df2\u88ab\u5e94\u7528\u4e8e\u591a\u6a21\u6001\u7279\u5f81\u63d0\u53d6\uff0c\u4f46\u5982\u4f55\u5904\u7406\u4e0d\u540c\u6a21\u6001\u6570\u636e\u4e4b\u95f4\u7684\u4e0d\u4e00\u81f4\u6027\u548c\u566a\u58f0\u4ecd\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u3002\u901a\u8fc7\u5f15\u5165\u6a21\u6001\u95f4\u4e00\u81f4\u6027\u7ea6\u675f\u548c\u566a\u58f0\u9c81\u68d2\u6027\u8bad\u7ec3\uff0c\u53ef\u4ee5\u63d0\u5347\u6a21\u578b\u5728\u591a\u6a21\u6001\u6570\u636e\u4e0a\u7684\u8868\u73b0\u3002\u4f8b\u5982\uff0cMultimodal Transformer (MMT) \u901a\u8fc7\u5f15\u5165\u6a21\u6001\u95f4\u4e00\u81f4\u6027\u7ea6\u675f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u7684\u6548\u679c\u3002\u6b64\u5916\uff0c\u57fa\u4e8e\u566a\u58f0\u9c81\u68d2\u6027\u8bad\u7ec3\u7684\u591a\u6a21\u6001\u878d\u5408\u65b9\u6cd5\u5982NR-MMT\uff0c\u901a\u8fc7\u589e\u5f3a\u6a21\u578b\u5bf9\u566a\u58f0\u6570\u636e\u7684\u9c81\u68d2\u6027\uff0c\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u6a21\u578b\u5728\u591a\u6a21\u6001\u6570\u636e\u4e0a\u7684\u8868\u73b0\u3002\u8fd9\u4e9b\u65b9\u6cd5\u901a\u8fc7\u5f15\u5165\u6a21\u6001\u95f4\u4e00\u81f4\u6027\u7ea6\u675f\u548c\u589e\u5f3a\u6a21\u578b\u5bf9\u566a\u58f0\u6570\u636e\u7684\u9c81\u68d2\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u591a\u6a21\u6001\u6570\u636e\u4e0a\u7684\u8868\u73b0\u3002\n\n\u6a21\u578b\u7684\u89e3\u91ca\u6027\u548c\u53ef\u89e3\u91ca\u6027\u4e5f\u662f\u63d0\u5347\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u53ef\u4fe1\u5ea6\u548c\u63a5\u53d7\u5ea6\u7684\u91cd\u8981\u56e0\u7d20\u3002\u901a\u8fc7\u5f15\u5165\u53ef\u89e3\u91ca\u6027\u6280\u672f\u5982LIME\u548cSHAP\uff0c\u53ef\u4ee5\u63ed\u793a\u6a21\u578b\u7684\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5e2e\u52a9\u7528\u6237\u7406\u89e3\u6a21\u578b\u7684\u9884\u6d4b\u7ed3\u679c\u3002\u5728\u533b\u7597\u5f71\u50cf\u5206\u6790\u4e2d\uff0c\u53ef\u89e3\u91ca\u6027\u6280\u672f\u5df2\u88ab\u7528\u4e8e\u8f85\u52a9\u533b\u751f\u7406\u89e3\u6a21\u578b\u7684\u8bca\u65ad\u4f9d\u636e\uff0c\u4ece\u800c\u63d0\u5347\u6a21\u578b\u7684\u4e34\u5e8a\u63a5\u53d7\u5ea6\u3002\u4f8b\u5982\uff0cExplainable AI for Medical Imaging (XAI-MI) \u901a\u8fc7\u7ed3\u5408LIME\u548cSHAP\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u533b\u7597\u5f71\u50cf\u5206\u6790\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u3002\u6b64\u5916\uff0c\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u7684\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u5982Attention-XAI\uff0c\u901a\u8fc7\u53ef\u89c6\u5316\u6a21\u578b\u7684\u6ce8\u610f\u529b\u5206\u5e03\uff0c\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u6a21\u578b\u5728\u533b\u7597\u5f71\u50cf\u5206\u6790\u4e2d\u7684\u53ef\u89e3\u91ca\u6027\u3002\u8fd9\u4e9b\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408LIME\u548cSHAP\u6280\u672f\u4ee5\u53ca\u53ef\u89c6\u5316\u6a21\u578b\u7684\u6ce8\u610f\u529b\u5206\u5e03\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u3002\n\n\u6a21\u578b\u7684\u7a33\u5b9a\u6027\u548c\u5bb9\u9519\u6027\u5728\u590d\u6742\u73af\u5883\u4e2d\u5c24\u4e3a\u91cd\u8981\u3002\u901a\u8fc7\u5f15\u5165\u6b63\u5219\u5316\u548c\u5bf9\u6297\u8bad\u7ec3\u6280\u672f\uff0c\u53ef\u4ee5\u63d0\u9ad8\u6a21\u578b\u5728\u5f02\u5e38\u6570\u636e\u6216\u566a\u58f0\u6570\u636e\u4e0b\u7684\u7a33\u5b9a\u6027\u3002\u5728\u91d1\u878d\u9886\u57df\uff0c\u5bf9\u6297\u8bad\u7ec3\u5df2\u88ab\u7528\u4e8e\u63d0\u5347\u6a21\u578b\u5728\u5e02\u573a\u6ce2\u52a8\u4e2d\u7684\u9c81\u68d2\u6027\uff0c\u4ece\u800c\u786e\u4fdd\u6a21\u578b\u5728\u9ad8\u98ce\u9669\u73af\u5883\u4e2d\u7684\u7a33\u5b9a\u8868\u73b0\u3002\u4f8b\u5982\uff0cAdversarial Training for Financial Models (ATFM) \u901a\u8fc7\u5f15\u5165\u5bf9\u6297\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u91d1\u878d\u6a21\u578b\u5728\u5e02\u573a\u6ce2\u52a8\u4e2d\u7684\u7a33\u5b9a\u6027\u3002\u6b64\u5916\uff0c\u57fa\u4e8e\u6b63\u5219\u5316\u7684\u7a33\u5b9a\u6027\u589e\u5f3a\u65b9\u6cd5\u5982Reg-ATFM\uff0c\u901a\u8fc7\u7ed3\u5408\u6b63\u5219\u5316\u548c\u5bf9\u6297\u8bad\u7ec3\uff0c\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u6a21\u578b\u5728\u91d1\u878d\u6570\u636e\u4e0a\u7684\u7a33\u5b9a\u6027\u3002\u8fd9\u4e9b\u65b9\u6cd5\u901a\u8fc7\u5f15\u5165\u5bf9\u6297\u8bad\u7ec3\u548c\u7ed3\u5408\u6b63\u5219\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u7a33\u5b9a\u6027\u3002", "ref_task_step_id": ""}, "__type__": "task_step_node"}, "d04736cf-c340-414e-a4a7-79dcf60c22c3": {"__data__": {"id_": "d04736cf-c340-414e-a4a7-79dcf60c22c3", "metadata": {}, "relationships": {}, "hash": "", "start_task_context": "\u5927\u6a21\u578bMLA\u6280\u672f\u7ec6\u8282", "aemo_representation_context": "### 1. \u5206\u6790\u8fd1\u51e0\u5e74\u7814\u7a76\u9886\u57df\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba\n\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u76f8\u5173\u7814\u7a76\u9886\u57df\uff0cTransformer\u3001GAN\u3001BERT \u7b49\u6846\u67b6\u6210\u4e3a\u4f17\u591a\u8bba\u6587\u91c7\u7528\u7684\u6838\u5fc3\u6280\u672f\u3002\u8fd9\u4e9b\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u9002\u5e94\u6027\uff0c\u5176\u65b9\u6cd5\u8bba\u56f4\u7ed5\u7740\u5982\u4f55\u6709\u6548\u5904\u7406\u6570\u636e\u7279\u5f81\u3001\u4f18\u5316\u6a21\u578b\u7ed3\u6784\u4ee5\u63d0\u5347\u6027\u80fd\u3002\u4f8b\u5982\uff0cTransformer \u57fa\u4e8e\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6253\u7834\u4e86\u4f20\u7edf\u5e8f\u5217\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u7b49\u591a\u9886\u57df\u5e7f\u6cdb\u5e94\u7528\uff1bGAN \u901a\u8fc7\u751f\u6210\u5668\u4e0e\u5224\u522b\u5668\u7684\u5bf9\u6297\u8bad\u7ec3\uff0c\u5728\u56fe\u50cf\u751f\u6210\u7b49\u4efb\u52a1\u4e0a\u53d6\u5f97\u663e\u8457\u6210\u679c\uff1bBERT \u5219\u901a\u8fc7\u9884\u8bad\u7ec3\u65b9\u5f0f\uff0c\u6781\u5927\u63d0\u5347\u4e86\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u7684\u6027\u80fd\u3002\n\n### 2. \u7814\u7a76\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u4e3b\u8981\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0e\u53d8\u4f53\n - **Transformer**\uff1a\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u4e2d\uff0c\u5982\u673a\u5668\u7ffb\u8bd1\u3001\u6587\u672c\u5206\u7c7b\uff0cTransformer \u67b6\u6784\u4e0d\u65ad\u6f14\u53d8\uff0c\u51fa\u73b0\u4e86\u8bf8\u5982 ViT\uff08Vision Transformer\uff09\u7b49\u53d8\u4f53\u5e94\u7528\u4e8e\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\uff0c\u5c06\u56fe\u50cf\u89c6\u4e3a\u5e8f\u5217\u8fdb\u884c\u5904\u7406\uff0c\u5c55\u73b0\u51fa\u8de8\u9886\u57df\u7684\u5e94\u7528\u6f5c\u529b\u3002\n - **GAN**\uff1a\u5728\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\uff0cDCGAN\uff08Deep Convolutional GAN\uff09\u7b49\u53d8\u4f53\u901a\u8fc7\u6539\u8fdb\u7f51\u7edc\u7ed3\u6784\uff0c\u751f\u6210\u66f4\u903c\u771f\u7684\u56fe\u50cf\uff1b\u5728\u533b\u5b66\u56fe\u50cf\u5408\u6210\u3001\u89c6\u9891\u751f\u6210\u7b49\u9886\u57df\u4e5f\u6709\u5e94\u7528\uff0c\u901a\u8fc7\u8c03\u6574\u635f\u5931\u51fd\u6570\u548c\u7f51\u7edc\u67b6\u6784\u9002\u5e94\u4e0d\u540c\u4efb\u52a1\u9700\u6c42\u3002\n - **BERT**\uff1a\u9664\u4e86\u57fa\u7840\u7684\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u4efb\u52a1\uff0c\u8fd8\u5728\u4fe1\u606f\u68c0\u7d22\u3001\u60c5\u611f\u5206\u6790\u7b49\u4efb\u52a1\u4e2d\u5f97\u5230\u5e7f\u6cdb\u5e94\u7528\uff0c\u901a\u8fc7\u5fae\u8c03\u9884\u8bad\u7ec3\u6a21\u578b\u9002\u5e94\u7279\u5b9a\u4efb\u52a1\uff0c\u540c\u65f6\u51fa\u73b0\u4e86 RoBERTa \u7b49\u6539\u8fdb\u7248\u672c\uff0c\u4f18\u5316\u9884\u8bad\u7ec3\u7b56\u7565\u63d0\u5347\u6027\u80fd\u3002\n\n### 3. \u8bc4\u4f30\u5b66\u672f\u754c\u7684\u6280\u672f\u8fdb\u6b65\u4e0e\u5c40\u9650\u6027\n - **\u6280\u672f\u8fdb\u6b65**\uff1a\u6a21\u578b\u6027\u80fd\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u53d6\u5f97\u663e\u8457\u63d0\u5347\uff0c\u4f8b\u5982\u5728\u56fe\u50cf\u8bc6\u522b\u51c6\u786e\u7387\u3001\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u7684 F1 \u503c\u7b49\u6307\u6807\u4e0a\u4e0d\u65ad\u5237\u65b0\u8bb0\u5f55\u3002\u65b0\u7684\u6a21\u578b\u67b6\u6784\u548c\u8bad\u7ec3\u65b9\u6cd5\u4e0d\u65ad\u6d8c\u73b0\uff0c\u63a8\u52a8\u4e86\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\u3002\n - **\u5c40\u9650\u6027**\uff1a\u4f9d\u7136\u5b58\u5728\u6a21\u578b\u504f\u5dee\u548c\u6570\u636e\u4f9d\u8d56\u95ee\u9898\u3002\u6a21\u578b\u5728\u67d0\u4e9b\u7279\u5b9a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u5176\u4ed6\u6570\u636e\u96c6\u6216\u73b0\u5b9e\u573a\u666f\u4e2d\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002\u90e8\u5206\u6a21\u578b\u5bf9\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u4f9d\u8d56\u4e25\u91cd\uff0c\u6570\u636e\u8d28\u91cf\u548c\u6570\u91cf\u5f71\u54cd\u6a21\u578b\u6027\u80fd\uff0c\u4e14\u6a21\u578b\u89e3\u91ca\u6027\u5dee\uff0c\u96be\u4ee5\u7406\u89e3\u5176\u51b3\u7b56\u8fc7\u7a0b\u3002\n\n### 4. \u63a2\u8ba8\u8ba1\u7b97\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\n - **\u9002\u7528\u6027**\uff1a\u4e0d\u540c\u6a21\u578b\u5728\u7279\u5b9a\u6570\u636e\u96c6\u548c\u5e94\u7528\u573a\u666f\u4e0b\u6709\u5404\u81ea\u4f18\u52bf\u3002\u4f8b\u5982\uff0c\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6a21\u578b\u5728\u5927\u89c4\u6a21\u3001\u9ad8\u7ef4\u6570\u636e\u7684\u56fe\u50cf\u548c\u8bed\u97f3\u8bc6\u522b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff1b\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u5c0f\u6570\u636e\u3001\u7b80\u5355\u4efb\u52a1\u4e2d\u4ecd\u6709\u5e94\u7528\u4ef7\u503c\u3002\n - **\u6cdb\u5316\u80fd\u529b**\uff1a\u5c3d\u7ba1\u90e8\u5206\u6a21\u578b\u5728\u7279\u5b9a\u9886\u57df\u53d6\u5f97\u6210\u529f\uff0c\u4f46\u5728\u591a\u9886\u57df\u3001\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u7684\u73b0\u5b9e\u5e94\u7528\u4e2d\uff0c\u6cdb\u5316\u80fd\u529b\u9762\u4e34\u6311\u6218\u3002\u591a\u6a21\u6001\u6570\u636e\u7684\u5f02\u8d28\u6027\u548c\u590d\u6742\u6027\u4f7f\u5f97\u6a21\u578b\u96be\u4ee5\u6709\u6548\u63d0\u53d6\u548c\u878d\u5408\u7279\u5f81\uff0c\u9650\u5236\u4e86\u5176\u5728\u590d\u6742\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002\n\n### 5. \u5206\u6790\u6700\u65b0\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027\n - **\u7a33\u5b9a\u6027\u4f18\u5316**\uff1a\u4e00\u4e9b\u7b97\u6cd5\u9488\u5bf9\u7a33\u5b9a\u6027\u8fdb\u884c\u4e86\u4f18\u5316\uff0c\u4f8b\u5982\u91c7\u7528\u6b63\u5219\u5316\u6280\u672f\u3001\u5bf9\u6297\u8bad\u7ec3\u7b49\u65b9\u6cd5\u63d0\u9ad8\u6a21\u578b\u5728\u590d\u6742\u73af\u5883\u4e0b\u7684\u7a33\u5b9a\u6027\u3002\u5728\u52a8\u6001\u73af\u5883\u4e2d\uff0c\u90e8\u5206\u7b97\u6cd5\u901a\u8fc7\u5728\u7ebf\u5b66\u4e60\u3001\u81ea\u9002\u5e94\u8c03\u6574\u53c2\u6570\u7b49\u65b9\u5f0f\u4fdd\u6301\u6027\u80fd\u7a33\u5b9a\u3002\n - **\u5927\u89c4\u6a21\u6570\u636e\u9002\u5e94\u6027**\uff1a\u5728\u5927\u89c4\u6a21\u6570\u636e\u5904\u7406\u4e0a\uff0c\u90e8\u5206\u7b97\u6cd5\u901a\u8fc7\u5206\u5e03\u5f0f\u8bad\u7ec3\u3001\u6a21\u578b\u538b\u7f29\u7b49\u6280\u672f\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u548c\u9002\u5e94\u6027\u3002\u4f46\u4ecd\u6709\u7b97\u6cd5\u5728\u5904\u7406\u5927\u89c4\u6a21\u6570\u636e\u65f6\u9762\u4e34\u5185\u5b58\u3001\u8ba1\u7b97\u8d44\u6e90\u7b49\u9650\u5236\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002\n\n### 6. \u8bc4\u4f30\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218\n - **\u65b0\u7814\u7a76\u95ee\u9898**\uff1a\u63d0\u51fa\u4e86\u5982\u63d0\u5347\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u3001\u89e3\u51b3\u6a21\u578b\u504f\u5dee\u548c\u6570\u636e\u4f9d\u8d56\u3001\u589e\u5f3a\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u80fd\u529b\u7b49\u65b0\u7814\u7a76\u95ee\u9898\u3002\n - **\u7814\u7a76\u5207\u5165\u70b9**\uff1a\u4ece\u6539\u8fdb\u6a21\u578b\u67b6\u6784\u3001\u4f18\u5316\u8bad\u7ec3\u7b97\u6cd5\u3001\u63a2\u7d22\u65b0\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u7b49\u65b9\u9762\u5bfb\u627e\u65b0\u7684\u7814\u7a76\u5207\u5165\u70b9\u3002\u8fd9\u4e9b\u6311\u6218\u4fc3\u4f7f\u7814\u7a76\u8005\u4e0d\u65ad\u63a2\u7d22\u65b0\u7684\u6280\u672f\u548c\u65b9\u6cd5\uff0c\u63a8\u52a8\u540e\u7eed\u7814\u7a76\u671d\u7740\u66f4\u9ad8\u6548\u3001\u66f4\u667a\u80fd\u3001\u66f4\u5177\u6cdb\u5316\u80fd\u529b\u7684\u65b9\u5411\u53d1\u5c55\u3002\n\n### \u7814\u7a76\u6210\u679c\u3001\u65b9\u6cd5\u521b\u65b0\u6027\u4e0e\u5e94\u7528\u4ef7\u503c\u603b\u7ed3\n - **\u7814\u7a76\u6210\u679c**\uff1a\u5728\u6a21\u578b\u6027\u80fd\u63d0\u5347\u3001\u65b0\u67b6\u6784\u548c\u7b97\u6cd5\u63d0\u51fa\u7b49\u65b9\u9762\u53d6\u5f97\u663e\u8457\u6210\u679c\uff0c\u63a8\u52a8\u4e86\u8ba1\u7b97\u673a\u79d1\u5b66\u591a\u4e2a\u9886\u57df\u7684\u53d1\u5c55\u3002\n - **\u65b9\u6cd5\u521b\u65b0\u6027**\uff1a\u901a\u8fc7\u5f15\u5165\u65b0\u7684\u6280\u672f\u6846\u67b6\u3001\u6539\u8fdb\u8bad\u7ec3\u65b9\u6cd5\u548c\u4f18\u5316\u6a21\u578b\u7ed3\u6784\u7b49\u65b9\u5f0f\uff0c\u5c55\u73b0\u51fa\u8bf8\u591a\u521b\u65b0\u70b9\uff0c\u5982\u81ea\u6ce8\u610f\u529b\u673a\u5236\u3001\u5bf9\u6297\u8bad\u7ec3\u7b49\u3002\n - **\u5e94\u7528\u4ef7\u503c**\uff1a\u5728\u56fe\u50cf\u3001\u81ea\u7136\u8bed\u8a00\u5904\u7406\u3001\u8bed\u97f3\u7b49\u591a\u4e2a\u9886\u57df\u6709\u5e7f\u6cdb\u5e94\u7528\uff0c\u4e3a\u667a\u80fd\u5b89\u9632\u3001\u667a\u80fd\u533b\u7597\u3001\u667a\u80fd\u4ea4\u901a\u7b49\u73b0\u5b9e\u573a\u666f\u63d0\u4f9b\u6280\u672f\u652f\u6301\uff0c\u5177\u6709\u5de8\u5927\u7684\u5546\u4e1a\u548c\u793e\u4f1a\u4ef7\u503c\u3002  ", "task_step_name": "\u5206\u6790\u6700\u65b0\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027", "task_step_description": "- **\u7a33\u5b9a\u6027\u4f18\u5316**\uff1a\u4e00\u4e9b\u7b97\u6cd5\u9488\u5bf9\u7a33\u5b9a\u6027\u8fdb\u884c\u4e86\u4f18\u5316\uff0c\u4f8b\u5982\u91c7\u7528\u6b63\u5219\u5316\u6280\u672f\u3001\u5bf9\u6297\u8bad\u7ec3\u7b49\u65b9\u6cd5\u63d0\u9ad8\u6a21\u578b\u5728\u590d\u6742\u73af\u5883\u4e0b\u7684\u7a33\u5b9a\u6027\u3002\u5728\u52a8\u6001\u73af\u5883\u4e2d\uff0c\u90e8\u5206\u7b97\u6cd5\u901a\u8fc7\u5728\u7ebf\u5b66\u4e60\u3001\u81ea\u9002\u5e94\u8c03\u6574\u53c2\u6570\u7b49\u65b9\u5f0f\u4fdd\u6301\u6027\u80fd\u7a33\u5b9a\u3002 - **\u5927\u89c4\u6a21\u6570\u636e\u9002\u5e94\u6027**\uff1a\u5728\u5927\u89c4\u6a21\u6570\u636e\u5904\u7406\u4e0a\uff0c\u90e8\u5206\u7b97\u6cd5\u901a\u8fc7\u5206\u5e03\u5f0f\u8bad\u7ec3\u3001\u6a21\u578b\u538b\u7f29\u7b49\u6280\u672f\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u548c\u9002\u5e94\u6027\u3002\u4f46\u4ecd\u6709\u7b97\u6cd5\u5728\u5904\u7406\u5927\u89c4\u6a21\u6570\u636e\u65f6\u9762\u4e34\u5185\u5b58\u3001\u8ba1\u7b97\u8d44\u6e90\u7b49\u9650\u5236\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002", "task_step_level": "4", "task_step_question": "\u5728\u5904\u7406\u5927\u89c4\u6a21\u6570\u636e\u65f6\uff0c\u90a3\u4e9b\u9762\u4e34\u5185\u5b58\u548c\u8ba1\u7b97\u8d44\u6e90\u9650\u5236\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u7684\u7b97\u6cd5\uff0c\u662f\u5426\u53ef\u4ee5\u501f\u9274\u91c7\u7528\u6b63\u5219\u5316\u6280\u672f\u3001\u5bf9\u6297\u8bad\u7ec3\u7b49\u7a33\u5b9a\u6027\u4f18\u5316\u65b9\u6cd5\u6765\u63d0\u5347\u5728\u5927\u89c4\u6a21\u6570\u636e\u73af\u5883\u4e0b\u7684\u7a33\u5b9a\u6027\u548c\u9002\u5e94\u6027\uff1f ", "task_step_question_context": [{"ref_id": "454848769189392150", "chunk_id": "4", "score": 0.185546875, "text": "# 3.2 SEPARATING BATCH STATISTICS IS NOT NECESSARY\nBatchNorm is a widely used normalization layer shown to improve performance and training stability of image classifiers ( Ioffe & Szegedy ,2015 ). We recall that a BatchNorm layer, given a batch as input, first normalizes it by subtracting the mean and dividing by the standard deviation computed over the entire batch, then it applies an affine transformation, with learnable scale and offset parameters. During training, it accumulates these so-called batch statistics to use during test time, so that the output of the classifier for each image is independent of the other images in the batch. The batch statistics can be seen an approximation of the statistics over the image distribution.  \n\nXie et al. (2019a ) show that optimizing the co-training loss in Eq. 1 can yield worse results on clean images than simple nominal training. This is especially the case when the network has a low capacity or the attack (i.e., the inner maximization) is too strong (such as using a large perturbation radius $\\epsilon$ ). To solve this issue, they propose AdvProp, which consists in using distinct BatchNorm layers for clean and adversarial images. They argue that \u201cmaintaining one set of [BatchNorm] statistics results in incorrect statistics estimation\u201d , which could be the reason for the performance degradation. We note that using two sets of BatchNorm layers for the clean and adversarial samples as in AdvProp creates two sets of batch statistics but also two sets of learnable scale and offset parameters. In the following we investigate whether having separate batch statistics is a necessary condition for successful co-training.  \n\n  \nFigure 2: Dual parameters are enough. We report the clean (solid lines) and robust accuracy (dashed lines) over training steps of R ES NET -50 trained on I MAGE NET with the co-training loss of Eq. 1 $(\\epsilon=4/255)$ : for models with dual layers. Clean accuracy refers to the clean mode and the robust accuracy to the robust mode .Left panel. We compare models with different normalization layers with no domain-specific parameters (Shared BatchNorm, Shared LayerNorm, Shared GroupNorm) to Dual BatchNorm as proposed by Xie et al. (2019a ): regardless of the type of normalization, the robustness of classifiers without dual layers drops to (almost) zero at the end of training. Right panel. We use domain-specific normalization layers (Dual BatchNorm, Dual LayerNorm, Dual GroupNorm) and a model with BatchNorm with shared batch statistics but domain-specific scale and offset (DualParams BatchNorm): all models achieve high clean and robust accuracy.  \n\nFigure 2 shows the clean and robust accuracy of various model architectures as training progresses. The left panel demonstrates that, if we share both batch statistics and scales/offsets (Shared BatchNorm, orange curves), the robust accuracy (orange dashed line) quickly drops, far from the one obtained by AdvProp (Dual BatchNorm, blue curve) which is above $34\\%$ . However, if we use a single set of batch statistics but specific scales and offsets for clean and adversarial images, we can observe on the right panel of Figure 2 that the robust accuracy (DualParams BatchNorm, orange dashed line) matches the one (blue dashed line) obtained by AdvProp. This demonstrates that it is possible to achieve nominal and robust classification results similar to those of AdvProp without separate batch statistics.  \n\nFurthermore, there exist normalization layers such as LayerNorm ( Ba et al. ,2016 ) or GroupNorm (Wu & He ,2018 ) which do not use batch statistics, as their normalization step is done per sample and not per batch. Hence, according to the hypothesis of Xie et al. (2019a ), these types of normalization layer should not suffer from performance degradation. Nevertheless, the left panel of Figure 2 shows that their robust accuracy (green and red dashed lines) does not match the robust accuracy of AdvProp (Dual BatchNorm), and is unstable over training steps. However, by making the scales and offsets of LayerNorm and GroupNorm specific to clean and adversarial images, their robust accuracy matches that obtained with dual BatchNorm layers, as shown in the right panel of Figure 2 . This suggests that a key element to make the co-training loss of Eq. 1 work for various normalization layers is to have trainable parameters which are specific to the clean and adversarial images.\n\n# 3.3 REVISITING ADAPTERS WITH ADVERSARIAL TRAINING\nThe last observation strongly relates this setting to the adapters literature where a single backbone architecture has some parameters, called adapters, which are specific to different domains while the rest of the parameters are shared among tasks. In our case, the clean images form one domain and the adversarial images constitute another domain. In this work, we go beyond having separate normalization layers for the clean and adversarial images and investigate other types of adapters.  \n\nFormally, the model parameters $\\pmb{\\theta}$ can be decomposed into parameters $\\psi$ which are shared among domains and parameters $\\phi$ which are specific to a domain. We call $\\phi_{\\mathrm{clean}}$ the parameters used when training on clean images and $\\phi_{\\mathrm{adv}}$ the parameters used when training on adversarial images. For example, in Section 3.2 , when we used dual LayerNorm layers, the scales and offsets of these normalization layers are contained in $\\phi_{\\mathrm{clean}}$ and $\\phi_{\\mathrm{adv}}$ whereas the rest of the model parameters are in $\\psi$ . Based on Eq. 1 , we optimize the following loss:  \n\n$$\n\\alpha L(f(\\pmb{x};\\psi\\cup\\phi_{\\mathrm{clean}}),y)+(1-\\alpha)\\operatorname*{max}_{\\pmb{\\delta\\in\\mathbb{S}}}L(f(\\pmb{x}+\\pmb{\\delta};\\psi\\cup\\phi_{\\mathrm{adv}}),y).\n$$  \n\nFinally, we introduce some notation for models with a s at inference time: we call $f(\\cdot;\\psi\\cup\\phi_{\\mathrm{clean}})$ the clean mode for prediction as we use the adapters $\\phi_{\\mathrm{clean}}$ trained on the clean data. Conversely, we call $f(\\cdot;\\psi\\cup\\phi_{\\mathrm{adv}})$ the robust mode when using the adapters $\\phi_{\\mathrm{adv}}$ trained on the perturbed data.  \n\nWortsman et al. (2022 ) propose model soups , which consist in averaging the weights of multiple models fine-tuned from the same pre-trained model. The resulting weight averaged model can benefit from the original models without incurring any extra compute and memory cost at inference time. Currently, in our setting the user would have to know at test time if the network should be in clean or robust mode . A model soup , by its ability to merge models, is a way to bypass this issue. We formulate the hypothesis that training with adapters enables model soups . With this in mind, we observe that training with adapters means that most of the model parameters are already shared, so model souping would simply consist in linearly interpolating the weights of the adapters for the two modes. We call adversarial model soups , the model soups with a model co-trained on clean and adversarial samples. We get the following parametrized model:  \n\n$$\nf(\\cdot;\\psi\\cup(\\beta\\phi_{\\mathrm{clean}}+(1-\\beta)\\phi_{\\mathrm{adv}}))\n$$  \n\nwhere $\\beta$ is the weighting factor when averaging the adapters. If $\\beta=1$ , the model soup boils down to the clean mode and conversely $\\beta=0$ corresponds to the robust mode . In Section 5.2 , we assess this hypothesis and show that forming model soups between independent nominal and robust models fails."}, {"ref_id": "454895453635358570", "chunk_id": "6", "score": 0.16796875, "text": "# 6.2 Implementation\nTo fairly evaluate our method against the baselines, we use the same set of hyperparameters for all the methods, for a given dataset. For details, see Appendix D. The code for Pyramid-BERT is made available as a supplementary material with the submission. The training and inference jobs are run separately on a NVIDIA Tesla V 100 GPU machine and a Intel Xeon Platinum 8000 series CPU machine respectively. All the accuracy and speedup scores are averaged over 20 trials.\n\n# 6.3 Results on GLUE benchmarks\nWe first examine the trade-off between accuracy and speedup. The accuracy results for $3X$ and $1.5X$ speedup are summarized in Table 1 and 2 respectively. The results for $3.5X$ and $2X$ speedup are given in the Table 11 and 13 in Appendix E.We observe that as the speedup increases the gap between our Coreset-select-opt and its competitors becomes large, where for $3X$ speedup, Coreset-select-opt outperforms the second best method Attention-select by $1\\%$ accuracy in average and beats the standard baselines by $2\\%$ or more. The Average-pool performs the worst in average across the GLUE benchmarks, specially on the COLA dataset. For detailed justification, see Appendix E. To better understand the performance of Coreset-select-opt with different values of $m$ ,an ablation study is shown in Section 7 . For mild speedup of $1.5X$ , we note that all methods (except Average-pool ) suffer only a small loss in accuracy and our method suffers no loss. A similar situation occurs when viewing the tradeoff between space complexity and accuracy, where we provide results for a memory reduction of $70\\%$ and $30\\%$ in the Tables 3 and 16 (in $\\S\\mathrm{~E~}$ ).\n\n# 6.4 Results on Long Range Arena\nWe show results on the following three datasets of LRA benchmark: (1) byte-level text classification using real-world data (IMDB), (2) Pathfinder task (long range spatial dependency problem), and (3) image classification on sequences of pixels converted from CIFAR-10.  \n\n<html><body><table><tr><td>Dataset</td><td>1st-I</td><td>1st</td><td>Rand</td><td>Pool</td><td>Att</td><td>CS-k-1</td><td>CS-opt</td><td>BERTBase</td></tr><tr><td>STS-B</td><td>86.4</td><td>86.4</td><td>86.8</td><td>81.6</td><td>87.0</td><td>87.0</td><td>87.0</td><td>87.9</td></tr><tr><td>MRPC</td><td>81.4</td><td>80.9</td><td>83.2</td><td>83.9</td><td>84.6</td><td>86.2</td><td>86.9</td><td>87.3</td></tr><tr><td>SST-2</td><td>83.8</td><td>84.4</td><td>85.6</td><td>85.2</td><td>86.0</td><td>87.3</td><td>89.6</td><td>92.4</td></tr><tr><td>QNLI</td><td>84.8</td><td>84.4</td><td>86.4</td><td>84.1</td><td>86.8</td><td>87.8</td><td>87.8</td><td>90.9</td></tr><tr><td>COLA</td><td>49.7</td><td>49.7</td><td>49.5</td><td>3.0</td><td>51.1</td><td>51.7</td><td>52.8</td><td>53.3</td></tr><tr><td>RTE</td><td>63.5</td><td>63.5</td><td>62.1</td><td>59.2</td><td>63.4</td><td>63.7</td><td>63.7</td><td>65.8</td></tr><tr><td>MNLI_M</td><td>77.8</td><td>76.7</td><td>81.4</td><td>75.4</td><td>82.5</td><td>82.4</td><td>82.5</td><td>84.0</td></tr><tr><td>MNLI_MM</td><td>75.9</td><td>75.6</td><td>78.7</td><td>76.7</td><td>82.7</td><td>82.6</td><td>82.7</td><td>84.6</td></tr><tr><td>QQP</td><td>80.8</td><td>80.4</td><td>87.0</td><td>79.4</td><td>87.3</td><td>87.3</td><td>87.3</td><td>87.5</td></tr><tr><td>Mean</td><td>76.0</td><td>76.1</td><td>77.9</td><td>69.6</td><td>79.0</td><td>79.6</td><td>80.0</td><td>81.5</td></tr></table></body></html>  \n\nTable 1: GLUE dev performance at $3X$ speedup. Here and everywhere else, F 1 scores are reported for QQP and MRPC, Spearman correlations are reported for STS-B, Matthew\u2019s correlations are reported for COLA, and accuracy scores are reported for the other tasks. Each value is averaged over 20 trials. Larger values indicates better performance.   \nTable 2: GLUE dev performance at $1.5X$ speedup.   \n\n\n<html><body><table><tr><td>Dataset</td><td>1st-I</td><td>1st</td><td>Rand</td><td>Pool</td><td>Att</td><td>CS-k-1</td><td>CS-opt</td><td>BERTBase</td></tr><tr><td>STS-B</td><td>87.9</td><td>87.9</td><td>87.8</td><td>87.8</td><td>87.9</td><td>87.7</td><td>87.9</td><td>87.9</td></tr><tr><td>MRPC</td><td>86.8</td><td>86.4</td><td>87.2</td><td>87.0</td><td>87.1</td><td>86.9</td><td>87.3</td><td>87.3</td></tr><tr><td>SST-2</td><td>92.1</td><td>91.5</td><td>91.9</td><td>90.3</td><td>92.3</td><td>92.4</td><td>92.4</td><td>92.4</td></tr><tr><td>QNLI</td><td>90.8</td><td>90.8</td><td>90.8</td><td>90.2</td><td>90.7</td><td>90.9</td><td>90.9</td><td>90.9</td></tr><tr><td>COLA</td><td>53.0</td><td>52.7</td><td>53.1</td><td>25.6</td><td>53.2</td><td>53.3</td><td>53.3</td><td>53.3</td></tr><tr><td>RTE</td><td>65.6</td><td>65.2</td><td>65.7</td><td>61.5</td><td>65.7</td><td>65.4</td><td>65.6</td><td>65.8</td></tr><tr><td>MNLI_M</td><td>84.0</td><td>83.8</td><td>83.9</td><td>80.9</td><td>84.0</td><td>84.0</td><td>84.0</td><td>84.0</td></tr><tr><td>MNLI_MM</td><td>84.1</td><td>84.0</td><td>83.9</td><td>84.0</td><td>84.5</td><td>84.6</td><td>84.6</td><td>84.6</td></tr><tr><td>QQP</td><td>87.1</td><td>86.9</td><td>87.4</td><td>85.7</td><td>87.4</td><td>87.5</td><td>87.5</td><td>87.5</td></tr><tr><td>Mean</td><td>81.3</td><td>81.0</td><td>81.3</td><td>76.8</td><td>81.4</td><td>81.4</td><td>81.5</td><td>81.5</td></tr></table></body></html>  \n\nFor baselines, we include First$.k$ -select and Random-select methods, but fail to include Attention-select .Attention-select requires a full attention matrix for selecting tokens which is not available in Big Bird (Zaheer et al. ,2020 ) and Performers (Choromanski et al. ,2020 ). In addition, the Transformers including Big Bird and Performers in LRA have shallow architectures because of no pre-training: the default number of encoders for text classification, path finder, and image classifi- cation datasets are four, four, and one, respectively. Thus, for both baselines and our method, we only reduce sequence length in the input layer , which is before the first encoder. For the sequence-length configurations, see Appendix C.2 . For Averagepool , due to its worst performance on the GLUE benchmarks and the shallow architectures of the models in LRA, we exclude it from the baselines.  \n\nTable 3: GLUE dev performance at $70\\%$ space complexity reduction.   \n\n\n<html><body><table><tr><td>Dataset</td><td>1st-I</td><td>1st</td><td>Rand</td><td>Pool</td><td>Att</td><td>CS-k-1</td><td>CS-opt</td><td>BERTBase</td></tr><tr><td>STS-B</td><td>85.3</td><td>85.1</td><td>85.6</td><td>78.7</td><td>85.4</td><td>86.5</td><td>86.7</td><td>87.9</td></tr><tr><td>MRPC</td><td>81.3</td><td>81.5</td><td>83.3</td><td>83.1</td><td>84.3</td><td>86.0</td><td>86.6</td><td>87.3</td></tr><tr><td>SST-2</td><td>83.3</td><td>84.6</td><td>84.9</td><td>85.1</td><td>87.2</td><td>87.6</td><td>87.7</td><td>92.4</td></tr><tr><td>QNLI</td><td>84.6</td><td>84.3</td><td>85.1</td><td>84.0</td><td>86.4</td><td>86.6</td><td>86.5</td><td>90.9</td></tr><tr><td>COLA</td><td>49.0</td><td>49.0</td><td>48.4</td><td>0.0</td><td>50.9</td><td>51.0</td><td>52.3</td><td>53.3</td></tr><tr><td>RTE</td><td>62.1</td><td>62.0</td><td>61.8</td><td>59.8</td><td>62.7</td><td>63.6</td><td>63.6</td><td>65.8</td></tr><tr><td>MNLI_M</td><td>76.9</td><td>76.3</td><td>79.0</td><td>75.2</td><td>80.5</td><td>80.9</td><td>81.0</td><td>84.0</td></tr><tr><td>MNLI_MM</td><td>74.9</td><td>74.5</td><td>79.3</td><td>76.3</td><td>80.7</td><td>81.6</td><td>81.8</td><td>84.6</td></tr><tr><td>QQP</td><td>80.6</td><td>80.0</td><td>86.6</td><td>82.9</td><td>87.0</td><td>87.2</td><td>87.3</td><td>87.5</td></tr><tr><td>Mean</td><td>75.3</td><td>75.3</td><td>77.1</td><td>69.5</td><td>78.3</td><td>79.0</td><td>79.3</td><td>81.5</td></tr></table></body></html>  \n\nTable 4: LRA test set performances at $70\\%$ space complexity reduction for Big Bird (top) and Performers (bottom) as the backbone Transformer. Here and everywhere else, accuracy scores are reported for all three tasks. Each value is averaged over 20 trials. Larger values indicates better performance.   \n\n\n<html><body><table><tr><td colspan=\"6\">BigBird</td></tr><tr><td>Dataset</td><td>1st</td><td>Rand</td><td>CS-k-1</td><td>CS-opt</td><td>Trans.-no-prune</td></tr><tr><td>CIFAR-10</td><td>26.9</td><td>39.4</td><td>38.6</td><td>43.3</td><td>40.9</td></tr><tr><td>PATHFINDER-32</td><td>55.6</td><td>69.9</td><td>69.3</td><td>71.7</td><td>73.5</td></tr><tr><td>IMDB (BYTE-LEVEL)</td><td>57.9</td><td>59.6</td><td>59.1</td><td>61.4</td><td>63.8</td></tr><tr><td>Mean</td><td>46.8</td><td>56.3</td><td>55.7</td><td>58.8</td><td>59.4</td></tr><tr><td colspan=\"6\">Performers</td></tr><tr><td>CIFAR-10</td><td>26.9</td><td>41.5</td><td>39.8</td><td>45.5</td><td>42.9</td></tr><tr><td>PATHFINDER-32</td><td>52.4</td><td>58.2</td><td>61.5</td><td>67.7</td><td>66.2</td></tr><tr><td>IMDB (BYTE-LEVEL)</td><td>59.9</td><td>59.9</td><td>59.7</td><td>62.8</td><td>64.3</td></tr><tr><td>Mean</td><td>46.4</td><td>53.2</td><td>53.7</td><td>58.7</td><td>57.8</td></tr></table></body></html>  \n\nThe results of accuracy scores for space complexity reduction at $70\\%$ and $30\\%$ are presented in Table 4 and Table 18 (in Appendix E), respectively. The Coreset-select-opt here represents the Coreset-select with $m\\ =\\ 1$ because of its superior performance over other $m\\quad\\in$ \u2208$\\{\\lceil0.1k\\rceil\\},\\lceil0.2k\\rceil,\\lceil0.3k\\rceil,\\lceil0.4k\\rceil\\}$ .  \n\nWe observe a similar pattern as discussed in GLUE benchmark evaluations: at high space complexity reduction $70\\%$ ,Coreset-select-opt significantly outperforms its competitors First$.k$ -select and Random-select by $12\\%$ and $2.5\\%$ in average for Big Bird $(12.3\\%$ and $5.5\\%$ in average for Performers ). Moreover, on CIFAR-10, our Coresetselect-opt is even better than the Big Bird and Performers without any sequence reduction with accuracy gain $2.4\\%$ and $2.6\\%$ , respectively (similarly for Performers on PATHFINDER-32). On the other hand, different from the GLUE evaluations, Coreset-select-k-1 does not show any significant advantages over the baseline methods. Our conjecture is that the input in the LRA datasets contain too many noisy or low level information which is not helpful for predicting the target. For an example, each pixel of an image (CIFAR-10) or a character in the byte-level text classification represents a token as the input. Our Coreset-select based strategy with $m=1$ does the most fine-grained token-level selection than its baselines and thus filter out the noisy information. Note, we do not include accuracy and speedup tables because of insignificant gains observed in speedup due to the shallow architectures of Transformers in LRA."}, {"ref_id": "454845533235189656", "chunk_id": "12", "score": 0.1552734375, "text": "# 4 Experiments\nWe now exhibit the results of our experiments, by showing the correlation between the feedback of our indicators, and the false sense of security given by badly-evaluated defenses.  \n\nExperimental setup. We run our attacks on an Intel \u00aeXeon \u00aeCPU E5-2670 v3, with 48 cores, 126 GB of RAM, and equipped with an Nvidia Quadro M6000 with $24\\:\\mathrm{GB}$ of memory. All the attacks and models have been wrapped and run by using the SecML library [ 20 ]. We select four defenses that have been reported as failing, and we show that our indicators would have detected such evaluation errors. For each of them, we set the hyperparameters for the attack as done in the original evaluation, in order to collect similar results.  \n\n-Winners-Take-All (kWTA) , the defense proposed by Xiao et al. [ 31 ] uses only the top-k outputs from each layer, generating many discontinuities in the loss landscape, and hence resulting in the non-converging failure due to noisy gradients $(F_{2})$ . We use the implementation provided by Tram\u00e8r et al. [ 30 ], trained on CIFAR10, and we test its robustness by attacking it with $\\ell_{\\infty}$ -PGD [ 17 ] with a step size of $\\alpha=0.003$ , maximum perturbation $\\epsilon=8/255$ and 50 iterations, with 5 restarts for each attack, scoring a robust accuracy of $58\\%$ on 100 samples.  \n\nDistillation , the defense proposed by Papernot et al. [ 22 ], works by training a model to have zero gradients around the training points, leading gradient-based attacks towards bad local optimum $(F_{3})$ .We re-implemented such defense, by training a distilled classifier on the MNIST dataset to mimic the evaluation. Then, we apply $\\ell_{\\infty}$ -PGD [ 17 ], with step size $\\alpha=0.01$ , maximum perturbation $\\epsilon=0.3$ for 50 iterations on 100 samples, resulting in a robust accuracy of $94\\small{,}2\\%$ .  \n\nEnsemble diversity , the defense proposed by Pang et al. [ 21 ] is composed with different neural networks, trained with a regularizer that encourages diversity. We adopt the implementation provided by Tram\u00e8r et al. [ 30 ]. Then, following its original evaluation, we apply $\\ell_{\\infty}$ -PGD [ 17 ], with step size $\\alpha=0.001$ , maximum perturbation $\\epsilon=0.01$ for 10 iterations on 100 samples, resulting in a robust accuracy of $38\\%$ .  \n\nTurning a Weakness into a Strenght (TWS) , the defense proposed by Yu et al. [ 32 ], applies a mechanism for detecting the presence of adversarial examples on top of an undefended model, measuring how much the decision changes locally around a sample. Even if the authors also apply other rejection mechanisms, we take into account only the described one, as we wish to show that attacks optimized neglecting such term will trigger the non-adaptive attack failure $(F_{4})$ .  \n\nWe apply this defended on a WideResNet model trained on CIFAR10, provided by RobustBench [ 14 ]. We attack this model with $\\ell_{\\infty}$ -PGD [ 17 ], with step size $\\alpha\\,=\\,0.1$ , maximum perturbation $\\epsilon=0.3$ for 50 iterations on 100 samples, and then we query the defended model with all the computed adversarial examples. While the attacks works against the standard model, some of them are rejected by the defense, resulting in a robust accuracy of $35\\%$ ,highlighted by the trigger of the $I_{5}$ indicator. In this case, we consider an attack unsuccessful if the original sample is not misclassified and the adversarial point is either belonging to the same class, or it is labeled as rejected.  \n\nEach of these attacks have been executed with 5 random restarts. We also attack all these models with the version of AutoPGD (APGD) [ 13 ] that uses the difference of logit (DLR) as a loss to optimize. This strategy will take care to automatically tune its hyperparameters while optimizing, reducing possible errors that occur while deciding the values of step size, and iterations. Lastly, we compute attacks that take into account all the mitigations we prescribed, and they will be analyzed further on in the paper.  \n\nIdentifying failures. We want now to understand if our indicators are correlated with faults  \n\n  \nFigure 5: Robust accuracy vs. average value of the indicators. Incorrect evaluations (denoted with $\\overrightarrow{\\circ}$ )report high robust accuracy but also trigger most of the indicators. Better evaluations, performed by either mitigating the attack failures (denoted with $\\mathbf{\\Phi}^{\\bullet}\\times\\mathbf{\\Phi}^{\\bullet})$ , or using APGD (denoted with $\\mathbf{\\Phi}^{,}\\star\\mathbf{\\Phi}$ ), correctly report a lower robust accuracy along with a lower average value of our indicators.  \n\nof the security evaluations of defenses. We collect the results of all the attacks against the selected targets, and we compute our indicators, by listing their values in Table 1, along with their mean score. With a glance, it is possible to grasp that out hypothesis is right: the detection of a failure is linked with higher values for the robust accuracy, and also the opposite. Each original evaluation is characterized by high values of one or more indicator, while the opposite happens for stronger attacks. For instance, APGD automatically tunes its hyperparameter while optimizing, hence it is able to apply some mitigations directly during the attack. To gain a quantitative evaluation of out hypothesis, we compute both the p-value and the correlation between the average score of the indicators and the robust accuracy, depicting this result in Fig. 5. Both p-value and correlation suggest a strong connection between these analyzed quantities, confirming our initial belief.  \n\nTable 1: Values of the Indicators of Attack Failures, computed for all the attacks against all the evaluated models. We denote the attacks that apply also the mitigations as $\\mathrm{PGD^{\\star}}$ .  \n\n\n<html><body><table><tr><td>Model</td><td>Attack</td><td>I1</td><td>12</td><td>13</td><td>14</td><td>15</td><td>T</td><td>RA</td></tr><tr><td rowspan=\"3\">k-WTA [31]</td><td>PGD</td><td>0.33</td><td>0.43</td><td>0.77</td><td></td><td></td><td>0.306</td><td>58%</td></tr><tr><td>APGD</td><td>-</td><td>0.31</td><td>0.33</td><td></td><td></td><td>0.128</td><td>36%</td></tr><tr><td>PGD*</td><td>0.07</td><td>0.48</td><td>0.55</td><td></td><td></td><td>0.220</td><td>6%</td></tr><tr><td rowspan=\"3\">Distillation[22]</td><td>PGD</td><td></td><td>0.98</td><td></td><td>0.97</td><td></td><td>0.39</td><td>94%</td></tr><tr><td>APGD</td><td>-</td><td>0.40</td><td>0.21</td><td></td><td></td><td>0.122</td><td>0%</td></tr><tr><td>PGD*</td><td></td><td>0.04</td><td></td><td>-</td><td></td><td>0.008</td><td>0%</td></tr><tr><td rowspan=\"3\">Ensemble Div.[21]</td><td>PGD</td><td></td><td>0.76</td><td></td><td></td><td></td><td>0.152</td><td>38%</td></tr><tr><td>APGD</td><td></td><td>0.37</td><td>0.14</td><td></td><td></td><td>0.102</td><td>0%</td></tr><tr><td>PGD*</td><td>0.08</td><td>0.17</td><td>0.15</td><td>-</td><td></td><td>0.080</td><td>%6</td></tr><tr><td rowspan=\"3\">TWS [32]</td><td>PGD</td><td></td><td>0.49</td><td>0.07</td><td></td><td>0.37</td><td>0.186</td><td>35%</td></tr><tr><td>APGD</td><td>-</td><td>0.41</td><td>0.09</td><td></td><td>-</td><td>0.100</td><td>0%</td></tr><tr><td>PGD*</td><td>-</td><td>0.37</td><td>0.10</td><td>-</td><td></td><td>0.094</td><td>0%</td></tr></table></body></html>  \n\nMitigating failures. We can now use our indicators to improve the quality of the security evaluations, and we apply the following pipeline: (i) we test the defense with a set of points with the original attack strategy proposed by the author of the defense; (ii) we select the failure cases and inspect the feedback of our indicators per-sample ; (iii) for each cause of failure, we apply the specific remediation suggested by the metric; and (iv) we show that the attack now succeeds, thus reducing the robust accuracy of the target model, and also the values of the indicators.  \n\nWe report all the results of this process in Table 2, where each row shows the original robust accuracy, and how it is decreased, mitigation after mitigation. Also, all the individual values of each indicator computed on these patched attacks can be found in Table 1, marked as $\\mathrm{PGD^{\\star}}$ .  \n\nMitigating $k$ -WTA failures. For many failing attacks, the $I I$ indicator triggers, implying that the attack found an adversarial example inside the path. We then apply mitigation $M_{I}$ , and we lower accordingly the robust accuracy of the model to $36\\small{,}4\\%$ . We then analyze the feedback of the $I_{3}$ indicator, the one that detects the presence of noisy gradients. We apply mitigation $M_{3}$ , and we change the loss of the attack as described by Tram\u00e8r et al. [ 30 ]. This loss is computed by averaging the gradient of each single point of the attack path with the information of the surrounding ones. The resulting direction is then able to correctly descent toward a minimum. We run $\\ell_{\\infty}$ -PGD with the same parameters, but smoothing the gradients by averaging 100 neighboring points from a normal distribution ${\\mathcal N}(\\mu=\\pmb{x}_{i},\\sigma=0.0\\dot{3}1)$ , where $x_{i}$ is a point in the attack path. After such mitigation, the robust accuracy drops to $6,4\\%$ , and so follows the indicator (Fig. 6a).  \n\nMitigating Distillation failures. All the attacks fail because of the absence of gradient information, leading the attack to a bad local optimum $(F_{3})$ , and such is highlighted by the feedback of the $I_{3}$ indicator. We apply mitigation $M_{3}$ , and we change the loss optimized during the attack, following the strategy applied by Carlini et al. [ 9 ], that computes the loss of the attack on the logit of the model rather than the final softmax layer. We repeat the PGD attack with such fix, and the robust accuracy drops to $0\\%$ , along with the indicator $I_{3}$ (Fig. 6b).  \n\nMitigating Ensemble diversity failures. Firstly, the $I_{I}$ indicator highlighted the presence of $F_{I}$ ,implying that some failing attacks are due to the implementation itself. We apply mitigation $M_{I}$ , and the robust accuracy decreases to $36\\%$ . Also, $I_{2}$ indicator is active, implying that the loss of of failing attacks could be optimized more. For this reason, we apply mitigation $M_{2}$ , and we increase the step size to 0 .05 and the iterations to 50 . This patch is enough for lowering the robust accuracy to $9\\bar{\\%}$ .(Fig. 6c).  \n\nMitigating TWS failures. The detector is rejecting adversarial attacks successfully computed on the undefended model, triggering the $I_{5}$ indicator. Hence we apply mitigation $M_{5}$ , and we adapt the attack to consider also the rejection class. This version of PGD minimizes the usual loss function of the attacker, but it also minimizes the score of the rejection class when encountered, allowing it to evade the rejection. We run such attack, and we obtain a new robust accuracy of $0\\%$ (Fig. 6d).  \n\n<html><body><table><tr><td>Model</td><td>Initial</td><td>M1</td><td>M2</td><td>M3</td><td>M4</td><td>M5</td><td>Final</td></tr><tr><td>k-WTA [31]</td><td>58.2%</td><td>36.4%</td><td>36.4%</td><td>6.4%</td><td>6.4%</td><td>6.4%</td><td>6.4%</td></tr><tr><td>Distillation [22]</td><td>94.2%</td><td>94.2%</td><td>94.2%</td><td>0.4%</td><td>0.4%</td><td>0.4%</td><td>0.4%</td></tr><tr><td>Ensemble Diversity [21]</td><td>38.0%</td><td>38.0%</td><td>9.0%</td><td>9.0%</td><td>9.0%</td><td>9.0%</td><td>9.0%</td></tr><tr><td>TWS [32]</td><td>35.0%</td><td>35.0%</td><td>35.0%</td><td>35.0%</td><td>35.0%</td><td>0.0%</td><td>0.0%</td></tr></table></body></html>\n\nTable 2: Robust accuracies $\\overline{{(\\%)}}$ after patching the security evaluations with the prescribed mitigations.  \n\n  \nFigure 6: The values of our indicators and the success rate (SR) of the attack, before (semi-transparent colored area) and after (solid colored area) fixing the failures, computed for the analyzed models."}], "task_step_question_answer": "\u9664\u4e86\u4e0a\u8ff0\u63d0\u5230\u7684\u65b9\u6cd5\u548c\u573a\u666f\uff0c\u5728\u5b9e\u65f6\u901a\u4fe1\u9886\u57df\uff0c\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027\u4e5f\u81f3\u5173\u91cd\u8981\u3002\u4f8b\u5982\u5728\u89c6\u9891\u4f1a\u8bae\u7cfb\u7edf\u4e2d\uff0c\u7f51\u7edc\u72b6\u51b5\u53ef\u80fd\u968f\u65f6\u53d1\u751f\u53d8\u5316\uff0c\u5b58\u5728\u4e22\u5305\u3001\u5ef6\u8fdf\u7b49\u95ee\u9898\u3002\u6b64\u65f6\uff0c\u7f16\u89e3\u7801\u7b97\u6cd5\u9700\u8981\u5177\u5907\u826f\u597d\u7684\u5bb9\u9519\u80fd\u529b\uff0c\u4ee5\u786e\u4fdd\u89c6\u9891\u548c\u97f3\u9891\u7684\u6d41\u7545\u64ad\u653e\u3002H.264/AVC \u7f16\u7801\u6807\u51c6\u91c7\u7528\u4e86\u591a\u79cd\u5bb9\u9519\u6280\u672f\uff0c\u5982\u7075\u6d3b\u5b8f\u5757\u6392\u5e8f\uff08FMO\uff09\u548c\u6570\u636e\u5206\u5272\u3002FMO \u53ef\u4ee5\u5c06\u56fe\u50cf\u5212\u5206\u4e3a\u591a\u4e2a\u72ec\u7acb\u7684\u533a\u57df\uff0c\u6bcf\u4e2a\u533a\u57df\u72ec\u7acb\u7f16\u7801\u548c\u4f20\u8f93\uff0c\u5f53\u67d0\u4e2a\u533a\u57df\u7684\u6570\u636e\u4e22\u5931\u65f6\uff0c\u5176\u4ed6\u533a\u57df\u7684\u6570\u636e\u4ecd\u80fd\u63d0\u4f9b\u4e00\u5b9a\u7684\u56fe\u50cf\u4fe1\u606f\uff0c\u51cf\u5c11\u56fe\u50cf\u8d28\u91cf\u7684\u4e0b\u964d\u3002\u6570\u636e\u5206\u5272\u5219\u5c06\u89c6\u9891\u6570\u636e\u5206\u4e3a\u4e0d\u540c\u7684\u90e8\u5206\uff0c\u91cd\u8981\u7684\u6570\u636e\u90e8\u5206\u4f18\u5148\u4f20\u8f93\uff0c\u5373\u4f7f\u90e8\u5206\u6570\u636e\u4e22\u5931\uff0c\u4e5f\u80fd\u4fdd\u8bc1\u57fa\u672c\u7684\u56fe\u50cf\u663e\u793a\u3002\n\n\u5728\u5de5\u4e1a\u81ea\u52a8\u5316\u9886\u57df\uff0c\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u76f4\u63a5\u5173\u7cfb\u5230\u751f\u4ea7\u7684\u5b89\u5168\u548c\u6548\u7387\u3002\u4f8b\u5982\u5728\u673a\u5668\u4eba\u63a7\u5236\u7b97\u6cd5\u4e2d\uff0c\u9700\u8981\u5b9e\u65f6\u5904\u7406\u4f20\u611f\u5668\u6570\u636e\uff0c\u63a7\u5236\u673a\u5668\u4eba\u7684\u8fd0\u52a8\u3002\u7531\u4e8e\u5de5\u4e1a\u73af\u5883\u4e2d\u5b58\u5728\u5404\u79cd\u5e72\u6270\u548c\u4e0d\u786e\u5b9a\u6027\uff0c\u5982\u632f\u52a8\u3001\u7535\u78c1\u5e72\u6270\u7b49\uff0c\u7b97\u6cd5\u5fc5\u987b\u5177\u5907\u9ad8\u5ea6\u7684\u7a33\u5b9a\u6027\u548c\u5bb9\u9519\u6027\u3002\u4e00\u4e9b\u5148\u8fdb\u7684\u673a\u5668\u4eba\u63a7\u5236\u7b97\u6cd5\u91c7\u7528\u4e86\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u6280\u672f\uff0c\u901a\u8fc7\u9884\u6d4b\u7cfb\u7edf\u7684\u672a\u6765\u72b6\u6001\uff0c\u63d0\u524d\u8c03\u6574\u63a7\u5236\u7b56\u7565\uff0c\u4ee5\u5e94\u5bf9\u5916\u754c\u5e72\u6270\u548c\u4e0d\u786e\u5b9a\u6027\u3002\u540c\u65f6\uff0c\u4e3a\u4e86\u63d0\u9ad8\u7b97\u6cd5\u7684\u5bb9\u9519\u6027\uff0c\u8fd8\u4f1a\u5f15\u5165\u5197\u4f59\u4f20\u611f\u5668\u548c\u6545\u969c\u8bca\u65ad\u673a\u5236\uff0c\u5f53\u67d0\u4e2a\u4f20\u611f\u5668\u51fa\u73b0\u6545\u969c\u65f6\uff0c\u80fd\u591f\u53ca\u65f6\u5207\u6362\u5230\u5907\u7528\u4f20\u611f\u5668\uff0c\u5e76\u5bf9\u6545\u969c\u8fdb\u884c\u8bca\u65ad\u548c\u4fee\u590d\u3002", "ref_task_step_id": ""}, "__type__": "task_step_node"}, "3ce7a6c2-7bf0-43d2-ab8f-ea242c24394d": {"__data__": {"id_": "3ce7a6c2-7bf0-43d2-ab8f-ea242c24394d", "metadata": {}, "relationships": {}, "hash": "", "start_task_context": "\u5927\u6a21\u578bMLA\u6280\u672f\u7ec6\u8282", "aemo_representation_context": "### 1. \u5206\u6790\u8fd1\u51e0\u5e74\u7814\u7a76\u9886\u57df\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba\n\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u76f8\u5173\u7814\u7a76\u9886\u57df\uff0cTransformer\u3001GAN\u3001BERT \u7b49\u6846\u67b6\u6210\u4e3a\u4f17\u591a\u8bba\u6587\u91c7\u7528\u7684\u6838\u5fc3\u6280\u672f\u3002\u8fd9\u4e9b\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u9002\u5e94\u6027\uff0c\u5176\u65b9\u6cd5\u8bba\u56f4\u7ed5\u7740\u5982\u4f55\u6709\u6548\u5904\u7406\u6570\u636e\u7279\u5f81\u3001\u4f18\u5316\u6a21\u578b\u7ed3\u6784\u4ee5\u63d0\u5347\u6027\u80fd\u3002\u4f8b\u5982\uff0cTransformer \u57fa\u4e8e\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6253\u7834\u4e86\u4f20\u7edf\u5e8f\u5217\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u7b49\u591a\u9886\u57df\u5e7f\u6cdb\u5e94\u7528\uff1bGAN \u901a\u8fc7\u751f\u6210\u5668\u4e0e\u5224\u522b\u5668\u7684\u5bf9\u6297\u8bad\u7ec3\uff0c\u5728\u56fe\u50cf\u751f\u6210\u7b49\u4efb\u52a1\u4e0a\u53d6\u5f97\u663e\u8457\u6210\u679c\uff1bBERT \u5219\u901a\u8fc7\u9884\u8bad\u7ec3\u65b9\u5f0f\uff0c\u6781\u5927\u63d0\u5347\u4e86\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u7684\u6027\u80fd\u3002\n\n### 2. \u7814\u7a76\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u4e3b\u8981\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0e\u53d8\u4f53\n - **Transformer**\uff1a\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u4e2d\uff0c\u5982\u673a\u5668\u7ffb\u8bd1\u3001\u6587\u672c\u5206\u7c7b\uff0cTransformer \u67b6\u6784\u4e0d\u65ad\u6f14\u53d8\uff0c\u51fa\u73b0\u4e86\u8bf8\u5982 ViT\uff08Vision Transformer\uff09\u7b49\u53d8\u4f53\u5e94\u7528\u4e8e\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\uff0c\u5c06\u56fe\u50cf\u89c6\u4e3a\u5e8f\u5217\u8fdb\u884c\u5904\u7406\uff0c\u5c55\u73b0\u51fa\u8de8\u9886\u57df\u7684\u5e94\u7528\u6f5c\u529b\u3002\n - **GAN**\uff1a\u5728\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\uff0cDCGAN\uff08Deep Convolutional GAN\uff09\u7b49\u53d8\u4f53\u901a\u8fc7\u6539\u8fdb\u7f51\u7edc\u7ed3\u6784\uff0c\u751f\u6210\u66f4\u903c\u771f\u7684\u56fe\u50cf\uff1b\u5728\u533b\u5b66\u56fe\u50cf\u5408\u6210\u3001\u89c6\u9891\u751f\u6210\u7b49\u9886\u57df\u4e5f\u6709\u5e94\u7528\uff0c\u901a\u8fc7\u8c03\u6574\u635f\u5931\u51fd\u6570\u548c\u7f51\u7edc\u67b6\u6784\u9002\u5e94\u4e0d\u540c\u4efb\u52a1\u9700\u6c42\u3002\n - **BERT**\uff1a\u9664\u4e86\u57fa\u7840\u7684\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u4efb\u52a1\uff0c\u8fd8\u5728\u4fe1\u606f\u68c0\u7d22\u3001\u60c5\u611f\u5206\u6790\u7b49\u4efb\u52a1\u4e2d\u5f97\u5230\u5e7f\u6cdb\u5e94\u7528\uff0c\u901a\u8fc7\u5fae\u8c03\u9884\u8bad\u7ec3\u6a21\u578b\u9002\u5e94\u7279\u5b9a\u4efb\u52a1\uff0c\u540c\u65f6\u51fa\u73b0\u4e86 RoBERTa \u7b49\u6539\u8fdb\u7248\u672c\uff0c\u4f18\u5316\u9884\u8bad\u7ec3\u7b56\u7565\u63d0\u5347\u6027\u80fd\u3002\n\n### 3. \u8bc4\u4f30\u5b66\u672f\u754c\u7684\u6280\u672f\u8fdb\u6b65\u4e0e\u5c40\u9650\u6027\n - **\u6280\u672f\u8fdb\u6b65**\uff1a\u6a21\u578b\u6027\u80fd\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u53d6\u5f97\u663e\u8457\u63d0\u5347\uff0c\u4f8b\u5982\u5728\u56fe\u50cf\u8bc6\u522b\u51c6\u786e\u7387\u3001\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u7684 F1 \u503c\u7b49\u6307\u6807\u4e0a\u4e0d\u65ad\u5237\u65b0\u8bb0\u5f55\u3002\u65b0\u7684\u6a21\u578b\u67b6\u6784\u548c\u8bad\u7ec3\u65b9\u6cd5\u4e0d\u65ad\u6d8c\u73b0\uff0c\u63a8\u52a8\u4e86\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\u3002\n - **\u5c40\u9650\u6027**\uff1a\u4f9d\u7136\u5b58\u5728\u6a21\u578b\u504f\u5dee\u548c\u6570\u636e\u4f9d\u8d56\u95ee\u9898\u3002\u6a21\u578b\u5728\u67d0\u4e9b\u7279\u5b9a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u5176\u4ed6\u6570\u636e\u96c6\u6216\u73b0\u5b9e\u573a\u666f\u4e2d\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002\u90e8\u5206\u6a21\u578b\u5bf9\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u4f9d\u8d56\u4e25\u91cd\uff0c\u6570\u636e\u8d28\u91cf\u548c\u6570\u91cf\u5f71\u54cd\u6a21\u578b\u6027\u80fd\uff0c\u4e14\u6a21\u578b\u89e3\u91ca\u6027\u5dee\uff0c\u96be\u4ee5\u7406\u89e3\u5176\u51b3\u7b56\u8fc7\u7a0b\u3002\n\n### 4. \u63a2\u8ba8\u8ba1\u7b97\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\n - **\u9002\u7528\u6027**\uff1a\u4e0d\u540c\u6a21\u578b\u5728\u7279\u5b9a\u6570\u636e\u96c6\u548c\u5e94\u7528\u573a\u666f\u4e0b\u6709\u5404\u81ea\u4f18\u52bf\u3002\u4f8b\u5982\uff0c\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6a21\u578b\u5728\u5927\u89c4\u6a21\u3001\u9ad8\u7ef4\u6570\u636e\u7684\u56fe\u50cf\u548c\u8bed\u97f3\u8bc6\u522b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff1b\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u5c0f\u6570\u636e\u3001\u7b80\u5355\u4efb\u52a1\u4e2d\u4ecd\u6709\u5e94\u7528\u4ef7\u503c\u3002\n - **\u6cdb\u5316\u80fd\u529b**\uff1a\u5c3d\u7ba1\u90e8\u5206\u6a21\u578b\u5728\u7279\u5b9a\u9886\u57df\u53d6\u5f97\u6210\u529f\uff0c\u4f46\u5728\u591a\u9886\u57df\u3001\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u7684\u73b0\u5b9e\u5e94\u7528\u4e2d\uff0c\u6cdb\u5316\u80fd\u529b\u9762\u4e34\u6311\u6218\u3002\u591a\u6a21\u6001\u6570\u636e\u7684\u5f02\u8d28\u6027\u548c\u590d\u6742\u6027\u4f7f\u5f97\u6a21\u578b\u96be\u4ee5\u6709\u6548\u63d0\u53d6\u548c\u878d\u5408\u7279\u5f81\uff0c\u9650\u5236\u4e86\u5176\u5728\u590d\u6742\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002\n\n### 5. \u5206\u6790\u6700\u65b0\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027\n - **\u7a33\u5b9a\u6027\u4f18\u5316**\uff1a\u4e00\u4e9b\u7b97\u6cd5\u9488\u5bf9\u7a33\u5b9a\u6027\u8fdb\u884c\u4e86\u4f18\u5316\uff0c\u4f8b\u5982\u91c7\u7528\u6b63\u5219\u5316\u6280\u672f\u3001\u5bf9\u6297\u8bad\u7ec3\u7b49\u65b9\u6cd5\u63d0\u9ad8\u6a21\u578b\u5728\u590d\u6742\u73af\u5883\u4e0b\u7684\u7a33\u5b9a\u6027\u3002\u5728\u52a8\u6001\u73af\u5883\u4e2d\uff0c\u90e8\u5206\u7b97\u6cd5\u901a\u8fc7\u5728\u7ebf\u5b66\u4e60\u3001\u81ea\u9002\u5e94\u8c03\u6574\u53c2\u6570\u7b49\u65b9\u5f0f\u4fdd\u6301\u6027\u80fd\u7a33\u5b9a\u3002\n - **\u5927\u89c4\u6a21\u6570\u636e\u9002\u5e94\u6027**\uff1a\u5728\u5927\u89c4\u6a21\u6570\u636e\u5904\u7406\u4e0a\uff0c\u90e8\u5206\u7b97\u6cd5\u901a\u8fc7\u5206\u5e03\u5f0f\u8bad\u7ec3\u3001\u6a21\u578b\u538b\u7f29\u7b49\u6280\u672f\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u548c\u9002\u5e94\u6027\u3002\u4f46\u4ecd\u6709\u7b97\u6cd5\u5728\u5904\u7406\u5927\u89c4\u6a21\u6570\u636e\u65f6\u9762\u4e34\u5185\u5b58\u3001\u8ba1\u7b97\u8d44\u6e90\u7b49\u9650\u5236\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002\n\n### 6. \u8bc4\u4f30\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218\n - **\u65b0\u7814\u7a76\u95ee\u9898**\uff1a\u63d0\u51fa\u4e86\u5982\u63d0\u5347\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u3001\u89e3\u51b3\u6a21\u578b\u504f\u5dee\u548c\u6570\u636e\u4f9d\u8d56\u3001\u589e\u5f3a\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u80fd\u529b\u7b49\u65b0\u7814\u7a76\u95ee\u9898\u3002\n - **\u7814\u7a76\u5207\u5165\u70b9**\uff1a\u4ece\u6539\u8fdb\u6a21\u578b\u67b6\u6784\u3001\u4f18\u5316\u8bad\u7ec3\u7b97\u6cd5\u3001\u63a2\u7d22\u65b0\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u7b49\u65b9\u9762\u5bfb\u627e\u65b0\u7684\u7814\u7a76\u5207\u5165\u70b9\u3002\u8fd9\u4e9b\u6311\u6218\u4fc3\u4f7f\u7814\u7a76\u8005\u4e0d\u65ad\u63a2\u7d22\u65b0\u7684\u6280\u672f\u548c\u65b9\u6cd5\uff0c\u63a8\u52a8\u540e\u7eed\u7814\u7a76\u671d\u7740\u66f4\u9ad8\u6548\u3001\u66f4\u667a\u80fd\u3001\u66f4\u5177\u6cdb\u5316\u80fd\u529b\u7684\u65b9\u5411\u53d1\u5c55\u3002\n\n### \u7814\u7a76\u6210\u679c\u3001\u65b9\u6cd5\u521b\u65b0\u6027\u4e0e\u5e94\u7528\u4ef7\u503c\u603b\u7ed3\n - **\u7814\u7a76\u6210\u679c**\uff1a\u5728\u6a21\u578b\u6027\u80fd\u63d0\u5347\u3001\u65b0\u67b6\u6784\u548c\u7b97\u6cd5\u63d0\u51fa\u7b49\u65b9\u9762\u53d6\u5f97\u663e\u8457\u6210\u679c\uff0c\u63a8\u52a8\u4e86\u8ba1\u7b97\u673a\u79d1\u5b66\u591a\u4e2a\u9886\u57df\u7684\u53d1\u5c55\u3002\n - **\u65b9\u6cd5\u521b\u65b0\u6027**\uff1a\u901a\u8fc7\u5f15\u5165\u65b0\u7684\u6280\u672f\u6846\u67b6\u3001\u6539\u8fdb\u8bad\u7ec3\u65b9\u6cd5\u548c\u4f18\u5316\u6a21\u578b\u7ed3\u6784\u7b49\u65b9\u5f0f\uff0c\u5c55\u73b0\u51fa\u8bf8\u591a\u521b\u65b0\u70b9\uff0c\u5982\u81ea\u6ce8\u610f\u529b\u673a\u5236\u3001\u5bf9\u6297\u8bad\u7ec3\u7b49\u3002\n - **\u5e94\u7528\u4ef7\u503c**\uff1a\u5728\u56fe\u50cf\u3001\u81ea\u7136\u8bed\u8a00\u5904\u7406\u3001\u8bed\u97f3\u7b49\u591a\u4e2a\u9886\u57df\u6709\u5e7f\u6cdb\u5e94\u7528\uff0c\u4e3a\u667a\u80fd\u5b89\u9632\u3001\u667a\u80fd\u533b\u7597\u3001\u667a\u80fd\u4ea4\u901a\u7b49\u73b0\u5b9e\u573a\u666f\u63d0\u4f9b\u6280\u672f\u652f\u6301\uff0c\u5177\u6709\u5de8\u5927\u7684\u5546\u4e1a\u548c\u793e\u4f1a\u4ef7\u503c\u3002  ", "task_step_name": "\u8bc4\u4f30\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218", "task_step_description": "- **\u65b0\u7814\u7a76\u95ee\u9898**\uff1a\u63d0\u51fa\u4e86\u5982\u63d0\u5347\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u3001\u89e3\u51b3\u6a21\u578b\u504f\u5dee\u548c\u6570\u636e\u4f9d\u8d56\u3001\u589e\u5f3a\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u80fd\u529b\u7b49\u65b0\u7814\u7a76\u95ee\u9898\u3002 - **\u7814\u7a76\u5207\u5165\u70b9**\uff1a\u4ece\u6539\u8fdb\u6a21\u578b\u67b6\u6784\u3001\u4f18\u5316\u8bad\u7ec3\u7b97\u6cd5\u3001\u63a2\u7d22\u65b0\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u7b49\u65b9\u9762\u5bfb\u627e\u65b0\u7684\u7814\u7a76\u5207\u5165\u70b9\u3002\u8fd9\u4e9b\u6311\u6218\u4fc3\u4f7f\u7814\u7a76\u8005\u4e0d\u65ad\u63a2\u7d22\u65b0\u7684\u6280\u672f\u548c\u65b9\u6cd5\uff0c\u63a8\u52a8\u540e\u7eed\u7814\u7a76\u671d\u7740\u66f4\u9ad8\u6548\u3001\u66f4\u667a\u80fd\u3001\u66f4\u5177\u6cdb\u5316\u80fd\u529b\u7684\u65b9\u5411\u53d1\u5c55\u3002", "task_step_level": "5", "task_step_question": "\u5728\u63d0\u5347\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u3001\u89e3\u51b3\u6a21\u578b\u504f\u5dee\u548c\u6570\u636e\u4f9d\u8d56\u3001\u589e\u5f3a\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u80fd\u529b\u8fd9\u4e9b\u65b0\u7814\u7a76\u95ee\u9898\u4e0b\uff0c\u4ece\u6539\u8fdb\u6a21\u578b\u67b6\u6784\u3001\u4f18\u5316\u8bad\u7ec3\u7b97\u6cd5\u3001\u63a2\u7d22\u65b0\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u7b49\u7814\u7a76\u5207\u5165\u70b9\u51fa\u53d1\uff0c\u6709\u54ea\u4e9b\u5df2\u7ecf\u53d6\u5f97\u4e00\u5b9a\u8fdb\u5c55\u7684\u5177\u4f53\u6280\u672f\u6216\u65b9\u6cd5\u5b9e\u4f8b\uff1f  ", "task_step_question_context": [{"ref_id": "454847062641060886", "chunk_id": "6", "score": 0.412109375, "text": "# 2 Related Work\nData Augmentation methods have showcased their capacity to improve DNNs\u2019 performance by expanding and diversifying training data [Maharana et al. , 2022]. Explicit augmentation directly incorporates augmented data into the training process, albeit at the expense of reduced training efficiency [Cubuk et al. , 2020; Taylor and Nitschke, 2018; Xu and Zhao, 2023]. Recently, Wang et al. [2019] introduced an implicit semantic data augmentation approach, named ISDA, which transforms the deep features of samples within the semantic space of DNNs and boils down to the optimization of a robust loss. Subsequent studies [Li et al. , 2021; Chen et al. , 2022] in image classification tasks have extended this approach. However, these methods still struggle with effectively improving model performance when dealing with data biases that go beyond the category level.  \n\nAdversarial and Anti-Adversarial Perturbations transform samples in directions that respectively move towards and away from the decision boundary, thereby modifying samples\u2019 learning difficulty [Lee et al. , 2023; Zhou et al. ,2023]. Consequently, models allocate varying levels of attention to samples subjected to their perturbations. Research has confirmed that incorporating adversarial and anti-adversarial samples during training assists models in achieving a better tradeoff between robustness and generalization [Zhou et al. ,2023; Zhu et al. , 2021]. However, existing adversarial training methods primarily focus on two specific types of perturbations that maximize and minimize losses [Xu et al. , 2021; Zhou et al. , 2023], posing limitations. Moreover, generating adversarial perturbations within the input space is timeconsuming [Madry et al. , 2018]. Different from prior studies, our approach randomly selects perturbation vectors from both adversarial and anti-adversarial perturbation distributions, enabling the generation of multiple distinct adversarial and antiadversarial samples. Furthermore, the perturbations are generated within the deep feature space, enhancing efficiency and ensuring universality across various data types.\n\n# 3 Implicit Adversarial Data Augmentation\nWe initially introduce a sample-wise adversarial data augmentation strategy to facilitate model training across various learning scenarios. By considering infinite augmentations, we then derive a surrogate loss for our augmentation strategy.\n\n# 3.1 Adversarial Data Augmentation\nConsider training a de $\\mathcal{F}$ weights $\\Phi$ on a training set, denoted as D$\\pmb{\\mathcal{D}}^{t r}=\\{(\\pmb{x}_{i},y_{i})\\}_{i=1}^{N}$ {, where Nrefers to the number of training resents the label of sample $\\pmb{x}_{i}$ . The deep feature (before logit) ples, and $\\bar{y_{i}}\\in\\{1,\\cdots,\\mathcal{C}\\}$ \u2208{ \u00b7 \u00b7 \u00b7 C} rep$\\mathcal{F}$ $\\pmb{x}_{i}$ is represented as a $\\mathcal{H}$ -dimensional vector $\\pmb{h}_{i}=\\mathcal{F}_{\\pmb{\\Phi}}(\\pmb{x}_{i})\\in\\mathbb{R}^{\\mathcal{H}}$ F\u2208.  \n\nOur augmentation strategy enhances samples within the deep feature space of DNNs. The perturbation vectors for the deep feature of each sample are randomly extracted from either its adversarial or anti-adversarial perturbation distributions. These distributions are modeled as multivariate normal distributions, $\\mathcal{N}(\\pmb{\\delta}_{i},\\pmb{\\Sigma}_{y_{i}})$ , where $\\delta_{i}$ refers to the sample perturbation, and $\\pmb{\\Sigma}_{y_{i}}$ represents the class-specific covariance matrix estimated from the features of all training samples in class $y_{i}$ . As samples undergo augmentation within the deep feature space, perturbations should also be generated within this space, facilitating semantic alterations for training samples. Consequently, the perturbation vector $\\delta_{i}$ for sample $\\pmb{x}_{i}$ is calculated as $\\epsilon_{i}{\\cdot}\\dot{s}i g n(\\dot{\\nabla}_{h_{i}}\\ell_{i}^{C E})$ , wh $s i g n(\\nabla_{h_{i}}\\ell_{i}^{C\\bar{E}})$ signifies the gradient sign of the CE loss $\\ell_{i}^{C E}$ with respect to $h_{i}$ .The parameter $\\epsilon_{i}$ plays a pivotal role in determining the perturbation strategy applied to $\\pmb{x}_{i}$ , encompassing both the perturbation direction and bound. Its positive or negative sign signifies adversarial or anti-adversarial perturbations, respecturbation bound. In practical applications, the value of tively. Furthermore, the absolute value $|\\epsilon_{i}|$ governs the \u03f5$\\epsilon_{i}$ eris dynamically computed through a perturbation network based on the training characteristics of $\\pmb{x}_{i}$ , which will be elaborated in Section 4. Additionally, the class-specific covariance matrix $\\pmb{\\Sigma}_{y_{i}}$ within this distribution aids in preserving the covariance structure of each class. Its value is estimated in real-time by aggregating statistics from all mini-batches, as detailed in Section I of the Appendix. Regarding the augmentation strength quantified by the number of augmented instances $\\mathcal{M}_{i}$ and for $\\pi_{y_{i}}$ $\\pmb{x}_{i}$ represents the proportion of class , we define $\\mathcal{M}_{i}$ as $\\mathcal{M}/\\pi_{y_{i}}$ , wher $y_{i}$ $\\mathcal{M}$ in the training is a constant data. Accordingly, a smaller proportion results in a larger number of augmented instances, ensuring class balance.  \n\n  \nFigure 2: The overview of our method pipeline. We initiate with a sample-wise adversarial data augmentation strategy (Box 1), enriching the deep features of samples using perturbation vectors extracted from their adversarial and anti-adversarial perturbation distributions. Subsequently, by considering an infinite number of augmented instances, we derive a novel robust loss, termed IADA (Box 2). Regularization analysis reveals the efficacy of IADA in improving model generalization, robustness, and inter-class fairness. To facilitate optimization with IADA, we then establish a meta-learning-based framework called Meta-IADA (Box 3). Within it, a perturbation network is tasked with generating perturbation strategies for samples (denoted as $\\epsilon_{x}$ ) in the IADA loss, leveraging a set of ( $K\\!=\\!15)$ ) training characteristics as inputs.  \n\nTo compute the augmented features $\\tilde{\\pmb{h}}_{i}$ from $h_{i}$ , we transform $h_{i}$ along random directions sampled from $\\mathcal{N}(\\pmb{\\delta}_{i},\\pmb{\\Sigma}_{y_{i}})$ .This transform ion yields $\\tilde{\\pmb{h}}_{i}\\sim\\mathcal{N}(\\pmb{h}_{i}+\\pmb{\\delta}_{i},\\alpha\\pmb{\\Sigma}_{y_{i}})$ , where the parameter \u03b1controls the extent of dispersion for augmented samples. In summary, our adversarial data augmentation strategy offers the following advantages:  \n\n\u2022 Instead of augmenting samples within the original data space, our approach enhances them within their adversarial and anti-adversarial perturbation distributions. This method effectively adjusts the learning difficulty distribution of training samples, fostering improved generalization and robustness in DNNs. \u2022 Our sample-wise augmentation distribution customizes the mean vector based on the unique training characteristics of each sample. This personalized strategy significantly enhances models\u2019 ability to address data biases, encompassing those beyond the category level."}, {"ref_id": "454846731731167836", "chunk_id": "9", "score": 0.341796875, "text": "# 2 Related Work\n\n# 2.1 Feature Weighting\nFeature weighting, vital for enhancing machine learning, includes several approaches [Chen and Guo, 2015; Chen and Hao, 2017b; Chowdhury et al. , 2023; Wang et al. , 2004; Yeung and Wang, 2002]. [Liu et al. , 2004], [Druck et al. ,2008], and [Raghavan et al. , 2006] explored feedback integration, model constraints, and active learning enhancement. [Wang et al. , 2013] proposed an active SVM method for image retrieval. Techniques like weighted bootstrapping [Barbe and Bertail, 1995], chi-squared tests, TabTransformer [Huang et al. , 2020], and cost-sensitive learning adjust weights through feature changes. These methods have limitations like overfitting or ignoring interactions. Our study focuses on adaptable weight distribution and improvement through feedback.\n\n# 2.2 Transformer\nThe Transformer architecture, introduced by [Vaswani et al. ,2017], has revolutionized many fields including natural language processing. Instead of relying on recurrence like its predecessors, it utilizes self-attention mechanisms to capture dependencies regardless of their distance in the input data. This innovation has led to several breakthroughs in various tasks. For instance, BERT model [Devlin et al. , 2018; Clark et al. , 2019], built upon the Transformer, set new records in multiple NLP benchmarks. Later, [Radford et al. ,2019] extended these ideas with GPT-2 and GPT-3 [Brown et al. , 2020], demonstrating impressive language generation capabilities. Concurrently, [Raffel et al. , 2020] proposed a unified text-to-text framework for NLP transfer learning, achieving state-of-the-art results across multiple tasks.\n\n# 3 Methodology\n\n# 3.1 Problem Formulation\n$\\{\\mathbf{F},\\mathbf{y}\\}$ {}nsider the problem be a dataset with $K$ ting of classif Nsamples tion. Let ${\\mathcal{D}}=$ define the feature matrix $\\textbf{F}=\\{\\mathbf{f}_{k}\\}_{k=1}^{K}$ {}.We use f${\\bf f}_{k}\\;\\;=\\;\\;$ $\\{f_{k}^{1},\\ldots,f_{k}^{i},\\ldots,f_{k}^{N}\\}^{\\top}$ }to denote the $k$ -th feature, $f_{k}^{i}$ is the value of i -th sample on the k-th feature. $\\textbf{y}=$ $[\\stackrel{\\cdot\\cdot}{y_{1}},\\dotsc,y_{N}]^{\\top}$ is the label vector. Without loss of generality, we assume the first $M$ features to be discrete, and the remaining $K-M$ features to be co  \n\nIn defining a weighting matrix W$\\textbf{W}\\in\\ \\mathbb{R}^{N\\times K}$ \u2208, each of whose elements corresponds to the elements of the feature matrix $\\mathbf{F}$ .This weighting matrix $\\mathbf{W}$ is applied elementwisely $\\mathbf{F}$ to produce a weighted matrix $\\bar{\\mathbf{F}_{r e w}}=\\mathbf{W}\\odot\\mathbf{F}.$ ,problem, we aim to find an optimized where \u2299denotes the Hadamard prod WIn the , so that $\\mathbf{F}_{r e w}$ can ting improve the downstream tasks\u2019 performance when substituting the original feature matrix $\\mathbf{F}$ in predicting y.\n\n# 3.2 Framework\nWe propose TFWT , a Tabular Feature Weighting with Transformer method for tabular data. We aim to improve downstream tasks\u2019 performance by effectively incorporating the attention mechanism to capture the relations and interactions between features. To achieve this goal, we design a Transformer-based feature weighting pipeline with a finetuning strategy. As Figure 2 shows, our method consists of three components: In the Feature Alignment , we align different types of original features so that they are in the same space. In the Feature Weighting , we encode the feature matrix to get its embedding via Transformer encoders, and then decode the embedding into feature weights. In the Fine-Tuning ,we design a reinforcement learning strategy to fine-tune the feature weights based on feedback from downstream tasks.\n\n# 3.3 Feature Alignment\nTo effectively extract tabular data\u2019s features while maintaining a streamlined computation, we convert both discrete and continuous features into numerical vectors.  \n\nDiscrete Feature Alignment. We first encode the discrete features into numerical values. The encoded numerical values are then passed to a dense embedding layer, transforming them into vectors for subsequent processes. For each discrete feature $\\mathbf{f}_{k}$ $(k=1,\\ldots,M)$ , the encoded vector is:  \n\n$$\n\\begin{array}{r}{\\mathbf{v}_{k}=\\mathrm{Dense}(\\mathbf{f}_{k}).}\\end{array}\n$$  \n\nContinuous Feature Alignment. We normalize all the continuous features with mean of 0 and variance of 1. We then design a linear layer to align their length with discrete features. For each continuous feature $\\mathbf{f}_{k}$ $(k=M+1,\\ldots,K)$ ,the encoded vector is:  \n\n$$\n\\mathbf{u}_{k}=\\mathrm{Linear}\\left(\\frac{\\mathbf{f}_{k}-\\mu_{k}}{\\sigma_{k}}\\right),\n$$  \n\nwhere $\\mu_{k}$ and $\\sigma_{k}$ are the mean and standard deviation of the $k$ -th feature, respectively. Then the aligned feature matrix $\\mathbf{F^{\\prime}}$ is formed by concatenating these vectors:  \n\n$$\n\\mathbf{F}^{\\prime}=[\\mathbf{v}_{1},\\ldots,\\mathbf{v}_{M},\\mathbf{u}_{M+1},\\ldots,\\mathbf{u}_{K}].\n$$\n\n# 3.4 Feature Weighting\nGiven aligned feature matrix $\\mathbf{F^{\\prime}}$ , we aim to explore the relationships between features and assign proper feature weights. Data Encoding. To enhance the model\u2019s understanding and extract latent patterns and relations from the data, we put $\\mathbf{F^{\\prime}}$ into the encoders with a multi-head self-attention mechanism. This mechanism processes the embedded feature matrix $\\mathbf{F^{\\prime}}$ by projecting it into query (Q), key (K), and value (V) spaces.  \n\nThe encoder then applies the self-attention mechanism to capture varying feature relations in the feature matrix and assigns distinct attention weights to them. Assuming $d_{k}$ is the dimensionality of the key vectors, the attention mechanism is formulated as:  \n\n$$\n\\mathrm{Attention}(Q,K,V)=\\mathrm{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_{k}}}\\right)V,\n$$  \n\nr$Q=W_{Q}\\cdot\\mathbf{F^{\\prime}}$ ,$K=W_{K}\\cdot\\mathbf{F}^{\\prime}$ , and $V=W_{V}\\cdot\\mathbf{F}^{\\prime},W_{\\mathbb{G}}$ ,$W_{K}$ ,$W_{V}$ are parameter matrices.  \n\nIn our method, we adopt the multi-head attention mechanism, where the results of each head are concatenated and linearly transformed. Assuming $W^{O}$ is an output projection matrix and $\\mathbf{Z}$ is the feature representation:  \n\n$$\n\\mathrm{{head}}_{i}=\\mathrm{{Attention}}(Q W_{i}^{Q},K W_{i}^{K},V W_{i}^{V}),\n$$  \n\n$$\n\\mathrm{MultiHead}(Q,K,V)=\\mathrm{Concat}(\\mathrm{head}_{1},...,\\mathrm{head}_{h})W^{O},\n$$  \n\n$$\n\\mathbf{Z}=\\operatorname{ResNet}(\\mathbf{MultiHead}(Q,K,V)),\n$$  \n\nwhere $W_{i}^{Q},\\;W_{i}^{K}$ , and $W_{i}^{V}$ are weights for query, key, and value. Through this process, we obtain the feature representation $\\mathbf{Z}$ that captures feature relationships. Specifically, $\\mathbf{Z}$ is obtained by passing the input feature matrix through multiple layers of the encoder, where each layer applies self-attention and residual connection-enhanced feedforward networks.  \n\nWeight Decoding. In this process, we aim to decode a weighting matrix Wfrom the embedding $\\mathbf{Z}$ . This decoding process iteratively updates Wuntil the downstream task\u2019s performance is satisfied. We initialize the Wby setting all its elements as 1. This is to ensure all features receive equal importance at the beginning. In each decoding layer, we do cross-attention on $\\mathbf{W}$ and $\\mathbf{Z}$ by:  \n\n$$\n\\mathrm{CrossAtention}(Q_{W},K_{Z},V_{Z})=\\mathrm{softmax}\\left(\\frac{Q_{W}K_{Z}^{T}}{\\sqrt{d_{z}}}\\right)V_{Z},\n$$  \n\nr$Q_{w}\\,=\\,W_{Q}\\cdot\\mathbf{W}$ ,$K_{Z}\\,=\\,K_{K}\\cdot\\mathbf{Z}$ , and $V=W_{V}\\cdot\\mathbf{Z}$ ,$W_{Q}$ ,$W_{K}$ ,$W_{V}$ are parameter matrices.  \n\nBy adopting a cross-attention mechanism, we generate a contextual representation that captures various relationships and dependencies in the feature matrix. After several weight decoding layers, we get an updated weighting matrix $\\mathbf{W}$ :  \n\n$$\n{\\bf W}={\\bf R e s N e t}(\\mathrm{CrossAttention}(Q_{W},K_{Z},V_{Z})).\n$$  \n\n  \nFigure 2: The framework consists of three components. In the alignment we convert discrete ( $f_{1}$ to $f_{M},$ ) and continuous $(f_{M+1}$ to $f_{K})$ )features into uniform-length vectors. In the weighting we initialize and reassign weights according to feature relationships. The fine-tuning process employs reinforcement learning to refine the weighting model.  \n\nWe finally use the the weighting matrix Wto derive a weighted feature matrix $\\mathbf{F}_{\\mathrm{rew}}$ by its Hadamard product with the original feature matrix $\\mathbf{F}$ :$\\mathbf{\\bar{F}}_{\\mathrm{rew}}\\;=\\;\\mathbf{W}\\odot\\bar{\\mathbf{F}}$ . With this weighted feature matrix, we reorganize the feature space and make features optimized for the downstream task. $\\mathbf{F}_{\\mathrm{rew}}$ is then used to substitute $\\mathbf{F}$ in the downstream tasks."}, {"ref_id": "454849433600195034", "chunk_id": "1", "score": 0.32421875, "text": "# CFurther Discussions\nWe summarize some empirical observations as follows.  \n\n1) The CLIP with four training tricks yields about $4\\%$ improvement at Rank-1 in Table 1 of the main paper. It can inspire future works in which the model performance could be boosted by applying these training tricks.  \n\n2) Data augmentation and loss function are common technologies used in various methods. The investigation of more than 20 data augmentations and about 10 loss functions on performance in Tables 2-5 of the main paper provides valuable guidance on future works. Researchers can select proper and effective augmentations and losses into the model for improving performance.   \n3) We explore the internal properties and functionalities of the model for the first time. These results can light future works on model compression, so as to develop a more lightweight and effective TBPS method.   \n4) There are very little research on few-shot TBPS, while this paper makes a preliminary study on CLIP-based fewshot TBPS, providing valuable observation for future research direction."}], "task_step_question_answer": "\u5728\u63a2\u8ba8\u672a\u6765\u7814\u7a76\u65b9\u5411\u65f6\uff0c\u8fd8\u9700\u5173\u6ce8\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u4f26\u7406\u548c\u6cd5\u5f8b\u95ee\u9898\u3002\u4f8b\u5982\uff0c\u63d0\u5347\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u4e0d\u4ec5\u6709\u52a9\u4e8e\u6280\u672f\u8fdb\u6b65\uff0c\u8fd8\u80fd\u5728\u6cd5\u5f8b\u5408\u89c4\u548c\u7528\u6237\u4fe1\u4efb\u65b9\u9762\u53d1\u6325\u91cd\u8981\u4f5c\u7528\u3002\u67d0\u7814\u7a76\u901a\u8fc7\u5f15\u5165\u89e3\u91ca\u6027\u6a21\u5757\uff0c\u4f7f\u6a21\u578b\u51b3\u7b56\u8fc7\u7a0b\u900f\u660e\u5316\uff0c\u6210\u529f\u5e94\u7528\u4e8e\u91d1\u878d\u98ce\u9669\u8bc4\u4f30\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u7528\u6237\u5bf9AI\u7cfb\u7edf\u7684\u4fe1\u4efb\u5ea6\u3002\u89e3\u51b3\u6a21\u578b\u504f\u5dee\u548c\u6570\u636e\u4f9d\u8d56\u95ee\u9898\uff0c\u53ef\u4ee5\u501f\u9274\u67d0\u7814\u7a76\u5728\u4eba\u8138\u8bc6\u522b\u4efb\u52a1\u4e2d\u901a\u8fc7\u591a\u6837\u5316\u6570\u636e\u96c6\u548c\u516c\u5e73\u6027\u7ea6\u675f\u51cf\u5c11\u79cd\u65cf\u548c\u6027\u522b\u504f\u5dee\u7684\u5b9e\u4f8b\uff0c\u5206\u6790\u5176\u5bf9\u63d0\u5347\u793e\u4f1a\u516c\u5e73\u6027\u7684\u6f5c\u5728\u5f71\u54cd\u3002\n\n\u5728\u7ec6\u5316\u7814\u7a76\u5207\u5165\u70b9\u65b9\u9762\uff0c\u6539\u8fdb\u6a21\u578b\u67b6\u6784\u7684\u5177\u4f53\u65b9\u6cd5\u8fd8\u53ef\u4ee5\u5305\u62ec\u5f15\u5165\u5143\u5b66\u4e60\u673a\u5236\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u81ea\u9002\u5e94\u4e0d\u540c\u4efb\u52a1\u9700\u6c42\u3002\u67d0\u7814\u7a76\u901a\u8fc7\u5143\u5b66\u4e60\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u6a21\u578b\u5728\u591a\u4e2a\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u5feb\u901f\u9002\u5e94\uff0c\u5c55\u793a\u4e86\u5176\u5728\u591a\u4efb\u52a1\u5b66\u4e60\u4e2d\u7684\u6f5c\u529b\u3002\u4f18\u5316\u8bad\u7ec3\u7b97\u6cd5\u7684\u5177\u4f53\u65b9\u6cd5\u53ef\u4ee5\u5305\u62ec\u96c6\u6210\u5f3a\u5316\u5b66\u4e60\uff0c\u901a\u8fc7\u5956\u52b1\u673a\u5236\u5f15\u5bfc\u6a21\u578b\u4f18\u5316\u65b9\u5411\uff0c\u67d0\u7814\u7a76\u5728\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e2d\u5e94\u7528\u6b64\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u51b3\u7b56\u6548\u7387\u548c\u5b89\u5168\u6027\u3002\n\n\u589e\u5f3a\u5bf9\u672a\u6765\u7814\u7a76\u65b9\u5411\u7684\u9884\u6d4b\u6027\uff0c\u8fd8\u9700\u5173\u6ce8AI\u6280\u672f\u5728\u53ef\u6301\u7eed\u53d1\u5c55\u9886\u57df\u7684\u5e94\u7528\u3002\u9884\u6d4b\u672a\u6765\u51e0\u5e74\u5185\uff0cAI\u5728\u80fd\u6e90\u4f18\u5316\u548c\u73af\u5883\u76d1\u6d4b\u65b9\u9762\u7684\u7814\u7a76\u5c06\u6210\u4e3a\u70ed\u70b9\u3002\u67d0\u7814\u7a76\u901a\u8fc7AI\u4f18\u5316\u7535\u7f51\u8c03\u5ea6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u80fd\u6e90\u5229\u7528\u6548\u7387\uff0c\u5c55\u793a\u4e86\u5176\u5728\u8282\u80fd\u51cf\u6392\u4e2d\u7684\u5de8\u5927\u6f5c\u529b\u3002\n\n\u589e\u52a0\u8de8\u5b66\u79d1\u89c6\u89d2\uff0c\u53ef\u4ee5\u63a2\u8ba8\u5c06\u795e\u7ecf\u79d1\u5b66\u4e2d\u7684\u8111\u673a\u63a5\u53e3\u6280\u672f\u878d\u5165AI\u7814\u7a76\uff0c\u63d0\u5347\u4eba\u673a\u4ea4\u4e92\u7684\u81ea\u7136\u6027\u548c\u6548\u7387\u3002\u67d0\u7814\u7a76\u901a\u8fc7\u8111\u7535\u4fe1\u53f7\u5206\u6790\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u4eba\u673a\u4ea4\u4e92\u7cfb\u7edf\uff0c\u5c55\u793a\u4e86\u5176\u5728\u8f85\u52a9\u6b8b\u969c\u4eba\u58eb\u65b9\u9762\u7684\u5e94\u7528\u524d\u666f\u3002\n\n\u5f3a\u5316\u5b9e\u8bc1\u652f\u6301\uff0c\u5f15\u7528\u67d0\u7814\u7a76\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u4e2d\u901a\u8fc7\u5927\u89c4\u6a21\u8bed\u6599\u5e93\u9a8c\u8bc1\u6a21\u578b\u6027\u80fd\u7684\u63d0\u5347\uff0c\u63d0\u4f9b\u8be6\u7ec6\u7684\u5b9e\u9a8c\u8bbe\u7f6e\u548c\u7ed3\u679c\u5206\u6790\uff0c\u589e\u5f3a\u4e86\u8bba\u8ff0\u7684\u5b9e\u8bc1\u57fa\u7840\u3002\n", "ref_task_step_id": ""}, "__type__": "task_step_node"}, "3e9a2ad0-3314-47be-933a-c61134edc408": {"__data__": {"id_": "3e9a2ad0-3314-47be-933a-c61134edc408", "metadata": {}, "relationships": {}, "hash": "", "start_task_context": "\u5927\u6a21\u578bMLA\u6280\u672f\u7ec6\u8282", "aemo_representation_context": "### 1. \u5206\u6790\u8fd1\u51e0\u5e74\u7814\u7a76\u9886\u57df\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba\n\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u76f8\u5173\u7814\u7a76\u9886\u57df\uff0cTransformer\u3001GAN\u3001BERT \u7b49\u6846\u67b6\u6210\u4e3a\u4f17\u591a\u8bba\u6587\u91c7\u7528\u7684\u6838\u5fc3\u6280\u672f\u3002\u8fd9\u4e9b\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u9002\u5e94\u6027\uff0c\u5176\u65b9\u6cd5\u8bba\u56f4\u7ed5\u7740\u5982\u4f55\u6709\u6548\u5904\u7406\u6570\u636e\u7279\u5f81\u3001\u4f18\u5316\u6a21\u578b\u7ed3\u6784\u4ee5\u63d0\u5347\u6027\u80fd\u3002\u4f8b\u5982\uff0cTransformer \u57fa\u4e8e\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6253\u7834\u4e86\u4f20\u7edf\u5e8f\u5217\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u7b49\u591a\u9886\u57df\u5e7f\u6cdb\u5e94\u7528\uff1bGAN \u901a\u8fc7\u751f\u6210\u5668\u4e0e\u5224\u522b\u5668\u7684\u5bf9\u6297\u8bad\u7ec3\uff0c\u5728\u56fe\u50cf\u751f\u6210\u7b49\u4efb\u52a1\u4e0a\u53d6\u5f97\u663e\u8457\u6210\u679c\uff1bBERT \u5219\u901a\u8fc7\u9884\u8bad\u7ec3\u65b9\u5f0f\uff0c\u6781\u5927\u63d0\u5347\u4e86\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u7684\u6027\u80fd\u3002\n\n### 2. \u7814\u7a76\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u4e3b\u8981\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0e\u53d8\u4f53\n - **Transformer**\uff1a\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u4e2d\uff0c\u5982\u673a\u5668\u7ffb\u8bd1\u3001\u6587\u672c\u5206\u7c7b\uff0cTransformer \u67b6\u6784\u4e0d\u65ad\u6f14\u53d8\uff0c\u51fa\u73b0\u4e86\u8bf8\u5982 ViT\uff08Vision Transformer\uff09\u7b49\u53d8\u4f53\u5e94\u7528\u4e8e\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\uff0c\u5c06\u56fe\u50cf\u89c6\u4e3a\u5e8f\u5217\u8fdb\u884c\u5904\u7406\uff0c\u5c55\u73b0\u51fa\u8de8\u9886\u57df\u7684\u5e94\u7528\u6f5c\u529b\u3002\n - **GAN**\uff1a\u5728\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\uff0cDCGAN\uff08Deep Convolutional GAN\uff09\u7b49\u53d8\u4f53\u901a\u8fc7\u6539\u8fdb\u7f51\u7edc\u7ed3\u6784\uff0c\u751f\u6210\u66f4\u903c\u771f\u7684\u56fe\u50cf\uff1b\u5728\u533b\u5b66\u56fe\u50cf\u5408\u6210\u3001\u89c6\u9891\u751f\u6210\u7b49\u9886\u57df\u4e5f\u6709\u5e94\u7528\uff0c\u901a\u8fc7\u8c03\u6574\u635f\u5931\u51fd\u6570\u548c\u7f51\u7edc\u67b6\u6784\u9002\u5e94\u4e0d\u540c\u4efb\u52a1\u9700\u6c42\u3002\n - **BERT**\uff1a\u9664\u4e86\u57fa\u7840\u7684\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u4efb\u52a1\uff0c\u8fd8\u5728\u4fe1\u606f\u68c0\u7d22\u3001\u60c5\u611f\u5206\u6790\u7b49\u4efb\u52a1\u4e2d\u5f97\u5230\u5e7f\u6cdb\u5e94\u7528\uff0c\u901a\u8fc7\u5fae\u8c03\u9884\u8bad\u7ec3\u6a21\u578b\u9002\u5e94\u7279\u5b9a\u4efb\u52a1\uff0c\u540c\u65f6\u51fa\u73b0\u4e86 RoBERTa \u7b49\u6539\u8fdb\u7248\u672c\uff0c\u4f18\u5316\u9884\u8bad\u7ec3\u7b56\u7565\u63d0\u5347\u6027\u80fd\u3002\n\n### 3. \u8bc4\u4f30\u5b66\u672f\u754c\u7684\u6280\u672f\u8fdb\u6b65\u4e0e\u5c40\u9650\u6027\n - **\u6280\u672f\u8fdb\u6b65**\uff1a\u6a21\u578b\u6027\u80fd\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u53d6\u5f97\u663e\u8457\u63d0\u5347\uff0c\u4f8b\u5982\u5728\u56fe\u50cf\u8bc6\u522b\u51c6\u786e\u7387\u3001\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u7684 F1 \u503c\u7b49\u6307\u6807\u4e0a\u4e0d\u65ad\u5237\u65b0\u8bb0\u5f55\u3002\u65b0\u7684\u6a21\u578b\u67b6\u6784\u548c\u8bad\u7ec3\u65b9\u6cd5\u4e0d\u65ad\u6d8c\u73b0\uff0c\u63a8\u52a8\u4e86\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\u3002\n - **\u5c40\u9650\u6027**\uff1a\u4f9d\u7136\u5b58\u5728\u6a21\u578b\u504f\u5dee\u548c\u6570\u636e\u4f9d\u8d56\u95ee\u9898\u3002\u6a21\u578b\u5728\u67d0\u4e9b\u7279\u5b9a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u5176\u4ed6\u6570\u636e\u96c6\u6216\u73b0\u5b9e\u573a\u666f\u4e2d\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002\u90e8\u5206\u6a21\u578b\u5bf9\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u4f9d\u8d56\u4e25\u91cd\uff0c\u6570\u636e\u8d28\u91cf\u548c\u6570\u91cf\u5f71\u54cd\u6a21\u578b\u6027\u80fd\uff0c\u4e14\u6a21\u578b\u89e3\u91ca\u6027\u5dee\uff0c\u96be\u4ee5\u7406\u89e3\u5176\u51b3\u7b56\u8fc7\u7a0b\u3002\n\n### 4. \u63a2\u8ba8\u8ba1\u7b97\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\n - **\u9002\u7528\u6027**\uff1a\u4e0d\u540c\u6a21\u578b\u5728\u7279\u5b9a\u6570\u636e\u96c6\u548c\u5e94\u7528\u573a\u666f\u4e0b\u6709\u5404\u81ea\u4f18\u52bf\u3002\u4f8b\u5982\uff0c\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6a21\u578b\u5728\u5927\u89c4\u6a21\u3001\u9ad8\u7ef4\u6570\u636e\u7684\u56fe\u50cf\u548c\u8bed\u97f3\u8bc6\u522b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff1b\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u5c0f\u6570\u636e\u3001\u7b80\u5355\u4efb\u52a1\u4e2d\u4ecd\u6709\u5e94\u7528\u4ef7\u503c\u3002\n - **\u6cdb\u5316\u80fd\u529b**\uff1a\u5c3d\u7ba1\u90e8\u5206\u6a21\u578b\u5728\u7279\u5b9a\u9886\u57df\u53d6\u5f97\u6210\u529f\uff0c\u4f46\u5728\u591a\u9886\u57df\u3001\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u7684\u73b0\u5b9e\u5e94\u7528\u4e2d\uff0c\u6cdb\u5316\u80fd\u529b\u9762\u4e34\u6311\u6218\u3002\u591a\u6a21\u6001\u6570\u636e\u7684\u5f02\u8d28\u6027\u548c\u590d\u6742\u6027\u4f7f\u5f97\u6a21\u578b\u96be\u4ee5\u6709\u6548\u63d0\u53d6\u548c\u878d\u5408\u7279\u5f81\uff0c\u9650\u5236\u4e86\u5176\u5728\u590d\u6742\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002\n\n### 5. \u5206\u6790\u6700\u65b0\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027\n - **\u7a33\u5b9a\u6027\u4f18\u5316**\uff1a\u4e00\u4e9b\u7b97\u6cd5\u9488\u5bf9\u7a33\u5b9a\u6027\u8fdb\u884c\u4e86\u4f18\u5316\uff0c\u4f8b\u5982\u91c7\u7528\u6b63\u5219\u5316\u6280\u672f\u3001\u5bf9\u6297\u8bad\u7ec3\u7b49\u65b9\u6cd5\u63d0\u9ad8\u6a21\u578b\u5728\u590d\u6742\u73af\u5883\u4e0b\u7684\u7a33\u5b9a\u6027\u3002\u5728\u52a8\u6001\u73af\u5883\u4e2d\uff0c\u90e8\u5206\u7b97\u6cd5\u901a\u8fc7\u5728\u7ebf\u5b66\u4e60\u3001\u81ea\u9002\u5e94\u8c03\u6574\u53c2\u6570\u7b49\u65b9\u5f0f\u4fdd\u6301\u6027\u80fd\u7a33\u5b9a\u3002\n - **\u5927\u89c4\u6a21\u6570\u636e\u9002\u5e94\u6027**\uff1a\u5728\u5927\u89c4\u6a21\u6570\u636e\u5904\u7406\u4e0a\uff0c\u90e8\u5206\u7b97\u6cd5\u901a\u8fc7\u5206\u5e03\u5f0f\u8bad\u7ec3\u3001\u6a21\u578b\u538b\u7f29\u7b49\u6280\u672f\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u548c\u9002\u5e94\u6027\u3002\u4f46\u4ecd\u6709\u7b97\u6cd5\u5728\u5904\u7406\u5927\u89c4\u6a21\u6570\u636e\u65f6\u9762\u4e34\u5185\u5b58\u3001\u8ba1\u7b97\u8d44\u6e90\u7b49\u9650\u5236\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002\n\n### 6. \u8bc4\u4f30\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218\n - **\u65b0\u7814\u7a76\u95ee\u9898**\uff1a\u63d0\u51fa\u4e86\u5982\u63d0\u5347\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u3001\u89e3\u51b3\u6a21\u578b\u504f\u5dee\u548c\u6570\u636e\u4f9d\u8d56\u3001\u589e\u5f3a\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u80fd\u529b\u7b49\u65b0\u7814\u7a76\u95ee\u9898\u3002\n - **\u7814\u7a76\u5207\u5165\u70b9**\uff1a\u4ece\u6539\u8fdb\u6a21\u578b\u67b6\u6784\u3001\u4f18\u5316\u8bad\u7ec3\u7b97\u6cd5\u3001\u63a2\u7d22\u65b0\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u7b49\u65b9\u9762\u5bfb\u627e\u65b0\u7684\u7814\u7a76\u5207\u5165\u70b9\u3002\u8fd9\u4e9b\u6311\u6218\u4fc3\u4f7f\u7814\u7a76\u8005\u4e0d\u65ad\u63a2\u7d22\u65b0\u7684\u6280\u672f\u548c\u65b9\u6cd5\uff0c\u63a8\u52a8\u540e\u7eed\u7814\u7a76\u671d\u7740\u66f4\u9ad8\u6548\u3001\u66f4\u667a\u80fd\u3001\u66f4\u5177\u6cdb\u5316\u80fd\u529b\u7684\u65b9\u5411\u53d1\u5c55\u3002\n\n### \u7814\u7a76\u6210\u679c\u3001\u65b9\u6cd5\u521b\u65b0\u6027\u4e0e\u5e94\u7528\u4ef7\u503c\u603b\u7ed3\n - **\u7814\u7a76\u6210\u679c**\uff1a\u5728\u6a21\u578b\u6027\u80fd\u63d0\u5347\u3001\u65b0\u67b6\u6784\u548c\u7b97\u6cd5\u63d0\u51fa\u7b49\u65b9\u9762\u53d6\u5f97\u663e\u8457\u6210\u679c\uff0c\u63a8\u52a8\u4e86\u8ba1\u7b97\u673a\u79d1\u5b66\u591a\u4e2a\u9886\u57df\u7684\u53d1\u5c55\u3002\n - **\u65b9\u6cd5\u521b\u65b0\u6027**\uff1a\u901a\u8fc7\u5f15\u5165\u65b0\u7684\u6280\u672f\u6846\u67b6\u3001\u6539\u8fdb\u8bad\u7ec3\u65b9\u6cd5\u548c\u4f18\u5316\u6a21\u578b\u7ed3\u6784\u7b49\u65b9\u5f0f\uff0c\u5c55\u73b0\u51fa\u8bf8\u591a\u521b\u65b0\u70b9\uff0c\u5982\u81ea\u6ce8\u610f\u529b\u673a\u5236\u3001\u5bf9\u6297\u8bad\u7ec3\u7b49\u3002\n - **\u5e94\u7528\u4ef7\u503c**\uff1a\u5728\u56fe\u50cf\u3001\u81ea\u7136\u8bed\u8a00\u5904\u7406\u3001\u8bed\u97f3\u7b49\u591a\u4e2a\u9886\u57df\u6709\u5e7f\u6cdb\u5e94\u7528\uff0c\u4e3a\u667a\u80fd\u5b89\u9632\u3001\u667a\u80fd\u533b\u7597\u3001\u667a\u80fd\u4ea4\u901a\u7b49\u73b0\u5b9e\u573a\u666f\u63d0\u4f9b\u6280\u672f\u652f\u6301\uff0c\u5177\u6709\u5de8\u5927\u7684\u5546\u4e1a\u548c\u793e\u4f1a\u4ef7\u503c\u3002  ", "task_step_name": "\u7814\u7a76\u6210\u679c\u3001\u65b9\u6cd5\u521b\u65b0\u6027\u4e0e\u5e94\u7528\u4ef7\u503c\u603b\u7ed3", "task_step_description": "- **\u7814\u7a76\u6210\u679c**\uff1a\u5728\u6a21\u578b\u6027\u80fd\u63d0\u5347\u3001\u65b0\u67b6\u6784\u548c\u7b97\u6cd5\u63d0\u51fa\u7b49\u65b9\u9762\u53d6\u5f97\u663e\u8457\u6210\u679c\uff0c\u63a8\u52a8\u4e86\u8ba1\u7b97\u673a\u79d1\u5b66\u591a\u4e2a\u9886\u57df\u7684\u53d1\u5c55\u3002 - **\u65b9\u6cd5\u521b\u65b0\u6027**\uff1a\u901a\u8fc7\u5f15\u5165\u65b0\u7684\u6280\u672f\u6846\u67b6\u3001\u6539\u8fdb\u8bad\u7ec3\u65b9\u6cd5\u548c\u4f18\u5316\u6a21\u578b\u7ed3\u6784\u7b49\u65b9\u5f0f\uff0c\u5c55\u73b0\u51fa\u8bf8\u591a\u521b\u65b0\u70b9\uff0c\u5982\u81ea\u6ce8\u610f\u529b\u673a\u5236\u3001\u5bf9\u6297\u8bad\u7ec3\u7b49\u3002 - **\u5e94\u7528\u4ef7\u503c**\uff1a\u5728\u56fe\u50cf\u3001\u81ea\u7136\u8bed\u8a00\u5904\u7406\u3001\u8bed\u97f3\u7b49\u591a\u4e2a\u9886\u57df\u6709\u5e7f\u6cdb\u5e94\u7528\uff0c\u4e3a\u667a\u80fd\u5b89\u9632\u3001\u667a\u80fd\u533b\u7597\u3001\u667a\u80fd\u4ea4\u901a\u7b49\u73b0\u5b9e\u573a\u666f\u63d0\u4f9b\u6280\u672f\u652f\u6301\uff0c\u5177\u6709\u5de8\u5927\u7684\u5546\u4e1a\u548c\u793e\u4f1a\u4ef7\u503c\u3002", "task_step_level": "6", "task_step_question": "\u5728\u6a21\u578b\u6027\u80fd\u63d0\u5347\u3001\u65b0\u67b6\u6784\u548c\u7b97\u6cd5\u63d0\u51fa\u7b49\u7814\u7a76\u6210\u679c\u65b9\u9762\uff0c\u5177\u4f53\u6709\u54ea\u4e9b\u6570\u636e\u6216\u5b9e\u4f8b\u53ef\u4ee5\u8fdb\u4e00\u6b65\u8bf4\u660e\u5176\u5bf9\u8ba1\u7b97\u673a\u79d1\u5b66\u591a\u4e2a\u9886\u57df\u53d1\u5c55\u7684\u63a8\u52a8\u4f5c\u7528\uff1f ", "task_step_question_context": [{"ref_id": "454847062641060886", "chunk_id": "6", "score": 0.3203125, "text": "# CFurther Discussions\nWe summarize some empirical observations as follows.  \n\n1) The CLIP with four training tricks yields about $4\\%$ improvement at Rank-1 in Table 1 of the main paper. It can inspire future works in which the model performance could be boosted by applying these training tricks.  \n\n2) Data augmentation and loss function are common technologies used in various methods. The investigation of more than 20 data augmentations and about 10 loss functions on performance in Tables 2-5 of the main paper provides valuable guidance on future works. Researchers can select proper and effective augmentations and losses into the model for improving performance.   \n3) We explore the internal properties and functionalities of the model for the first time. These results can light future works on model compression, so as to develop a more lightweight and effective TBPS method.   \n4) There are very little research on few-shot TBPS, while this paper makes a preliminary study on CLIP-based fewshot TBPS, providing valuable observation for future research direction."}, {"ref_id": "454848267426884226", "chunk_id": "6", "score": 0.2451171875, "text": "# Ethical Considerations\nOur experiments replicate prior work under comparable experimental conditions. For this reason, we do not expect our work to introduce any novel ethical issues, although our experiments may inherit a similar set of issues concerning PLM (especially large-scale ones), as outlined by various prior work ( Gehman et al. ,2020 ;Bender et al. ,2021 ;Rae et al. ,2021 ;Dinan et al. ,2021 ;Bommasani et al. ,2021 ;Kenton et al. ,2021 ;Weidinger et al. ,2021 ,inter alia ). We remark, however, that conducting these principled comparisons across different models \u2014 which requires a degree of hyper-parameter tuning for each model (both at pre-training and fine-tuning stages) in order to enable a fair comparison \u2014 requires a large number of computational resources, which may contribute to increased carbon emissions ( Strubell et al. ,2019 ;Patterson et al. ,2021 ).\n\n# Acknowledgement\nWe thank Chris Dyer, John T. Hale, and Laura Rimell at DeepMind, Noah A. Smith at the University of Washington & Allen Institute for Artificial Intelligence, and Qi Huang at Bloomberg for their valuable insights, feedback, and suggestions.\n\n\n\n# A Model Comparison\nIn Table 3 , we outline a summary of various aspects of some commonly-used PLMs that have been proposed to date, summarizing the size of the model, the training data and its size, the text pre-processing scheme, the pre-training objective, and some hyper-parameter details. This table reveals a large variation in the design choices of these PLMs, hence rendering it difficult to conduct apple-to-apple comparisons between different approaches. Common patterns include scaling the model while also using different (often larger) pretraining data, as well as using different training regimes altogether. Each design choice impacts model performance in different ways ( Sennrich and Zhang ,2019 ;Jiao et al. ,2019 ), emphasizing the importance of conducting thorough ablations and apple-to-apple comparisons.  \n\nmodel over the others. We summarize some key design choices in Table 4 .\n\n# B.1 Hyper-Paramaters for Fine-tuning\nWe refer to the original BERT\u2019s hyper-parameters for fine-tuning, but experiment with more finetuning learning rates. In addition to that, we also use three different random seeds. The finetuning hyper-parameters that we used are as follows, which is partially based on prior work ( Joshi et al. ,2020 ):  \n\n\u2022Batch sizes: {16, 32}   \n\u2022Learning rates: {2e-4, 1e-4, 5e-5, 3e-5, 2e5, 1e-5, 5e-6}   \n\u2022Epoch: 4   \n\u2022Random seeds : {1, 41, 386}\n\n# BDetailed Hyper-Parameters\nWhen conducting experiments, we follow BERT\u2019s architecture, training data, and overall hyperparameter choices ( Devlin et al. ,2019 ). One noticeable difference, however, is that we follow RoBERTa to train the model without using a nextsentence prediction loss ( Liu et al. ,2019 ), which has been shown to have a minimal impact on model performance. Each of the three models in our rerun not only has the exact same design choices in terms of training data, text processing, model architecture, etc., but is also implemented on the exact same codebase. Concretely, we use the BERT codebase as implemented on Huggingface, and conduct some slight modifications in terms of removing the next-sentence prediction loss as stated above. When implementing the other models, we take the BERT implementation, and simply change the masking function and pre-training objective in order to replicate the results of GPT-1 and ELMo under comparable conditions as our BERT model. This means that our reruns of the GPT-1 and ELMo models benefit from the exact same technical implementation details as our BERT model by virtue of using identical positional encoding, segment embeddings, etc. We tune the pre-training and fine-tuning learning rate of each model independently (hence the final learning rate for each model may be different), although we strive to dedicate the same amount of compute resources in tuning the hyper-parameters of each model, in order to avoid favoring one"}, {"ref_id": "454846731731167836", "chunk_id": "9", "score": 0.2412109375, "text": "# 4 Paying Off the Scientific Debt: Recommendations and Lessons Learnt\nWe proceed to outline several key recommendations and lessons learnt for encouraging, incentivizing, and accelerating progress in this line of work.  \n\nEstablish standard, publicly available pretraining corpora at multiple data scales. As seen in $\\S3$ , the size and quality of the pre-training data is an important driver behind model performance ( Liu et al. ,2019 ;Hoffmann et al. ,2022 ), which makes a rigorous comparison between different PLMs difficult. Hence, our first recommendation is to establish standard pre-training corpora that are publicly available. We further recommend releasing the pre-training corpora under multiple data scales, as approaches that work best under strict compute or data resource requirements may be different from the case where there is a large amount of compute and data available (\u00a7 3 ,Clark et al. ,2020 ;Treviso et al. ,2022 ). Note that this does not mean that we are discouraging the use of non-standard or even-larger corpora than those that are publicly available. On the contrary, researchers should continue to push the boundaries of what is possible by training on more, better quality, and more recent data. In such cases, we recommend researchers to also release versions of their models that are trained on the standard pre-training corpora \u2014 above and beyond the version trained on proprietary & large-scale data that would presumably be necessary to achieve a new state-of-the-art \u2014 in order to facilitate a fair and principled comparison with prior work. We encourage the community to continually release new standardized pre-training datasets as time passes to avoid the effect of pre-training data staleness (Lazaridou et al. ,2021 ).  \n\n<html><body><table><tr><td>Model</td><td>CoLA</td><td>MNLI(-m)</td><td>MRPC</td><td>QNLI</td><td>QQP</td><td>RTE</td><td>SST-2</td><td>STS-B</td><td>Avg</td></tr><tr><td colspan=\"10\">Original published results</td></tr><tr><td>BERT</td><td>52.1</td><td>84.6</td><td>88.9</td><td>90.5</td><td>71.2</td><td>66.4</td><td>93.5</td><td>85.8</td><td>79.1</td></tr><tr><td>BERTLarge</td><td>60.5</td><td>86.7</td><td>89.3</td><td>92.7</td><td>72.1</td><td>70.1</td><td>94.9</td><td>86.5</td><td>81.6</td></tr><tr><td>GPT-1</td><td>45.4</td><td>82.1</td><td>82.3</td><td>87.4</td><td>70.3</td><td>56.0</td><td>91.3</td><td>80.0</td><td>74.4</td></tr><tr><td>BiLSTM+ELMo+Attn</td><td>36.0</td><td>76.4</td><td>84.9</td><td>79.8</td><td>64.8</td><td>56.8</td><td>90.4</td><td>73.3</td><td>70.3</td></tr><tr><td colspan=\"10\">Our replication with proper controls & comparable experimental conditions</td></tr><tr><td>BERTRerun</td><td>50.8</td><td>84.5</td><td>89.0</td><td>90.5</td><td>71.0</td><td>61.0</td><td>93.1</td><td>84.4</td><td>78.0</td></tr><tr><td>Comparable GPT-1 Rerun - L2R</td><td>41.6</td><td>87.4</td><td>84.7</td><td>86.6</td><td>68.8</td><td>62.9</td><td>91.8</td><td>79.3</td><td>75.4</td></tr><tr><td>Comparable GPT-1 Rerun -R2L</td><td>42.5</td><td>82.0</td><td>85.5</td><td>88.3</td><td>69.1</td><td>57.6</td><td>92.8</td><td>79.1</td><td>74.6</td></tr><tr><td>ComparableELMo-variantRerun</td><td>46.8</td><td>83.6</td><td>85.8</td><td>89.9</td><td>70.8</td><td>61.9</td><td>93.1</td><td>82.1</td><td>76.8</td></tr><tr><td>Ensemble of Comparable GPT-1: L2R + R2L</td><td>45.1</td><td>83.7</td><td>85.8</td><td>88.9</td><td>70.8</td><td>62.4</td><td>92.9</td><td>81.0</td><td>76.3</td></tr><tr><td>Ensemble of Comparable GPT-1: L2R + L2R</td><td>42.4</td><td>83.5</td><td>85.1</td><td>87.8</td><td>70.0</td><td>63.1</td><td>93.1</td><td>79.9</td><td>75.6</td></tr></table></body></html>  \n\nTable 1: GLUE test results. We use F1 scores for MRPC and QQP, Matthew\u2019s Correlation for CoLA, SpearmanR for STS-B, and accuracy for the rest; all models are pre-trained with the same batch size & compute (1M steps).   \n\n\n<html><body><table><tr><td>Model</td><td>CoLA</td><td>MNLI(-m)</td><td>MRPC</td><td>QNLI</td><td>QQP</td><td>RTE</td><td>SST-2</td><td>STS-B</td><td>Avg</td></tr><tr><td>BERT Rerun</td><td>43.8</td><td>80.9</td><td>86.4</td><td>87.9</td><td>69.3</td><td>59.3</td><td>90.0</td><td>80.4</td><td>74.8</td></tr><tr><td>Comparable GPT-1 Rerun: L2R</td><td>43.5</td><td>80.3</td><td>84.1</td><td>86.3</td><td>68.4</td><td>63.0</td><td>91.0</td><td>77.8</td><td>74.3</td></tr><tr><td>Comparable GPT-1 Rerun: R2L</td><td>36.2</td><td>80.6</td><td>82.4</td><td>88.2</td><td>68.7</td><td>53.7</td><td>93.0</td><td>77.8</td><td>72.6</td></tr><tr><td>Ensemble of GPT-1 Rerun: L2R + R2L</td><td>45.1</td><td>82.6</td><td>84.4</td><td>88.3</td><td>70.8</td><td>62.9</td><td>93.5</td><td>79.9</td><td>75.9</td></tr></table></body></html>\n\nTable 2: GLUE test set results using the pre-trained model, after training for 200,000 steps followed by fine-tuning.  \n\nExplicitly delineate the different types of contributions behind each work, including both the key novelty and engineering contributions. We recommend that PLM research explicitly state the key novelty behind each work ( e.g., the bidirectional masked LM loss for BERT), delineate and explicitly state other contributions (including engineering ones) and design choices that can impact performance, and outline how these differ from prior work ( e.g., better model partitioning for training larger models on multiple devices, better filtering of the training data, more extensive hyperparameter tuning, etc.). Combined with strong baselines and extensive ablations (see below), this will enable us to better understand how much of the performance gains can be attributed to each factor, including the key novelty behind the approach.  \n\nInvest comparable effort into tuning both the baselines and the newly-proposed models. In practice, many of the contributions ( e.g., better hyper-parameters, better data, etc.) would also be applicable to the baselines. We recommend each PLM work to discern which of their design choices can also be applied to the baselines, and apply those techniques in order to create stronger baselines that may nevertheless rival the performance of more recent models (\u00a7 3 ,Melis et al. ,2018 ;Lei ,More extensive ablation studies. When proposing multiple contributions at once (as many PLM papers do), we recommend conducting as many ablation studies as is feasible to isolate the impact of each component under comparable conditions. In light of recent trends where models \u2014 including open-sourced ones \u2014 are publicly released without technical reports or papers that outline technical details regarding model evaluation and benchmarking ( Taori et al. ,2023 ;Chiang et al. ,2023 ), we argue that our recommendation for conducting more thorough evaluations is even more critical.  \n\nBetter credit assignment is needed. As shown in Table 1 , the vast gap between BERT and ELMo can nearly be bridged by using (i) the same (larger) pre-training data, (ii) Transformer architectures, and (iii) whole model fine-tuning; all of which were already used and proposed by the GPT-1 model. As these techniques account for a more significant chunk of the performance difference than the bidirectional masked LM loss, disentangling each factor\u2019s contribution thus provides an opportunity to conduct better credit assignment in the field.  \n\nStrike a balance between pushing the stateof-the-art and advancing our scientific understanding. In some sense, recent rapid progress is made possible by a strong emphasis on building the next state-of-the-art PLMs and foundation models, although it comes at a cost of understanding \u2014 from the scientific point of view \u2014 where the performance improvements are coming from, and which techniques work best under what circumstances. We argue that both lines of work \u2014 one that pushes the state-of-the-art at breakneck speed and through all available means, and another that aims to resolve the scientific and technical debt by disentangling the impact of multiple factors of model improvements (which we argue is still currently underrepresented in the field) \u2014 should be conducted, encouraged, and rewarded within the field. We outline two concrete recommendations for striking a better balance between the two lines of work. First, public release of PLMs or their downstream applications should be promptly accompanied by a technical description of the model, ideally in the form of a technical report or a scientific paper. This would enable the community to better understand the key component behind these models\u2019 success, allow future work to replicate the results, and promptly disentangle the different components behind model improvements. Second, we as a community should not necessarily expect both types of contributions under the same paper. Just like how the cleaning up of technical debt happens after the initial code has been written, it is often the case that prior work that resolves the scientific debt through principled comparisons was only conducted after substantial progress in advancing the state-of-the-art (often through all available means for improving model performance) had been made. We should, however, encourage the community to conduct such understanding line of work promptly after major milestones or exciting results.  \n\nReward and encourage a line of work that focuses on understanding (not just those that chase a new state-of-the-art), even when they are imperfect. The current, rapid pace of the field provides an incentive to spend one\u2019s (finite) computational resources and effort for building the next state-of-the-art, albeit at the expense of scientific rigour and principled comparisons. Given a fi- nite amount of compute, there is arguably more incentive in tuning one\u2019s proposed approach through all possible means ( e.g., using larger datasets and larger models, training for longer, etc.), topping the leaderboards, and publishing the paper, even if this leaves no computational resources to tune the baselines and conduct rigorous ablations. Furthermore, the rapidly increasing cost of training ever-larger PLMs means that any principled comparisons are most likely imperfect (\u00a7 7 ) \u2014 e.g., how do our findings in $\\S3$ change with models that are trained for longer, like RoBERTa? Or with encoder-decoder models like T5? Or in other languages? Indeed, our experiments in $\\S3$ are fairly narrow in scope, involving only three nonrecent models (BERT, ELMo, GPT-1) and a training dataset that is small by today\u2019s standards. Yet due to the rigorous hyper-parameter tuning of all three models, conducting these principled comparisons required an enormous amount of compute resources \u2014 equivalent to training 10 BERTs from scratch. This cost would have been even higher with the inclusion more models, languages, and larger datasets. On this point, we remark that doing such principled comparisons \u2014 even when they are limited in scope and done on smaller models \u2014 still contributes towards paying off the scientific debt, better understanding where our current progress is coming from, and deriving valuable insights that can contribute to the development of next generation PLMs. We additionally call on those in our community who serve as reviewers to recognize and reward these types of research contributions, which are complementary (if perhaps equally important) to a parallel line of work that pushes the state-of-the-art in PLM research through all possible means.  \n\nWe need more comprehensive PLM scaling laws. Our experiments and recommendations still leave a major open question: How can we scale these kinds of investigations to much larger PLMs, which are much more computationally expensive? To that end, scaling laws (Kaplan et al. ,2020 ;Hoffmann et al. ,2022 ) provide an account of how PLM performance changes with respect to different factors, allowing us to accurately extrapolate that a PLM with X parameters and Y training steps should achieve a perplexity of Z. However, we argue that current scaling laws are still overly narrow in scope: Concretely, existing scaling laws often only apply to decoder-only / unidirectional PLMs, and only provide an account of how their performance changes with respect to (i) model size and (ii) the number of training tokens. We call on the community to develop more comprehensive scaling laws that take into account and characterize how other factors impact LM performance and downstream behavior, including how model performance and behavior change with respect to the choice of the objective function and model hyper-parameters, and the quality of the pretraining data. The existence of such scaling laws \u2014 which can happen by pooling community data on various PLM pre-training runs and their corresponding perplexity and downstream performance \u2014 would allow other researchers to accurately extrapolate how their findings would generalize to other PLM model sizes, objective functions, etc. Most importantly, comprehensive scaling laws can disentangle and quantify how these different factors contribute to determine the final model performance under various experimental conditions.  \n\nHow conducting rigorous experiments and ablation studies can lead to new state-of-the-art results. Lastly, we argue that conducting rigorous experiments and ablation studies for paying off the scientific debt should not necessarily come at the expense of achieving a new state-of-theart. In contrast, doing so can be a key ingredient for building the next state-of-the-art PLMs. In 2020, Kaplan et al. (2020 ) proposed a seminal scaling law that showed how larger PLMs are more sample-efficient, and that one should always increase model size when increasing the pre-training compute budget, leading the community to develop ever-larger PLMs in response ( Rae et al. ,2021 ;Smith et al. ,2022 ,inter alia ). Nevertheless, subsequent rigorous experiments from Hoffmann et al. (2022 ) demonstrated that the optimal pretraining compute allocation should, in fact, also be scaled in another dimension: The amount of pretraining data that the model is trained on. This insight was then used to build smaller, more efficient, and cheaper-to-run PLMs that, at the time of its release, achieved new state-of-the-art results that outperformed much larger PLMs that were undertrained in comparison. Going forward, we conjecture that rigorous experiments and ablation studies that look at factors above and beyond model size and data quantity, such as the quality of the pre-training data, the exact hyper-parameters, the pre-training objective, etc., will not only be useful to understand how these factors improve performance and thus pay off the scientific debt, but also form a key ingredient for building the next generation of better PLMs."}], "task_step_question_answer": "\u6b64\u5916\uff0c\u5728\u5de5\u4e1a\u5236\u9020\u9886\u57df\uff0c\u5229\u7528\u8ba1\u7b97\u673a\u89c6\u89c9\u6280\u672f\u548c\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\uff0c\u53ef\u4ee5\u5b9e\u73b0\u5bf9\u751f\u4ea7\u7ebf\u4e0a\u4ea7\u54c1\u7684\u8d28\u91cf\u68c0\u6d4b\u548c\u7f3a\u9677\u8bc6\u522b\uff0c\u5927\u5927\u63d0\u9ad8\u4e86\u751f\u4ea7\u6548\u7387\u548c\u4ea7\u54c1\u8d28\u91cf\u3002\u901a\u8fc7\u5bf9\u751f\u4ea7\u6570\u636e\u7684\u5b9e\u65f6\u5206\u6790\uff0c\u8fd8\u80fd\u4f18\u5316\u751f\u4ea7\u6d41\u7a0b\uff0c\u964d\u4f4e\u751f\u4ea7\u6210\u672c\u3002\u5728\u4ea4\u901a\u9886\u57df\uff0c\u7814\u7a76\u6210\u679c\u4e3a\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u6709\u529b\u652f\u6301\u3002\u4f8b\u5982\uff0c\u901a\u8fc7\u5bf9\u4ea4\u901a\u6d41\u91cf\u6570\u636e\u7684\u5206\u6790\u548c\u9884\u6d4b\uff0c\u53ef\u4ee5\u5b9e\u73b0\u667a\u80fd\u4ea4\u901a\u4fe1\u53f7\u706f\u7684\u4f18\u5316\u63a7\u5236\uff0c\u51cf\u5c11\u62e5\u5835\uff0c\u63d0\u9ad8\u9053\u8def\u901a\u884c\u80fd\u529b\u3002\u81ea\u52a8\u9a7e\u9a76\u6280\u672f\u7684\u4e0d\u65ad\u8fdb\u6b65\uff0c\u4e5f\u6709\u671b\u63d0\u5347\u4ea4\u901a\u5b89\u5168\u548c\u51fa\u884c\u6548\u7387\u3002", "ref_task_step_id": ""}, "__type__": "task_step_node"}}, "task_step_store/ref_task_step_info": {"": {"node_ids": ["8da2e2af-46ef-4ee5-8766-c77589af9874", "0b08a81b-65f3-4cf0-9626-683e37b4bdaa", "fc9ce0a4-4b19-4d3e-a44a-994be0efc0fa", "02c99ed3-dc6a-4b2d-bbff-109a51351344", "d04736cf-c340-414e-a4a7-79dcf60c22c3", "3ce7a6c2-7bf0-43d2-ab8f-ea242c24394d", "3e9a2ad0-3314-47be-933a-c61134edc408", "3e9a2ad0-3314-47be-933a-c61134edc408", "8da2e2af-46ef-4ee5-8766-c77589af9874", "0b08a81b-65f3-4cf0-9626-683e37b4bdaa", "d04736cf-c340-414e-a4a7-79dcf60c22c3", "02c99ed3-dc6a-4b2d-bbff-109a51351344", "fc9ce0a4-4b19-4d3e-a44a-994be0efc0fa", "3ce7a6c2-7bf0-43d2-ab8f-ea242c24394d", "8da2e2af-46ef-4ee5-8766-c77589af9874", "02c99ed3-dc6a-4b2d-bbff-109a51351344", "d04736cf-c340-414e-a4a7-79dcf60c22c3", "0b08a81b-65f3-4cf0-9626-683e37b4bdaa", "3e9a2ad0-3314-47be-933a-c61134edc408", "fc9ce0a4-4b19-4d3e-a44a-994be0efc0fa", "8da2e2af-46ef-4ee5-8766-c77589af9874", "3ce7a6c2-7bf0-43d2-ab8f-ea242c24394d", "02c99ed3-dc6a-4b2d-bbff-109a51351344", "d04736cf-c340-414e-a4a7-79dcf60c22c3", "0b08a81b-65f3-4cf0-9626-683e37b4bdaa", "3e9a2ad0-3314-47be-933a-c61134edc408", "fc9ce0a4-4b19-4d3e-a44a-994be0efc0fa", "3ce7a6c2-7bf0-43d2-ab8f-ea242c24394d", "8da2e2af-46ef-4ee5-8766-c77589af9874", "8da2e2af-46ef-4ee5-8766-c77589af9874", "8da2e2af-46ef-4ee5-8766-c77589af9874", "8da2e2af-46ef-4ee5-8766-c77589af9874", "02c99ed3-dc6a-4b2d-bbff-109a51351344", "02c99ed3-dc6a-4b2d-bbff-109a51351344", "0b08a81b-65f3-4cf0-9626-683e37b4bdaa", "0b08a81b-65f3-4cf0-9626-683e37b4bdaa", "fc9ce0a4-4b19-4d3e-a44a-994be0efc0fa", "fc9ce0a4-4b19-4d3e-a44a-994be0efc0fa", "8da2e2af-46ef-4ee5-8766-c77589af9874", "8da2e2af-46ef-4ee5-8766-c77589af9874", "d04736cf-c340-414e-a4a7-79dcf60c22c3", "d04736cf-c340-414e-a4a7-79dcf60c22c3", "3e9a2ad0-3314-47be-933a-c61134edc408", "3e9a2ad0-3314-47be-933a-c61134edc408", "3ce7a6c2-7bf0-43d2-ab8f-ea242c24394d", "3ce7a6c2-7bf0-43d2-ab8f-ea242c24394d", "02c99ed3-dc6a-4b2d-bbff-109a51351344", "02c99ed3-dc6a-4b2d-bbff-109a51351344", "0b08a81b-65f3-4cf0-9626-683e37b4bdaa", "0b08a81b-65f3-4cf0-9626-683e37b4bdaa", "fc9ce0a4-4b19-4d3e-a44a-994be0efc0fa", "fc9ce0a4-4b19-4d3e-a44a-994be0efc0fa", "3ce7a6c2-7bf0-43d2-ab8f-ea242c24394d", "3ce7a6c2-7bf0-43d2-ab8f-ea242c24394d", "3e9a2ad0-3314-47be-933a-c61134edc408", "3e9a2ad0-3314-47be-933a-c61134edc408", "8da2e2af-46ef-4ee5-8766-c77589af9874", "8da2e2af-46ef-4ee5-8766-c77589af9874", "d04736cf-c340-414e-a4a7-79dcf60c22c3", "d04736cf-c340-414e-a4a7-79dcf60c22c3", "fc9ce0a4-4b19-4d3e-a44a-994be0efc0fa", "fc9ce0a4-4b19-4d3e-a44a-994be0efc0fa", "0b08a81b-65f3-4cf0-9626-683e37b4bdaa", "0b08a81b-65f3-4cf0-9626-683e37b4bdaa", "8da2e2af-46ef-4ee5-8766-c77589af9874", "8da2e2af-46ef-4ee5-8766-c77589af9874", "3e9a2ad0-3314-47be-933a-c61134edc408", "3e9a2ad0-3314-47be-933a-c61134edc408", "d04736cf-c340-414e-a4a7-79dcf60c22c3", "d04736cf-c340-414e-a4a7-79dcf60c22c3", "3ce7a6c2-7bf0-43d2-ab8f-ea242c24394d", "3ce7a6c2-7bf0-43d2-ab8f-ea242c24394d", "02c99ed3-dc6a-4b2d-bbff-109a51351344"], "metadata": {}}}, "task_step_store/metadata": {"8da2e2af-46ef-4ee5-8766-c77589af9874": {"task_step_hash": "", "ref_task_step_id": ""}, "0b08a81b-65f3-4cf0-9626-683e37b4bdaa": {"task_step_hash": "", "ref_task_step_id": ""}, "fc9ce0a4-4b19-4d3e-a44a-994be0efc0fa": {"task_step_hash": "", "ref_task_step_id": ""}, "02c99ed3-dc6a-4b2d-bbff-109a51351344": {"task_step_hash": "", "ref_task_step_id": ""}, "d04736cf-c340-414e-a4a7-79dcf60c22c3": {"task_step_hash": "", "ref_task_step_id": ""}, "3ce7a6c2-7bf0-43d2-ab8f-ea242c24394d": {"task_step_hash": "", "ref_task_step_id": ""}, "3e9a2ad0-3314-47be-933a-c61134edc408": {"task_step_hash": "", "ref_task_step_id": ""}}}