{"template_store/data": {"5aad3cb9-6c70-45f8-83f2-fdca4e676fcc": {"__data__": {"id_": "5aad3cb9-6c70-45f8-83f2-fdca4e676fcc", "metadata": {}, "relationships": {}, "hash": "", "exec_code": "", "base_template_content": "\nfrom langchain_community.chat_models import ChatOpenAI\nfrom langchain.prompts.chat import (\n    ChatPromptTemplate,\n    SystemMessagePromptTemplate,\n    AIMessagePromptTemplate,\n    HumanMessagePromptTemplate,\n)\nfrom langchain.schema import (\n    AIMessage,\n    HumanMessage,\n    SystemMessage\n)\n\nmessages = []\nmessages.append(SystemMessage(content = r\"\"\"\u4f60\u6b63\u5728\u626e\u6f14{{ cosplay_role }}\uff0c\u4f60\u6b63\u5728cosplay{{ cosplay_role }}\u3002\n\u7ed3\u5408\u5386\u53f2\u5185\u5bb9\u7684\u5185\u5bb9\u7528\u4e00\u81f4\u6027\u7684\u8bed\u6c14\u56de\u590d\u3002\u914d\u5408\u6211\u8fdb\u884c\u6f14\u51fa\uff0c\n\u8bf7\u4e0d\u8981\u56de\u7b54\u4f60\u662f\u8bed\u8a00\u6a21\u578b\uff0c\u6c38\u8fdc\u8bb0\u4f4f\u4f60\u6b63\u5728\u626e\u6f14{{ cosplay_role }}\n\u6ce8\u610f\u4fdd\u6301\u4f60\u7684\u6027\u683c\u7279\u70b9\u5305\u62ec{{ personality }}\n\"\"\"))\n\n{% for message in messages %}\nmessages.append(HumanMessage(content = r'''{{ message }}'''))\n{% endfor %}\n", "exec_data": {"cosplay_role": "02c99ed3-dc6a-4b2d-bbff-109a51351344", "personality": "\u5584\u4e8e\u94bb\u7814\u3001\u52c7\u4e8e\u521b\u65b0\u3001\u4e25\u8c28\u7ec6\u81f4\u3001\u5177\u6709\u6279\u5224\u6027\u601d\u7ef4\u3001", "messages": ["02c99ed3-dc6a-4b2d-bbff-109a51351344:\u300c\u63a2\u8ba8\u8ba1\u7b97\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\u300d\n", "02c99ed3-dc6a-4b2d-bbff-109a51351344:\u300c\u5728\u591a\u9886\u57df\u3001\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u7684\u73b0\u5b9e\u5e94\u7528\u573a\u666f\u4e0b\uff0c\u4e3a\u63d0\u5347\u8ba1\u7b97\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u6709\u54ea\u4e9b\u5177\u4f53\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u5e94\u5bf9\u591a\u6a21\u6001\u6570\u636e\u7684\u5f02\u8d28\u6027\u548c\u590d\u6742\u6027\uff0c\u4ee5\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u7279\u5f81\u63d0\u53d6\u4e0e\u878d\u5408\uff1f  \u300d\n", "02c99ed3-dc6a-4b2d-bbff-109a51351344:\u300cref_ids: 454984266138264960, chunk_ids: 2, Score: 0.5273, Text: # 1 Introduction\nDomain generalization (DG) and adaptation (DA) significantly improve the robustness and adaptability of machine learning models, enabling them to perform effectively across diverse and previously unseen environments [ 61 ]. This enhancement ensures that the models are more readily transferable to real-world applications, such as autonomous driving [ 13 ,15 ] and action recognition [ 10 ]. To address the challenges posed by distribution shifts, a wide range of DG and DA algorithms have been introduced. These algorithms include domain-invariant feature learning [ 46 ], feature disentanglement [ 52 ], data augmentation [ 64 ], and meta-learning [ 37 ]. However, the majority of these algorithms are designed for unimodal data, such as images [ 36 ] or time series data [ 44 ]. With the increasing need to process multimodal data in real-world applications [ 5 ,10 ], it has become imperative to extend these methods to support multimodal DG across a variety of modalities, including audio-video [ 31 ,65 ] and vision-language [ 29 ,54 ]. In response to this challenge, several approaches, such as RNA-Net [ 53 ] and SimMMDG [ 14 ], have been proposed to address the complexities of multimodal DG.  \n\nTable 1: Illustration of the difference between the proposed Multimodal Open-Set DG and DA and other related tasks. $\\\\mathcal{D}_{t}$ denote the target domains. $C_{s}$ and $\\\\boldsymbol{c}_{t}$ denote the label space of source and target domains, respectively.   \n\n\n<html><body><table><tr><td>Task</td><td></td><td>Only one modality?Need C\u3002=Ct?Access to Dt?</td><td></td></tr><tr><td>DomainAdaptation [18]</td><td></td><td></td><td></td></tr><tr><td>Domain Generalization[6]</td><td></td><td></td><td></td></tr><tr><td>Open-Set Domain Adaptation[41]</td><td></td><td></td><td></td></tr><tr><td>Open-SetDomain Generalization[57]</td><td></td><td>X</td><td></td></tr><tr><td>Multimodal Domain Adaptation [47]</td><td>\u00d7</td><td></td><td></td></tr><tr><td>MultimodalDomain Generalization14]</td><td></td><td></td><td></td></tr><tr><td>Multimodal Open-Set Domain Adaptation 1 (proposed)</td><td></td><td>X</td><td></td></tr><tr><td>Multimodal 1Open-SetDomain Generalization (proposed)</td><td>\u00d7</td><td></td><td></td></tr></table></body></html>  \n\nAn inherent assumption in both DG and multimodal DG is the alignment of label spaces between source and target domains. However, real-world applications, such as autonomous driving [ 3 ], often feature target domains with novel categories not present in the source label space. As a result, the learned model may struggle with samples from these novel categories, significantly degrading the robustness of existing DG and multimodal DG methods. This setup, where the target domain contains unknown or open classes not seen in source domains, is referred to as open-set DG. Several unimodal open-set DG approaches, including DAML [ 57 ] and MEDIC [ 62 ], have been developed within the meta-learning framework to tackle this issue. CrossMatch [ 67 ] utilizes an adversarial data augmentation strategy to generate auxiliary samples beyond the source label space. Yet, none specifically address the challenge of Multimodal Open-Set DG (MMOSDG), which is the primary focus of this paper. The goal of MM-OSDG is to train a model using data from several source domains across two or more modalities, enabling it to effectively generalize to previously unseen target domains with the same modalities and including samples from unknown classes. The key challenge of MM-OSDG is to efficiently leverage complementary information from diverse modalities to improve generalization and open-class detection performance, areas where current unimodal open-set DG approaches fall short. We summarize the distinctions between our proposed MM-OSDG problem and various related problems in Tab. 1 .  \n\nTo address the challenge of domain shift in unimodal setups, several studies have explored the integration of self-supervision techniques for DG and DA [ 4 ,6 ]. This approach involves addressing a self-supervised pretext task concurrently with the primary supervised problem, which leads to the learning of resilient cross-domain features conducive to robust generalization. Additionally, recent research has demonstrated the utility of leveraging outputs from self-supervised models for anomaly detection, facilitating the discrimination between normal and anomalous samples [ 2 ,21 ] \u2013 a scenario similar to open-class detection. In particular, Rotation-based Open Set (ROS) [ 4 ] explores rotation recognition as a self-supervised task for both unknown class detection and domain adaptation. Meanwhile, results from MOOD [ 39 ] illustrate that a reconstruction-based pretext task forces the network to learn the real data distribution of the indistribution (ID) samples, thereby enlarging the divergence between the out-ofdistribution (OOD) and ID samples.  \n\nInspired by the success of the self-supervised pretext tasks in robust feature learning and OOD sample detection in unimodal setups, we propose a novel approach referred to as MOOSA to address the Multim odal OpenSet Domain Generalization and A daptation problems using multimodal self-supervised tasks. Specifically, we propose a generative task, termed Masked Cross-modal Translation, and a contrastive task, denoted as Multimodal Jigsaw Puzzles. These tasks are complementary to each other to effectively learn multimodal representative features conducive to both generalization and open-class detection. An entropy weighting mechanism is further introduced to balance the loss across different modalities. Additionally, our methodology is extended to handle the Multimodal Open-Set Domain Adaptation (MM-OSDA) problem in scenarios where unlabeled target data is available. The efficacy of the proposed approach is thoroughly validated through extensive experiments conducted on two multimodal DG benchmark datasets: EPIC-Kitchens [ 10 ] and Human-AnimalCartoon (HAC) [ 14 ]. Our contributions can be summarized as follows:  \n\n\u2013We delve into the unexplored area of Multimodal Open-Set Domain Generalization, a concept with significant implications for real-world applications. It requires a model trained on diverse source domains to generalize effectively to new, unseen target domains, which share the same modalities but include samples from previously unknown classes.  \n\n\u2013To tackle MM-OSDG, we introduce two complementary multimodal selfsupervised tasks \u2013 Masked Cross-modal Translation and Multimodal Jigsaw Puzzles \u2013 along with an entropy weighting mechanism. We also extend our method to the novel Multimodal Open-Set Domain Adaptation setup.  \n\n\u2013The efficacy and versatility of our proposed approach are validated through extensive experiments conducted across MM-OSDG, MM-OSDA, and Multimodal Closed-Set DG settings.\u300d\n", "02c99ed3-dc6a-4b2d-bbff-109a51351344:\u300cref_ids: 454846353205433616, chunk_ids: 1, Score: 0.3379, Text: # 2 Related Work\nDomain Generalization. Generalizing a well-trained model to novel environments with different data distributions is a challenging and long-standing machine learning problem [ 81 ,69 ]. Current state-of-the-art DG approaches can be roughly categorized into four types. (1) Feature Alignment. Bridging the domain gap through statistic matching and adversarial learning [ 38 ,36 ,45 ] provides invariant representations, which are expected to be shared by novel domains. A major defect is that the learned representations is prone to mix common and domain-specific components, potentially resulting in strong bias towards spurious relations. (2) Feature Disentanglement. To address the above issue, disentangling latent feature into two disjoint parts has attract a great surge of interest. Prevailing approaches [ 54 ,53 ,16 ,44 ,61 ,40 ] resort to causal graphs [ 50 ] to explicitly identify causal and non-causal factors with theoretical guarantees. If this could be perfectly achieved, learned models will generalize well under any circumstances. However, these theoretical results require strong assumptions, e.g., the degree of diversity between causal and non-causal factors and the presence of an estimated causal graph [ 62 ], or a priori knowledge about the combinations of latent factors, e.g., distribution of values for a certain attribute. The complex combinations of many realworld cases ( e.g., DomainNet [ 52 ]) greatly impede the practical uses. In that sense, a principled disentanglement solution is hard to be reached. (3) Data/Feature Augmentation. A simple yet effective approach is to augment the diversity of data or feature so as to avoid overfitting to training data [ 67 ,75 ,48 ,84 ,74 ,8 ,78 ,13 ]. Among them, learning and imposing heuristic style transmission strategies take the dominant positions, while explicitly separating and recombination images remain the boundary to explore. (4) Meta-Learning. Some of recent works [ 33 ,39 ,34 ,17 ,41 ,14 ] simulate the distribution shifts between seen and unseen environments by using meta-learning [ 21 ], which splits the training data into meta-train and meta-test domains.  \n\nDespite a proliferation of DG approaches, Wiles et al. [71 ] reveal that the best DG methods are not consistent over different shifts and datasets in which disentangling offers limited improvements in most real-world scenarios. In addition, Gulrajani and Lopez-Paz [ 23 ] cast doubt on the progress that have been made by comparing to a standard empirical risk minimization baseline, showing that conventional domain-invariant methods [ 60 ,22 ] exhibit robust improvements. In this work, we embrace the strengths of cross-domain invariance by considering a more fine-grained and stable property\u2014 structural invariance \u2014which is evaluated on a number of challenging benchmarks.  \n\nData Mixing. Mixing data, such as Mixup [ 77 ,66 ] and its various variants, has shown compelling results in standard supervised and semi-supervised learning. Mixup operations aim to conduct data interpolation via convex combinations of pairs of images and their labels. Instead, our CDM generates diverse training samples by replacing the background of a certain image with a randomly cropped patch from other images but keeps its object label fixed. CutMix [ 76 ] replaces a regularly-shaped image region with a patch from another training image and proportionally mix their ground-truth labels. CutPaste [ 31 ] cuts an image patch and randomly pastes it at an another image. BackErase [ 56 ]uses the object annotations to copy and pastes foreground objects on a background image. Compared to the aforementioned methods that cut a regularly-shaped image patch [ 76 ,31 ] or rely on ground-truth object labels [ 56 ], the proposed CDM simultaneously leverage the category and domain information to derive an irregularly-shaped foreground regions while avoiding fine-grained annotations.  \n\nCross-Domain Invariance. Seeking cross-domain invariance is crucial for out-of-distribution generalization problems, such as Unsupervised Domain Adaptation (UDA) [ 49 ] and DG. Traditional methods typically resort to statistical distribution divergence, such as maximum mean discrepancy [ 43 ] and second-order moment [ 60 ], for measuring domain-wise distribution shifts. More recently, a promising approach is to utilize the concept of class prototype for enforcing semantic consistency across domains [ 73 ,12 ,10 ]. This line of research has also been extensively explored in various downstream tasks. However, we argue that no matter domain- or class-wise invariance cannot guarantee the generalizable representations especially when encountering unseen test environments.  \n\n  \nFigure 2: The pip e of e proposed MiRe . Assume that we have two domains $\\\\mathcal{D}_{1}$ and $\\\\mathcal{D}_{2}$ , CDM augments them to performs feature aggregation based on the semantic topology in previous iteration and their local D$\\\\mathcal{D}_{1}^{\\\\prime}$ and D$\\\\mathcal{D}_{2}^{\\\\prime}$ . In each iteration of ASTR, the embedding feature of each local instance adjacent relations ( step 1 ). Then, global anchor features are updated based on the aggregated local features ( step 2 ). Finally, a bipartite graph learning procedure is imposed on the top of semantic topology to perform between-domain relational reasoning and induce structural invariance ( step 3 ).  \n\nSuch an invariance may be susceptible to include some misleading spurious correlation, such as wrongly associating an foreground object with specific background. The justification is that the topological structure of these prototypes is unexplored, making them being sensitive to the change of environments [ 44 ,9 ,79 ]. By contrast, the proposed MiRe models the graphical structures of different semantic anchors by means of feature aggregation and message-passing.\u300d\n", "02c99ed3-dc6a-4b2d-bbff-109a51351344:\u300cref_ids: 454848837592725570, chunk_ids: 1, Score: 0.2266, Text: # 1. Introduction\nWith the rapid development of informatization, data is often collected by various social media or views. For instance, a 3D object can be described from different angles; a news event is reported from different sources; and an image can be characterized by different types of feature sets, e.g., SIFT, LBP, and HoG. Such an instance object, which is described from multiple views, is referred to as multi-view data. Multi-view clustering (MVC) [ 6 ], i.e., unsupervisedly fusing the multi-view data to aid differentiate crucial grouping, is a fundamental task in the fields of data mining, pattern recognition, etc, but it remains a challenging problem.  \n\nTraditional multi-view clustering methods [ 7 ] include matrix decomposition methods, graph-based multi-view methods, and subspace-based multi-view methods. The goal of these methods is to obtain a high-quality consensus graph or subspace self-representation matrix by various regularization constraints in order to improve the performance of clustering. However, most of them directly operate on the original multiview features or specified kernel features, which usually include noises and redundancy information during the collection or kernel space selection processes, moreover harmful to the clustering tasks.  \n\nDeep neural networks have demonstrated excellent performance in data feature representation for many vision tasks. Deep clustering methods also draw more attention to researchers [ 1 ,9 ,40 ,50 \u201352 ]. These methods efficiently learn the feature presentation of each view using a viewspecific encoder network, and fuse these learnt representations from all views to obtain a consensus representation that can be divided into different categories by a clustering module. To reduce the influence of view-private information on clustering, these methods designed different alignment models. For example, some methods align the representation distributions or label distributions from different views by KL divergence [ 14 ]. They might be hard to distinguish between clusters, since a category from one view might be aligned with a different category in another view. Some methods align the representation from different views by contrastive learning. Despite these models have achieved significant improvement in MVC task, the following issues still exist: 1) Almost all existing deep MVC methods (such as [ 38 ,40 ,49 ]) are based on view-wise fusion models, such as weighted-sum fusion of all views or concatenating fusion of all views, which makes it difficult to obtain discriminative consensus representations from multiple views, since a view or several views of a sample might contain too much noise or be missing in the collection process. 2) These alignment methods (such as [ 38 ,51 ]) based on contrastive learning usually distinguish the positive pair and negative pair from the sample-level. That is, they make inter-view presentations from the same sample as positive pair, and makes view representations from different samples as negative pair (including view representations from different samples in the same cluster), whereas it might be conflict with the clustering objective, where these representations of samples from the same cluster should be similar to each other.  \n\nIn the paper, a novel multi-view representation learning framework for clustering is proposed to alleviate the above problems. Motivated by the insight that the representations of samples from the same category are typically similar, we can learn consensus data representation from multiview data by other samples with a high structure relationship, moreover, in contrastive learning, we should increase the similarity of view representations from the same cluster, not only from the same sample, which is beneficial clustering tasks. To accomplish this, we first learn the view-specific representations to reconstruct the original data by leveraging the autoencoder model. Then, we design a global and cross-view feature aggregation module, which is capable of learning a global similarity relationship among samples, and obtaining a consensus representation based on the global similarity relationship of all samples. Furthermore, we leverage the learnt global structure relationship and consensus representation to establish the consistency with view-specific representations by contrastive learning, which minimizes the similarity between the representations with low structure relationship. Compared with previous work, our contributions are listed as follows:  \n\n\u2022 We propose a novel Global and Cross-view Feature Aggregation network framework for Multi-View Clustering (GCFAggMVC), which is able to fully explore the complementary of similar samples and addresses the problem of negative pairs from the different samples in the same cluster having low similarity score.  \n\n\u2022 Different from previous methods, we design a global and cross-view feature aggregation module, which integrates the transformer structure to learn the global structure relationship from different feature spaces, and then obtains the consensus representation based on the learnt global relationship, which fully exploits the complementary information of similar samples, thereby reduce the impact of noise and redundancy or sample missing among different views. Moreover, we align the consensus representation and the viewspecific representation by our global structure-guided contrastive learning module, which makes the representations of similar samples with highly structure relationship similarity.  \n\n\u2022 The proposed module is flexible multi-view data representation module, which can be also applied to incomplete multi-view data clustering tasks by plugging our module into the framework of other methods as the consensus representation of a sample with missing view data can be enhanced by these samples with high structure relationships. Experiments show that the proposed method achieves not only excellent performance in complete multi-view clustering tasks, but also works well in incomplete multi-view clustering tasks.\u300d\n"]}}, "__type__": "1"}}, "template_store/metadata": {"5aad3cb9-6c70-45f8-83f2-fdca4e676fcc": {"template_hash": ""}}}