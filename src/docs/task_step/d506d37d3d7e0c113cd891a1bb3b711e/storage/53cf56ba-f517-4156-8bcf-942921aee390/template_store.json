{"template_store/data": {"b2195ba1-deb7-4445-9859-fa10ae39ff27": {"__data__": {"id_": "b2195ba1-deb7-4445-9859-fa10ae39ff27", "metadata": {}, "relationships": {}, "hash": "", "exec_code": "", "base_template_content": "\nfrom langchain_community.chat_models import ChatOpenAI\nfrom langchain.prompts.chat import (\n    ChatPromptTemplate,\n    SystemMessagePromptTemplate,\n    AIMessagePromptTemplate,\n    HumanMessagePromptTemplate,\n)\nfrom langchain.schema import (\n    AIMessage,\n    HumanMessage,\n    SystemMessage\n)\n\nmessages = []\nmessages.append(SystemMessage(content = r\"\"\"\u4f60\u6b63\u5728\u626e\u6f14{{ cosplay_role }}\uff0c\u4f60\u6b63\u5728cosplay{{ cosplay_role }}\u3002\n\u7ed3\u5408\u5386\u53f2\u5185\u5bb9\u7684\u5185\u5bb9\u7528\u4e00\u81f4\u6027\u7684\u8bed\u6c14\u56de\u590d\u3002\u914d\u5408\u6211\u8fdb\u884c\u6f14\u51fa\uff0c\n\u8bf7\u4e0d\u8981\u56de\u7b54\u4f60\u662f\u8bed\u8a00\u6a21\u578b\uff0c\u6c38\u8fdc\u8bb0\u4f4f\u4f60\u6b63\u5728\u626e\u6f14{{ cosplay_role }}\n\u6ce8\u610f\u4fdd\u6301\u4f60\u7684\u6027\u683c\u7279\u70b9\u5305\u62ec{{ personality }}\n\"\"\"))\n\n{% for message in messages %}\nmessages.append(HumanMessage(content = r'''{{ message }}'''))\n{% endfor %}\n", "exec_data": {"cosplay_role": "53cf56ba-f517-4156-8bcf-942921aee390", "personality": "\u6df1\u5165\u7684\u7814\u7a76\u80fd\u529b\u3001\u521b\u65b0\u601d\u7ef4\u3001\u7cfb\u7edf\u5316\u601d\u8003\u3001\u524d\u77bb\u6027\u3001\u6df1\u5165\u7684\u7814\u7a76\u80fd\u529b\u3001\u521b\u65b0\u601d\u7ef4\u3001\u7cfb\u7edf\u5316\u601d\u8003\u3001\u524d\u77bb\u6027\u3001", "messages": ["53cf56ba-f517-4156-8bcf-942921aee390:\u300c\u5206\u6790\u8fd1\u51e0\u5e74\u7814\u7a76\u9886\u57df\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba\u300d\n", "53cf56ba-f517-4156-8bcf-942921aee390:\u300c### \u95ee\u9898\n\n\u5728\u5206\u6790\u8fd1\u51e0\u5e74\u7814\u7a76\u9886\u57df\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba\u65f6\uff0c\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u5982\u5377\u79ef\u795e\u7ecf\u7f51\u7edc (CNN)\u3001\u5faa\u73af\u795e\u7ecf\u7f51\u7edc (RNN) \u53ca\u5176\u53d8\u4f53\u3001Transformer\u67b6\u6784\u3001\u5f3a\u5316\u5b66\u4e60\u548c\u751f\u6210\u6a21\u578b\u7b49\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u5c55\u73b0\u4e86\u663e\u8457\u7684\u5e94\u7528\u4ef7\u503c\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u6846\u67b6\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u4ecd\u9762\u4e34\u4e00\u4e9b\u6311\u6218\u548c\u5c40\u9650\u6027\u3002\u57fa\u4e8e\u6b64\uff0c\u63d0\u51fa\u4ee5\u4e0b\u95ee\u9898\uff1a\n\n**\u95ee\u9898**\uff1a\u5728\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u7684\u5e7f\u6cdb\u5e94\u7528\u4e2d\uff0c\u5982\u4f55\u6709\u6548\u7ed3\u5408\u4e0d\u540c\u6846\u67b6\u7684\u4f18\u52bf\uff08\u5982CNN\u3001RNN\u3001Transformer\u7b49\uff09\u4ee5\u6784\u5efa\u6df7\u5408\u6a21\u578b\uff0c\u4ece\u800c\u5728\u7279\u5b9a\u4efb\u52a1\uff08\u5982\u89c6\u9891\u5206\u6790\u6216\u591a\u6a21\u6001\u5b66\u4e60\uff09\u4e2d\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\uff1f\u540c\u65f6\uff0c\u5982\u4f55\u89e3\u51b3\u6df7\u5408\u6a21\u578b\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u8fc7\u7a0b\u4e2d\u53ef\u80fd\u51fa\u73b0\u7684\u8ba1\u7b97\u590d\u6742\u6027\u548c\u8d44\u6e90\u6d88\u8017\u95ee\u9898\uff1f\u300d\n", "53cf56ba-f517-4156-8bcf-942921aee390:\u300cref_ids: 454918900731096864, chunk_ids: 1, Score: 0.4668, Text: # 2 RELATED WORK\n\n# 2.1 Convolution Neural Networks (CNNs)\nBeginning with AlexNet [46], convolutional neural networks (CNNs) have dominated a broad range of vision tasks [21], [55], [109]. In the past few years, more and more effective CNNs have been proposed and achieved great success in the image understanding, including VGG [72], Inception [75], ResNet [35], ResNeXt [96], DenseNet [38], MobileNet [37], ShuffleNet [108] and EfficientNet [76]. As for video understanding, it is natural to apply 3D convolution on a stack of frames [80]. However, 3D CNNs suffer from difficult optimization problem and large computation cost. To resolve these issues, the prior works try to inflate the pre-trained 2D convolution kernels for better optimization [11] and factorize the 3D convolution kernels in different dimensions to reduce complexity [28], [29], [65], [81], [82]. Additionally, other studies [41], [49], [50], [52], [54] focus on enhancing the temporal modeling ability for 2D CNNs via well-designed plug-and-play modules, such as temporal shift [54], [62], motion enhancement [41], [52], [58] and spatiotemporal excitation [49], [50]. Unfortunately, due to the limited reception field, typical spatial and temporal convolution struggle to capture long-range dependency even if stacked deeper.\n\n# 2.2 Vision Transformers (ViTs)\nInspired by the success of self-attention mechanism and Transformer architectures in NLP, Vision Transformer (ViT) applies pure Transformer encoder to encode a sequence of image tokens, achieving competitive superior performance to CNNs with suffi- cient data and detailed data augmentation. Following works mainly concentrates on improve ViT from different perspectives, such as improved patch embedding [51], data-efficient training [78], efficient self-attention [22], [56], [97] and multi-scale architectures [27], [89]. Besides, many efforts have been devoted to applying ViTs for various vision tasks, including object detection [8], [111], semantic segmentation [14], [43], [95], pose estimation [51], [98], [102], re-identification [36], and low-level image processing [12], [53]. Furthermore, other works propose different variants for spatiotemporal representation learning [1], [3], [5], [27], [57], [64], such as video object tracking [88], video object segmentation [24], video retrieval [25], [30] and video super-resolution [7]. Theses results verify the outstanding ability of the transformer to capture long-term information. However, the self-attention mechanism is inefficient to encode low-level features, hindering their high potential for efficient representation learning.\n\n# 2.3 Combination of CNN and ViT\nThe prior works have demonstrate that self-attention can perform convolution [19], [68], but they suggest replacing convolution instead of combining them. Current combination of CNN and ViT mainly focus on image understanding, which try to enhance ViTs with convolution in different ways, e.g., adding convolutional patch stem for fast convergence [94], [99], introducing convolutional position embedding [16], [22], inserting depthwise convolution into feed-forward network to encode local features [99], [102], utilizing squeezed convolutional projection to reduce computation [91] and combining MBConv [69] with Transformer [20]. However, the above works lack analysis for the representation learning ability and property of convolution and self-attention. As for video understanding, the combination is almost straightforward, e.g., inserting self-attention as global attention [90] or using convolution as patch stem [59]. Though Video Swin [57] advocates an inductive bias of locality with shift window, it is inefficient to encode lowlevel features via self-attention. In this work, we analyze the learning features of pure self-attention and the relation between convolution and self-attention, thus we propose a unified relation aggregator to form our effective backbone for visual recognition.\n\n# 3 METHOD\nIn this section, we introduce the proposed UniFormer in detail. First, we describe the overview of our UniFormer block. Then, we explain its key modules such as multi-head relation aggregator and dynamic position embedding. Moreover, we discuss the distinct relations between our UniFormer and existing convolution/transformer blocks, showing its preferable design for accuracy-computation balance. Finally, we stack UniFormer blocks hierarchically to build up our backbone for visual recognition, and propose an effective adaption for various downstream vision tasks.\n\n# 3.1 Overview\nFigure 3 shows our concise Unified transFormer (UniFormer). For simple description, we take a video with $T$ frames as an example and an image input can be seen as a video with a single frame. Hence, the dimensions highlighted in red only exit for the video input, while all of them are equal to one for image input. One can see that, our UniFormer is a basic transformer format, while we elaborately design it to tackle computational redundancy and capture complex dependency.  \n\nSpecifically, our UniFormer block consist of three key modules: Dynamic Position Embedding (DPE), Multi-Head Relation Aggregator (MHRA) and Feed-Forward Network (FFN):  \n\n$$\n\\\\begin{array}{r l}&{{\\\\mathbf X}=\\\\mathrm{DPE}\\\\left({\\\\mathbf X}_{i n}\\\\right)+{\\\\mathbf X}_{i n},}\\\\\\\\ &{{\\\\mathbf Y}=\\\\mathrm{MHRA}\\\\left(\\\\mathrm{Norm}\\\\left({\\\\mathbf X}\\\\right)\\\\right)+{\\\\mathbf X},}\\\\\\\\ &{{\\\\mathbf Z}=\\\\mathrm{FFN}\\\\left(\\\\mathrm{Norm}\\\\left({\\\\mathbf Y}\\\\right)\\\\right)+{\\\\mathbf Y}.}\\\\end{array}\n$$  \n\nConsidering the input token tensor ${\\\\bf X}_{i n}\\\\in\\\\mathbb{R}^{C\\\\times T\\\\times H\\\\times W}$ $T=1$ for an image input), we first introduce DPE to dynamically integrate position information into all the tokens (Eq. 1). It is friendly to arbitrary input resolution and makes good use of token order for better visual recognition. Then, we use MHRA to enhance each token by exploiting its contextual tokens with relation learning (Eq. 2). Via flexibly designing the token affinity in the shallow and deep layers, our MHRA can smartly unify convolution and self-attention to reduce local redundancy and learn global dependency. Finally, we add FFN like traditional ViTs [23], which consists of two linear layers and one non-linear function, i.e., GELU (Eq. 3). The channel number is first expanded by the ratio of 4 and then recovered, thus each token will be enhanced individually.\u300d\n", "53cf56ba-f517-4156-8bcf-942921aee390:\u300cref_ids: 454849250644648612, chunk_ids: 1, Score: 0.3379, Text: # Model Reprogramming: Resource-Efficient Cross-Domain Machine Learning\nPin-Yu Chen 1  \n\n1 IBM Research\n\n# Abstract\nIn data-rich domains such as vision, language, and speech, deep learning prevails to deliver high-performance taskspecific models and can even learn general task-agnostic representations for efficient finetuning to downstream tasks. However, deep learning in resource-limited domains still faces multiple challenges including (i) limited data, (ii) constrained model development cost, and (iii) lack of adequate pre-trained models for effective finetuning. This paper provides an overview of model reprogramming to bridge this gap. Model reprogramming enables resource-efficient crossdomain machine learning by repurposing and reusing a welldeveloped pre-trained model from a source domain to solve tasks in a target domain without model finetuning, where the source and target domains can be vastly different. In many applications, model reprogramming outperforms transfer learning and training from scratch. This paper elucidates the methodology of model reprogramming, summarizes existing use cases, provides a theoretical explanation of the success of model reprogramming, and concludes with a discussion on open-ended research questions and opportunities. A list of model reprogramming studies is actively maintained and updated at https://github.com/IBM/model-reprogramming .\u300d\n", "53cf56ba-f517-4156-8bcf-942921aee390:\u300cref_ids: 454848644537032228, chunk_ids: 1, Score: 0.3008, Text: # 2. Related Work\n\n# 2.1. Vision Transformers\nThe Transformer [ 54 ] was initially developed for natural language processing tasks and has since been adapted for computer vision tasks through the introduction of the Vision Transformer (ViT) [ 11 ]. Further improvements to ViT have been achieved through knowledge distillation or more intricate data augmentation, as demonstrated by DeiT [ 52 ]. However, Transformers do not consider the quadratic complexity of high-resolution images or the 2D structure of images, which are challenges in vision tasks. To address these issues and improve the performance of vision Transformers, various methods have been proposed, including multi-scale architectures [ 3 ,32 ,56 ,63 ], lightweight convolution layers [ 14 ,28 ,60 ], and local self-attention mechanisms [ 32 ,6 ,65 ,71 ].\n\n# 2.2. Convolutional Neural Networks\nConvolutional neural networks (CNNs) have been the main force behind the revival of deep neural networks in computer vision. Since the introduction of AlexNet [ 25 ], VGGNet [ 44 ], and ResNet [ 17 ], CNNs have rapidly become the standard framework for computer vision tasks. The design principles of CNNs have been advanced by subsequent models such as Inception [ 47 ,48 ], ResNeXt [ 62 ], Res2Net [ 13 ] and MixNet [ 51 ], which promote the use of building blocks with multiple parallel convolutional paths. Other works such as MobileNet [ 20 ] and ShuffleNet [ 73 ]have focused on the efficiency of CNNs. To further improve the performance of CNNs, attention-based models such as SE-Net [ 21 ], Non-local Networks [ 58 ], and CBAM [ 59 ]have been proposed to enhance the modeling of channel or spatial attention. EfficientNets [ 49 ,50 ] and MobileNetV3 [ 19 ] have employed neural architecture search (NAS) [ 77 ] to develop efficient network architectures. ConvNeXt [ 33 ] adopts the hierarchical design of Vision Transformers to enhance CNN performance while retaining the simplicity and effectiveness of CNNs. Recently, several studies [ 15 ,18 ,64 ] have utilized convolutional modulation as a replacement for self-attention, resulting in improved performance. Specifically, FocalNet [ 64 ] utilizes a stack of depth-wise convolutional layers to encode features across short to long ranges and then injects the modulator into the tokens using an element-wise affine transformation. Conv2Former [ 18 ] achieves good recognition performance using a simple $11\\\\times11$ depth-wise convolution. In contrast, our scale-aware modulation also employs depth-wise convolution as a basic operation but introduces multi-head mixed convolution and scale-aware aggregation.  \n\n  \nFigure 2: (a) The architecture of the Scale-Aware Modulation Transformer (SMT); (b) Mix Block: a series of SAM blocks and MSA blocks that are stacked successively (as presented in Sec. 3.3 ). SAM and MSA denote the scale-aware modulation module and multi-head self-attention module, respectively.\n\n# 2.3. Hybrid CNN-Transformer Networks\nA popular topic in visual recognition is the development of hybrid CNN-Transformer architectures. Recently, several studies [ 14 ,45 ,60 ,76 ] have demonstrated the effectiveness of combining Transformers and convolutions to leverage the strengths of both architectures. CvT [ 60 ] first introduced depth-wise and point-wise convolutions before self-attention. CMT [ 14 ] proposed a hybrid network that utilizes Transformers to capture long-range dependencies and CNNs to model local features. MobileViT [ 37 ], EdgeNeXt [ 36 ], MobileFormer [ 5 ], and EfficientFormer [ 27 ]reintroduced convolutions to Transformers for efficient network design and demonstrated exceptional performance in image classification and downstream applications. However, the current hybrid networks lack the ability to model range dependency transitions, making it challenging to improve their performance. In this paper, we propose an evolutionary hybrid network that addresses this limitation and showcases its importance.\n\n# 3. Method\n\n# 3.1. Overall Architecture\nThe overall architecture of our proposed Scale-Aware Modulation Transformer (SMT) is illustrated in Fig. 2 . The network comprises four stages, each with downsampling rates of free network, we first adopt our proposed Scale-Aware $\\\\{4,8,16,32\\\\}$ . Instead of constructing an attentionModulation (SAM) in the top two stages, followed by a penultimate stage where we sequentially stack one SAM block and one Multi-Head Self-Attention (MSA) block to model the transition from capturing local to global dependencies. For the last stage, we solely use MSA blocks to capture long-range dependencies effectively. For the FeedForward Network (FFN) in each block, we adopt the detailspecific feedforward layers as used in Shunted [ 42 ].\u300d\n"]}}, "__type__": "1"}}, "template_store/metadata": {"b2195ba1-deb7-4445-9859-fa10ae39ff27": {"template_hash": ""}}}