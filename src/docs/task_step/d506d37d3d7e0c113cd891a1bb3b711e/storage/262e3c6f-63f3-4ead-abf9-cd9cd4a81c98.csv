角色,内容,分镜
262e3c6f-63f3-4ead-abf9-cd9cd4a81c98,评估论文中提出的未来研究方向与挑战,5
262e3c6f-63f3-4ead-abf9-cd9cd4a81c98,"### 问题

在评估论文中提出的未来研究方向与挑战时，如何平衡模型的可解释性与隐私保护的需求，特别是在资源受限的环境下？具体来说，有哪些创新的方法可以在保证模型透明度的同时，有效应用差分隐私或联邦学习等技术，以应对低质量数据和计算资源有限的挑战？这些方法在实际应用中如何提升用户对AI系统的信任，并促进AI在复杂环境中的广泛应用？",5
262e3c6f-63f3-4ead-abf9-cd9cd4a81c98,"ref_ids: 454984255704670602, chunk_ids: 1, Score: 0.5508, Text: # 7Conclusion
In the past decade, an explosion in data collection has led to huge strides forward in machine learning, but the use of sensitive personal data in machine learning also represents a serious privacy concern. We present an approach based on a new protocol called FLDP that ensures differential privacy for the trained model, without the need for a trusted data aggregator. Using FLDP allows a highly accurate model to be trained in a federated (distributed) manner while guaranteeing the privacy of data owners, even against powerful and colluding adversaries. Our empirical results show that these accurate models are trainable within a feasible time frame for practical applications, especially when accuracy and low trust burdens are critical.  

The promising results presented in our evaluation also suggest directions for future research. For example, gradient compression techniques can substantially reduce incommunication overhead for distributed training [ 28 ]. Paired with FLDP , these techniques could further reduce the time per batch for larger models, and potentially improve our scalability with respect to model complexity. Moreover, we apply FLDP to the very specific case of privacy preserving federated learning, but additional research could consider how these techniques scale with simpler, yet important, data problems. For example, the core noise addition and secure aggregation methods described in this paper could be adapted to privacypreserving database queries, while eliminating the need for a central database.



# [1] Amazon EC2 z1d instances, 2021.
[2] Martín Abadi, Andy Chu, Ian J. Goodfellow, H. Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. Deep learning with differential privacy. In Edgar R. Weippl, Stefan Katzenbeisser, Christopher Kruegel, Andrew C. Myers, and Shai Halevi, editors, Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security, Vienna, Austria, October 24-28, 2016 , pages 308–318. ACM, 2016.  









[11] Keith Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H. Brendan McMahan, Sarvar Patel, Daniel Ramage, Aaron Segal, and Karn Seth. Practical secure aggregation for privacy-preserving machine learning. In Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security , CCS ’17, page 1175–1191, New York, NY, USA, 2017. Association for Computing Machinery.  






[17] Morten Dahl. Secret sharing, part 2 efficient sharing with the fast fourier transform, Jun 2017.  





[24] Bargav Jayaraman, Lingxiao Wang, David Evans, and Quanquan Gu. Distributed learning without distress: Privacy-preserving empirical risk minimization. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicolò Cesa-Bianchi, and Roman Garnett, editors, Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada , pages 6346–6357, 2018.   
[36] Adi Shamir. How to share a secret. Communications of the ACM , 22(11):612–613, 1979.   
[42] Gathen Joachim von zur and Gerhard Jürgen. Modern Computer algebra . Cambridge University Press, 2013.

# A Proof of security
Suppose the ideal functionality of noisy vector addition as $F$ , an adversary $A$ . Let $\\nu_{i}$ and $x_{i}$ be input and view of client $i$ respectively. Let $x_{s}$ be the view of the server. $n$ is the LWE security parameter. Suppose a maliciously secure aggregation protocol $\\operatorname{Sagg}(X,t)$ . Let $V$ be the output of $\\pi$ .  

Let $U$ be the set of clients, and $C\\subset U\\cup\\{S\\}$ be the set of corrupt parties.  

In the malicious model, we consider dropping out an adversarial behavior without loss of generality.  

Suppose the simulator has access to an oracle $\\mathtt{I D E A L}(t,\\nu_{u})_{u\\in U\\backslash C}$ where:  

$$
\\texttt{I D E A L}(t,\\nu_{u})_{u\\in U\\setminus C}=\\left\\{\\begin{array}{l l}{\\sum_{u\\in U\\setminus C}\\nu_{u}}&{|U\\setminus C|>t}\\\\ {\\perp}&{o t h e r w i s e}\\end{array}\\right.
$$  

Theorem 2 There exists a PPT simulator SIM such that for all t, U, $C$  

$$
R E\\lambda L_{\\pi,C}^{U}(n,t;\\nu_{U\\backslash C})\\equiv S T M_{C}^{U,\\,T D E A L(t,\\nu_{u})}(n,t;x_{C})
$$  

Proven through the hybrid argument.  

1. This hybrid is a random variable distributed exactly like $\\mathtt{R E A L}_{\\mathrm{FLDP},C}^{U}(n,t;\\nu_{U\\setminus C})$  

2. In this hybrid SIM has access to $\\{x_{i}|i\\in U\\}$ .SIM runs the full protocol and outputs a view of the adversary from the previous hybrid.  

3. In this hybrid, SIM has corrupt parties receive an ABORT if the server sends a $U_{1}$ such that $t>\\left|U_{1}\\right|$ .  

4. In this hybrid, SIM replaces $V$ with the output of $F$ from any $x_{C}$ .  

5. In this hybrid, SIM generates the ideal inputs of the corrupt parties using the IDEAL oracle, SIM generates a set of random inputs $V_{C}$ such that $\\Sigma_{i\\in C}\\nu_{i}=F\\big(\\nu_{U}\\big)-$ $\\mathtt{I D E A L}(t,\\nu_{u})_{u\\in U\\backslash C}.$ . The output domain of FLDP is any vector $V\\in\\mathbb{F}_{q}^{m}$ and ABORT .SIM can replicate any vector output using this process. Therefore, this hybrid is indistinguishable from the previous hybrid.  

6. In this hybrid, SIM replaces $s$ , the sum of secret vectors with a vector of random field elements distributed by $\\chi*$ $k$ . Because $s$ is not used to reconstruct $G$ , and is normally distributed by $\\chi*k$ , this hybrid is indistinguishable from the previous hybrid.  

7. In this hybrid, SIM replaces $H$ with $V+A s$ .  

8. In this hybrid, SIM replaces the run of protocol Sagg with the ideal simulation of Sagg . If Sagg returns ABORT ,SIM returns ABORT . Because Sagg is secure, this hybrid is indistinguishable from the previous hybrid using each parties $s_{i}$ as input.  

9. In this hybrid, SIM replaces the $s_{i}$ of each client with a vector of elements distributed by $\\chi$ . Because $s_{i}$ is typically distributed by $\\chi$ and each $s_{i}$ is not used to compute $s$ anymore, this hybrid is indistinguishable from the previous hybrid.  

10. In this hybrid, SIM replaces the $b_{i}$ of each client with a vector of uniformly distributed field elements in $\\mathbb{F}_{q}^{m}$ .Given the LWE assumption, $b_{i}$ should be indistinguishable from random field elements, so this hybrid is indistinguishable from the previous hybrid from the perspective of the adversary.  

11. In this hybrid, SIM replaces $h_{i}$ of each client with a vector of uniformly distributed field elements in $\\mathbb{F}_{q}$ . By the definition of one time pad, this hybrid should be indistinguishable from the previous hybrid. Additionally this hybrid does not use any input from the honest parties and thus concludes the proof.  

After these steps, the simulator no longer needs any input from the honest clients to simulate Protocol 3, implying that it is secure in the malicious threat model.  

Notably, our malicious threat model subsumes the semihonest threat model. Therefore this proof proves security in that threat model as well. In the case of a semi-honest threat model, the security of Sagg can also eased to semi-honest.",5
262e3c6f-63f3-4ead-abf9-cd9cd4a81c98,"ref_ids: 454845858389724938, chunk_ids: 5, Score: 0.3047, Text: # LBROADER IMPACTS
Our study is among the efforts to extend the capability of AI systems from the closed world to the open world. Particularly, it will play a positive role in fostering next-generation AI systems with the capability of categorizing and organizing open-world data automatically. However, our method still has several limitations. First, though we have achieved encouraging results on the public datasets, the interpretability still needs improvement, as the underlying principles of how the decisions are made by the systems remain not crystal clear. Second, the cross-domain robustness is not satisfactory, as can be seen from the results on the setting of GCD with domain shifts, though our method has achieved the best overall results and new class discovery results, the performance still has significant room to improve. Additionally, in the vanilla GCD setting, methods typically rely on a pre-trained model ( e.g ., DINO) as a feature extractor, which may inherit its drawbacks ( e.g ., discrimination and privacy issues).",5
262e3c6f-63f3-4ead-abf9-cd9cd4a81c98,"ref_ids: 454845753539980440, chunk_ids: 0, Score: 0.2969, Text: # 1 Introduction
In recent years, statistical machine learning models have been deployed in many domains such as health care, education, criminal justice, or social studies [Chen et al., 2021, He et al., 2019, Jiang et al., 2017]. However, the release of statistical estimates based on these sensitive data comes with the risk of leaking personal information of individuals in the original dataset. One naive solution for this problem is to remove all the identifying information such as names, races, or social security numbers. Unfortunately, this is usually not enough to preserve privacy. It has been shown in various works that an adversary can take advantages of structural properties of the rest of the dataset to reconstruct information about certain individuals [Backstrom et al., 2007, Dinur and Nissim, 2003]. Thus, we would need a stronger privacy-preserving mechanism. Over the past couple of decades, differential privacy [Dwork et al., 2006] has emerged as the dominant privacy notion for machine learning problems.  

finition rential Privacy [Dw th, 2014]) .A randomiz $M$ :$\\mathcal{X}^{N}\\mapsto$ $\\mathbb{R}^{d}$ satisfies $(\\epsilon,\\delta)-$ −differential pri $((\\epsilon,\\delta){-}D P)$ )-DP) if for any two data sets $D,\\check{D^{\\prime}}\\in\\mathcal{X}^{N}$ ∈X differing by at most one element and any event $E\\subseteq\\mathbb{R}^{d}$ ⊆, it holds that:  

$$
P\\left[M(D)\\in E\\right]\\leq\\exp(\\epsilon)P\\left[M(D^{\\prime})\\in E\\right]+\\delta
$$  

Roughly speaking, differential privacy guarantees that the outputs of two neighboring datasets (datasets that differ in at most one datapoint) are almost the same with high probability, thus preventing the adversary from identifying any individual’s data.  

In this paper, we are interested in designing $(\\epsilon,\\delta)$ −private a rithms for non al risk minimization (ERM) probl s. In ERM proble en Ni.i.d amples $x_{1},...,x_{N}\\;\\in\\;\\mathcal{X}$ ∈X from some unknown distribution $P$ , the goal is to find $w\\in\\bar{\\mathbb{R}}^{d}$ ∈such that wminimizes the empirical loss defined as follows:  

$$
F(w)\\triangleq\\frac{1}{N}\\sum_{i=1}^{N}f(w,x_{i})
$$  

36th Conference on Neural Information Processing Systems (NeurIPS 2022).  

where $f:\\mathbb{R}^{d}\\times\\mathcal{X}\\mapsto\\mathbb{R}$ is the loss function associated with the learning problem. This is a setting that commonly arises in modern machine learning problem. For example, in image classification problems, the data point $x$ would be a tuple of (image, label), $w$ denotes the parameters of our model, and $f(w,x)$ represents composing the model predictions with some loss function such as cross-entropy. We are interested in finding a critical point, or a point such that the norm of the empirical gradient $\\|\\nabla F(w)\\|$ is as small as po ible. Further, we want all the outputs $w_{1},w_{2},...,w_{T}$ to be differentially private with respect to the Ntraining samples.  

Private ERM has been well studied in the convex settings. The approaches in this line of work can be classified into three main categories: output perturbation [Dwork et al., 2006, Chaudhuri et al., 2011, Zhang et al., 2017, Wu et al., 2017], objective perturbation [Chaudhuri et al., 2011, Kifer et al., 2012, Iyengar et al., 2019, Talwar et al., 2014], and gradient perturbation [Bassily et al., 2014, Wang et al., 2017, Jayaraman et al., 2018, Wang et al., 2018]. All of these approaches have been shown to achieve the asymptotically optimal bound $\\begin{array}{r}{\\tilde{O}\\left(\\frac{\\sqrt{d}}{\\epsilon N}\\right)}\\end{array}$ for smooth convex loss (with output perturbation requiring strong convexity to get the optimal bound) in (near) linear time. On the other hand, the literature on private non-convex ERM is nowhere as comprehensive. The first theoretical bound in private non-convex ERM is from [Zhang et al., 2017]. They propose an algorithm called Random Round Private Stochastic Gradient Descent (RRSGD) which is inspired by the results from [Bassily et al., 2014, Ghadimi and Lan, 2013]. RRSGD is able to guarantee the utility bound of $\\begin{array}{r}{O\\left(\\frac{(d\\log(n/\\delta)\\log(1/\\delta))^{1/4}}{\\sqrt{\\epsilon N}}\\right)}\\end{array}$ . However, RRSGD takes $O\\left(N^{2}d\\right)$  gradient computations to achieve this, which can be troublesome for high-dimensional problem. [Wang et al., 2018] then improves upon this utility bound by a factor of $O$ $\\left((\\log(n/\\delta))^{1/\\bar{4}}\\right)$ . They achieve this rate by using full-batch gradient descent which is not a common practice in non-private machine learning in which very large batch sizes actually require careful work to make training efficient. Recently, [Wang et al., 2019b] tackles both runtime and utility issues by introducing a private version of the Stochastic Recursive Momentum (DP-SRM) [Cutkosky and Orabona, 2019]. By appealing to variance reduction as well as privacy amplification by subsampling [Balle et al., 2018, Abadi et al., 2016], DP-SRM achieve the bound $O\\left(\\frac{(d\\log(1/\\delta))^{1/4}}{\\sqrt{\\epsilon N}}\\right)$ in $\\begin{array}{r}{\\bar{O}\\left(\\frac{(\\epsilon\\bar{N})^{3/2}}{d^{3/4}}+\\frac{\\epsilon N}{\\sqrt{d}}\\right)}\\end{array}$ gradient complexity. However, DRSRM still requires the batch size to be $\\begin{array}{r}{O\\left(\\frac{\\sqrt{\\epsilon N}}{d^{1/4}}\\right)}\\end{array}$ for the analysis to work. Finally, although our focus in this paper is the ERM problem, there are also various works on private stochastic nonconvex/convex optimization [Bassily et al., 2019, 2021a,b, Feldman et al., 2018, 2020, Zhou et al., 2020, Asi et al., 2021, Wang et al., 2019a, Kulkarni et al., 2021].  

Contributions. We first provide the analysis for the private version of Normalized SGD (DP-NSGD) [Cutkosky and Mehta, 2020] for unconstrained non-convex ERM. By using the tree-aggregation technique [Chan et al., 2011, Dwork et al., 2010] to compute the momentum privately, we can ensure the privacy guarantee while adding noise of only $\\begin{array}{r}{\\tilde{O}\\left(\\frac{\\sqrt{T}}{\\epsilon\\sqrt{N}}\\right)}\\end{array}$ (where $T$ is the total number of iterations). This allows us to achieve the same asymptotic bound $\\begin{array}{r}{\\tilde{O}\\left(\\frac{d^{1/4}}{\\sqrt{\\epsilon N}}\\right)}\\end{array}$ on the expectation of the gradient as [Zhang et al., 2017, Wang et al., 2018, 2019b] without appealing to privacy amplification techniques which is usually required for private SGD to have a good utility guarantee [Abadi et al., 2016]. DP-NSGD also does not require a large batch size as in [Wang et al., 2018, 2019b]; it has utility guarantee for any batch size. This tree-aggregation technique is morally similar to the approach in [Kairouz et al., 2021, Guha Thakurta and Smith, 2013] for online learning. However, unlike in [Kairouz et al., 2021, Guha Thakurta and Smith, 2013], we do not restrict our loss function to be convex and we will also extend tree-aggregation technique to SGD with momentum. Further, we provide a new variant of Normalized SGD that takes advantages of the fact that the gradients of the nearby iterates are close to each other due to smoothness. This new algorithm is able to guarantee an error of $\\begin{array}{r}{\\tilde{O}\\left(\\frac{d^{1/3}}{(\\epsilon N)^{2/3}}\\right)}\\end{array}$ in $\\begin{array}{r}{\\mathcal{O}\\left(\\frac{N^{7/3}\\epsilon^{4/3}}{d^{2/3}}\\right)}\\end{array}$ gradient computations which, to our knowledge, is the best known rate for private non-convex ERM.  

Organization. The rest of the paper is organized as follows. In section 2, we define our problem of interest and the assumptions that we make on the problem settings. We also provide some background on Differential Privacy as well as some high-level intuition on tree-aggregation technique. We then formally describe our first private variant of Normalized SGD in section 3 and discuss its privacy guarantee and theoretical utility bound. In section 4, we introduce a novel sensitivity-reduced analysis for Normalized SGD that allows us to improve upon the utility bound in section 3. Finally, we conclude with a discussion in section 5.",5
