角色,内容,分镜
07d08b9c-62e7-47f6-aae2-a814b7a455cc,分析最新算法的稳定性与容错性,4
07d08b9c-62e7-47f6-aae2-a814b7a455cc,"### 问题

在分析最新算法的稳定性与容错性时，如何通过引入对抗样本进行训练（如PGD对抗训练）来提升模型的对抗鲁棒性？同时，在分布式训练（如联邦学习）中，如何设计有效的容错机制（如冗余机制）来确保模型在数据隐私保护下的稳定性？这些方法在实际应用中如何平衡模型的可靠性与大规模适应性？",4
07d08b9c-62e7-47f6-aae2-a814b7a455cc,"ref_ids: 454847291425149688, chunk_ids: 6, Score: 0.4355, Text: # Towards the Robustness of Differentially Private Federated Learning
Tao $\\mathbf{Q}^{\\mathbf{i}^{1*}}$ , Huili Wang 1 , Yongfeng Huang  

1 Department of Electronic Engineering, Tsinghua University, Beijing 100084, China 2 Zhongguancun Laboratory, Beijing 100094, China , whl21 $@$ mails.tsinghua.edu.cn,

# Abstract
Robustness and privacy protection are two important factors of trustworthy federated learning (FL). Existing FL works usually secure data privacy by perturbing local model gradients via the differential privacy (DP) technique, or defend against poisoning attacks by filtering the local gradients in the outlier of the gradient distribution before aggregation. However, these two issues are often addressed independently in existing works, and how to secure federated learning in both privacy and robustness still needs further exploration. In this paper, we unveil that although DP noisy perturbation can improve the learning robustness, DP-FL frameworks are not inherently robust and are vulnerable to a carefully-designed attack method. Furthermore, we reveal that it is challenging for existing robust FL methods to defend against attacks on DPFL. This can be attributed to the fact that the local gradients of DP-FL are perturbed by random noise, and the selected central gradients inevitably incorporate a higher proportion of poisoned gradients compared to conventional FL. To address this problem, we further propose a new defense method for DP-FL (named Robust-DPFL ), which can effectively distinguish poisoned and clean local gradients in DP-FL and robustly update the global model. Experiments on three benchmark datasets demonstrate that baseline methods cannot ensure task accuracy, data privacy, and robustness simultaneously, while Robust-DPFL can effectively enhance the privacy protection and robustness of federated learning meanwhile maintain the task performance.",4
07d08b9c-62e7-47f6-aae2-a814b7a455cc,"ref_ids: 454847099910337174, chunk_ids: 0, Score: 0.4062, Text: # 2 Related Work
Adversarial defense methods can be categorized into two distinct types: static defense and adaptive test-time defense.

# 2.1 Static Defense
Within the realm of static defense, both the inputs and the parameters of the model remain constant during the inference process. One of the most effective defenses in this category is adversarial training [Madry et al. , 2017], which involves training models with adversarial examples [Zhang et al. , 2019; Cheng et al. , 2020; Gowal et al. , 2020]. However, many adversarial training approaches can only defend against specific attacks they were trained with [Madry et al. ,2017] and often experience a significant accuracy drop on clean data [Laidlaw et al. , 2020].  

Conversely, some methods [Li et al. , 2021; Mustafa et al. ,bustness solely by relying on clean examples to avoid over2019; Pang ""et al. %(%) , 2019] focus on improving adversarial ro#fitting to adversarial perturbations. Many of these methods aim to reduce inter-class distances, thereby increasing interclass margins. For instance, Mustafa et al. [Mustafa et al. ,2019] introduced an approach to enhance robustness by compelling the features for each class to reside within a convex polytope that is maximally separated from the polytopes of other classes. However, the increased margin is based on the feature space of clean examples, thus achieving poor robustness when countering adversarial perturbations compared to adversarial training. Therefore, some margin-based methods [Mustafa et al. , 2019; Pang et al. , 2019] need to be combined with adversarial training to further improve robustness.  

Different from the aforementioned methods, the proposed FPCC comprises strategies to address the situation of increasing interferential features and decreasing critical features when adversarial perturbations are added to clean examples, along with techniques that constrain the features to uphold the correct feature pattern.

# 2.2 Adaptive Test-time Defense
Adaptive test-time defense constitutes another pivotal category within adversarial defense methods, wherein the inputs or parameters of the model undergo dynamic alterations during the test phase. Adversarial purification [Wang et al. , 2022; Nie et al. , 2022; Hill et al. , 2020] is one of the most popular and effective methods in this category. It utilizes a generative model (e.g., GANs [Goodfellow et al. ,2014a], Diffusion models [Ho et al. , 2020; Song et al. , 2020; Ma et al. , 2023a; Li et al. , 2023]) to remove perturbations from adversarial examples. While these methods are plugand-play and successfully defend against most attacks, they do incur significant additional computational costs during inference and do not inherently enhance the model’s robustness [Croce et al. , 2022].  

Furthermore, some works [Wang et al. , 2021; Chen et al. ,2021; Kang et al. , 2021; Dong et al. , 2022; Fu et al. , 2021] propose modifying parameters or activations during inference, essentially aiming to reduce the impact of perturbations during network prediction. However, these methods also incur computational overhead, and the improved robustness achieved through these approaches is limited, owing to the complexity of adversarial perturbation and the lack of diversity in training data.  

In contrast to adaptive test-time defense methods, the proposed FPCC does not require integrating additional modules or dynamically adjusting defense strategies during inference. Therefore, it incurs no additional computational overhead.

# 3 Method
In this section, we conduct an analysis of feature behavior in the final layer of classification networks for correctly predicted samples, leading to the introduction of the concept of feature pattern . We extend this concept to other layers of the network, suggesting that a network only achieves correct predictions when its latent features align with these correct feature patterns. Furthermore, we propose the FPCC, which includes Spatial-wise Feature Modification and Channel-wise Feature Selection to address the situation of increasing interferential features and decreasing critical features when adversarial perturbations are added to clean examples. Additionally, FPCC incorporates Pattern-based Robustness Optimization to constrain the modified and selected features, ensuring the maintenance of the correct feature pattern.

# 3.1 The Feature Pattern
Given a classification network with $K(K\\,\\geq\\,2)$ categories, the softmax function is typically used to calculate the probability of an input sample $x_{i}$ belonging to the $y_{i}$ -th category, where $y_{i}\\in\\{1,2,\\ldots,K\\}$ s the true label of $x_{i}$ . For the feature vector $z_{i}\\,\\in\\,\\mathbb{R}^{K}$ ∈of xat the final layer of the network, the softmax function is defined as follows:  

$$
S(z_{i}[y_{i}])=\\frac{\\exp(z_{i}[y_{i}])}{\\sum_{k=1}^{K}\\exp(z_{i}[k])},
$$  

where $z_{i}[k]$ is the value in the $k$ -th dimension of $z_{i}$ . Given that $y_{i}$ is the true label of $x_{i}$ , the network is usually trained using cross-entropy loss to maximize the probability $S(z_{i}[y_{i}])$ :  

$$
\\mathcal{L}_{C E,i}=-l o g(S(z_{i}[y_{i}])).
$$  

Generally, the network’s ability to correctly predict the label of $x_{i}$ is not solely based on the specific values of $z_{i}$ , but rather on whether the value in the $y_{i}$ -th dimension $z_{i}[y_{i}]$ is relatively larger than the values in other dimensions. We define this relative size among dimensions in the feature vector as the feature pattern . In the network’s final layer, each category has a distinct feature pattern, similar to the true label’s one-hot vector. Correct predictions are made when the sample’s feature pattern is correct, meaning it closely resembles the true category’s feature pattern. We propose that this concept of feature pattern can be generalized to other layers of the network.  

  
Figure 2: Feature patterns of a correctly predicted sample ,an incorrectly predicted sample , and their corresponding ground-truth category . The feature pattern of the ground-truth category is derived by averaging the feature patterns of the top 10 correctly predicted samples, identified based on the highest predicted probabilities. The horizontal axis denotes the feature dimensions, while the vertical axis represents the relative magnitude of the features. Both correct and incorrect samples are randomly selected from the ’dog’ category of the CIFAR-10 dataset. To streamline the illustration, only the first 10 dimensions of the penultimate layer (fully connected layer) of the VGG-16 network are displayed.  

Next, we formalize the definition of the feature pattern, taking the $l$ -th layer as an example and assuming it to be a fully connected layer (the case of convolutional layers will be discussed later). The feature pattern $p_{i}^{(l)}$ for a given sample $x_{i}$ at the $l$ -th layer is articulated as the z-score normalization of the features $\\bar{x_{i}^{(l)}}$ at the same layer. This normalization procedure is employed to eliminate the feature scale, thereby upholding solely the relative sizes of the features across each dimension:  

$$
p_{i}^{(l)}=\\frac{x_{i}^{(l)}-\\overline{{x}}_{i}^{(l)}}{\\sigma(x_{i}^{(l)})+\\epsilon},
$$  

where σ$\\begin{array}{r l r}{\\sigma(x_{i}^{(l)})}&{=}&{\\sqrt{\\frac{1}{D^{(l)}}\\sum_{d=1}^{D^{(l)}}\\left(x_{i}^{(l)}[d]-\\overline{{x}}_{i}^{(l)}\\right)}}\\end{array}$ P,$\\begin{array}{r l}{\\overline{{x}}_{i}^{(l)}}&{{}=}\\end{array}$ $\\begin{array}{r}{\\frac{1}{D^{(l)}}\\sum_{d=1}^{D^{(l)}}x_{i}^{(l)}[d],L}\\end{array}$ P$D^{(l)}$ is the dimensionality of the features at the -th layer, and $x_{i}^{(l)}[d]$ denotes the value of the $d$ -th dimension within the feature $x_{i}^{(l)}$ .  

For a convolutional layer at the $l$ -th layer, in contrast to fully connected layers , the latent features are derived by averaging the output feature map:  

$$
x_{i}^{(l)}=\\frac{1}{H^{(l)}}\\frac{1}{W^{(l)}}\\sum_{h=1}^{H^{(l)}}\\sum_{w=1}^{W^{(l)}}f_{i}^{(l)},
$$  

where $f_{i}^{(l)}\\in\\mathbb{R}^{D^{(l)}\\times H^{(l)}\\times W^{(l)}}$ ∈es the feature map of $x_{i}$ at the $l$ -th layer, with $H^{(l)}$ and $W^{(l)}$ representing the height and width of the feature map at the $l$ -th layer, respectively. Subsequently, the corresponding feature pattern $p_{i}^{(l)}$ can be determined using Eqn. (3) as delineated above.  

As depicted in Fig. 2, within an intermediate layer of the network, the feature pattern of incorrectly predicted samples deviates significantly from the feature pattern of the groundtruth category. In contrast, the feature pattern of correctly predicted samples closely mirrors the feature pattern of the ground-truth category. Consequently, akin to the final layer of the network, at the $l$ -th layer, correct predictions hinge on the correctness of the feature pattern $p_{i}^{(l)}$ for $x_{i}$ , i.e., its similarity to the feature pattern $d_{y_{i}}^{(l)}$ of the true category. The similarity between the feature pattern of a sample $x_{i}$ and the groundtruth category $y_{i}$ can be quantified by the L1 distance between them:",4
07d08b9c-62e7-47f6-aae2-a814b7a455cc,"ref_ids: 454959876536011888, chunk_ids: 1, Score: 0.4023, Text: # Related Work

# Federated Learning
Federated learning is a representative machine learning paradigm that can train model parameters from decentralized data in a privacy-preserving way (McMahan et al. 2017; Yang et al. 2019; Zhang et al. 2021). Its core idea is to exchange model gradients instead of the local data for model training (Bonawitz et al. 2017). For example, McMahan et al. (2017) first formulate the framework of federated training: the clients locally train the model parameters and then upload the local model updates to the server, and the server collects and averages local updates to learn the global model. Furthermore, to speed up the model convergence, many works study the adaptive federated learning optimization strategies that can effectively smooth the learning of the global model (Reddi et al. 2021; Yuan and Li 2022; Karimireddy et al. 2020; Zhang et al. 2020; Khanduri et al. 2021; Yuan, Zaheer, and Reddi 2021). In conclusion, the conventional federated learning methods usually focus on how to effectively learn model parameters from decentralized data. However, these conventional FL methods are based on a distributed training framework that is exposed to an open environment, which faces serious risks in terms of both data privacy and model robustness. These risks also promote a line of research to secure federated learning, including differentially private federated learning and robust federated learning, which are reviewed in the following sections.

# Differentially Private Federated Learning
Differential privacy techniques can offer theoretical guarantees on the privacy protection of communicated data (Kenny et al. 2021). The main idea of the DP technique is to perturb the communicated data via independent random noise to pose challenges to user privacy identification. Furthermore, DP still allows us to accurately estimate some statistical characteristics of the communicated data since the DP noise can be effectively reduced by aggregating the perturbed data. Thus, the DP technique can be naturally applied to protect user privacy in federated learning, which is widely studied in previous works (Wei et al. 2020; Girgis et al. 2021; Geyer, Klein, and Nabi 2017; Truex et al. 2020; Sun, Qian, and Chen 2021). For example, Truex et al. (2020) proposed to utilize the Gaussian noise to perturb the local model gradients, and then update the global model based on the aggregation of perturbed gradients. Sun, Qian, and Chen (2021) proposed a parameter shuffling-based differentially private FL method that can enhance the trade-off between task accuracy and privacy protection. In conclusion, most of the existing differentially private federated learning (DP-FL) methods focus on studying how to improve the effectiveness of model training under a given privacy protection level (Sun and Lyu 2021). However, these DP-FL methods are also vulnerable to poisoning attacks, which still have serious risks in real-world applications. Different from these methods, we study how to improve the model robustness of differentially private federated learning.

# Robust Federated Learning
Poisoning attack is a serious threat to the security of federated learning (Cao et al. 2019; Shejwalkar et al. 2022; Fang et al. 2020). In federated learning, the local model training of a client is invisible to the outside, making it highly convenient for an adversary to poison the local gradients. Thus, in the general framework of existing federated poisoning attack methods, the adversary first employs certain strategies to poison local model gradients and further uploads them to the server to poison the global model (Yin et al. 2018; Shejwalkar and Houmansadr 2021). Most federated poisoning attack methods can be broadly classified into three categories according to their attack purposes (Shejwalkar et al. 2022): (1) targeted attack aiming to degrade the model accuracy on samples in certain groups (Bhagoji et al. 2019; Tolpegin et al. 2020), (2) untargeted attack aiming to degrade the overall task accuracy (Fang et al. 2020; Blanchard et al. 2017), (3) backdoor attack aiming to control the model predictions on poisoned samples embedded with the backdoor triggers (Bagdasaryan et al. 2020; Wang et al. 2020; Xie et al. 2020). For example, Bhagoji et al. (2019) proposed to flip the label of a part of local training data for the untargeted attack, and Bagdasaryan et al. (2020) proposed to learn poisoned model updates based on backdoored training data. In conclusion, these works disclose the vulnerability of federated learning to poisoning attacks and show that it is important to study robust federated learning methods.  

There is a line of works studying how to defend against poisoning attacks in federated learning. In practical attack settings, the malicious clients controlled by the adversary should be the minority group in the participating clients. Thus, most of the robust FL methods assume that the poisoned gradients are the outliers in the gradient distribution, and filtering the outliers and only aggregating the gradients in the distribution center can avoid integrating the poisoned gradients into the global model. For example, Yin et al. (2018) proposed to update the global model based on the median of local gradients in each dimension. Blanchard et al. (2017) proposed to select the local model gradient that is most relevant to other gradients to update the global model. Generally speaking, most of the existing robust gradient aggregation methods are designed for conventional FL, which is difficult to be applied in differentially private FL. This is because in DP-FL local gradients are perturbed by DP noise, which may make the outlier assumption not hold. Besides, many current robust FL methods update the global model based on a very small fraction of local gradients (Blanchard et al. 2017; Yin et al. 2018), which is difficult to reduce the damage of DP noise on task accuracy. Different from these works, we propose a new robust differentially private FL framework, which can simultaneously ensure the data privacy, model robustness, and model accuracy.",4
