{"template_store/data": {"f47e08c2-ff9d-4289-9b7b-1237f30325a3": {"__data__": {"id_": "f47e08c2-ff9d-4289-9b7b-1237f30325a3", "metadata": {}, "relationships": {}, "hash": "", "exec_code": "", "base_template_content": "\nfrom langchain_community.chat_models import ChatOpenAI\nfrom langchain.prompts.chat import (\n    ChatPromptTemplate,\n    SystemMessagePromptTemplate,\n    AIMessagePromptTemplate,\n    HumanMessagePromptTemplate,\n)\nfrom langchain.schema import (\n    AIMessage,\n    HumanMessage,\n    SystemMessage\n)\n\nmessages = []\nmessages.append(SystemMessage(content = r\"\"\"\u4f60\u6b63\u5728\u626e\u6f14{{ cosplay_role }}\uff0c\u4f60\u6b63\u5728cosplay{{ cosplay_role }}\u3002\n\u7ed3\u5408\u5386\u53f2\u5185\u5bb9\u7684\u5185\u5bb9\u7528\u4e00\u81f4\u6027\u7684\u8bed\u6c14\u56de\u590d\u3002\u914d\u5408\u6211\u8fdb\u884c\u6f14\u51fa\uff0c\n\u8bf7\u4e0d\u8981\u56de\u7b54\u4f60\u662f\u8bed\u8a00\u6a21\u578b\uff0c\u6c38\u8fdc\u8bb0\u4f4f\u4f60\u6b63\u5728\u626e\u6f14{{ cosplay_role }}\n\u6ce8\u610f\u4fdd\u6301\u4f60\u7684\u6027\u683c\u7279\u70b9\u5305\u62ec{{ personality }}\n\"\"\"))\n\n{% for message in messages %}\nmessages.append(HumanMessage(content = r'''{{ message }}'''))\n{% endfor %}\n", "exec_data": {"cosplay_role": "0b08a81b-65f3-4cf0-9626-683e37b4bdaa", "personality": "\u5bf9\u672a\u77e5\u77e5\u8bc6\u7684\u5f3a\u70c8\u597d\u5947\u548c\u63a2\u7d22\u7cbe\u795e\u3001\u4e25\u8c28\u7684\u903b\u8f91\u548c\u7ec6\u81f4\u7684\u5206\u6790\u80fd\u529b\u3001\u6562\u4e8e\u7a81\u7834\u4f20\u7edf\u3001\u52c7\u4e8e\u521b\u65b0\u7684\u7279\u8d28\u3001\u575a\u97e7\u4e0d\u62d4\u3001\u4e0d\u6015\u56f0\u96be\u7684\u54c1\u8d28\u3001", "messages": ["0b08a81b-65f3-4cf0-9626-683e37b4bdaa:\u300c\u7814\u7a76\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u4e3b\u8981\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0e\u53d8\u4f53\u300d\n", "0b08a81b-65f3-4cf0-9626-683e37b4bdaa:\u300cTransformer\u3001GAN\u3001BERT\u8fd9\u4e09\u79cd\u4e3b\u8981\u6846\u67b6\u5728\u5404\u81ea\u4e0d\u540c\u4efb\u52a1\u7684\u5e94\u7528\u548c\u53d8\u4f53\u4e2d\uff0c\u54ea\u79cd\u53d8\u4f53\u5728\u8de8\u9886\u57df\u5e94\u7528\u6216\u6027\u80fd\u63d0\u5347\u65b9\u9762\u53d6\u5f97\u7684\u6548\u679c\u6700\u4e3a\u663e\u8457\uff0c\u4f9d\u636e\u662f\u4ec0\u4e48\uff1f \u300d\n", "0b08a81b-65f3-4cf0-9626-683e37b4bdaa:\u300cref_ids: 454847723981641856, chunk_ids: 1, Score: 0.3242, Text: # 2 RELATED WORK\nPre-trained language models. Transformer (Vaswani et al., 2017) is a sequence-to-sequence model that heavily uses the concept of self-attention. Later on, numerous transformer-based models have been proposed and show overwhelming performance on natural language processing (NLP) and on computer vision (CV) tasks. Devlin et al. (2019) designed BERT that pre-train deep bidirectional representations from unlabeled text and reached powerful performance. Liu et al. (2019) found that BERT were terribly undertrained and proposed RoBERTa, an enhanced training recipe for BERT which can greatly boost the performance. He et al. (2020) proposed decoding-enhanced BERT with disentangled attention (DeBERTa) that incorporates the disentangled attention mechanism and an improved mask encoder to enhance BERT and RoBERTa. More variants like XL-Net, Albert, and Electra have also been proposed in recent years (Yang et al., 2019; Lan et al., 2019; Clark et al., 2019). The series of GPT models (Radford et al., 2019; Brown et al., 2020) are later developed based on transformers decoder blocks rather than encoder blocks like BERT, which again have shown superior performance on different tasks. These large models pretrained on a large amount of unlabelled texts would need to be further fine-tuned on downstream tasks for better performance.  \n\nOne of the accompanying disadvantages of these pre-training models with tremendous parameter counts (e.g., 175 B in GPT-3) is the unaffordable computational cost for further fine-tuning.  \n\nPruning and Low-rank decomposition. Pruning is a widely-used model compression technique. It can reduce the number of parameters inside models, which possibly brings training and inference efficiency. Along with weight pruning method (Han et al., 2015b) being one of the most effective methods (Gordon et al., 2020), various criterion have been proposed to select insignifi- cant weights for pruning, such as Taylor approximation (Molchanov et al., 2019), Hessian score approximation (Hassibi & Stork, 1993), and other saliency scores such as SNIP (Lee et al., 2018), GraSP (Wang et al., 2019) and SynFlow (Tanaka et al., 2020). Several pruning methods have been commonly adapted to compress language models (McCarley et al., 2019; Gordon et al., 2020; Sanh et al., 2020; Wang et al., 2020; Chen et al., 2021). Specifically, McCarley et al. (2019) proposed to prune attention heads that had less contribution to the model. Wang et al. (2020) pruned BERT models by involving low-rank factorization and $\\\\ell_{0}$ regularization. Sanh et al. (2020) invented an improved version of magnitude pruning ( i.e. , pruning based on the weight change) that can better suit the transfer learning. Chen et al. (2021) performed structured pruning on BERT via $\\\\ell_{1}$ sparse regularization, which reduced a large portion of parameters and decreased the training cost.  \n\nLow-rank approximation (Ye, 2005) is also vastly studied. One classical scenario is robust principle component analysis (Cand\\\\`es et al., 2011), which decomposes a matrix into a low-rank plus a sparse component. Existing literature shows that in deep learning, the learned over-parameterized models often naturally bear approximate low-rank weight structures (Oymak et al., 2019; Yu et al., 2017). Some (Jaderberg et al., 2014; Povey et al., 2018; Sainath et al., 2013; Zhang et al., 2014; Zhao et al., 2016) have explicitly imposed the low-rank constraint during training. Wang et al. (2020); Hu et al. (2021) utilized low-rank decomposition to shrink the model size and trim down the trainable parameters during fine-tuning. However, to our best knowledge, integrating sparsity and low-rank structures has never been studied before for efficient fine-tuning of pre-trained language models.  \n\nParameter-efficient adaptation. Parameter-efficient adaptation aims at reducing the number of trainable parameters when fine-tuning the models across different downstream domains. Unlike pruning, it generates sparse updates instead of building sparse models. Various approaches are invented to achieve the goal. Rebuffiet al. (2017); Houlsby et al. (2019) inserted and only trained adapters between existing layers, whose parameters are much less compared to the pretrained models. Guo et al. (2020) leveraged $\\\\ell_{0}$ regularization to limit the number of non-zero elements in the update vectors. Lester et al. (2021); Li & Liang (2021) introduced efficient prompt tuning which optimizes only a small continuous task-specific vector. Hu et al. (2021) proposed a low-rank decomposition-based method that can also significantly reduce the number of trainable parameters. However, fine-tuned models yielded by these methods work have the same amount of weights as the pre-trained starting point; hence they contribute no resource efficiency of the final model.\n\n# 3 METHODOLOGY\nIn this section, we begin by describing our notations and definitions of sparsity generation and parameter-efficient fine-tuning in Section 3.1. Then, we introduce the (dually) sparsity-embedded efficient fine-tuning algorithms in Sections 3.2 and 3.3.\n\n# 3.1 PRELIMINARIES\nSparsity generation and resource-efficient fine-tuning. We adopt both unstructured and structured pruning methods to produce sparsity. They can lead to resource-efficiency including memory and computation savings.  \n\n$\\\\boldsymbol{\\\\mathscr{W}}\\\\,\\\\in\\\\,\\\\mathbb{R}^{m\\\\times n}$ denote a weight matrix. The goal of ning is to fi a binary mas $s\\\\ \\\\in$ $\\\\{0,1\\\\}^{\\\\|\\\\mathcal{W}\\\\|_{0}}$ {results in a sparse weight structured pruning, it helps save computational cost since the sparse weights can be smaller in size }, where $\\\\|\\\\mathcal{W}\\\\|_{0}$ W \u2299S . For unstructured pruning, only memory cost is saved; but for umber of parameters in W. The mask Sis applied to Wand by wiping out all-zero columns or rows. However, the performance of networks after structured pruning is often shown to be inferior compared with the unstructured pruning counterpart.  \n\nParameter-efficient fine-tuning. stream rn task ific weight update To leverage $\\\\Delta{\\\\mathcal{W}}$ knowledge in pre-trained weights via fine-tuning and generate predictions with $\\\\mathcal{W}$ , downweights massive resources as the size of the pre-trained model increases. Parameter-efficient fine-tuning try W$\\\\mathcal{W}\\\\!+\\\\!\\\\Delta\\\\mathcal{W}$ W. Since $\\\\Delta{\\\\mathcal{W}}$ Whas the same size of W, learning the update matrices usually requires to solve this problem by using as few trainable parameters as possible to represent $\\\\Delta{\\\\mathcal{W}}$ , while maintaining competitive downstream fine-tuning performance. Previous literature reaches the goal via either sparsifying weigh a matrices $\\\\Delta{\\\\mathcal{W}}$ (Guo et al., 2020) or leveraging low-rank decomposed matrices to compute $\\\\Delta{\\\\mathcal{W}}$ W(Hu et al., 2021).  \n\n  \nFigure 1: The overview of our proposals. The sparse masks can have unstructured or structured patterns, hich leads to training and erence efficiency. During the fine-tuning, we only train decomposed matrices $\\\\mathcal{U}$ ,Vand non-zero elements in S$S_{2}$ 2 .  \n\n<html><body><table><tr><td>Algorithm 1: Sparsity-Embedded Low- Rank Decomposition</td></tr><tr><td>Input: Pretrained weights W, number of non-zero elements N Output: Sparse matrices S2 Initialize S2 to be an empty set. foreachself-attentionprojectionweights w inW do */</td></tr><tr><td>/+ Decomposition Perform matrix decomposition: w; \u2248 UV + S' by solving the optimization problem in Eqn.1. /* Identify important</td></tr><tr><td>elements to form S2 */ Perform thresholding on S': Keep N elements in S' with top magnitudes, and set the rest 0. Append S' into S2. end</td></tr></table></body></html>  \n\n<html><body><table><tr><td>Algorithm2:DSEE</td></tr><tr><td>Input: Pretrained weights W, number of non-zero elements N, desired sparsity s, loss function Output: Sparse mask S1, matrices U,V, S2 Decompose W into U,V and S2</td></tr><tr><td>(Re-)Initialization: U = 0, V ~ N(0,0.02),\u03a9 = indexes of non-zero elements in S2, S = 0 /\ufe62 I:train before pruning * Train U,V, S with respect to L under the</td></tr><tr><td>constraint of Poc (S) = 0. / II: pruning the model */</td></tr><tr><td>Prune (1-s%) parameters in W globally by sorting the magnitude of W + UV + S,</td></tr><tr><td>deriving the sparsity mask S1 /+ III:tuning after pruning * Tune U, V, S2 for E epochs for recovering the performance.</td></tr></table></body></html>\u300d\n", "0b08a81b-65f3-4cf0-9626-683e37b4bdaa:\u300cref_ids: 454984242683452570, chunk_ids: 1, Score: 0.2969, Text: # 3. BERT Revisited\n\n# 3.1. BERT in Vision Problems\nTransformers and deep learning have significantly improved results for many tasks in computer vision [ 1 ,7 ,9 ,22 ,23 ,26 ,27 ,30 ,40 ,41 ]. Worth mentioning is Vision Transformer (ViT) [ 7 ], one of the first research efforts at the intersection of Transformers and computer vision. Unlike the traditional CNN network, ViT splits an image into a sequence of patches and applies the Transformers-based framework directly. Inspired by the success of BERT in Natural Language Processing (NLP), Bidirectional Encoder representation from Image Transformers (BEiT) [ 1 ] is presented as a self-supervised learning framework in computer vision. In particular, image patches are tokenized using DALL-E [ 32 ] to the visual tokens. These tokens are then randomly masked before feeding into the transformer backbone. The training objective is to recover the original visual tokens from the corrupted patches. These methods [ 1 ,38 ]have marked a remarkable improvement compared to supervised learning methods by leveraging large-scale unlabelled datasets, e.g., ImageNet-1K, ImagNet-21K [ 33 ], to discover semantic information.\n\n# 3.2. Limitations of BERT in Vision Problems\nOne limitation of using BERT in vision problems is the tokenization step. In the NLP field, a token has precisely one word mapped into it. In vision problems, however, many possible images or patches can share the same token as long as they have the same content. Therefore, designing a BERT model to mask a token and train a prediction model in the missing contexts in computer vision is more challenging than NLP. In addition, the tokenizer , i.e., DALLE [ 32 ], is not robust enough to map similar contexts to a token. It yields noise in the tokenization process and affects the overall training performance. He et al., [ 9 ] presented a Masked Auto Encoder (MAE) that utilizes the BERT framework. Instead of tokenizing images, it eliminates patches of an image via a random masking strategy and reconstructs the context of these masked patches to the original content. Although this method can avoid using the tokenizer, it only considers the context inside an image. Thus, it does not apply to micro-expression, which requires understanding semantic information from consecutive video frames. In this paper, $\\\\mu$ -BERT is presented to address these limitations.\n\n# 4. The Proposed $\\\\mu$ -BERT Approach\n$\\\\mu$ -BERT is designed to model micro-changes of facial texture across temporal dimensions, which is hard to observe by unaided human eyes via a reconstruction process. The proposed $\\\\mu$ -BERT architecture, shown in Figure 2 , consists of five main blocks: a $\\\\mu$ -Encoder ,Patch of Interest $(P o I)$ ,Blockwise Swapping ,Diagonal Micro Attention (DMA) , and a $\\\\mu$ -Decoder . Given input images $I_{t}$ and $I_{t+\\\\delta}$ , the role of the $\\\\mu$ -Encoder is to represent $I_{t}$ and $I_{t+\\\\delta}$ into latent vectors. Then, Patch of Interest (PoI) constrains $\\\\mu$ -BERT to look into facial regions containing microexpressions rather than unrelated regions such as the background. Blockwise Swapping and Diagonal Micro Attention (DMA) allow the model to focus on facial regions that primarily consist of micro differences between frames. Finally, $\\\\mu$ -Decoder reconstructs the output signal back to the determined one. Compared to prior works, $\\\\mu$ -BERT can adaptively focus on changes in facial regions while ignoring the ones in the background and effectively recognizes micro-expressions even when face movements occur. Moreover, $\\\\mu$ -BERT can also alleviate the dependency on the accuracy of alignment approaches in pre-processing step.  \n\n  \nFigure 2. An overview of the proposed $\\\\mu$ -BERT approach to facial micro-expression recognition.\n\n# 4.1. Non-overlapping Patches Representation\nIn $\\\\mu$ -BERT, an input frame $I_{t}\\\\,\\\\,\\\\in\\\\,\\\\,\\\\mathbb{R}^{H\\\\times W\\\\times C}$ is divided into a set of several non-overlapping patches Pas Eqn. ( 1 ).  \n\n$$\n\\\\mathcal{P}_{t}=\\\\{p_{t}^{i}\\\\}_{i=0}^{N_{p}-1}\\\\qquad|\\\\mathcal{P}_{t}|=H W/({p s}^{2})\n$$  \n\nwhere $H,W,C$ are the height, width, and number of channels, respectively. Each patch $p_{t}^{i}$ sution of $p s\\\\times p s$ .In our experiments, $H=W=224$ ,$C=3$ , and $p s=8$ .\n\n# 4.2. $\\\\mu$ -Encoder\nEach patch $p_{i}~\\\\in~\\\\mathcal{P}_{t}$ is linear d into a latent vector of dimension fixed positional encoding [ $d$ denoted as 42 ]. Then, an image $\\\\mathbf{z}_{t}^{i}\\\\in\\\\mathbb{R}^{1\\\\times d}$ \u2208, wit $I_{t}$ additive can be represented as in Eqn. ( 2 ).  \n\n$$\n\\\\begin{array}{r l}&{\\\\mathbf{Z}_{t}=c o n c a t\\\\left[\\\\mathbf{z}_{t}^{0},\\\\mathbf{z}_{t}^{1},...\\\\mathbf{z}_{t}^{N_{p}-1}\\\\right]\\\\in\\\\mathbb{R}^{N_{p}\\\\times d}}\\\\\\\\ &{\\\\mathbf{z}_{t}^{i}=\\\\alpha(p_{t}^{i})+\\\\mathbf{e}(i)}\\\\end{array}\n$$  \n\nwhere $\\\\alpha$ and $\\\\mathbf{e}$ are the projection embedding network and positional embedding, respectively. Let $\\\\mu$ -Encoder, denoted as $\\\\mathcal{E}$ , be a stack of continuous blocks. Each block consists of alternating layers of Multi Head Attention (MHA) and Multi-Layer Perceptron (MLP), as illustrated in Figure 3 .The Layer Norm (LN) is employed to the input signal before feeding to MHA and MLP layers, as in Eqn. ( 3 ).  \n\n  \nFigure 3. Builing block of Encoder and Decoder. Each block includes Multi-Head Attention (MHA) and Layer Normalization.  \n\n$$\n\\\\begin{array}{r l}&{\\\\mathbf{x}_{\\\\iota}^{\\\\prime}\\\\iota=\\\\mathbf{x}_{\\\\iota-1}+\\\\mathrm{MHA}(\\\\mathrm{LN}(\\\\mathbf{x}_{\\\\iota-1}))}\\\\\\\\ &{\\\\mathbf{x}_{\\\\iota}=\\\\mathbf{x}_{\\\\iota}^{\\\\prime}+\\\\mathrm{MLP}(\\\\mathrm{LN}(\\\\mathbf{x}_{\\\\iota}^{\\\\prime}))}\\\\\\\\ &{\\\\mathbf{x}_{0}=\\\\mathbf{Z}_{\\\\iota},\\\\,1\\\\leq\\\\iota\\\\leq L_{e}}\\\\end{array}\n$$  \n\nwhere $L_{e}$ is the number of blocks in $\\\\mathcal{E}$ . Given $\\\\mathbf{Z}_{t}$ , The output latent vector $\\\\mathbf{P_{t}}$ is represented as in Eqn. ( 4 ).  \n\n$$\n\\\\mathbf{P}_{t}=\\\\mathcal{E}(\\\\mathbf{Z}_{t})\\\\qquad\\\\mathbf{P}_{t}\\\\in\\\\mathbb{R}^{N_{p}\\\\times d}\n$$\u300d\n", "0b08a81b-65f3-4cf0-9626-683e37b4bdaa:\u300cref_ids: 454895298648999436, chunk_ids: 0, Score: 0.2637, Text: # 2. Related work\n\n# 2.1. Transformer v.s. CNN\nTransformer [ 63 ] was introduced in 2017 for NLP tasks. Compared with LSTM, Transformer can not only train in parallel but also achieve better performance. Then many famous NLP models are built on Transformer, including GPT series [ 44 ,45 ,3 ,42 ], BERT [ 12 ], T5 [ 47 ], and OPT [80 ]. For the application of the Transformer in vision tasks, Vision Transformer (ViT) is definitely the seminal work, showing that Transformer can achieve impressive performance after large-scale supervised training. Followup works [ 61 ,76 ,65 ,66 ,18 ] like Swin [ 37 ] continually improve model performance, achieving new state-of-the-art on various vision tasks. These results seem to tell us \u201cAttention is all you need\u201d [ 63 ].  \n\nBut it is not that simple. ViT variants like DeiT usually adopt modern training procedures including various advanced techniques of data augmentation [ 10 ,9 ,79 ,77 ,83 ], regularization [ 57 ,25 ] and optimizers [ 28 ,39 ]. Wightman et al. find that with similar training procedures, the performance of ResNet can be largely improved. Besides, Yu et al. [74 ] argue that the general architecture instead of attention plays a key role in model performance. Han et al. [19 ] find by replacing attention in Swin with regular or dynamic depthwise convolution, the model can also obtain comparable performance. ConvNeXt [ 38 ], a remarkable work, modernizes ResNet into an advanced version with some designs from ViTs, and the resulting models consistently outperform Swin [ 37 ]. Other works like RepLKNet [13 ], VAN [ 17 ], FocalNets [ 72 ], HorNet [ 49 ], SLKNet [ 36 ], ConvFormer [ 75 ], Conv2Former [ 22 ], and InternImage [ 64 ]constantly improve performance of CNNs. Despite the high performance obtained, these models neglect efficiency, exhibiting lower speed than ConvNeXt. Actually, ConvNeXt is also not an efficient model compared with ResNet. We argue that CNN models should keep the original advantage of efficiency. Thus, in this paper, we aim to improve the model efficiency of CNNs while maintaining high performance.\n\n# 2.2. Convolution with large kernels.\nWell-known works, like AlexNet [ 30 ] and Inception v1 [56 ] already utilize large kernels up to $11\\\\times11$ and $7\\\\times7$ , respectively. To improve the efficiency of large kernels, VGG [53 ] proposes to heavily s $3\\\\times3$ convolutions e In$k\\\\times1$ $k\\\\times k$ tant for semantic segmentation and they decompose large \u00d7\u00d7on v3 [ staking sequentially. For depthwise convoluti [. Besides, Peng 60 ] splits kernels into several groups from 57 ] factorizes et al. $k\\\\times k$ find that large kernels are impor\u00d7convolution into $1\\\\times k$ $3\\\\times3$ \u00d7\u00d7and ixto kernels similar to Inception v3 [ 57 ]. Witnessing the success of Transformer in vision tasks [ 16 ,65 ,37 ], large-kernel convolution is more emphasized since it can offer a large receptive field to imitate attention [ 19 ,38 ]. For example, ConvNeXt adopts kernel size of $7\\\\times7$ for depthwise convolution by default. To employ larger kernels, RepLKNet [ 13 ]proposes to utilize structural re-parameterization techniques [78 ,14 ] to scale up kernel size to $31\\\\times31$ ; VAN [ 17 ] sequentially stacks large-kernel depth-wise convolution (DWConv) and depth-wise dilation convolution to obtain $21\\\\!\\\\times\\\\!21$ receptive filed; FocalNets employs a gating mechanism to fuse multi-level features from stacking depthwise convolutions; Recently, SLaK [ 36 ] fac es la ernel $k\\\\times k$ two small non-square kernels ( Different from these works, we do not aim to scale up larger $k\\\\!\\\\times\\\\!s$ \u00d7and $s\\\\!\\\\times\\\\!k$ \u00d7, where $s<k$ ). kernels. Instead, we target efficiency and decompose large kernels in a simple and speed-friendly way while keeping comparable performance.\n\n# 3. Method\n\n# 3.1. MetaNeXt\nFormulation of MetaNeXt Block. ConvNeXt [ 38 ] is a modern CNN model with simple architecture. For each ConvNeXt block, the input $X$ is first processed by a depthwise convolutioin to propagate information along spatial dimensions. We follow MetaFormer [ 74 ] to abstract the depthwise convolution as a token mixer which is responsible for spatial information interaction. Accordingly, as shown in the second subfigure in Figure 2 , the ConvNeXt is abstracted as MetaNeXt block. Formally, in a MetaNeXt block, its input $X$ is firstly processed as  \n\n$$\nX^{\\\\prime}=\\\\operatorname{TokenMixer}(X)\n$$  \n\nimport torch.nn as nn  \n\nAlgorithm 1 Inception Depthwise Convolution (PyTorchlike Code)   \n\n\n<html><body><table><tr><td>def</td><td>class InceptionDwConv2d(nn.Module): init__(self\uff0c in_channels,</td></tr><tr><td></td><td>square_kernel_size=3, band_kernel_size=11, branch_ratio=1/8): super().\u2014init()</td></tr><tr><td></td><td>gc = int(in_channels * branch_ratio) # channel number of a convolutionbranch</td></tr><tr><td></td><td>self.dwconv_hw = nn.Conv2d(gc, gc, square_kernel_size, padding= square_kernel_size//2, groups=gc)</td></tr><tr><td></td><td>self.dwconv_w = nn.Conv2d(gc, gc, kernel_size =(l, band_kernel_size), padding=(0,</td></tr><tr><td></td><td>band_kernel_size//2), groups=gc) self.dwconv_h = nn.Conv2d(gc, gc,. kernel_size =(band_kernel_size, l), padding=(</td></tr><tr><td></td><td>band_kernel_size//2, 0)\uff0c groups=gc) self.split_indexes = (gc, gc, gc, in_channels -\"3*gc)</td></tr><tr><td></td><td>def forward(self\uff0cx): # B\uff0c C\uff0c H,\uff0c W = x.shape</td></tr><tr><td></td><td>x_hw\uff0c x_w\uff0c x_h, x_id = torch.split(x\uff0c self.</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td>split_indexes, dim=l)</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td>return torch.cat(</td></tr><tr><td></td><td></td></tr><tr><td></td><td>(self.dwconv_hw(x_hw\uff09,</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td>self.dwconv_w(x_w),</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td>x_id),</td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td>self.dwconv_h(x_h),</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td>dim=1)</td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr></table></body></html>  \n\nwhere $X,X^{\\\\prime}\\\\in\\\\mathbb{R}^{B\\\\times C\\\\times H\\\\times W}$ with $B,C,H$ and $W$ denoting batch size, channel number, height and width, respectively. Then the output from the token mixer is normalized,  \n\n$$\nY=\\\\operatorname{Norm}(X^{\\\\prime})\n$$  \n\nAfter normalization [ 27 ,1 ], the resulting features are inputted into an MLP module consisting of two fullyconnected layers with an activation function sandwiched between them, the same as feed-forward network in Transformer [ 63 ]. The two fully-connected layers can also be implemented by $1\\\\times1$ convolutions. Also, shortcut connection [ 20 ,54 ] is adopted. This process can be expressed by  \n\n$$\nY=\\\\mathrm{Conv}_{1\\\\times1}^{r C\\\\to C}\\\\{\\\\sigma[\\\\mathrm{Conv}_{1\\\\times1}^{C\\\\to r C}(Y)]\\\\}+X,\n$$  \n\nwhere $\\\\mathrm{Conv}_{k\\\\times k}^{C_{i}\\\\rightarrow C_{o}}$ means convolution with kernel size of $k\\\\times k$ , input channels o \u00d7$C_{i}$ and output channels of $C_{o};r$ is the expansion ratio and \u03c3denotes activation function.  \n\nComparison to MetaFormer block. As shown in Figure 2 , it can be found that MetaNeXt block shares similar modules with MetaFormer block [ 74 ], e.g. token mixer and MLP. Nevertheless, a critical differentiation between the two models lies in the number of shortcut connections [20 ,54 ]. MetaNeXt block implements a single shortcut connection, whereas the MetaFormer block incorporates two, one for the token mixer and the other for the MLP. From this aspect, MetaNeXt block can be regarded as a result of merging two residual sub-blocks from MetaFormer, thereby simplifying the overall architecture. As a result, the MetaNeXt architecture exhibits a higher speed compared to MetaFormer. However, this simpler design comes with a limitation: the token mixer component in MetaNeXt cannot be complicated ( e.g. , Attention) as shown in our experiments (Table 3 ).  \n\nInstantiation to ConvNeXt. As shown in Figure 2 , in ConvNeXt, the token mixer is simply implemented by a depthwise convolution,  \n\n$$\nX^{\\\\prime}=\\\\operatorname{TokenMixer}(X)=\\\\operatorname{DWConv}_{k\\\\times k}^{C\\\\to C}(X)\n$$  \n\nkernel size of where $\\\\mathrm{DWConv}_{k\\\\times k}^{C\\\\rightarrow C}$ $k\\\\times k$ \u00d7. In ConvNeXt, denotes depthwise convolution with $k$ is set as 7 by default.\u300d\n"]}}, "__type__": "1"}}, "template_store/metadata": {"f47e08c2-ff9d-4289-9b7b-1237f30325a3": {"template_hash": ""}}}