角色,内容,分镜
3e9a2ad0-3314-47be-933a-c61134edc408,研究成果、方法创新性与应用价值总结,6
3e9a2ad0-3314-47be-933a-c61134edc408,在模型性能提升、新架构和算法提出等研究成果方面，具体有哪些数据或实例可以进一步说明其对计算机科学多个领域发展的推动作用？ ,6
3e9a2ad0-3314-47be-933a-c61134edc408,"ref_ids: 454847062641060886, chunk_ids: 6, Score: 0.3203, Text: # CFurther Discussions
We summarize some empirical observations as follows.  

1) The CLIP with four training tricks yields about $4\\%$ improvement at Rank-1 in Table 1 of the main paper. It can inspire future works in which the model performance could be boosted by applying these training tricks.  

2) Data augmentation and loss function are common technologies used in various methods. The investigation of more than 20 data augmentations and about 10 loss functions on performance in Tables 2-5 of the main paper provides valuable guidance on future works. Researchers can select proper and effective augmentations and losses into the model for improving performance.   
3) We explore the internal properties and functionalities of the model for the first time. These results can light future works on model compression, so as to develop a more lightweight and effective TBPS method.   
4) There are very little research on few-shot TBPS, while this paper makes a preliminary study on CLIP-based fewshot TBPS, providing valuable observation for future research direction.",6
3e9a2ad0-3314-47be-933a-c61134edc408,"ref_ids: 454848267426884226, chunk_ids: 6, Score: 0.2451, Text: # Ethical Considerations
Our experiments replicate prior work under comparable experimental conditions. For this reason, we do not expect our work to introduce any novel ethical issues, although our experiments may inherit a similar set of issues concerning PLM (especially large-scale ones), as outlined by various prior work ( Gehman et al. ,2020 ;Bender et al. ,2021 ;Rae et al. ,2021 ;Dinan et al. ,2021 ;Bommasani et al. ,2021 ;Kenton et al. ,2021 ;Weidinger et al. ,2021 ,inter alia ). We remark, however, that conducting these principled comparisons across different models — which requires a degree of hyper-parameter tuning for each model (both at pre-training and fine-tuning stages) in order to enable a fair comparison — requires a large number of computational resources, which may contribute to increased carbon emissions ( Strubell et al. ,2019 ;Patterson et al. ,2021 ).

# Acknowledgement
We thank Chris Dyer, John T. Hale, and Laura Rimell at DeepMind, Noah A. Smith at the University of Washington & Allen Institute for Artificial Intelligence, and Qi Huang at Bloomberg for their valuable insights, feedback, and suggestions.



# A Model Comparison
In Table 3 , we outline a summary of various aspects of some commonly-used PLMs that have been proposed to date, summarizing the size of the model, the training data and its size, the text pre-processing scheme, the pre-training objective, and some hyper-parameter details. This table reveals a large variation in the design choices of these PLMs, hence rendering it difficult to conduct apple-to-apple comparisons between different approaches. Common patterns include scaling the model while also using different (often larger) pretraining data, as well as using different training regimes altogether. Each design choice impacts model performance in different ways ( Sennrich and Zhang ,2019 ;Jiao et al. ,2019 ), emphasizing the importance of conducting thorough ablations and apple-to-apple comparisons.  

model over the others. We summarize some key design choices in Table 4 .

# B.1 Hyper-Paramaters for Fine-tuning
We refer to the original BERT’s hyper-parameters for fine-tuning, but experiment with more finetuning learning rates. In addition to that, we also use three different random seeds. The finetuning hyper-parameters that we used are as follows, which is partially based on prior work ( Joshi et al. ,2020 ):  

•Batch sizes: {16, 32}   
•Learning rates: {2e-4, 1e-4, 5e-5, 3e-5, 2e5, 1e-5, 5e-6}   
•Epoch: 4   
•Random seeds : {1, 41, 386}

# BDetailed Hyper-Parameters
When conducting experiments, we follow BERT’s architecture, training data, and overall hyperparameter choices ( Devlin et al. ,2019 ). One noticeable difference, however, is that we follow RoBERTa to train the model without using a nextsentence prediction loss ( Liu et al. ,2019 ), which has been shown to have a minimal impact on model performance. Each of the three models in our rerun not only has the exact same design choices in terms of training data, text processing, model architecture, etc., but is also implemented on the exact same codebase. Concretely, we use the BERT codebase as implemented on Huggingface, and conduct some slight modifications in terms of removing the next-sentence prediction loss as stated above. When implementing the other models, we take the BERT implementation, and simply change the masking function and pre-training objective in order to replicate the results of GPT-1 and ELMo under comparable conditions as our BERT model. This means that our reruns of the GPT-1 and ELMo models benefit from the exact same technical implementation details as our BERT model by virtue of using identical positional encoding, segment embeddings, etc. We tune the pre-training and fine-tuning learning rate of each model independently (hence the final learning rate for each model may be different), although we strive to dedicate the same amount of compute resources in tuning the hyper-parameters of each model, in order to avoid favoring one",6
3e9a2ad0-3314-47be-933a-c61134edc408,"ref_ids: 454846731731167836, chunk_ids: 9, Score: 0.2412, Text: # 4 Paying Off the Scientific Debt: Recommendations and Lessons Learnt
We proceed to outline several key recommendations and lessons learnt for encouraging, incentivizing, and accelerating progress in this line of work.  

Establish standard, publicly available pretraining corpora at multiple data scales. As seen in $\\S3$ , the size and quality of the pre-training data is an important driver behind model performance ( Liu et al. ,2019 ;Hoffmann et al. ,2022 ), which makes a rigorous comparison between different PLMs difficult. Hence, our first recommendation is to establish standard pre-training corpora that are publicly available. We further recommend releasing the pre-training corpora under multiple data scales, as approaches that work best under strict compute or data resource requirements may be different from the case where there is a large amount of compute and data available (§ 3 ,Clark et al. ,2020 ;Treviso et al. ,2022 ). Note that this does not mean that we are discouraging the use of non-standard or even-larger corpora than those that are publicly available. On the contrary, researchers should continue to push the boundaries of what is possible by training on more, better quality, and more recent data. In such cases, we recommend researchers to also release versions of their models that are trained on the standard pre-training corpora — above and beyond the version trained on proprietary & large-scale data that would presumably be necessary to achieve a new state-of-the-art — in order to facilitate a fair and principled comparison with prior work. We encourage the community to continually release new standardized pre-training datasets as time passes to avoid the effect of pre-training data staleness (Lazaridou et al. ,2021 ).  

<html><body><table><tr><td>Model</td><td>CoLA</td><td>MNLI(-m)</td><td>MRPC</td><td>QNLI</td><td>QQP</td><td>RTE</td><td>SST-2</td><td>STS-B</td><td>Avg</td></tr><tr><td colspan=""10"">Original published results</td></tr><tr><td>BERT</td><td>52.1</td><td>84.6</td><td>88.9</td><td>90.5</td><td>71.2</td><td>66.4</td><td>93.5</td><td>85.8</td><td>79.1</td></tr><tr><td>BERTLarge</td><td>60.5</td><td>86.7</td><td>89.3</td><td>92.7</td><td>72.1</td><td>70.1</td><td>94.9</td><td>86.5</td><td>81.6</td></tr><tr><td>GPT-1</td><td>45.4</td><td>82.1</td><td>82.3</td><td>87.4</td><td>70.3</td><td>56.0</td><td>91.3</td><td>80.0</td><td>74.4</td></tr><tr><td>BiLSTM+ELMo+Attn</td><td>36.0</td><td>76.4</td><td>84.9</td><td>79.8</td><td>64.8</td><td>56.8</td><td>90.4</td><td>73.3</td><td>70.3</td></tr><tr><td colspan=""10"">Our replication with proper controls & comparable experimental conditions</td></tr><tr><td>BERTRerun</td><td>50.8</td><td>84.5</td><td>89.0</td><td>90.5</td><td>71.0</td><td>61.0</td><td>93.1</td><td>84.4</td><td>78.0</td></tr><tr><td>Comparable GPT-1 Rerun - L2R</td><td>41.6</td><td>87.4</td><td>84.7</td><td>86.6</td><td>68.8</td><td>62.9</td><td>91.8</td><td>79.3</td><td>75.4</td></tr><tr><td>Comparable GPT-1 Rerun -R2L</td><td>42.5</td><td>82.0</td><td>85.5</td><td>88.3</td><td>69.1</td><td>57.6</td><td>92.8</td><td>79.1</td><td>74.6</td></tr><tr><td>ComparableELMo-variantRerun</td><td>46.8</td><td>83.6</td><td>85.8</td><td>89.9</td><td>70.8</td><td>61.9</td><td>93.1</td><td>82.1</td><td>76.8</td></tr><tr><td>Ensemble of Comparable GPT-1: L2R + R2L</td><td>45.1</td><td>83.7</td><td>85.8</td><td>88.9</td><td>70.8</td><td>62.4</td><td>92.9</td><td>81.0</td><td>76.3</td></tr><tr><td>Ensemble of Comparable GPT-1: L2R + L2R</td><td>42.4</td><td>83.5</td><td>85.1</td><td>87.8</td><td>70.0</td><td>63.1</td><td>93.1</td><td>79.9</td><td>75.6</td></tr></table></body></html>  

Table 1: GLUE test results. We use F1 scores for MRPC and QQP, Matthew’s Correlation for CoLA, SpearmanR for STS-B, and accuracy for the rest; all models are pre-trained with the same batch size & compute (1M steps).   


<html><body><table><tr><td>Model</td><td>CoLA</td><td>MNLI(-m)</td><td>MRPC</td><td>QNLI</td><td>QQP</td><td>RTE</td><td>SST-2</td><td>STS-B</td><td>Avg</td></tr><tr><td>BERT Rerun</td><td>43.8</td><td>80.9</td><td>86.4</td><td>87.9</td><td>69.3</td><td>59.3</td><td>90.0</td><td>80.4</td><td>74.8</td></tr><tr><td>Comparable GPT-1 Rerun: L2R</td><td>43.5</td><td>80.3</td><td>84.1</td><td>86.3</td><td>68.4</td><td>63.0</td><td>91.0</td><td>77.8</td><td>74.3</td></tr><tr><td>Comparable GPT-1 Rerun: R2L</td><td>36.2</td><td>80.6</td><td>82.4</td><td>88.2</td><td>68.7</td><td>53.7</td><td>93.0</td><td>77.8</td><td>72.6</td></tr><tr><td>Ensemble of GPT-1 Rerun: L2R + R2L</td><td>45.1</td><td>82.6</td><td>84.4</td><td>88.3</td><td>70.8</td><td>62.9</td><td>93.5</td><td>79.9</td><td>75.9</td></tr></table></body></html>

Table 2: GLUE test set results using the pre-trained model, after training for 200,000 steps followed by fine-tuning.  

Explicitly delineate the different types of contributions behind each work, including both the key novelty and engineering contributions. We recommend that PLM research explicitly state the key novelty behind each work ( e.g., the bidirectional masked LM loss for BERT), delineate and explicitly state other contributions (including engineering ones) and design choices that can impact performance, and outline how these differ from prior work ( e.g., better model partitioning for training larger models on multiple devices, better filtering of the training data, more extensive hyperparameter tuning, etc.). Combined with strong baselines and extensive ablations (see below), this will enable us to better understand how much of the performance gains can be attributed to each factor, including the key novelty behind the approach.  

Invest comparable effort into tuning both the baselines and the newly-proposed models. In practice, many of the contributions ( e.g., better hyper-parameters, better data, etc.) would also be applicable to the baselines. We recommend each PLM work to discern which of their design choices can also be applied to the baselines, and apply those techniques in order to create stronger baselines that may nevertheless rival the performance of more recent models (§ 3 ,Melis et al. ,2018 ;Lei ,More extensive ablation studies. When proposing multiple contributions at once (as many PLM papers do), we recommend conducting as many ablation studies as is feasible to isolate the impact of each component under comparable conditions. In light of recent trends where models — including open-sourced ones — are publicly released without technical reports or papers that outline technical details regarding model evaluation and benchmarking ( Taori et al. ,2023 ;Chiang et al. ,2023 ), we argue that our recommendation for conducting more thorough evaluations is even more critical.  

Better credit assignment is needed. As shown in Table 1 , the vast gap between BERT and ELMo can nearly be bridged by using (i) the same (larger) pre-training data, (ii) Transformer architectures, and (iii) whole model fine-tuning; all of which were already used and proposed by the GPT-1 model. As these techniques account for a more significant chunk of the performance difference than the bidirectional masked LM loss, disentangling each factor’s contribution thus provides an opportunity to conduct better credit assignment in the field.  

Strike a balance between pushing the stateof-the-art and advancing our scientific understanding. In some sense, recent rapid progress is made possible by a strong emphasis on building the next state-of-the-art PLMs and foundation models, although it comes at a cost of understanding — from the scientific point of view — where the performance improvements are coming from, and which techniques work best under what circumstances. We argue that both lines of work — one that pushes the state-of-the-art at breakneck speed and through all available means, and another that aims to resolve the scientific and technical debt by disentangling the impact of multiple factors of model improvements (which we argue is still currently underrepresented in the field) — should be conducted, encouraged, and rewarded within the field. We outline two concrete recommendations for striking a better balance between the two lines of work. First, public release of PLMs or their downstream applications should be promptly accompanied by a technical description of the model, ideally in the form of a technical report or a scientific paper. This would enable the community to better understand the key component behind these models’ success, allow future work to replicate the results, and promptly disentangle the different components behind model improvements. Second, we as a community should not necessarily expect both types of contributions under the same paper. Just like how the cleaning up of technical debt happens after the initial code has been written, it is often the case that prior work that resolves the scientific debt through principled comparisons was only conducted after substantial progress in advancing the state-of-the-art (often through all available means for improving model performance) had been made. We should, however, encourage the community to conduct such understanding line of work promptly after major milestones or exciting results.  

Reward and encourage a line of work that focuses on understanding (not just those that chase a new state-of-the-art), even when they are imperfect. The current, rapid pace of the field provides an incentive to spend one’s (finite) computational resources and effort for building the next state-of-the-art, albeit at the expense of scientific rigour and principled comparisons. Given a fi- nite amount of compute, there is arguably more incentive in tuning one’s proposed approach through all possible means ( e.g., using larger datasets and larger models, training for longer, etc.), topping the leaderboards, and publishing the paper, even if this leaves no computational resources to tune the baselines and conduct rigorous ablations. Furthermore, the rapidly increasing cost of training ever-larger PLMs means that any principled comparisons are most likely imperfect (§ 7 ) — e.g., how do our findings in $\\S3$ change with models that are trained for longer, like RoBERTa? Or with encoder-decoder models like T5? Or in other languages? Indeed, our experiments in $\\S3$ are fairly narrow in scope, involving only three nonrecent models (BERT, ELMo, GPT-1) and a training dataset that is small by today’s standards. Yet due to the rigorous hyper-parameter tuning of all three models, conducting these principled comparisons required an enormous amount of compute resources — equivalent to training 10 BERTs from scratch. This cost would have been even higher with the inclusion more models, languages, and larger datasets. On this point, we remark that doing such principled comparisons — even when they are limited in scope and done on smaller models — still contributes towards paying off the scientific debt, better understanding where our current progress is coming from, and deriving valuable insights that can contribute to the development of next generation PLMs. We additionally call on those in our community who serve as reviewers to recognize and reward these types of research contributions, which are complementary (if perhaps equally important) to a parallel line of work that pushes the state-of-the-art in PLM research through all possible means.  

We need more comprehensive PLM scaling laws. Our experiments and recommendations still leave a major open question: How can we scale these kinds of investigations to much larger PLMs, which are much more computationally expensive? To that end, scaling laws (Kaplan et al. ,2020 ;Hoffmann et al. ,2022 ) provide an account of how PLM performance changes with respect to different factors, allowing us to accurately extrapolate that a PLM with X parameters and Y training steps should achieve a perplexity of Z. However, we argue that current scaling laws are still overly narrow in scope: Concretely, existing scaling laws often only apply to decoder-only / unidirectional PLMs, and only provide an account of how their performance changes with respect to (i) model size and (ii) the number of training tokens. We call on the community to develop more comprehensive scaling laws that take into account and characterize how other factors impact LM performance and downstream behavior, including how model performance and behavior change with respect to the choice of the objective function and model hyper-parameters, and the quality of the pretraining data. The existence of such scaling laws — which can happen by pooling community data on various PLM pre-training runs and their corresponding perplexity and downstream performance — would allow other researchers to accurately extrapolate how their findings would generalize to other PLM model sizes, objective functions, etc. Most importantly, comprehensive scaling laws can disentangle and quantify how these different factors contribute to determine the final model performance under various experimental conditions.  

How conducting rigorous experiments and ablation studies can lead to new state-of-the-art results. Lastly, we argue that conducting rigorous experiments and ablation studies for paying off the scientific debt should not necessarily come at the expense of achieving a new state-of-theart. In contrast, doing so can be a key ingredient for building the next state-of-the-art PLMs. In 2020, Kaplan et al. (2020 ) proposed a seminal scaling law that showed how larger PLMs are more sample-efficient, and that one should always increase model size when increasing the pre-training compute budget, leading the community to develop ever-larger PLMs in response ( Rae et al. ,2021 ;Smith et al. ,2022 ,inter alia ). Nevertheless, subsequent rigorous experiments from Hoffmann et al. (2022 ) demonstrated that the optimal pretraining compute allocation should, in fact, also be scaled in another dimension: The amount of pretraining data that the model is trained on. This insight was then used to build smaller, more efficient, and cheaper-to-run PLMs that, at the time of its release, achieved new state-of-the-art results that outperformed much larger PLMs that were undertrained in comparison. Going forward, we conjecture that rigorous experiments and ablation studies that look at factors above and beyond model size and data quantity, such as the quality of the pre-training data, the exact hyper-parameters, the pre-training objective, etc., will not only be useful to understand how these factors improve performance and thus pay off the scientific debt, but also form a key ingredient for building the next generation of better PLMs.",6
