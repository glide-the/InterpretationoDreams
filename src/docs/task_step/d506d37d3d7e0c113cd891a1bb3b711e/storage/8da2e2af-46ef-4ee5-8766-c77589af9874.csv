角色,内容,分镜
8da2e2af-46ef-4ee5-8766-c77589af9874,分析近几年研究领域的技术框架与方法论,0
8da2e2af-46ef-4ee5-8766-c77589af9874,近几年在计算机科学相关研究领域，Transformer、GAN、BERT 等框架是如何具体围绕有效处理数据特征和优化模型结构来提升性能的？ ,0
8da2e2af-46ef-4ee5-8766-c77589af9874,"ref_ids: 454847723981641856, chunk_ids: 1, Score: 0.4590, Text: # 2 RELATED WORK
Pre-trained language models. Transformer (Vaswani et al., 2017) is a sequence-to-sequence model that heavily uses the concept of self-attention. Later on, numerous transformer-based models have been proposed and show overwhelming performance on natural language processing (NLP) and on computer vision (CV) tasks. Devlin et al. (2019) designed BERT that pre-train deep bidirectional representations from unlabeled text and reached powerful performance. Liu et al. (2019) found that BERT were terribly undertrained and proposed RoBERTa, an enhanced training recipe for BERT which can greatly boost the performance. He et al. (2020) proposed decoding-enhanced BERT with disentangled attention (DeBERTa) that incorporates the disentangled attention mechanism and an improved mask encoder to enhance BERT and RoBERTa. More variants like XL-Net, Albert, and Electra have also been proposed in recent years (Yang et al., 2019; Lan et al., 2019; Clark et al., 2019). The series of GPT models (Radford et al., 2019; Brown et al., 2020) are later developed based on transformers decoder blocks rather than encoder blocks like BERT, which again have shown superior performance on different tasks. These large models pretrained on a large amount of unlabelled texts would need to be further fine-tuned on downstream tasks for better performance.  

One of the accompanying disadvantages of these pre-training models with tremendous parameter counts (e.g., 175 B in GPT-3) is the unaffordable computational cost for further fine-tuning.  

Pruning and Low-rank decomposition. Pruning is a widely-used model compression technique. It can reduce the number of parameters inside models, which possibly brings training and inference efficiency. Along with weight pruning method (Han et al., 2015b) being one of the most effective methods (Gordon et al., 2020), various criterion have been proposed to select insignifi- cant weights for pruning, such as Taylor approximation (Molchanov et al., 2019), Hessian score approximation (Hassibi & Stork, 1993), and other saliency scores such as SNIP (Lee et al., 2018), GraSP (Wang et al., 2019) and SynFlow (Tanaka et al., 2020). Several pruning methods have been commonly adapted to compress language models (McCarley et al., 2019; Gordon et al., 2020; Sanh et al., 2020; Wang et al., 2020; Chen et al., 2021). Specifically, McCarley et al. (2019) proposed to prune attention heads that had less contribution to the model. Wang et al. (2020) pruned BERT models by involving low-rank factorization and $\\ell_{0}$ regularization. Sanh et al. (2020) invented an improved version of magnitude pruning ( i.e. , pruning based on the weight change) that can better suit the transfer learning. Chen et al. (2021) performed structured pruning on BERT via $\\ell_{1}$ sparse regularization, which reduced a large portion of parameters and decreased the training cost.  

Low-rank approximation (Ye, 2005) is also vastly studied. One classical scenario is robust principle component analysis (Cand\\`es et al., 2011), which decomposes a matrix into a low-rank plus a sparse component. Existing literature shows that in deep learning, the learned over-parameterized models often naturally bear approximate low-rank weight structures (Oymak et al., 2019; Yu et al., 2017). Some (Jaderberg et al., 2014; Povey et al., 2018; Sainath et al., 2013; Zhang et al., 2014; Zhao et al., 2016) have explicitly imposed the low-rank constraint during training. Wang et al. (2020); Hu et al. (2021) utilized low-rank decomposition to shrink the model size and trim down the trainable parameters during fine-tuning. However, to our best knowledge, integrating sparsity and low-rank structures has never been studied before for efficient fine-tuning of pre-trained language models.  

Parameter-efficient adaptation. Parameter-efficient adaptation aims at reducing the number of trainable parameters when fine-tuning the models across different downstream domains. Unlike pruning, it generates sparse updates instead of building sparse models. Various approaches are invented to achieve the goal. Rebuffiet al. (2017); Houlsby et al. (2019) inserted and only trained adapters between existing layers, whose parameters are much less compared to the pretrained models. Guo et al. (2020) leveraged $\\ell_{0}$ regularization to limit the number of non-zero elements in the update vectors. Lester et al. (2021); Li & Liang (2021) introduced efficient prompt tuning which optimizes only a small continuous task-specific vector. Hu et al. (2021) proposed a low-rank decomposition-based method that can also significantly reduce the number of trainable parameters. However, fine-tuned models yielded by these methods work have the same amount of weights as the pre-trained starting point; hence they contribute no resource efficiency of the final model.

# 3 METHODOLOGY
In this section, we begin by describing our notations and definitions of sparsity generation and parameter-efficient fine-tuning in Section 3.1. Then, we introduce the (dually) sparsity-embedded efficient fine-tuning algorithms in Sections 3.2 and 3.3.

# 3.1 PRELIMINARIES
Sparsity generation and resource-efficient fine-tuning. We adopt both unstructured and structured pruning methods to produce sparsity. They can lead to resource-efficiency including memory and computation savings.  

$\\boldsymbol{\\mathscr{W}}\\,\\in\\,\\mathbb{R}^{m\\times n}$ denote a weight matrix. The goal of ning is to fi a binary mas $s\\ \\in$ $\\{0,1\\}^{\\|\\mathcal{W}\\|_{0}}$ {results in a sparse weight structured pruning, it helps save computational cost since the sparse weights can be smaller in size }, where $\\|\\mathcal{W}\\|_{0}$ W ⊙S . For unstructured pruning, only memory cost is saved; but for umber of parameters in W. The mask Sis applied to Wand by wiping out all-zero columns or rows. However, the performance of networks after structured pruning is often shown to be inferior compared with the unstructured pruning counterpart.  

Parameter-efficient fine-tuning. stream rn task ific weight update To leverage $\\Delta{\\mathcal{W}}$ knowledge in pre-trained weights via fine-tuning and generate predictions with $\\mathcal{W}$ , downweights massive resources as the size of the pre-trained model increases. Parameter-efficient fine-tuning try W$\\mathcal{W}\\!+\\!\\Delta\\mathcal{W}$ W. Since $\\Delta{\\mathcal{W}}$ Whas the same size of W, learning the update matrices usually requires to solve this problem by using as few trainable parameters as possible to represent $\\Delta{\\mathcal{W}}$ , while maintaining competitive downstream fine-tuning performance. Previous literature reaches the goal via either sparsifying weigh a matrices $\\Delta{\\mathcal{W}}$ (Guo et al., 2020) or leveraging low-rank decomposed matrices to compute $\\Delta{\\mathcal{W}}$ W(Hu et al., 2021).  

  
Figure 1: The overview of our proposals. The sparse masks can have unstructured or structured patterns, hich leads to training and erence efficiency. During the fine-tuning, we only train decomposed matrices $\\mathcal{U}$ ,Vand non-zero elements in S$S_{2}$ 2 .  

<html><body><table><tr><td>Algorithm 1: Sparsity-Embedded Low- Rank Decomposition</td></tr><tr><td>Input: Pretrained weights W, number of non-zero elements N Output: Sparse matrices S2 Initialize S2 to be an empty set. foreachself-attentionprojectionweights w inW do */</td></tr><tr><td>/+ Decomposition Perform matrix decomposition: w; ≈ UV + S' by solving the optimization problem in Eqn.1. /* Identify important</td></tr><tr><td>elements to form S2 */ Perform thresholding on S': Keep N elements in S' with top magnitudes, and set the rest 0. Append S' into S2. end</td></tr></table></body></html>  

<html><body><table><tr><td>Algorithm2:DSEE</td></tr><tr><td>Input: Pretrained weights W, number of non-zero elements N, desired sparsity s, loss function Output: Sparse mask S1, matrices U,V, S2 Decompose W into U,V and S2</td></tr><tr><td>(Re-)Initialization: U = 0, V ~ N(0,0.02),Ω = indexes of non-zero elements in S2, S = 0 /﹢ I:train before pruning * Train U,V, S with respect to L under the</td></tr><tr><td>constraint of Poc (S) = 0. / II: pruning the model */</td></tr><tr><td>Prune (1-s%) parameters in W globally by sorting the magnitude of W + UV + S,</td></tr><tr><td>deriving the sparsity mask S1 /+ III:tuning after pruning * Tune U, V, S2 for E epochs for recovering the performance.</td></tr></table></body></html>",0
8da2e2af-46ef-4ee5-8766-c77589af9874,"ref_ids: 454847724073130116, chunk_ids: 3, Score: 0.4023, Text: # 2. Related work

# 2.1. Transformer v.s. CNN
Transformer [ 63 ] was introduced in 2017 for NLP tasks. Compared with LSTM, Transformer can not only train in parallel but also achieve better performance. Then many famous NLP models are built on Transformer, including GPT series [ 44 ,45 ,3 ,42 ], BERT [ 12 ], T5 [ 47 ], and OPT [80 ]. For the application of the Transformer in vision tasks, Vision Transformer (ViT) is definitely the seminal work, showing that Transformer can achieve impressive performance after large-scale supervised training. Followup works [ 61 ,76 ,65 ,66 ,18 ] like Swin [ 37 ] continually improve model performance, achieving new state-of-the-art on various vision tasks. These results seem to tell us “Attention is all you need” [ 63 ].  

But it is not that simple. ViT variants like DeiT usually adopt modern training procedures including various advanced techniques of data augmentation [ 10 ,9 ,79 ,77 ,83 ], regularization [ 57 ,25 ] and optimizers [ 28 ,39 ]. Wightman et al. find that with similar training procedures, the performance of ResNet can be largely improved. Besides, Yu et al. [74 ] argue that the general architecture instead of attention plays a key role in model performance. Han et al. [19 ] find by replacing attention in Swin with regular or dynamic depthwise convolution, the model can also obtain comparable performance. ConvNeXt [ 38 ], a remarkable work, modernizes ResNet into an advanced version with some designs from ViTs, and the resulting models consistently outperform Swin [ 37 ]. Other works like RepLKNet [13 ], VAN [ 17 ], FocalNets [ 72 ], HorNet [ 49 ], SLKNet [ 36 ], ConvFormer [ 75 ], Conv2Former [ 22 ], and InternImage [ 64 ]constantly improve performance of CNNs. Despite the high performance obtained, these models neglect efficiency, exhibiting lower speed than ConvNeXt. Actually, ConvNeXt is also not an efficient model compared with ResNet. We argue that CNN models should keep the original advantage of efficiency. Thus, in this paper, we aim to improve the model efficiency of CNNs while maintaining high performance.

# 2.2. Convolution with large kernels.
Well-known works, like AlexNet [ 30 ] and Inception v1 [56 ] already utilize large kernels up to $11\\times11$ and $7\\times7$ , respectively. To improve the efficiency of large kernels, VGG [53 ] proposes to heavily s $3\\times3$ convolutions e In$k\\times1$ $k\\times k$ tant for semantic segmentation and they decompose large ××on v3 [ staking sequentially. For depthwise convoluti [. Besides, Peng 60 ] splits kernels into several groups from 57 ] factorizes et al. $k\\times k$ find that large kernels are impor×convolution into $1\\times k$ $3\\times3$ ××and ixto kernels similar to Inception v3 [ 57 ]. Witnessing the success of Transformer in vision tasks [ 16 ,65 ,37 ], large-kernel convolution is more emphasized since it can offer a large receptive field to imitate attention [ 19 ,38 ]. For example, ConvNeXt adopts kernel size of $7\\times7$ for depthwise convolution by default. To employ larger kernels, RepLKNet [ 13 ]proposes to utilize structural re-parameterization techniques [78 ,14 ] to scale up kernel size to $31\\times31$ ; VAN [ 17 ] sequentially stacks large-kernel depth-wise convolution (DWConv) and depth-wise dilation convolution to obtain $21\\!\\times\\!21$ receptive filed; FocalNets employs a gating mechanism to fuse multi-level features from stacking depthwise convolutions; Recently, SLaK [ 36 ] fac es la ernel $k\\times k$ two small non-square kernels ( Different from these works, we do not aim to scale up larger $k\\!\\times\\!s$ ×and $s\\!\\times\\!k$ ×, where $s<k$ ). kernels. Instead, we target efficiency and decompose large kernels in a simple and speed-friendly way while keeping comparable performance.

# 3. Method

# 3.1. MetaNeXt
Formulation of MetaNeXt Block. ConvNeXt [ 38 ] is a modern CNN model with simple architecture. For each ConvNeXt block, the input $X$ is first processed by a depthwise convolutioin to propagate information along spatial dimensions. We follow MetaFormer [ 74 ] to abstract the depthwise convolution as a token mixer which is responsible for spatial information interaction. Accordingly, as shown in the second subfigure in Figure 2 , the ConvNeXt is abstracted as MetaNeXt block. Formally, in a MetaNeXt block, its input $X$ is firstly processed as  

$$
X^{\\prime}=\\operatorname{TokenMixer}(X)
$$  

import torch.nn as nn  

Algorithm 1 Inception Depthwise Convolution (PyTorchlike Code)   


<html><body><table><tr><td>def</td><td>class InceptionDwConv2d(nn.Module): init__(self， in_channels,</td></tr><tr><td></td><td>square_kernel_size=3, band_kernel_size=11, branch_ratio=1/8): super().—init()</td></tr><tr><td></td><td>gc = int(in_channels * branch_ratio) # channel number of a convolutionbranch</td></tr><tr><td></td><td>self.dwconv_hw = nn.Conv2d(gc, gc, square_kernel_size, padding= square_kernel_size//2, groups=gc)</td></tr><tr><td></td><td>self.dwconv_w = nn.Conv2d(gc, gc, kernel_size =(l, band_kernel_size), padding=(0,</td></tr><tr><td></td><td>band_kernel_size//2), groups=gc) self.dwconv_h = nn.Conv2d(gc, gc,. kernel_size =(band_kernel_size, l), padding=(</td></tr><tr><td></td><td>band_kernel_size//2, 0)， groups=gc) self.split_indexes = (gc, gc, gc, in_channels -""3*gc)</td></tr><tr><td></td><td>def forward(self，x): # B， C， H,， W = x.shape</td></tr><tr><td></td><td>x_hw， x_w， x_h, x_id = torch.split(x， self.</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td>split_indexes, dim=l)</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td>return torch.cat(</td></tr><tr><td></td><td></td></tr><tr><td></td><td>(self.dwconv_hw(x_hw）,</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td>self.dwconv_w(x_w),</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td>x_id),</td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td>self.dwconv_h(x_h),</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td>dim=1)</td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr></table></body></html>  

where $X,X^{\\prime}\\in\\mathbb{R}^{B\\times C\\times H\\times W}$ with $B,C,H$ and $W$ denoting batch size, channel number, height and width, respectively. Then the output from the token mixer is normalized,  

$$
Y=\\operatorname{Norm}(X^{\\prime})
$$  

After normalization [ 27 ,1 ], the resulting features are inputted into an MLP module consisting of two fullyconnected layers with an activation function sandwiched between them, the same as feed-forward network in Transformer [ 63 ]. The two fully-connected layers can also be implemented by $1\\times1$ convolutions. Also, shortcut connection [ 20 ,54 ] is adopted. This process can be expressed by  

$$
Y=\\mathrm{Conv}_{1\\times1}^{r C\\to C}\\{\\sigma[\\mathrm{Conv}_{1\\times1}^{C\\to r C}(Y)]\\}+X,
$$  

where $\\mathrm{Conv}_{k\\times k}^{C_{i}\\rightarrow C_{o}}$ means convolution with kernel size of $k\\times k$ , input channels o ×$C_{i}$ and output channels of $C_{o};r$ is the expansion ratio and σdenotes activation function.  

Comparison to MetaFormer block. As shown in Figure 2 , it can be found that MetaNeXt block shares similar modules with MetaFormer block [ 74 ], e.g. token mixer and MLP. Nevertheless, a critical differentiation between the two models lies in the number of shortcut connections [20 ,54 ]. MetaNeXt block implements a single shortcut connection, whereas the MetaFormer block incorporates two, one for the token mixer and the other for the MLP. From this aspect, MetaNeXt block can be regarded as a result of merging two residual sub-blocks from MetaFormer, thereby simplifying the overall architecture. As a result, the MetaNeXt architecture exhibits a higher speed compared to MetaFormer. However, this simpler design comes with a limitation: the token mixer component in MetaNeXt cannot be complicated ( e.g. , Attention) as shown in our experiments (Table 3 ).  

Instantiation to ConvNeXt. As shown in Figure 2 , in ConvNeXt, the token mixer is simply implemented by a depthwise convolution,  

$$
X^{\\prime}=\\operatorname{TokenMixer}(X)=\\operatorname{DWConv}_{k\\times k}^{C\\to C}(X)
$$  

kernel size of where $\\mathrm{DWConv}_{k\\times k}^{C\\rightarrow C}$ $k\\times k$ ×. In ConvNeXt, denotes depthwise convolution with $k$ is set as 7 by default.",0
8da2e2af-46ef-4ee5-8766-c77589af9874,"ref_ids: 454845616393828768, chunk_ids: 2, Score: 0.3945, Text: # 2 Related Work

# 2.1 Transformers for Graph
Recently, Transformer [ 33 ] has shown its superiority in an increasing number of domains [ 7 ,8 ,40 ], e.g. Bert [ 7 ] in NLP and ViT [ 8 ] in CV. Existing works attempting to generalize Transformer to graph data mainly focus on two problems: (1) How to design dedicated positional encoding for the nodes; (2) How to alleviate the quadratic computational complexity of the vanilla Transformer and scale the Graph Transformer to large graphs. As for the positional encoding, GT [ 9 ] firstly uses Laplacian eigenvectors to enhance node features. Graph-Bert [ 41 ] studies employing Weisfeiler-Lehman code to encode structural information. Graphormer [ 38 ] utilizes centrality encoding to enhance node features while incorporating edge information with spatial (SPD-indexed attention bias) and edge encoding. SAN [ 21 ] further replaces the static Laplacian eigenvectors with learnable positional encodings and designs an attention mechanism that distinguishes local connectivity. For the scalability issue, one immediate idea is to restrict the number of attending nodes. For example, GAT [ 34 ] and GT-Sparse [9 ] only consider the 1-hop neighboring nodes; Gophormer [ 46 ] uses GraphSAGE [ 11 ] sampling to uniformly sample ego-graphs with pre-defined maximum depth; Graph-Bert [ 41 ] restricts the receptive field of each node to the nodes with top$\\cdot\\mathbf{k}$ intimacy scores (e.g., Katz and PPR). However, these fixed node sampling strategies sacrifice the advantage of the Transformer architecture. SAC [22 ] tries to use an LSTM edge predictor to predict edges for self-attention operations. However, the fact that LSTM can hardly be parallelized reduces the computational efficiency of the Transformer.

# 2.2 Sparse Transformers
In parallel, many efforts have been devoted to reducing the computational complexity of the Transformer in the field of NLP [ 23 ] and CV [ 32 ]. In the domain of NLP, Longformer [ 2 ] applies block-wise or strode patterns while only fixing on fixed neighbors. Reformer [ 19 ] replaces dot-product attention by using approximate attention computation based on locality-sensitive hashing. Routing Transformer [30] employs online $\\mathbf{k}$ -means clustering on the tokens. Linformer [35] demonstrates that the self-attention mechanism can be approximated by a low-rank matrix and reduces the complexity from ${\\mathcal{O}}(n^{2})$ scheme which brings greater efficiency by limiting self-attention computation to non-overlapping to $\\mathcal{O}(n)$ . As for vision transformers, Swin Transformer [ 24 ] proposes the shifted windowing local windows while also allowing for cross-window connection. Focal Transformer [ 37 ] presents a new mechanism incorporating both fine-grained local and coarse-grained global attention to capture short- and long-range visual dependencies efficiently. However, these sparse transformers do not take the unique graph properties into consideration.

# 2.3 Graph Neural Networks and Node Sampling
Graph neural networks (GNNs) [ 18 ,11 ,12 ,44 ,43 ,31 ,45 ,42 ] follow a message-passing schema that iteratively updates the representation of a node by aggregating representations from neighboring nodes. When generalizing to large graphs, Graph Neural Networks face a similar scalability issue. This is mainly due to the uncontrollable neighborhood expansion in the aggregation stage of GNN. Several node sampling algorithms have been proposed to limit the neighborhood expansion, which mainly falls into node-wise sampling methods and layer-wise sampling methods. In node-wise sampling, each node samples $k$ neighbors from its sampling distribution, then the total number of nodes in the $l$ -th layer becomes $\\mathcal{O}(\\breve{k}^{l})$ . GraphSage [ 11 ] is one of the most well-known node-wise sampling methods with the uniform sampling distribution. GCN-BS [ 25 ] introduces a variance reduced sampler based on multi-armed bandits. To alleviate the exponential neighbor expansion $\\mathcal{O}(k^{l})$ of the node-wise samplers, layer-wise samplers define the sampling distribution as a probability of sampling nodes given a set of nodes in the upper layer [ 4 ,16 ,49 ]. From another perspective, these sampling methods can also be categorized into fixed sampling strategies [ 11 ,4 ,49 ] and adaptive strategies [ 25 ,39 ]. However, none of the above sampling methods in GNNs can be directly applied in Graph Transformer as Graph Transformer does not follow the message passing schema.

# 3 Preliminaries

# 3.1 Problem Definition
Let $G=(A,X)$ denote t ted graph where $A\\in\\mathbb{R}^{n\\times n}$ represents the symmetric adjac matrix with nnodes, and $X\\in\\mathbb{R}^{n\\times p}$ ∈is the attribute matrix of pattributes r node. The element $A_{i j}$ in the adjacency matrix equals to 1 if there exists an edge between node $v_{i}$ and node $v_{j}$ , otherwise $A_{i j}~=~0$ . The label of node $v_{i}$ is $y_{i}$ . In the node classification problem, the classifier has the knowledge of the labels of a subset of nodes $V_{L}$ . The goal of semi-supervised node classification is to infer the labels of nodes in $V\\backslash V_{L}$ by learning a classification function.

# 3.2 Transformer Architecture
The Transformer architecture consists of a series of Transformer layers [ 33 ]. Each Transformer layer has two parts: a multi-head self-attention (MHA) module and a position-wise feed-forward network (FFN). Let $\\mathbf{H}=\\left[h_{1},\\cdot\\cdot\\cdot\\mathbf{\\Phi},h_{m}\\right]^{\\top}\\in\\mathbb{R}^{m\\times d}$ denote the input to the self-attention module where $d$ is  

the hidden dimension, $h_{i}\\in\\mathbb{R}^{d\\times1}$ is the hidden represe ation at position $i$ , and $m$ is the number of positions. The MHA module firstly projects the input Hto query-, key-, value-spaces, denoted as $\\mathbf{Q},\\mathbf{K},\\mathbf{V}$ , using three matrices $\\mathbf{W}_{Q}\\in\\mathbb{R}^{d\\times d_{K}}$ ,$\\mathbf{W}_{K}\\in\\mathbb{R}^{d\\times d_{K}^{\\textbf{\\texttt{L}}}}$ and $\\dot{\\mathbf{W}_{V}}\\in\\mathbb{R}^{d\\times d_{V}}$ :  

$$
\\mathbf{Q}=\\mathbf{H}\\mathbf{W}_{Q},\\quad\\mathbf{K}=\\mathbf{H}\\mathbf{W}_{K},\\quad\\mathbf{V}=\\mathbf{H}\\mathbf{W}_{V}.
$$  

Then, in each head $i\\,\\in\\,\\{1,2,\\dots,B\\}$ ($B$ is the to heads), the scaled dot-product attention mechanism is applied to the corresponding {$\\{\\mathbf{Q}_{i},\\mathbf{K}_{i},\\mathbf{V}_{i}\\}$ }:  

$$
\\mathrm{head}_{i}=\\mathrm{Softmax}\\left(\\frac{\\mathbf{Q}_{i}\\mathbf{K}_{i}^{T}}{\\sqrt{d_{K}}}\\right)\\mathbf{V}_{i}.
$$  

Finally, the outputs from different heads are further concatenated and transformed to obtain the final output of MHA:  

$$
\\mathrm{MHA}(\\mathbf{H})=\\mathrm{Concat}\\left(\\mathbf{\\Pi}_{\\mathrm{head}_{1},\\mathbf{\\Pi}_{\\cdot}\\mathbf{\\Pi}_{\\cdot},\\mathrm{...},\\mathrm{head}_{B}}\\right)\\mathbf{W}_{O},
$$  

where $\\mathbf{W}_{O}\\in\\mathbb{R}^{d\\times d}$ . In this work, we employ $d_{K}=d_{V}=d/B$ .",0
