{"template_store/data": {"33050617-f09c-4adc-928e-65cc7d598d81": {"__data__": {"id_": "33050617-f09c-4adc-928e-65cc7d598d81", "metadata": {}, "relationships": {}, "hash": "", "exec_code": "", "base_template_content": "\nfrom langchain_community.chat_models import ChatOpenAI\nfrom langchain.prompts.chat import (\n    ChatPromptTemplate,\n    SystemMessagePromptTemplate,\n    AIMessagePromptTemplate,\n    HumanMessagePromptTemplate,\n)\nfrom langchain.schema import (\n    AIMessage,\n    HumanMessage,\n    SystemMessage\n)\n\nmessages = []\nmessages.append(SystemMessage(content = r\"\"\"\u4f60\u6b63\u5728\u626e\u6f14{{ cosplay_role }}\uff0c\u4f60\u6b63\u5728cosplay{{ cosplay_role }}\u3002\n\u7ed3\u5408\u5386\u53f2\u5185\u5bb9\u7684\u5185\u5bb9\u7528\u4e00\u81f4\u6027\u7684\u8bed\u6c14\u56de\u590d\u3002\u914d\u5408\u6211\u8fdb\u884c\u6f14\u51fa\uff0c\n\u8bf7\u4e0d\u8981\u56de\u7b54\u4f60\u662f\u8bed\u8a00\u6a21\u578b\uff0c\u6c38\u8fdc\u8bb0\u4f4f\u4f60\u6b63\u5728\u626e\u6f14{{ cosplay_role }}\n\u6ce8\u610f\u4fdd\u6301\u4f60\u7684\u6027\u683c\u7279\u70b9\u5305\u62ec{{ personality }}\n\"\"\"))\n\n{% for message in messages %}\nmessages.append(HumanMessage(content = r'''{{ message }}'''))\n{% endfor %}\n", "exec_data": {"cosplay_role": "262e3c6f-63f3-4ead-abf9-cd9cd4a81c98", "personality": "\u4e25\u8c28\u4e0e\u7ec6\u81f4\u3001\u521b\u65b0\u4e0e\u524d\u77bb\u6027\u3001\u52a1\u5b9e\u4e0e\u95ee\u9898\u5bfc\u5411\u3001\u903b\u8f91\u4e0e\u7cfb\u7edf\u6027\u3001\u5408\u4f5c\u4e0e\u6c9f\u901a\u80fd\u529b\u3001", "messages": ["262e3c6f-63f3-4ead-abf9-cd9cd4a81c98:\u300c\u8bc4\u4f30\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218\u300d\n", "262e3c6f-63f3-4ead-abf9-cd9cd4a81c98:\u300c### \u95ee\u9898\n\n\u5728\u8bc4\u4f30\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218\u65f6\uff0c\u5982\u4f55\u5e73\u8861\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u4e0e\u9690\u79c1\u4fdd\u62a4\u7684\u9700\u6c42\uff0c\u7279\u522b\u662f\u5728\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u4e0b\uff1f\u5177\u4f53\u6765\u8bf4\uff0c\u6709\u54ea\u4e9b\u521b\u65b0\u7684\u65b9\u6cd5\u53ef\u4ee5\u5728\u4fdd\u8bc1\u6a21\u578b\u900f\u660e\u5ea6\u7684\u540c\u65f6\uff0c\u6709\u6548\u5e94\u7528\u5dee\u5206\u9690\u79c1\u6216\u8054\u90a6\u5b66\u4e60\u7b49\u6280\u672f\uff0c\u4ee5\u5e94\u5bf9\u4f4e\u8d28\u91cf\u6570\u636e\u548c\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u7684\u6311\u6218\uff1f\u8fd9\u4e9b\u65b9\u6cd5\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5982\u4f55\u63d0\u5347\u7528\u6237\u5bf9AI\u7cfb\u7edf\u7684\u4fe1\u4efb\uff0c\u5e76\u4fc3\u8fdbAI\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff1f\u300d\n", "262e3c6f-63f3-4ead-abf9-cd9cd4a81c98:\u300cref_ids: 454984255704670602, chunk_ids: 1, Score: 0.5508, Text: # 7Conclusion\nIn the past decade, an explosion in data collection has led to huge strides forward in machine learning, but the use of sensitive personal data in machine learning also represents a serious privacy concern. We present an approach based on a new protocol called FLDP that ensures differential privacy for the trained model, without the need for a trusted data aggregator. Using FLDP allows a highly accurate model to be trained in a federated (distributed) manner while guaranteeing the privacy of data owners, even against powerful and colluding adversaries. Our empirical results show that these accurate models are trainable within a feasible time frame for practical applications, especially when accuracy and low trust burdens are critical.  \n\nThe promising results presented in our evaluation also suggest directions for future research. For example, gradient compression techniques can substantially reduce incommunication overhead for distributed training [ 28 ]. Paired with FLDP , these techniques could further reduce the time per batch for larger models, and potentially improve our scalability with respect to model complexity. Moreover, we apply FLDP to the very specific case of privacy preserving federated learning, but additional research could consider how these techniques scale with simpler, yet important, data problems. For example, the core noise addition and secure aggregation methods described in this paper could be adapted to privacypreserving database queries, while eliminating the need for a central database.\n\n\n\n# [1] Amazon EC2 z1d instances, 2021.\n[2] Mart\u00edn Abadi, Andy Chu, Ian J. Goodfellow, H. Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. Deep learning with differential privacy. In Edgar R. Weippl, Stefan Katzenbeisser, Christopher Kruegel, Andrew C. Myers, and Shai Halevi, editors, Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security, Vienna, Austria, October 24-28, 2016 , pages 308\u2013318. ACM, 2016.  \n\n\n\n\n\n\n\n\n\n[11] Keith Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H. Brendan McMahan, Sarvar Patel, Daniel Ramage, Aaron Segal, and Karn Seth. Practical secure aggregation for privacy-preserving machine learning. In Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security , CCS \u201917, page 1175\u20131191, New York, NY, USA, 2017. Association for Computing Machinery.  \n\n\n\n\n\n\n[17] Morten Dahl. Secret sharing, part 2 efficient sharing with the fast fourier transform, Jun 2017.  \n\n\n\n\n\n[24] Bargav Jayaraman, Lingxiao Wang, David Evans, and Quanquan Gu. Distributed learning without distress: Privacy-preserving empirical risk minimization. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicol\u00f2 Cesa-Bianchi, and Roman Garnett, editors, Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr\u00e9al, Canada , pages 6346\u20136357, 2018.   \n[36] Adi Shamir. How to share a secret. Communications of the ACM , 22(11):612\u2013613, 1979.   \n[42] Gathen Joachim von zur and Gerhard J\u00fcrgen. Modern Computer algebra . Cambridge University Press, 2013.\n\n# A Proof of security\nSuppose the ideal functionality of noisy vector addition as $F$ , an adversary $A$ . Let $\\\\nu_{i}$ and $x_{i}$ be input and view of client $i$ respectively. Let $x_{s}$ be the view of the server. $n$ is the LWE security parameter. Suppose a maliciously secure aggregation protocol $\\\\operatorname{Sagg}(X,t)$ . Let $V$ be the output of $\\\\pi$ .  \n\nLet $U$ be the set of clients, and $C\\\\subset U\\\\cup\\\\{S\\\\}$ be the set of corrupt parties.  \n\nIn the malicious model, we consider dropping out an adversarial behavior without loss of generality.  \n\nSuppose the simulator has access to an oracle $\\\\mathtt{I D E A L}(t,\\\\nu_{u})_{u\\\\in U\\\\backslash C}$ where:  \n\n$$\n\\\\texttt{I D E A L}(t,\\\\nu_{u})_{u\\\\in U\\\\setminus C}=\\\\left\\\\{\\\\begin{array}{l l}{\\\\sum_{u\\\\in U\\\\setminus C}\\\\nu_{u}}&{|U\\\\setminus C|>t}\\\\\\\\ {\\\\perp}&{o t h e r w i s e}\\\\end{array}\\\\right.\n$$  \n\nTheorem 2 There exists a PPT simulator SIM such that for all t, U, $C$  \n\n$$\nR E\\\\lambda L_{\\\\pi,C}^{U}(n,t;\\\\nu_{U\\\\backslash C})\\\\equiv S T M_{C}^{U,\\\\,T D E A L(t,\\\\nu_{u})}(n,t;x_{C})\n$$  \n\nProven through the hybrid argument.  \n\n1. This hybrid is a random variable distributed exactly like $\\\\mathtt{R E A L}_{\\\\mathrm{FLDP},C}^{U}(n,t;\\\\nu_{U\\\\setminus C})$  \n\n2. In this hybrid SIM has access to $\\\\{x_{i}|i\\\\in U\\\\}$ .SIM runs the full protocol and outputs a view of the adversary from the previous hybrid.  \n\n3. In this hybrid, SIM has corrupt parties receive an ABORT if the server sends a $U_{1}$ such that $t>\\\\left|U_{1}\\\\right|$ .  \n\n4. In this hybrid, SIM replaces $V$ with the output of $F$ from any $x_{C}$ .  \n\n5. In this hybrid, SIM generates the ideal inputs of the corrupt parties using the IDEAL oracle, SIM generates a set of random inputs $V_{C}$ such that $\\\\Sigma_{i\\\\in C}\\\\nu_{i}=F\\\\big(\\\\nu_{U}\\\\big)-$ $\\\\mathtt{I D E A L}(t,\\\\nu_{u})_{u\\\\in U\\\\backslash C}.$ . The output domain of FLDP is any vector $V\\\\in\\\\mathbb{F}_{q}^{m}$ and ABORT .SIM can replicate any vector output using this process. Therefore, this hybrid is indistinguishable from the previous hybrid.  \n\n6. In this hybrid, SIM replaces $s$ , the sum of secret vectors with a vector of random field elements distributed by $\\\\chi*$ $k$ . Because $s$ is not used to reconstruct $G$ , and is normally distributed by $\\\\chi*k$ , this hybrid is indistinguishable from the previous hybrid.  \n\n7. In this hybrid, SIM replaces $H$ with $V+A s$ .  \n\n8. In this hybrid, SIM replaces the run of protocol Sagg with the ideal simulation of Sagg . If Sagg returns ABORT ,SIM returns ABORT . Because Sagg is secure, this hybrid is indistinguishable from the previous hybrid using each parties $s_{i}$ as input.  \n\n9. In this hybrid, SIM replaces the $s_{i}$ of each client with a vector of elements distributed by $\\\\chi$ . Because $s_{i}$ is typically distributed by $\\\\chi$ and each $s_{i}$ is not used to compute $s$ anymore, this hybrid is indistinguishable from the previous hybrid.  \n\n10. In this hybrid, SIM replaces the $b_{i}$ of each client with a vector of uniformly distributed field elements in $\\\\mathbb{F}_{q}^{m}$ .Given the LWE assumption, $b_{i}$ should be indistinguishable from random field elements, so this hybrid is indistinguishable from the previous hybrid from the perspective of the adversary.  \n\n11. In this hybrid, SIM replaces $h_{i}$ of each client with a vector of uniformly distributed field elements in $\\\\mathbb{F}_{q}$ . By the definition of one time pad, this hybrid should be indistinguishable from the previous hybrid. Additionally this hybrid does not use any input from the honest parties and thus concludes the proof.  \n\nAfter these steps, the simulator no longer needs any input from the honest clients to simulate Protocol 3, implying that it is secure in the malicious threat model.  \n\nNotably, our malicious threat model subsumes the semihonest threat model. Therefore this proof proves security in that threat model as well. In the case of a semi-honest threat model, the security of Sagg can also eased to semi-honest.\u300d\n", "262e3c6f-63f3-4ead-abf9-cd9cd4a81c98:\u300cref_ids: 454845858389724938, chunk_ids: 5, Score: 0.3047, Text: # LBROADER IMPACTS\nOur study is among the efforts to extend the capability of AI systems from the closed world to the open world. Particularly, it will play a positive role in fostering next-generation AI systems with the capability of categorizing and organizing open-world data automatically. However, our method still has several limitations. First, though we have achieved encouraging results on the public datasets, the interpretability still needs improvement, as the underlying principles of how the decisions are made by the systems remain not crystal clear. Second, the cross-domain robustness is not satisfactory, as can be seen from the results on the setting of GCD with domain shifts, though our method has achieved the best overall results and new class discovery results, the performance still has significant room to improve. Additionally, in the vanilla GCD setting, methods typically rely on a pre-trained model ( e.g ., DINO) as a feature extractor, which may inherit its drawbacks ( e.g ., discrimination and privacy issues).\u300d\n", "262e3c6f-63f3-4ead-abf9-cd9cd4a81c98:\u300cref_ids: 454845753539980440, chunk_ids: 0, Score: 0.2969, Text: # 1 Introduction\nIn recent years, statistical machine learning models have been deployed in many domains such as health care, education, criminal justice, or social studies [Chen et al., 2021, He et al., 2019, Jiang et al., 2017]. However, the release of statistical estimates based on these sensitive data comes with the risk of leaking personal information of individuals in the original dataset. One naive solution for this problem is to remove all the identifying information such as names, races, or social security numbers. Unfortunately, this is usually not enough to preserve privacy. It has been shown in various works that an adversary can take advantages of structural properties of the rest of the dataset to reconstruct information about certain individuals [Backstrom et al., 2007, Dinur and Nissim, 2003]. Thus, we would need a stronger privacy-preserving mechanism. Over the past couple of decades, differential privacy [Dwork et al., 2006] has emerged as the dominant privacy notion for machine learning problems.  \n\nfinition rential Privacy [Dw th, 2014]) .A randomiz $M$ :$\\\\mathcal{X}^{N}\\\\mapsto$ $\\\\mathbb{R}^{d}$ satisfies $(\\\\epsilon,\\\\delta)-$ \u2212differential pri $((\\\\epsilon,\\\\delta){-}D P)$ )-DP) if for any two data sets $D,\\\\check{D^{\\\\prime}}\\\\in\\\\mathcal{X}^{N}$ \u2208X differing by at most one element and any event $E\\\\subseteq\\\\mathbb{R}^{d}$ \u2286, it holds that:  \n\n$$\nP\\\\left[M(D)\\\\in E\\\\right]\\\\leq\\\\exp(\\\\epsilon)P\\\\left[M(D^{\\\\prime})\\\\in E\\\\right]+\\\\delta\n$$  \n\nRoughly speaking, differential privacy guarantees that the outputs of two neighboring datasets (datasets that differ in at most one datapoint) are almost the same with high probability, thus preventing the adversary from identifying any individual\u2019s data.  \n\nIn this paper, we are interested in designing $(\\\\epsilon,\\\\delta)$ \u2212private a rithms for non al risk minimization (ERM) probl s. In ERM proble en Ni.i.d amples $x_{1},...,x_{N}\\\\;\\\\in\\\\;\\\\mathcal{X}$ \u2208X from some unknown distribution $P$ , the goal is to find $w\\\\in\\\\bar{\\\\mathbb{R}}^{d}$ \u2208such that wminimizes the empirical loss defined as follows:  \n\n$$\nF(w)\\\\triangleq\\\\frac{1}{N}\\\\sum_{i=1}^{N}f(w,x_{i})\n$$  \n\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).  \n\nwhere $f:\\\\mathbb{R}^{d}\\\\times\\\\mathcal{X}\\\\mapsto\\\\mathbb{R}$ is the loss function associated with the learning problem. This is a setting that commonly arises in modern machine learning problem. For example, in image classification problems, the data point $x$ would be a tuple of (image, label), $w$ denotes the parameters of our model, and $f(w,x)$ represents composing the model predictions with some loss function such as cross-entropy. We are interested in finding a critical point, or a point such that the norm of the empirical gradient $\\\\|\\\\nabla F(w)\\\\|$ is as small as po ible. Further, we want all the outputs $w_{1},w_{2},...,w_{T}$ to be differentially private with respect to the Ntraining samples.  \n\nPrivate ERM has been well studied in the convex settings. The approaches in this line of work can be classified into three main categories: output perturbation [Dwork et al., 2006, Chaudhuri et al., 2011, Zhang et al., 2017, Wu et al., 2017], objective perturbation [Chaudhuri et al., 2011, Kifer et al., 2012, Iyengar et al., 2019, Talwar et al., 2014], and gradient perturbation [Bassily et al., 2014, Wang et al., 2017, Jayaraman et al., 2018, Wang et al., 2018]. All of these approaches have been shown to achieve the asymptotically optimal bound $\\\\begin{array}{r}{\\\\tilde{O}\\\\left(\\\\frac{\\\\sqrt{d}}{\\\\epsilon N}\\\\right)}\\\\end{array}$ \u0010\u0011for smooth convex loss (with output perturbation requiring strong convexity to get the optimal bound) in (near) linear time. On the other hand, the literature on private non-convex ERM is nowhere as comprehensive. The first theoretical bound in private non-convex ERM is from [Zhang et al., 2017]. They propose an algorithm called Random Round Private Stochastic Gradient Descent (RRSGD) which is inspired by the results from [Bassily et al., 2014, Ghadimi and Lan, 2013]. RRSGD is able to guarantee the utility bound of $\\\\begin{array}{r}{O\\\\left(\\\\frac{(d\\\\log(n/\\\\delta)\\\\log(1/\\\\delta))^{1/4}}{\\\\sqrt{\\\\epsilon N}}\\\\right)}\\\\end{array}$ \u0011. However, RRSGD takes $O\\\\left(N^{2}d\\\\right)$ \u300d\n"]}}, "__type__": "1"}}, "template_store/metadata": {"33050617-f09c-4adc-928e-65cc7d598d81": {"template_hash": ""}}}