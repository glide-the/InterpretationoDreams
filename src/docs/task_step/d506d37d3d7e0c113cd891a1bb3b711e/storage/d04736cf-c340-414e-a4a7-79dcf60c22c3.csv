角色,内容,分镜
d04736cf-c340-414e-a4a7-79dcf60c22c3,分析最新算法的稳定性与容错性,4
d04736cf-c340-414e-a4a7-79dcf60c22c3,在处理大规模数据时，那些面临内存和计算资源限制导致性能下降的算法，是否可以借鉴采用正则化技术、对抗训练等稳定性优化方法来提升在大规模数据环境下的稳定性和适应性？ ,4
d04736cf-c340-414e-a4a7-79dcf60c22c3,"ref_ids: 454848769189392150, chunk_ids: 4, Score: 0.1855, Text: # 3.2 SEPARATING BATCH STATISTICS IS NOT NECESSARY
BatchNorm is a widely used normalization layer shown to improve performance and training stability of image classifiers ( Ioffe & Szegedy ,2015 ). We recall that a BatchNorm layer, given a batch as input, first normalizes it by subtracting the mean and dividing by the standard deviation computed over the entire batch, then it applies an affine transformation, with learnable scale and offset parameters. During training, it accumulates these so-called batch statistics to use during test time, so that the output of the classifier for each image is independent of the other images in the batch. The batch statistics can be seen an approximation of the statistics over the image distribution.  

Xie et al. (2019a ) show that optimizing the co-training loss in Eq. 1 can yield worse results on clean images than simple nominal training. This is especially the case when the network has a low capacity or the attack (i.e., the inner maximization) is too strong (such as using a large perturbation radius $\\epsilon$ ). To solve this issue, they propose AdvProp, which consists in using distinct BatchNorm layers for clean and adversarial images. They argue that “maintaining one set of [BatchNorm] statistics results in incorrect statistics estimation” , which could be the reason for the performance degradation. We note that using two sets of BatchNorm layers for the clean and adversarial samples as in AdvProp creates two sets of batch statistics but also two sets of learnable scale and offset parameters. In the following we investigate whether having separate batch statistics is a necessary condition for successful co-training.  

  
Figure 2: Dual parameters are enough. We report the clean (solid lines) and robust accuracy (dashed lines) over training steps of R ES NET -50 trained on I MAGE NET with the co-training loss of Eq. 1 $(\\epsilon=4/255)$ : for models with dual layers. Clean accuracy refers to the clean mode and the robust accuracy to the robust mode .Left panel. We compare models with different normalization layers with no domain-specific parameters (Shared BatchNorm, Shared LayerNorm, Shared GroupNorm) to Dual BatchNorm as proposed by Xie et al. (2019a ): regardless of the type of normalization, the robustness of classifiers without dual layers drops to (almost) zero at the end of training. Right panel. We use domain-specific normalization layers (Dual BatchNorm, Dual LayerNorm, Dual GroupNorm) and a model with BatchNorm with shared batch statistics but domain-specific scale and offset (DualParams BatchNorm): all models achieve high clean and robust accuracy.  

Figure 2 shows the clean and robust accuracy of various model architectures as training progresses. The left panel demonstrates that, if we share both batch statistics and scales/offsets (Shared BatchNorm, orange curves), the robust accuracy (orange dashed line) quickly drops, far from the one obtained by AdvProp (Dual BatchNorm, blue curve) which is above $34\\%$ . However, if we use a single set of batch statistics but specific scales and offsets for clean and adversarial images, we can observe on the right panel of Figure 2 that the robust accuracy (DualParams BatchNorm, orange dashed line) matches the one (blue dashed line) obtained by AdvProp. This demonstrates that it is possible to achieve nominal and robust classification results similar to those of AdvProp without separate batch statistics.  

Furthermore, there exist normalization layers such as LayerNorm ( Ba et al. ,2016 ) or GroupNorm (Wu & He ,2018 ) which do not use batch statistics, as their normalization step is done per sample and not per batch. Hence, according to the hypothesis of Xie et al. (2019a ), these types of normalization layer should not suffer from performance degradation. Nevertheless, the left panel of Figure 2 shows that their robust accuracy (green and red dashed lines) does not match the robust accuracy of AdvProp (Dual BatchNorm), and is unstable over training steps. However, by making the scales and offsets of LayerNorm and GroupNorm specific to clean and adversarial images, their robust accuracy matches that obtained with dual BatchNorm layers, as shown in the right panel of Figure 2 . This suggests that a key element to make the co-training loss of Eq. 1 work for various normalization layers is to have trainable parameters which are specific to the clean and adversarial images.

# 3.3 REVISITING ADAPTERS WITH ADVERSARIAL TRAINING
The last observation strongly relates this setting to the adapters literature where a single backbone architecture has some parameters, called adapters, which are specific to different domains while the rest of the parameters are shared among tasks. In our case, the clean images form one domain and the adversarial images constitute another domain. In this work, we go beyond having separate normalization layers for the clean and adversarial images and investigate other types of adapters.  

Formally, the model parameters $\\pmb{\\theta}$ can be decomposed into parameters $\\psi$ which are shared among domains and parameters $\\phi$ which are specific to a domain. We call $\\phi_{\\mathrm{clean}}$ the parameters used when training on clean images and $\\phi_{\\mathrm{adv}}$ the parameters used when training on adversarial images. For example, in Section 3.2 , when we used dual LayerNorm layers, the scales and offsets of these normalization layers are contained in $\\phi_{\\mathrm{clean}}$ and $\\phi_{\\mathrm{adv}}$ whereas the rest of the model parameters are in $\\psi$ . Based on Eq. 1 , we optimize the following loss:  

$$
\\alpha L(f(\\pmb{x};\\psi\\cup\\phi_{\\mathrm{clean}}),y)+(1-\\alpha)\\operatorname*{max}_{\\pmb{\\delta\\in\\mathbb{S}}}L(f(\\pmb{x}+\\pmb{\\delta};\\psi\\cup\\phi_{\\mathrm{adv}}),y).
$$  

Finally, we introduce some notation for models with a s at inference time: we call $f(\\cdot;\\psi\\cup\\phi_{\\mathrm{clean}})$ the clean mode for prediction as we use the adapters $\\phi_{\\mathrm{clean}}$ trained on the clean data. Conversely, we call $f(\\cdot;\\psi\\cup\\phi_{\\mathrm{adv}})$ the robust mode when using the adapters $\\phi_{\\mathrm{adv}}$ trained on the perturbed data.  

Wortsman et al. (2022 ) propose model soups , which consist in averaging the weights of multiple models fine-tuned from the same pre-trained model. The resulting weight averaged model can benefit from the original models without incurring any extra compute and memory cost at inference time. Currently, in our setting the user would have to know at test time if the network should be in clean or robust mode . A model soup , by its ability to merge models, is a way to bypass this issue. We formulate the hypothesis that training with adapters enables model soups . With this in mind, we observe that training with adapters means that most of the model parameters are already shared, so model souping would simply consist in linearly interpolating the weights of the adapters for the two modes. We call adversarial model soups , the model soups with a model co-trained on clean and adversarial samples. We get the following parametrized model:  

$$
f(\\cdot;\\psi\\cup(\\beta\\phi_{\\mathrm{clean}}+(1-\\beta)\\phi_{\\mathrm{adv}}))
$$  

where $\\beta$ is the weighting factor when averaging the adapters. If $\\beta=1$ , the model soup boils down to the clean mode and conversely $\\beta=0$ corresponds to the robust mode . In Section 5.2 , we assess this hypothesis and show that forming model soups between independent nominal and robust models fails.",4
d04736cf-c340-414e-a4a7-79dcf60c22c3,"ref_ids: 454895453635358570, chunk_ids: 6, Score: 0.1680, Text: # 6.2 Implementation
To fairly evaluate our method against the baselines, we use the same set of hyperparameters for all the methods, for a given dataset. For details, see Appendix D. The code for Pyramid-BERT is made available as a supplementary material with the submission. The training and inference jobs are run separately on a NVIDIA Tesla V 100 GPU machine and a Intel Xeon Platinum 8000 series CPU machine respectively. All the accuracy and speedup scores are averaged over 20 trials.

# 6.3 Results on GLUE benchmarks
We first examine the trade-off between accuracy and speedup. The accuracy results for $3X$ and $1.5X$ speedup are summarized in Table 1 and 2 respectively. The results for $3.5X$ and $2X$ speedup are given in the Table 11 and 13 in Appendix E.We observe that as the speedup increases the gap between our Coreset-select-opt and its competitors becomes large, where for $3X$ speedup, Coreset-select-opt outperforms the second best method Attention-select by $1\\%$ accuracy in average and beats the standard baselines by $2\\%$ or more. The Average-pool performs the worst in average across the GLUE benchmarks, specially on the COLA dataset. For detailed justification, see Appendix E. To better understand the performance of Coreset-select-opt with different values of $m$ ,an ablation study is shown in Section 7 . For mild speedup of $1.5X$ , we note that all methods (except Average-pool ) suffer only a small loss in accuracy and our method suffers no loss. A similar situation occurs when viewing the tradeoff between space complexity and accuracy, where we provide results for a memory reduction of $70\\%$ and $30\\%$ in the Tables 3 and 16 (in $\\S\\mathrm{~E~}$ ).

# 6.4 Results on Long Range Arena
We show results on the following three datasets of LRA benchmark: (1) byte-level text classification using real-world data (IMDB), (2) Pathfinder task (long range spatial dependency problem), and (3) image classification on sequences of pixels converted from CIFAR-10.  

<html><body><table><tr><td>Dataset</td><td>1st-I</td><td>1st</td><td>Rand</td><td>Pool</td><td>Att</td><td>CS-k-1</td><td>CS-opt</td><td>BERTBase</td></tr><tr><td>STS-B</td><td>86.4</td><td>86.4</td><td>86.8</td><td>81.6</td><td>87.0</td><td>87.0</td><td>87.0</td><td>87.9</td></tr><tr><td>MRPC</td><td>81.4</td><td>80.9</td><td>83.2</td><td>83.9</td><td>84.6</td><td>86.2</td><td>86.9</td><td>87.3</td></tr><tr><td>SST-2</td><td>83.8</td><td>84.4</td><td>85.6</td><td>85.2</td><td>86.0</td><td>87.3</td><td>89.6</td><td>92.4</td></tr><tr><td>QNLI</td><td>84.8</td><td>84.4</td><td>86.4</td><td>84.1</td><td>86.8</td><td>87.8</td><td>87.8</td><td>90.9</td></tr><tr><td>COLA</td><td>49.7</td><td>49.7</td><td>49.5</td><td>3.0</td><td>51.1</td><td>51.7</td><td>52.8</td><td>53.3</td></tr><tr><td>RTE</td><td>63.5</td><td>63.5</td><td>62.1</td><td>59.2</td><td>63.4</td><td>63.7</td><td>63.7</td><td>65.8</td></tr><tr><td>MNLI_M</td><td>77.8</td><td>76.7</td><td>81.4</td><td>75.4</td><td>82.5</td><td>82.4</td><td>82.5</td><td>84.0</td></tr><tr><td>MNLI_MM</td><td>75.9</td><td>75.6</td><td>78.7</td><td>76.7</td><td>82.7</td><td>82.6</td><td>82.7</td><td>84.6</td></tr><tr><td>QQP</td><td>80.8</td><td>80.4</td><td>87.0</td><td>79.4</td><td>87.3</td><td>87.3</td><td>87.3</td><td>87.5</td></tr><tr><td>Mean</td><td>76.0</td><td>76.1</td><td>77.9</td><td>69.6</td><td>79.0</td><td>79.6</td><td>80.0</td><td>81.5</td></tr></table></body></html>  

Table 1: GLUE dev performance at $3X$ speedup. Here and everywhere else, F 1 scores are reported for QQP and MRPC, Spearman correlations are reported for STS-B, Matthew’s correlations are reported for COLA, and accuracy scores are reported for the other tasks. Each value is averaged over 20 trials. Larger values indicates better performance.   
Table 2: GLUE dev performance at $1.5X$ speedup.   


<html><body><table><tr><td>Dataset</td><td>1st-I</td><td>1st</td><td>Rand</td><td>Pool</td><td>Att</td><td>CS-k-1</td><td>CS-opt</td><td>BERTBase</td></tr><tr><td>STS-B</td><td>87.9</td><td>87.9</td><td>87.8</td><td>87.8</td><td>87.9</td><td>87.7</td><td>87.9</td><td>87.9</td></tr><tr><td>MRPC</td><td>86.8</td><td>86.4</td><td>87.2</td><td>87.0</td><td>87.1</td><td>86.9</td><td>87.3</td><td>87.3</td></tr><tr><td>SST-2</td><td>92.1</td><td>91.5</td><td>91.9</td><td>90.3</td><td>92.3</td><td>92.4</td><td>92.4</td><td>92.4</td></tr><tr><td>QNLI</td><td>90.8</td><td>90.8</td><td>90.8</td><td>90.2</td><td>90.7</td><td>90.9</td><td>90.9</td><td>90.9</td></tr><tr><td>COLA</td><td>53.0</td><td>52.7</td><td>53.1</td><td>25.6</td><td>53.2</td><td>53.3</td><td>53.3</td><td>53.3</td></tr><tr><td>RTE</td><td>65.6</td><td>65.2</td><td>65.7</td><td>61.5</td><td>65.7</td><td>65.4</td><td>65.6</td><td>65.8</td></tr><tr><td>MNLI_M</td><td>84.0</td><td>83.8</td><td>83.9</td><td>80.9</td><td>84.0</td><td>84.0</td><td>84.0</td><td>84.0</td></tr><tr><td>MNLI_MM</td><td>84.1</td><td>84.0</td><td>83.9</td><td>84.0</td><td>84.5</td><td>84.6</td><td>84.6</td><td>84.6</td></tr><tr><td>QQP</td><td>87.1</td><td>86.9</td><td>87.4</td><td>85.7</td><td>87.4</td><td>87.5</td><td>87.5</td><td>87.5</td></tr><tr><td>Mean</td><td>81.3</td><td>81.0</td><td>81.3</td><td>76.8</td><td>81.4</td><td>81.4</td><td>81.5</td><td>81.5</td></tr></table></body></html>  

For baselines, we include First$.k$ -select and Random-select methods, but fail to include Attention-select .Attention-select requires a full attention matrix for selecting tokens which is not available in Big Bird (Zaheer et al. ,2020 ) and Performers (Choromanski et al. ,2020 ). In addition, the Transformers including Big Bird and Performers in LRA have shallow architectures because of no pre-training: the default number of encoders for text classification, path finder, and image classifi- cation datasets are four, four, and one, respectively. Thus, for both baselines and our method, we only reduce sequence length in the input layer , which is before the first encoder. For the sequence-length configurations, see Appendix C.2 . For Averagepool , due to its worst performance on the GLUE benchmarks and the shallow architectures of the models in LRA, we exclude it from the baselines.  

Table 3: GLUE dev performance at $70\\%$ space complexity reduction.   


<html><body><table><tr><td>Dataset</td><td>1st-I</td><td>1st</td><td>Rand</td><td>Pool</td><td>Att</td><td>CS-k-1</td><td>CS-opt</td><td>BERTBase</td></tr><tr><td>STS-B</td><td>85.3</td><td>85.1</td><td>85.6</td><td>78.7</td><td>85.4</td><td>86.5</td><td>86.7</td><td>87.9</td></tr><tr><td>MRPC</td><td>81.3</td><td>81.5</td><td>83.3</td><td>83.1</td><td>84.3</td><td>86.0</td><td>86.6</td><td>87.3</td></tr><tr><td>SST-2</td><td>83.3</td><td>84.6</td><td>84.9</td><td>85.1</td><td>87.2</td><td>87.6</td><td>87.7</td><td>92.4</td></tr><tr><td>QNLI</td><td>84.6</td><td>84.3</td><td>85.1</td><td>84.0</td><td>86.4</td><td>86.6</td><td>86.5</td><td>90.9</td></tr><tr><td>COLA</td><td>49.0</td><td>49.0</td><td>48.4</td><td>0.0</td><td>50.9</td><td>51.0</td><td>52.3</td><td>53.3</td></tr><tr><td>RTE</td><td>62.1</td><td>62.0</td><td>61.8</td><td>59.8</td><td>62.7</td><td>63.6</td><td>63.6</td><td>65.8</td></tr><tr><td>MNLI_M</td><td>76.9</td><td>76.3</td><td>79.0</td><td>75.2</td><td>80.5</td><td>80.9</td><td>81.0</td><td>84.0</td></tr><tr><td>MNLI_MM</td><td>74.9</td><td>74.5</td><td>79.3</td><td>76.3</td><td>80.7</td><td>81.6</td><td>81.8</td><td>84.6</td></tr><tr><td>QQP</td><td>80.6</td><td>80.0</td><td>86.6</td><td>82.9</td><td>87.0</td><td>87.2</td><td>87.3</td><td>87.5</td></tr><tr><td>Mean</td><td>75.3</td><td>75.3</td><td>77.1</td><td>69.5</td><td>78.3</td><td>79.0</td><td>79.3</td><td>81.5</td></tr></table></body></html>  

Table 4: LRA test set performances at $70\\%$ space complexity reduction for Big Bird (top) and Performers (bottom) as the backbone Transformer. Here and everywhere else, accuracy scores are reported for all three tasks. Each value is averaged over 20 trials. Larger values indicates better performance.   


<html><body><table><tr><td colspan=""6"">BigBird</td></tr><tr><td>Dataset</td><td>1st</td><td>Rand</td><td>CS-k-1</td><td>CS-opt</td><td>Trans.-no-prune</td></tr><tr><td>CIFAR-10</td><td>26.9</td><td>39.4</td><td>38.6</td><td>43.3</td><td>40.9</td></tr><tr><td>PATHFINDER-32</td><td>55.6</td><td>69.9</td><td>69.3</td><td>71.7</td><td>73.5</td></tr><tr><td>IMDB (BYTE-LEVEL)</td><td>57.9</td><td>59.6</td><td>59.1</td><td>61.4</td><td>63.8</td></tr><tr><td>Mean</td><td>46.8</td><td>56.3</td><td>55.7</td><td>58.8</td><td>59.4</td></tr><tr><td colspan=""6"">Performers</td></tr><tr><td>CIFAR-10</td><td>26.9</td><td>41.5</td><td>39.8</td><td>45.5</td><td>42.9</td></tr><tr><td>PATHFINDER-32</td><td>52.4</td><td>58.2</td><td>61.5</td><td>67.7</td><td>66.2</td></tr><tr><td>IMDB (BYTE-LEVEL)</td><td>59.9</td><td>59.9</td><td>59.7</td><td>62.8</td><td>64.3</td></tr><tr><td>Mean</td><td>46.4</td><td>53.2</td><td>53.7</td><td>58.7</td><td>57.8</td></tr></table></body></html>  

The results of accuracy scores for space complexity reduction at $70\\%$ and $30\\%$ are presented in Table 4 and Table 18 (in Appendix E), respectively. The Coreset-select-opt here represents the Coreset-select with $m\\ =\\ 1$ because of its superior performance over other $m\\quad\\in$ ∈$\\{\\lceil0.1k\\rceil\\},\\lceil0.2k\\rceil,\\lceil0.3k\\rceil,\\lceil0.4k\\rceil\\}$ .  

We observe a similar pattern as discussed in GLUE benchmark evaluations: at high space complexity reduction $70\\%$ ,Coreset-select-opt significantly outperforms its competitors First$.k$ -select and Random-select by $12\\%$ and $2.5\\%$ in average for Big Bird $(12.3\\%$ and $5.5\\%$ in average for Performers ). Moreover, on CIFAR-10, our Coresetselect-opt is even better than the Big Bird and Performers without any sequence reduction with accuracy gain $2.4\\%$ and $2.6\\%$ , respectively (similarly for Performers on PATHFINDER-32). On the other hand, different from the GLUE evaluations, Coreset-select-k-1 does not show any significant advantages over the baseline methods. Our conjecture is that the input in the LRA datasets contain too many noisy or low level information which is not helpful for predicting the target. For an example, each pixel of an image (CIFAR-10) or a character in the byte-level text classification represents a token as the input. Our Coreset-select based strategy with $m=1$ does the most fine-grained token-level selection than its baselines and thus filter out the noisy information. Note, we do not include accuracy and speedup tables because of insignificant gains observed in speedup due to the shallow architectures of Transformers in LRA.",4
d04736cf-c340-414e-a4a7-79dcf60c22c3,"ref_ids: 454845533235189656, chunk_ids: 12, Score: 0.1553, Text: # 4 Experiments
We now exhibit the results of our experiments, by showing the correlation between the feedback of our indicators, and the false sense of security given by badly-evaluated defenses.  

Experimental setup. We run our attacks on an Intel ®Xeon ®CPU E5-2670 v3, with 48 cores, 126 GB of RAM, and equipped with an Nvidia Quadro M6000 with $24\\:\\mathrm{GB}$ of memory. All the attacks and models have been wrapped and run by using the SecML library [ 20 ]. We select four defenses that have been reported as failing, and we show that our indicators would have detected such evaluation errors. For each of them, we set the hyperparameters for the attack as done in the original evaluation, in order to collect similar results.  

-Winners-Take-All (kWTA) , the defense proposed by Xiao et al. [ 31 ] uses only the top-k outputs from each layer, generating many discontinuities in the loss landscape, and hence resulting in the non-converging failure due to noisy gradients $(F_{2})$ . We use the implementation provided by Tramèr et al. [ 30 ], trained on CIFAR10, and we test its robustness by attacking it with $\\ell_{\\infty}$ -PGD [ 17 ] with a step size of $\\alpha=0.003$ , maximum perturbation $\\epsilon=8/255$ and 50 iterations, with 5 restarts for each attack, scoring a robust accuracy of $58\\%$ on 100 samples.  

Distillation , the defense proposed by Papernot et al. [ 22 ], works by training a model to have zero gradients around the training points, leading gradient-based attacks towards bad local optimum $(F_{3})$ .We re-implemented such defense, by training a distilled classifier on the MNIST dataset to mimic the evaluation. Then, we apply $\\ell_{\\infty}$ -PGD [ 17 ], with step size $\\alpha=0.01$ , maximum perturbation $\\epsilon=0.3$ for 50 iterations on 100 samples, resulting in a robust accuracy of $94\\small{,}2\\%$ .  

Ensemble diversity , the defense proposed by Pang et al. [ 21 ] is composed with different neural networks, trained with a regularizer that encourages diversity. We adopt the implementation provided by Tramèr et al. [ 30 ]. Then, following its original evaluation, we apply $\\ell_{\\infty}$ -PGD [ 17 ], with step size $\\alpha=0.001$ , maximum perturbation $\\epsilon=0.01$ for 10 iterations on 100 samples, resulting in a robust accuracy of $38\\%$ .  

Turning a Weakness into a Strenght (TWS) , the defense proposed by Yu et al. [ 32 ], applies a mechanism for detecting the presence of adversarial examples on top of an undefended model, measuring how much the decision changes locally around a sample. Even if the authors also apply other rejection mechanisms, we take into account only the described one, as we wish to show that attacks optimized neglecting such term will trigger the non-adaptive attack failure $(F_{4})$ .  

We apply this defended on a WideResNet model trained on CIFAR10, provided by RobustBench [ 14 ]. We attack this model with $\\ell_{\\infty}$ -PGD [ 17 ], with step size $\\alpha\\,=\\,0.1$ , maximum perturbation $\\epsilon=0.3$ for 50 iterations on 100 samples, and then we query the defended model with all the computed adversarial examples. While the attacks works against the standard model, some of them are rejected by the defense, resulting in a robust accuracy of $35\\%$ ,highlighted by the trigger of the $I_{5}$ indicator. In this case, we consider an attack unsuccessful if the original sample is not misclassified and the adversarial point is either belonging to the same class, or it is labeled as rejected.  

Each of these attacks have been executed with 5 random restarts. We also attack all these models with the version of AutoPGD (APGD) [ 13 ] that uses the difference of logit (DLR) as a loss to optimize. This strategy will take care to automatically tune its hyperparameters while optimizing, reducing possible errors that occur while deciding the values of step size, and iterations. Lastly, we compute attacks that take into account all the mitigations we prescribed, and they will be analyzed further on in the paper.  

Identifying failures. We want now to understand if our indicators are correlated with faults  

  
Figure 5: Robust accuracy vs. average value of the indicators. Incorrect evaluations (denoted with $\\overrightarrow{\\circ}$ )report high robust accuracy but also trigger most of the indicators. Better evaluations, performed by either mitigating the attack failures (denoted with $\\mathbf{\\Phi}^{\\bullet}\\times\\mathbf{\\Phi}^{\\bullet})$ , or using APGD (denoted with $\\mathbf{\\Phi}^{,}\\star\\mathbf{\\Phi}$ ), correctly report a lower robust accuracy along with a lower average value of our indicators.  

of the security evaluations of defenses. We collect the results of all the attacks against the selected targets, and we compute our indicators, by listing their values in Table 1, along with their mean score. With a glance, it is possible to grasp that out hypothesis is right: the detection of a failure is linked with higher values for the robust accuracy, and also the opposite. Each original evaluation is characterized by high values of one or more indicator, while the opposite happens for stronger attacks. For instance, APGD automatically tunes its hyperparameter while optimizing, hence it is able to apply some mitigations directly during the attack. To gain a quantitative evaluation of out hypothesis, we compute both the p-value and the correlation between the average score of the indicators and the robust accuracy, depicting this result in Fig. 5. Both p-value and correlation suggest a strong connection between these analyzed quantities, confirming our initial belief.  

Table 1: Values of the Indicators of Attack Failures, computed for all the attacks against all the evaluated models. We denote the attacks that apply also the mitigations as $\\mathrm{PGD^{\\star}}$ .  


<html><body><table><tr><td>Model</td><td>Attack</td><td>I1</td><td>12</td><td>13</td><td>14</td><td>15</td><td>T</td><td>RA</td></tr><tr><td rowspan=""3"">k-WTA [31]</td><td>PGD</td><td>0.33</td><td>0.43</td><td>0.77</td><td></td><td></td><td>0.306</td><td>58%</td></tr><tr><td>APGD</td><td>-</td><td>0.31</td><td>0.33</td><td></td><td></td><td>0.128</td><td>36%</td></tr><tr><td>PGD*</td><td>0.07</td><td>0.48</td><td>0.55</td><td></td><td></td><td>0.220</td><td>6%</td></tr><tr><td rowspan=""3"">Distillation[22]</td><td>PGD</td><td></td><td>0.98</td><td></td><td>0.97</td><td></td><td>0.39</td><td>94%</td></tr><tr><td>APGD</td><td>-</td><td>0.40</td><td>0.21</td><td></td><td></td><td>0.122</td><td>0%</td></tr><tr><td>PGD*</td><td></td><td>0.04</td><td></td><td>-</td><td></td><td>0.008</td><td>0%</td></tr><tr><td rowspan=""3"">Ensemble Div.[21]</td><td>PGD</td><td></td><td>0.76</td><td></td><td></td><td></td><td>0.152</td><td>38%</td></tr><tr><td>APGD</td><td></td><td>0.37</td><td>0.14</td><td></td><td></td><td>0.102</td><td>0%</td></tr><tr><td>PGD*</td><td>0.08</td><td>0.17</td><td>0.15</td><td>-</td><td></td><td>0.080</td><td>%6</td></tr><tr><td rowspan=""3"">TWS [32]</td><td>PGD</td><td></td><td>0.49</td><td>0.07</td><td></td><td>0.37</td><td>0.186</td><td>35%</td></tr><tr><td>APGD</td><td>-</td><td>0.41</td><td>0.09</td><td></td><td>-</td><td>0.100</td><td>0%</td></tr><tr><td>PGD*</td><td>-</td><td>0.37</td><td>0.10</td><td>-</td><td></td><td>0.094</td><td>0%</td></tr></table></body></html>  

Mitigating failures. We can now use our indicators to improve the quality of the security evaluations, and we apply the following pipeline: (i) we test the defense with a set of points with the original attack strategy proposed by the author of the defense; (ii) we select the failure cases and inspect the feedback of our indicators per-sample ; (iii) for each cause of failure, we apply the specific remediation suggested by the metric; and (iv) we show that the attack now succeeds, thus reducing the robust accuracy of the target model, and also the values of the indicators.  

We report all the results of this process in Table 2, where each row shows the original robust accuracy, and how it is decreased, mitigation after mitigation. Also, all the individual values of each indicator computed on these patched attacks can be found in Table 1, marked as $\\mathrm{PGD^{\\star}}$ .  

Mitigating $k$ -WTA failures. For many failing attacks, the $I I$ indicator triggers, implying that the attack found an adversarial example inside the path. We then apply mitigation $M_{I}$ , and we lower accordingly the robust accuracy of the model to $36\\small{,}4\\%$ . We then analyze the feedback of the $I_{3}$ indicator, the one that detects the presence of noisy gradients. We apply mitigation $M_{3}$ , and we change the loss of the attack as described by Tramèr et al. [ 30 ]. This loss is computed by averaging the gradient of each single point of the attack path with the information of the surrounding ones. The resulting direction is then able to correctly descent toward a minimum. We run $\\ell_{\\infty}$ -PGD with the same parameters, but smoothing the gradients by averaging 100 neighboring points from a normal distribution ${\\mathcal N}(\\mu=\\pmb{x}_{i},\\sigma=0.0\\dot{3}1)$ , where $x_{i}$ is a point in the attack path. After such mitigation, the robust accuracy drops to $6,4\\%$ , and so follows the indicator (Fig. 6a).  

Mitigating Distillation failures. All the attacks fail because of the absence of gradient information, leading the attack to a bad local optimum $(F_{3})$ , and such is highlighted by the feedback of the $I_{3}$ indicator. We apply mitigation $M_{3}$ , and we change the loss optimized during the attack, following the strategy applied by Carlini et al. [ 9 ], that computes the loss of the attack on the logit of the model rather than the final softmax layer. We repeat the PGD attack with such fix, and the robust accuracy drops to $0\\%$ , along with the indicator $I_{3}$ (Fig. 6b).  

Mitigating Ensemble diversity failures. Firstly, the $I_{I}$ indicator highlighted the presence of $F_{I}$ ,implying that some failing attacks are due to the implementation itself. We apply mitigation $M_{I}$ , and the robust accuracy decreases to $36\\%$ . Also, $I_{2}$ indicator is active, implying that the loss of of failing attacks could be optimized more. For this reason, we apply mitigation $M_{2}$ , and we increase the step size to 0 .05 and the iterations to 50 . This patch is enough for lowering the robust accuracy to $9\\bar{\\%}$ .(Fig. 6c).  

Mitigating TWS failures. The detector is rejecting adversarial attacks successfully computed on the undefended model, triggering the $I_{5}$ indicator. Hence we apply mitigation $M_{5}$ , and we adapt the attack to consider also the rejection class. This version of PGD minimizes the usual loss function of the attacker, but it also minimizes the score of the rejection class when encountered, allowing it to evade the rejection. We run such attack, and we obtain a new robust accuracy of $0\\%$ (Fig. 6d).  

<html><body><table><tr><td>Model</td><td>Initial</td><td>M1</td><td>M2</td><td>M3</td><td>M4</td><td>M5</td><td>Final</td></tr><tr><td>k-WTA [31]</td><td>58.2%</td><td>36.4%</td><td>36.4%</td><td>6.4%</td><td>6.4%</td><td>6.4%</td><td>6.4%</td></tr><tr><td>Distillation [22]</td><td>94.2%</td><td>94.2%</td><td>94.2%</td><td>0.4%</td><td>0.4%</td><td>0.4%</td><td>0.4%</td></tr><tr><td>Ensemble Diversity [21]</td><td>38.0%</td><td>38.0%</td><td>9.0%</td><td>9.0%</td><td>9.0%</td><td>9.0%</td><td>9.0%</td></tr><tr><td>TWS [32]</td><td>35.0%</td><td>35.0%</td><td>35.0%</td><td>35.0%</td><td>35.0%</td><td>0.0%</td><td>0.0%</td></tr></table></body></html>

Table 2: Robust accuracies $\\overline{{(\\%)}}$ after patching the security evaluations with the prescribed mitigations.  

  
Figure 6: The values of our indicators and the success rate (SR) of the attack, before (semi-transparent colored area) and after (solid colored area) fixing the failures, computed for the analyzed models.",4
