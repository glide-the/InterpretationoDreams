角色,内容,分镜
a9901553-4ed2-4163-9b05-360018088c92,研究论文中采用的主要框架在不同任务中的应用与变体,1
a9901553-4ed2-4163-9b05-360018088c92,"### 问题

在“研究论文中采用的主要框架在不同任务中的应用与变体”这一任务步骤中，**Transformer变体**如T5和Vision Transformer (ViT) 在不同任务中的应用展现了显著的跨领域适应性。基于此，**如何进一步优化Transformer架构以提升其在多模态任务（如图像与文本联合理解）中的性能与效率？** 具体来说，可以考虑以下方面：

1. **架构改进**：是否可以通过引入新的注意力机制或模块化设计来增强Transformer在多模态数据中的表示能力？
2. **训练策略**：如何设计更高效的多模态预训练策略，以减少对大规模标注数据的依赖？
3. **资源优化**：在资源受限的设备上，如何平衡Transformer模型的性能与计算效率？
4. **泛化能力**：如何提升Transformer在跨领域任务中的泛化能力，使其在未见过的数据分布上表现更稳定？

通过探讨这些问题，可以进一步推动Transformer架构在多模态任务中的应用与创新。",1
a9901553-4ed2-4163-9b05-360018088c92,"ref_ids: 454846687148335366, chunk_ids: 1, Score: 0.6992, Text: # Related Work

# Vision Transformer
Inspired by the great success of transformer (Vaswani et al. 2017) on NLP tasks, many recent studies have explored the introduction of transformer architecture to multiple computer vision tasks (Carion et al. 2020; Chen et al. 2023; Chen, Fan, and Panda 2021; Chen et al. 2021; Li et al. 2021; Dai et al. 2021; Touvron et al. 2021a; Liu et al. 2021; Wang et al. 2023, 2021b; Meng et al. 2022). Following ViT (Dosovitskiy et al. 2021), a variety of ViT variants have been proposed to improve the recognition performance as well as training and inference efficiency. DeiT (Touvron et al. 2021a) incorporates distillation strategies to improve the training efficiency of ViTs, outperforming standard CNNs without pretraining on large-scale datasets like JFT (Sun et al. 2017). LV-ViT (Jiang et al. 2021) leverages all tokens to compute the training loss, and the location-specific supervision label of each patch token is generated by a machine annotator. GFNet (Rao et al. 2021b) replaces selfattention in ViT with three key operations that learn longrange spatial dependencies with log-linear complexity in the frequency domain. CrossViT (Chen, Fan, and Panda 2021) achieves new SOTA performance using a two-branch transformer to combine different patch sizes to recognize objects across multiple scales. DeiT III (Touvron, Cord, and J´egou 2022) boosts the supervised training performance of ViT models on ImageNet to a new benchmark by improving the training strategy. Wave-ViT (Yao et al. 2022) seeks a better trade-off between efficiency and accuracy by formulating reversible downsampling via wavelet transform and self-attentive learning. Spectformer (Patro, Namboodiri, and Agneeswaran 2023) first uses Fourier transform operations to implement a frequency domain layer used to extract image features at shallow locations in the network. In this paper, we focus on improving the performance of a generic ViT backbone, so our work is orthogonal to designing efficient ViT backbones.

# Adaptive Inference in Vision Transformer
The human brain processes visual information using hierarchical and varied attention scales, enriching its environmental perception and object recognition (Gupta et al. 2021; Zhang et al. 2017). This mirrors the adaptive inference rationale, which leverages the significant variances within network inputs as well as the redundancy in network architectures to improve efficiency through instance-specific inference strategies. In particular, previous techniques applied to CNNs have investigated various approaches, such as modifying input samples (Wu et al. 2020; Zuxuan et al. 2021), skipping network layers (Wu et al. 2018; Wang et al. 2018) and channels (Lin et al. 2017; Bejnordi, Blankevoort, and Welling 2020), as well as employing early exiting with a multi-classifier structure (Bolukbasi et al. 2017; Huang et al. 2018; Li et al. 2019). In recent studies, researchers have explored the use of adaptive inference strategies to improve the inference efficiency of ViT models (Wang et al. 2021b; Chen et al. 2023; Xu et al. 2022; Tang et al. 2022a). Some studies (Yin et al. 2022; Meng et al. 2022; Xu et al. 2022; Rao et al. 2021a) attempt to prune unimportant tokens dynamically and progressively during inference. DVT (Wang et al. 2021b) endows a proper token number for each input image by cascading three transformers. CF-ViT (Chen et al. 2023) performs further fine-grained partitioning of informative regions scattered throughout the image to improve ViT model performance. Compared to the aforementioned methods, our LF-ViT ingeniously harnesses the inherent spatial redundancy within images. By pinpointing and focusing on regions of class-discriminative within high-resolution images, we approach the issue from the perspective of spatial redundancy in images, effectively reducing the computational costs of the ViT model.

# Preliminaries
Vision Transformer (ViT) (Dosovitskiy et al. 2021) splits images into sequences of patches as input, and then uses multiple stacked multi-head self-attention (MSA) and feedforward network (FFN) building blocks to model the longrange dependencies between them. Formally, for each input image $I^{C\\times H\\times W}$ , ViT first splits into 2D patches with fixed size $\\textbf{X}=~[\\mathbf{x}_{1},\\mathbf{x}_{2},...,\\mathbf{x}_{N}]$ , where $N$ is the number of patches, $C,\\,H$ , and $W$ denote the channel, height and width of the input image, respectively. These patches are then mapped to $D$ -dimensional patch embeddings $\\mathbf{Z=}$ $[\\mathbf{z}_{1},\\mathbf{z}_{2},...,\\mathbf{z}_{N}]$ with a linear layer, i.e., tokens. Subsequently, a learnable class token $\\mathbf{z}_{c l s}$ is appended to the tokens serving as a representation of the whole image. The positional embedding ${\\bf{E}}_{p o s}$ is also added to these tokens to enhance their positional information. Thus, the sequence of tokens input to the ViT model is:  

$$
{\\bf Z}=[{\\bf z}_{c l s};{\\bf z}_{1},{\\bf z}_{2},...,{\\bf z}_{N}]+{\\bf E}_{p o s}
$$  

where $\\mathbf{z}\\in\\mathbb{R}^{D}$ and $\\mathbf{E}_{p o s}\\in\\mathbb{R}^{(N+1)\\times D}$ respectively.  

The backbone network of a ViT model consists of $L$ building blocks, each of which consists of a MSA and a FFN. In particular, the $l$ -th encoder in a single-head, the toce $\\mathbf{Z}_{l-1}$ is pro matrix $\\mathbf{Q}_{l}\\mathbf{\\Lambda}\\in$ $\\mathbb{R}^{(N+1)\\top}$ $\\mathbf{V}_{l}~\\in~\\mathbb{R}^{\\dot{(}N+1)\\times D}$ . Then, the self-attention matrix $\\mathbf{K}_{l}~\\in~\\mathbb{R}^{(N+1)\\times D}$ ∈, and a value $\\mathbf{A}_{l}\\in\\mathbb{R}^{(N+1)\\times(N+1)}$ ∈is computed as:  

$$
\\mathbf{A}_{l}=\\mathrm{Softmax}(\\frac{\\mathbf{Q}_{l}\\mathbf{K}_{l}^{T}}{\\sqrt{D}})\\mathbf{V}_{l}=[\\mathbf{a}_{c l s,l};\\mathbf{a}_{1,l},\\mathbf{a}_{2,l},...,\\mathbf{a}_{N,l}]\\mathbf{V}_{l}
$$  

The $\\mathbf{a}_{c l s,l}\\in\\mathbb{R}^{(N+1)}$ is known as class attention, reflecting the interactions between class tokens and other patch tokens. For more effective attention to different representation subspaces, multi-head self-attention concatenates the output from several single-head attentions and projects it with another parameter matrix:  

$$
\\mathbf{head}_{i,l}=\\mathbf{A}(\\mathbf{Z}_{l}\\mathbf{W}_{i,l}^{Q},\\mathbf{Z}_{l}\\mathbf{W}_{i,l}^{K},\\mathbf{Z}_{l}\\mathbf{W}_{i,l}^{V})
$$  

$$
\\mathrm{MSA}(\\mathbf{Z}_{l})=\\mathrm{Concat}(\\mathbf{head}_{i,l},...,\\mathbf{head}_{H,l})\\mathbf{W}_{l}^{O}
$$  

where $\\mathbf{W}_{i,l}^{Q},\\,\\mathbf{W}_{i,l}^{K},\\,\\mathbf{W}_{i,l}^{V},\\,\\mathbf{W}_{l}^{O}$ are the parameter matrices in the $i$ -th attention head of the $l$ -th build block, and $\\mathbf{Z}_{l}$ denotes the input at the $l$ -th block. The output from MSA is then fed into FFN to produce the output of the build block $\\mathbf{Z}_{l+1}$ . Residual connections are also applied on both MSA and FFN as follows:  

$$
\\begin{array}{r}{{\\bf Z}_{l}^{\\prime}=\\mathrm{MSA}({\\bf Z}_{l})+{\\bf Z}_{l},\\quad{\\bf Z}_{l+1}=\\mathrm{FFN}({\\bf Z}_{l}^{\\prime})+{\\bf Z}_{l}^{\\prime}}\\end{array}
$$  

The final prediction is produced by the classifier taking the class token $\\mathbf{z}_{c l s,L}$ from the last build block as inputs.",1
a9901553-4ed2-4163-9b05-360018088c92,"ref_ids: 454847026521023802, chunk_ids: 2, Score: 0.6719, Text: # Related Works
Vision Transformers. Transformer and self-attention mechanism have significantly impacted Natural Language Processing (Vaswani et al. 2017), and their application to vision tasks has been made possible by the pioneering Vision Transformer (ViT) (Dosovitskiy et al. 2021). Researchers have further extended ViT models in multiple directions, focusing on position encoding (Chu et al. 2023), data efficiency (Touvron et al. 2021), and optimization techniques (Li et al. 2022b). These advancements have led to significant progress in vision tasks (Khan et al. 2022). Several recent works have focused on enhancing ViT’ performance on downstream tasks by exploring pyramid structures, surpassing convolution-based methods. PVT (Wang et al. 2021, 2022) introduces sparse location sampling in the feature map to form key and value pairs. Swin (Liu et al. 2021) utilizes non-overlapping windows with window shifts between consecutive blocks. CSwin (Dong et al. 2022) extends this approach with cross-shape windows to enhance model capacity. PaCa-ViT (Grainger et al. 2023) introduces a new approach where queries start with patches, while keys and values are based on clustering, learned end-to-end. HaloNet (Vaswani et al. 2021) introduced a haloing mechanism that localizes self-attention for blocks of pixels instead of pixelwise, aiming to address the lack of an efficient sliding window attention. Similarly, NAT (Hassani et al. 2023) adopts neighborhood attention, considering specific scenarios for corner pixels. FocalNets (Yang et al. 2022) replaces selfattention with a Focal module using gated aggregation technique for token interaction modeling in vision tasks. While, these advances have been successful and achieved impressive results on vision tasks, they often come with higher complexity compared to vanilla ViT counterparts. With SeTformer, we’ve aimed to achieve a balance between performance and complexity, addressing this issue.  

Kernel Methods for Transformers. When dealing with large-scale input, a more efficient approach is to directly reduce the complexity of the theoretical calculations. Kernelization accelerates self-attention by transforming the computation complexity from quadratic to linear. By utilizing kernel feature maps, we can bypass the computation of the full attention matrix, which is a major bottleneck in softmax. Recent advances in scalable transformers include (Choromanski et al. 2021; Peng et al. 2021; Chowdhury et al. 2022; Choromanski et al. 2023; Zandieh et al. 2023), where the self-attention matrix is approximated as a low-rank matrix for long sequences. These methods are either very simplistic (Chowdhury et al. 2022), while others are mathematically well explained but complex (Choromanski et al. 2021; Zandieh et al. 2023). Several of these methods rely on the Fourier transform, leading to sin and cos random features, which are unsuitable for transformers due to negative values in the attention matrix. To address this, Choromanski et al. (2021) proposed a solution using positive valued random features known as $\\operatorname{FAVOR+}$ for self-attention approximation. This approach was improved by Likhosherstov et al. (2022), through carefully selecting linear combination parameters, allowing for one parameter set to be used across all approximated values. While all these methods achieved decent efficiency, they often falls behind popular vision transformers like Swin in terms of performance (Figure 2). However, we explore the concept of optimal transport and kernel learning to potentially overcome this performance gap while still maintaining the efficiency advantages of Transformers.  

Our model’s corresponding kernel is a matching kernel (Tolias, Avrithis, and J´egou 2013), which uses a similarity function to compare pairs of features (the input vector and reference set). Recent methods have also explored kernels based on matching features using wasserstein distance (Khamis et al. 2023; Kolouri et al. 2021). Prior studies by Skianis et al. (2020) and Mialon et al. (2021) analyze similarity costs between input and reference features in biological data. They employ dot-product operation (Vaswani et al. 2017) for element-wise comparison. In contrast, our model employs transport plans to compute attention weights, providing a new perspective on Transformer’s attention via kernel methods. Unlike other kernel-based methods like Performer that is not compatible with positional encoding techniques, our model incorporates this information, which is crucial in visual modeling tasks.

# Proposed Method

# From Self-Attention to Self-optimal Transport
Consider an input sequence $x{=}\\{x_{1},...,x_{n}\\}\\ \\in\\ R^{d}$ of $n$ tokens. The DPSA is a mapping that inputs matrices $Q,K,V\\in R^{n\\times d}$ . These matrices are interpreted as queries, keys, and values, respectively,  

$$
\\begin{array}{c}{{A t t(Q,K,V)=D^{-1}A V}}\\\\ {{A=\\exp\\Big(Q K^{T}/\\sqrt{d}\\Big);\\;\\;\\;D=\\mathrm{diag}(A{\\bf1}_{n}^{T})}}\\end{array}
$$  

where $\\mathbf{1_{n}}$ is the ones vector, and $A,D\\,\\in\\,R^{n\\times n}$ . The Softmax operation normalizes the attention weights A , allowing each token to be a weighted average of all token values. However, the quadratic complexity of the softmax becomes a bottleneck as the number of tokens increase. We aim to develop a powerful and efficient self-attention that is, above all, simple. We do not add any complex modules like convolution (Wu et al. 2021), shifted windows (Hassani et al. 2023), or attention bias (Li et al. 2022a) to improve the vision task’s performance. We indeed take a different strategy. SeT leverages the important properties of softmax, including non-negativity and reweighting mechanism (Gao and Pavel 2017), while also prioritizing efficiency in its design. The use of RKHS with a positive definite (PD) kernel avoids aggregating negative-correlated information. SeT incorporates a nonlinear reweighting scheme through OT. This involves computing alignment scores between input and reference sets within RKHS. This process introduces nonlinearity to the alignment scores, assigning weights to elements to highlight their significance. This helps the model in capturing complex relationships and emphasizing local correlations.  

  
Figure 3: An input feature vector $x$ is transported onto reference $y$ via transport plan $T(x,y)$ , that aggregates $x$ features w.r.t. $y$ , yielding $A_{y}(x)$ . In DPSA, each $x_{i}$ aggregates with all $x$ features, forming a large sparse matrix. Our model aggregates based on best-matched $x$ and $y$ features through OT.  

Representing local image neighborhoods in an RKHS. In order to maintain a linear computation, we embed the input feature vector into a RKHS, in which point evaluation takes the form of linear function (Jagarlapudi and Jawanpuria 2020). Kernel methods enable us to map data from its original spac $\\mathcal{X}$ to a higher-dimensional Hilbert space kern $\\kappa$ ature space) , Wang, and Nehorai 2020). For a funcFthrough a positive definite (PD) tio $u\\colon\\mathcal{X}\\ \\rightarrow\\ \\mathcal{F}$ , the PD kern denoted an infinite-dimensional, the kernel technique (Williams and as K$K(x,x^{\\prime})\\ =\\ \\langle u(x),u(x^{\\prime})\\rangle_{\\mathcal{F}}$ ⟨⟩F. Given that $u(x)$ can be Seeger 2001) allows to derive a finite-dimensional representation $v(x)$ in $R^{k}$ , with an inner product $\\langle v(x_{i}),v(\\Bar{x_{j}^{\\prime}})\\rangle$ ⟩denoting $\\boldsymbol{\\kappa}(\\boldsymbol{x},\\boldsymbol{x}^{\\prime})$ . As sh by (Fukum $\\kappa$ is posaligns with the non-negativity property of softmax operator. itive definite, for any $x,\\,x^{\\prime}$ , we have K$K(x,x^{\\prime})\\,\\geq\\,0$ ≥, which Optimal transport (OT). A fundamental role in our model is to aggregate related tokens by learning a mapping between them. Our weighted aggregation relies on the transport plan between elements $x$ and $x^{\\prime}$ treated as distinct measures or weighted point clouds. OT has found extensive use in alignment problems (Khamis et al. 2023), and has an impressive capacity to capture the geometry of the data (Liu et al. 2023). We focus throughout this paper on the Kantorovich form of OT (Peyr´e and Cuturi 2019) with entropic regularization for smoothing transportation plans. Let $g$ in $\\mu_{n}$ and $h$ in $\\mu_{n^{\\prime}}$ denote the weights of discrete measures $\\sum_{i}g_{i}\\delta_{x_{i}}$ $\\sum_{j}h_{j}\\delta_{x_{j}^{\\prime}}$ for the elements $x$ and $x^{\\prime}$ . Here, $\\delta_{x}$ is the unit mass with $x$ .cost matrix $C\\;\\in\\;R^{n\\times n^{\\prime}}$ has entries c$c(x_{i},x_{j}^{\\prime})$ for the ($(i,j)$ pairs. Instead of computing a pairwise dot product between distributions, OT finds the minimal effort based on the ground cost to shift the mass from one distribution to another, and can be defined by  

$$
O T_{\\mathrm{kant}}=\\operatorname*{min}_{T\\in U(g,h)}\\sum_{i j}C_{i j}T_{i j}+\\epsilon H(T)
$$  

where the negative entropy function is defined as $H(T)=$ $\\begin{array}{r}{\\sum_{i,j}T_{i j}(\\log(\\bar{T}_{i j})-1)}\\end{array}$ with regularization parameter $\\epsilon$ . The transport plan $T_{i j}$ describes the amount of mass flowing from location $i$ to location $j$ with minimal cost, and the constraint $\\begin{array}{r}{U(g,h){=}\\{T\\,\\in\\,R_{+}^{n\\times n^{\\prime}}:\\,T1_{n}{=}g\\}\\ T^{T}1_{n^{\\prime}}{=}h\\}}\\end{array}$ represents the uniform transport polytope. The computation of OT in (2) is efficiently done using a matrix scaling procedure derived from Sinkhorn’s algorithm (Cuturi 2013). OT assigns different weights to individual elements/tokens based on their significance within the input, similar to the reweighting scheme in softmax attention. Furthermore, OT enforces non-negativity by optimizing alignments between input elements (Sinkhorn and Knopp 1967; Cuturi 2013), preserving the non-negative nature of attention weights, as in softmax attention. Indeed, kernels capture the nonlinear transformation of the input, while OT finds optimal alignments between sets of features with fast computation.",1
a9901553-4ed2-4163-9b05-360018088c92,"ref_ids: 454849056084511186, chunk_ids: 2, Score: 0.6172, Text: # 2. Related Work

# 2.1. Vision Transformers
The Transformer [ 54 ] was initially developed for natural language processing tasks and has since been adapted for computer vision tasks through the introduction of the Vision Transformer (ViT) [ 11 ]. Further improvements to ViT have been achieved through knowledge distillation or more intricate data augmentation, as demonstrated by DeiT [ 52 ]. However, Transformers do not consider the quadratic complexity of high-resolution images or the 2D structure of images, which are challenges in vision tasks. To address these issues and improve the performance of vision Transformers, various methods have been proposed, including multi-scale architectures [ 3 ,32 ,56 ,63 ], lightweight convolution layers [ 14 ,28 ,60 ], and local self-attention mechanisms [ 32 ,6 ,65 ,71 ].

# 2.2. Convolutional Neural Networks
Convolutional neural networks (CNNs) have been the main force behind the revival of deep neural networks in computer vision. Since the introduction of AlexNet [ 25 ], VGGNet [ 44 ], and ResNet [ 17 ], CNNs have rapidly become the standard framework for computer vision tasks. The design principles of CNNs have been advanced by subsequent models such as Inception [ 47 ,48 ], ResNeXt [ 62 ], Res2Net [ 13 ] and MixNet [ 51 ], which promote the use of building blocks with multiple parallel convolutional paths. Other works such as MobileNet [ 20 ] and ShuffleNet [ 73 ]have focused on the efficiency of CNNs. To further improve the performance of CNNs, attention-based models such as SE-Net [ 21 ], Non-local Networks [ 58 ], and CBAM [ 59 ]have been proposed to enhance the modeling of channel or spatial attention. EfficientNets [ 49 ,50 ] and MobileNetV3 [ 19 ] have employed neural architecture search (NAS) [ 77 ] to develop efficient network architectures. ConvNeXt [ 33 ] adopts the hierarchical design of Vision Transformers to enhance CNN performance while retaining the simplicity and effectiveness of CNNs. Recently, several studies [ 15 ,18 ,64 ] have utilized convolutional modulation as a replacement for self-attention, resulting in improved performance. Specifically, FocalNet [ 64 ] utilizes a stack of depth-wise convolutional layers to encode features across short to long ranges and then injects the modulator into the tokens using an element-wise affine transformation. Conv2Former [ 18 ] achieves good recognition performance using a simple $11\\times11$ depth-wise convolution. In contrast, our scale-aware modulation also employs depth-wise convolution as a basic operation but introduces multi-head mixed convolution and scale-aware aggregation.  

  
Figure 2: (a) The architecture of the Scale-Aware Modulation Transformer (SMT); (b) Mix Block: a series of SAM blocks and MSA blocks that are stacked successively (as presented in Sec. 3.3 ). SAM and MSA denote the scale-aware modulation module and multi-head self-attention module, respectively.

# 2.3. Hybrid CNN-Transformer Networks
A popular topic in visual recognition is the development of hybrid CNN-Transformer architectures. Recently, several studies [ 14 ,45 ,60 ,76 ] have demonstrated the effectiveness of combining Transformers and convolutions to leverage the strengths of both architectures. CvT [ 60 ] first introduced depth-wise and point-wise convolutions before self-attention. CMT [ 14 ] proposed a hybrid network that utilizes Transformers to capture long-range dependencies and CNNs to model local features. MobileViT [ 37 ], EdgeNeXt [ 36 ], MobileFormer [ 5 ], and EfficientFormer [ 27 ]reintroduced convolutions to Transformers for efficient network design and demonstrated exceptional performance in image classification and downstream applications. However, the current hybrid networks lack the ability to model range dependency transitions, making it challenging to improve their performance. In this paper, we propose an evolutionary hybrid network that addresses this limitation and showcases its importance.

# 3. Method

# 3.1. Overall Architecture
The overall architecture of our proposed Scale-Aware Modulation Transformer (SMT) is illustrated in Fig. 2 . The network comprises four stages, each with downsampling rates of free network, we first adopt our proposed Scale-Aware $\\{4,8,16,32\\}$ . Instead of constructing an attentionModulation (SAM) in the top two stages, followed by a penultimate stage where we sequentially stack one SAM block and one Multi-Head Self-Attention (MSA) block to model the transition from capturing local to global dependencies. For the last stage, we solely use MSA blocks to capture long-range dependencies effectively. For the FeedForward Network (FFN) in each block, we adopt the detailspecific feedforward layers as used in Shunted [ 42 ].",1
