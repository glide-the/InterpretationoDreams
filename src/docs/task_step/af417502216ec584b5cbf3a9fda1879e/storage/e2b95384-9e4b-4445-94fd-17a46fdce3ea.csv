角色,内容,分镜
e2b95384-9e4b-4445-94fd-17a46fdce3ea,分析最新算法的稳定性与容错性,4
e2b95384-9e4b-4445-94fd-17a46fdce3ea,LayerNorm 和 RMSNorm 在复杂动态环境及大规模数据下是如何具体优化以保持稳定性的，以及在极端情况下面对数据噪声和异常值，提升它们容错性的可能方向有哪些？ ,4
e2b95384-9e4b-4445-94fd-17a46fdce3ea,"ref_ids: 454845924254196540, chunk_ids: 7, Score: 0.3652, Text: # B.2 MODEL LAYERS
In this section, we give the formal definition of LayerNorm $\\operatorname{LN}(\\cdot)$ and RMS Norm ${\\mathrm{RMS}}\\left(\\cdot\\right)$ .  

Definition 1 (LayerNorm) .LayerNorm $L N(\\cdot;\\mu,\\beta,\\epsilon)$ of dimension $D$ is defined as:  

$$
L N(\\mathbf{x};\\pmb{\\mu},\\beta,\\epsilon)=\\frac{\\mathbf{x}-\\mathbb{E}[\\mathbf{x}]}{\\sqrt{\\mathrm{Var}[\\mathbf{x}]+\\epsilon}}\\odot\\pmb{\\mu}+\\beta,
$$  

where $\\mathbf{x},\\pmb{\\mu},\\beta\\in\\mathbb{R}^{D}$ .  

Definition 2 (RMSNorm) .RMS Norm $R M S(\\cdot;\\mu,\\epsilon)$ of dimension $D$ is defined as:  

$$
R M S(\\mathbf{x};\\pmb{\\mu},\\epsilon)=\\frac{\\mathbf{x}}{\\sqrt{\\frac{1}{D}\\sum_{i=1}^{D}(\\mathbf{x}[i])^{2}+\\epsilon}}\\odot\\pmb{\\mu},
$$  

where x,$\\pmb{\\mu}\\in\\mathbb{R}^{D}$ .  

Remark. In neural networks, inputs of normalization layers are usually high dimension tensors. In this case, LayerNorm and RMSNorm normally apply to the last dimension separately.

# B.3 LOSSLESS EXPANSION IN VECTOR SPACE
In this section, we first give the general definition of lossless expansion in vector space.  

dimensions satisfy dim it is invertible. Definition 3 (Lossless $(\\bar{\\mathcal{T}})\\geq d i m(S)$ T≥S, a vector space expansion ector space) .Given $\\boldsymbol{S}$ and V$\\tau$ $\\mathcal{V}:\\mathcal{S}\\rightarrow\\mathcal{T}$ S →T is said to be lossless if ector spaces where the  

Remark. Note that the identity function Id is lossless with its inverse being itself.  

Then we give a few examples of lossless vector space expansions. These examples will also be used in LEMON.  

Example B.3.1 (Vector average expansion $\\mathcal{V}_{\\mathrm{avg.}}$ ).Let $\\mathbf{\\widetilde{x}}\\in\\mathbb{R}^{D_{S}}$ be a vector of dimension $D_{S}$ and its average $\\begin{array}{r}{\\lambda_{V}g(\\mathbf{x})=\\mathbb{E}[\\mathbf{x}]=\\frac{1}{D_{S}}\\sum_{i}^{D_{S}}\\mathbf{x}[i]}\\end{array}$ P].$\\mathbf{x}_{a\\nu g}^{*}$ is called the average expanded xof dimension $D_{T}$  

with $D_{T}\\geq D_{S}$ if  

$$
\\mathbf{x}_{a v g}^{*}=\\mathcal{V}_{a v g}(\\mathbf{x})=C o n c a t\\left[\\underbrace{\\mathbf{x}^{\\mathsf{T}},\\cdots,\\mathbf{x}^{\\mathsf{T}}}_{\\lfloor D_{T}/D s\\rfloor},\\underbrace{A v g(\\mathbf{x}),\\cdots,A v g(\\mathbf{x})}_{D_{T}\\mathrm{~mod~}D_{S}}\\right]^{\\mathsf{T}}\\in\\mathbb{R}^{D_{T}}.
$$  

Example B.3.2 (Vector z o expansion $\\mathcal{V}_{\\mathrm{zero.}}$ ).Le $\\mathbf{x}\\in\\mathbb{R}^{D_{S}}$ be a vector of dimension $D_{S}$ .$\\mathbf{x}_{z e r o}^{*}$ is called the zero expanded xof dimension $D_{T}$ with $D_{T}\\geq D_{S}$ ≥if  

$$
\\begin{array}{r}{\\mathbf{x}_{z e r o}^{*}=\\mathcal{V}_{z e r o}(\\mathbf{x})=C o n c a t\\left[\\underbrace{\\mathbf{x^{\\mathsf{T}}},\\cdots,\\mathbf{x^{\\mathsf{T}}}}_{\\lfloor D_{T}/D_{S}\\rfloor},\\underbrace{0,\\cdots,0}_{D_{T}\\mathrm{~mod~}D_{S}}\\right]^{\\mathsf{T}}\\in\\mathbb{R}^{D_{T}}.}\\end{array}
$$  

Example B.3.3 (Vector circula expansion $\\mathcal{V}_{\\mathrm{circ}})$ Let $\\mathbf{x}\\in\\mathbb{R}^{D_{S}}$ a vector of dimension $D_{S}$ .${\\bf x}_{c i r c}^{*}$ is called the circular expanded xof dimension $D_{T}$ with $D_{T}\\geq D_{S}$ ≥if  

$$
\\begin{array}{r}{\\mathbf{x}_{c i r c}^{*}=\\mathcal{V}_{c i r c}(\\mathbf{x})=C o n c a t\\underbrace{\\left[\\mathbf{x}^{\\mathsf{T}},\\cdots,\\mathbf{x}^{\\mathsf{T}},\\mathbf{x}^{\\mathsf{T}}[\\colon D_{T}\\bmod D_{S}]\\right]^{\\mathsf{T}}\\in\\mathbb{R}^{D_{T}}}_{[D_{T}/D_{S}]}.}\\end{array}
$$  

Example B.3.4 (Vector random expansion $\\mathcal{V}_{\\mathrm{rand.}}$ Let $\\mathbf{\\Deltax}\\in\\mathbb{R}^{D_{S}}$ a vector of dimension $D_{S}$ .${\\bf x}_{r a n d}^{*}$ is called the random expanded xof dimension $D_{T}$ with $D_{T}\\geq D_{S}$ ≥if  

$$
\\begin{array}{r}{\\mathbf{x}_{r a n d}^{*}=\\mathcal{V}_{r a n d}(\\mathbf{x};\\zeta)=C o n c a t\\left[\\underbrace{\\mathbf{x^{\\intercal}},\\cdots,\\mathbf{x^{\\intercal}}}_{\\lfloor D_{T}/D_{S}\\rfloor},\\zeta^{\\intercal}\\right]^{\\intercal}\\in\\mathbb{R}^{D_{T}},}\\end{array}
$$  

where $\\zeta\\in\\mathbb{R}^{D_{T}}$ mod $D_{S}$ is an arbitrary vector.  

Remark. (1) All vector expansion examples above follow the same pattern. Specifically, when $D_{T}$ expanding from di mod s by $D_{S}$ entries differently. (2) The random vector ating $\\textbf{x}\\lfloor D_{T}/D_{S}\\rfloor D_{S}$ ⌊$D_{S}$ ⌋$D_{T}$ number of times. , all vector expansion methods pad first $\\zeta$ in vector random expansion is arbitrary, Each method deals with the remaining $\\lfloor D_{T}/D_{S}\\rfloor D_{S}$ enso $\\mathcal{V}_{a\\nu g}$ ,$\\mathcal{V}_{z e r o}$ ,$\\mathcal{V}_{c i r c}\\subset\\mathcal{V}_{r a n d}$ . (3) Here all three examples are expansion methods for vectors. In practice, neural networks like Transformers are dealing high dimensional tensors. These tensors can essentially be thought of as collections of vectors. In such scenarios, we can apply the expansion methods separately to the last dimension of these tensors.  

In the following claim, we show that vectors expanded by these operators are lossless.  

$\\mathcal{V}_{c i r c}$ V, and vector random expansion m 1. Vector average expansio V$\\gamma_{r a n d}$ $\\mathcal{V}_{a\\nu g},$ are all lossless expansion for vectors. , vector zero expansion $\\mathcal{V}_{z e r o}$ , vector circular expansion Proof. The inverse function $\\mathcal{V}^{-1}:\\mathbb{R}^{D_{T}}\\rightarrow\\mathbb{R}^{D_{S}}$ of these vector expansion methods is  

$$
\\nu^{-1}({\\bf x})={\\bf x}[:D_{S}].
$$  

Remark. In practice, we want inverse mapping of expansion methods to be easily computed just like the example above.

# B.4LOSSLESS EXPANSION FOR OPERATORS
We then give the definition of lossless expansion for operators. These operators apply on tensors, hence our definition of lossless operator expansion is based on lossless expansion in vector space. These operators can be different layers used in Transformer architectures, including LayerNorm, convolutional layers, and fully-connected layers, etc.  

Definit ansio der vector spaces $S^{i n},S^{o u t},\\mathcal{T}^{i n}$ and $\\mathcal{T}^{o u t}$ such that with $g(\\cdot):S^{i n}\\rightarrow S^{o u t}$ ·$n(S^{i n})\\leq d i m(T^{i n})$ S→S or space e T. We say the ope and dim $d i m\\big(S^{\\bar{o}u t}\\big)\\leq d i m\\big(T^{o u t}\\big)$ S$\\mathcal{V}_{i n}:S^{i\\bar{n}}\\to\\mathcal{T}^{i n}$ ≤TEMo is $(\\mathcal{V}_{i n},\\mathcal{V}_{o u t})$ VVess output vector space expansion ppose the op -lossless for $g(\\cdot)$ ·or is denoted if there exist $\\mathcal{V}_{o u t}:S^{o u t}\\to\\mathcal{T}^{o u t}$ VS→T such that V$\\mathcal{V}_{o u t}(g(\\mathbf{x}))=\\mathcal{E}[g](\\mathcal{V}_{i n}(\\mathbf{x})),\\forall\\mathbf{x}\\in S^{i n}$ EV∀∈S .  

$(\\mathcal{V}_{i n},\\mathcal{V}_{o u t})$ Remark. losslessly expanded input, the output of the to be invertible, we do not have restrictions on the operator expansion VV(1) Intuitively, a lossless operator -lossless for the origina $g(\\cdot)$ ·tput. (2) For conciseness, we use ‘ ’ interchangeably. (3) We only require the v Eexpanded oper pansion can be understood a $^{\\cdot}\\mathcal{E}[g]$ Eis a is $(\\mathcal{V}_{i n},\\mathcal{V}_{o u t})$ EVtor expansions .V$\\nu_{o u t}$ ows: when using losslessly expa -lossles V$\\mathcal{V}_{i n}$ and ‘ and $\\mathcal{E}$ V$\\nu_{o u t}$ $\\mathcal{V}_{i n}$ ed",4
e2b95384-9e4b-4445-94fd-17a46fdce3ea,"ref_ids: 454847819065993190, chunk_ids: 1, Score: 0.3457, Text: # 3.3 A TRANSFORMATION PER BLOCK
Now that every LayerNorm in the transformer has been converted to RMSNorm, we can select any $\\mathbf{Q}$ to modify the model. Our initial plan was to collect signals from the model, construct an orthogonal matrix using those signals and to delete parts of the network. We quickly saw that the signals at different blocks of the network were not aligned, and that we would need to apply a different orthogonal matrix at each block, $\\mathbf{Q}_{\\ell}$ .  

Allowing the orthogonal matrix used in each block to differ can be shown to leave the model unchanged using the same proof as Theorem 1 ,  

  
Figure 3: Converting a transformer network from LayerNorm to RMSNorm: the scale matrix diag $(\\alpha)$ is absorbed into the subsequent matrix $\\mathbf{W}_{\\mathrm{in}}$ . Figure shows the block in combined colors. We use $(\\alpha)$ for brevity. The mean-subtraction matrix $\\mathbf{M}$ is applied to each matrix $\\mathbf{W}_{\\mathrm{out}}$ . Layernorm becomes RMSNorm, up to a constant $\\bar{\\sqrt{D}}$ (not shown). Here, the scaling $(\\alpha^{\\prime})$ comes from the previous block.  

  
Figure 4: With the network converted to RMSNorm (see Figure 3 ), we apply the computational-invariance idea. The input weight matrices $\\mathrm{diag}(\\alpha)\\mathbf{W}_{\\mathrm{in}}$ are pre-multiplied by $\\mathbf{Q}^{\\top}$ . The output matrices $\\mathbf{W}_{\\mathrm{out}}\\mathbf{M}$ are post-multiplied by $\\mathbf{Q}$ . In the skip-connection, a new linear layer is added $\\mathbf{Q}_{\\ell}^{\\top}\\mathbf{Q}_{\\ell+1}$ . After these modifications, the matrices can be sliced (hatched areas).  

with the exception of line 5 of Algorithm 1 . Here we see that the residual connection and the output of the block must have the same rotation. To fix this, we modify the residual connection by applying the linear transformation applied to different blocks with the additional linear operation in the residual connection. Unlike the $\\mathbf{Q}_{\\ell-1}^{\\top}\\mathbf{Q}_{\\ell}$ −to the residual. Figure 4 shows how different rotations can be modifications to the weight matrices, these additional operations cannot be pre-computed and add a small $(D\\times D)$ overhead to the model. Nonetheless, they are needed to allow slicing the model (Section 3.4 ) and we see real speedup overall (Section 4 ).  

To compute the matrices $\\mathbf{Q}_{\\ell}$ , we use PCA. We select a calibration dataset from the training set, run it through the model (after converting LayerNorm operations into RMSNorm), and extract the orthogonal matrix of the layer. We use the output of the transformed network to calculate the orthogonal matrices of the next layers. More precisely, if $\\mathbf{X}_{\\ell,i}$ is the output of the $\\ell^{\\mathrm{th}}$ RMSNorm block for the $i^{\\mathrm{th}}$ sequence in the calibration dataset, we compute  

$$
\\mathbf{C}_{\\ell}=\\sum_{i}\\mathbf{X}_{\\ell,i}^{\\top}\\mathbf{X}_{\\ell,i}
$$  

and set $\\mathbf{Q}_{\\ell}$ to the be the eigenvectors of $\\mathbf{C}_{\\ell}$ , sorted by decreasing eigenvalues.

# 3.4 SLICING
The goal of Principal Component Analysis is usually to take a data matrix $\\mathbf{X}$ and compute a lower dimensional representation $\\mathbf{Z}$ , and an approximate reconstruction $\\tilde{\\mathbf{X}}$ :  

$$
\\mathbf{Z}=\\mathbf{X}\\mathbf{Q}\\mathbf{D}\\,,\\qquad\\tilde{\\mathbf{X}}=\\mathbf{Z}\\mathbf{D}^{\\top}\\mathbf{Q}^{\\top}\\,.
$$  

where $\\mathbf{Q}$ is the ectors of ${\\bf X}^{\\top}{\\bf X}$ , and $\\mathbf{D}$ is a $D\\times D_{\\mathrm{small}}$ deletion matrix (containing $D_{\\mathrm{small}}$ The reconstruction is columns of the $D\\times D$ ×$L_{2}$ identity matrix), which removes some of the columns of the matrix to the left. optimal, in the sense that QD is a linear mapping that minimizes $\\lVert\\mathbf{X}-\\tilde{\\mathbf{X}}\\rVert^{2}$ .  

When we apply PCA to the signal matrix $\\mathbf{X}$ bween blocks, we never materialize the $N\\times D$ signal matrix, but we apply the deletion matrix Dto the operations preceding and succeeding the construction of that matrix, which have already been multiplied by $\\mathbf{Q}$ in the above. We delete rows of $\\mathbf{W}_{\\mathrm{in}}$ that we have inserted into the residual connection (see Figure and columns of $\\mathbf{W}_{\\mathrm{out}}$ and $\\mathbf{W}_{\\mathrm{embd}}$ . We also delete both rows 4 ). and columns of the matrix $\\mathbf{Q}_{\\ell-1}^{\\top}\\mathbf{Q}_{\\ell}$ −

# 4 EXPERIMENTAL VALIDATION
Setup We use HuggingFace Transformers ( Wolf et al. ,2019 ) to implement our code with PyTorch (Paszke et al. ,2019 ). The computation of $\\mathbf{Q}$ is performed on a single H100 GPU with 80GB of memory, taking approximately 3.5 hours to complete for the L LAMA -2 70B model. During the PCA calculation, we use double precision for computing the eigenvectors of the covariance matrix. We find that using single precision for eigenvector calculations in PyTorch leads to a discrepancy in the final accuracy, as detailed in Appendix A.2 .  

We experiment with two different calibration sets: 1024 samples from the WikiText-2 training dataset ( Merity et al. ,2016 ) and 5000 samples from the Alpaca training dataset ( Taori et al. ,2023 ). Sequence lengths are chosen as the maximum of each language model. An ablation study on the calibration set size and sequence length is presented in Appendix A.3 .  

Models, Tasks, and GPUs We evaluate all our experiments on OPT ( Zhang et al. ,2022 ), L LAMA -2 (Touvron et al. ,2023 ) model families, and additionally evaluate Phi-2 (in our zero-shot task) experiments. We exclude OPT 175B, as it is outperformed by smaller L LAMA -2 models. Nonetheless, we anticipate that this larger model will yield improved results, as larger models typically offer more promising opportunities for compression (see Section 4.1 ). We evaluate our scheme on both language generation as well as popular zero-shot tasks. To demonstrate the comprehensive speedup achieved by SliceGPT we use: Quadro RTX6000 GPUs with 24GB of memory as a representative example of consumer-level GPUs; 40GB A100s and 80GB H100s to provide datacenter-level benchmarks.  

Baseline Setup We initially planned to compare our results against a scheme that pruned columns (or rows) with the smallest norm but found that this baseline was very poor, with the perplexity of the model soaring into the 1000s after pruning just a few columns. Instead, we compare SliceGPT against SparseGPT ( Frantar & Alistarh ,2023 ) employing a 2:4 sparsity ratio, as this is the only sparsity scheme which achieves speedup ( Mishra et al. ,2021 ).",4
e2b95384-9e4b-4445-94fd-17a46fdce3ea,"ref_ids: 454846008144214678, chunk_ids: 3, Score: 0.3203, Text: # 3.3 BIAS NORM
Conformer (Gulati et al., 2020) utilizes LayerNorm (Ba et al., 2016) to normalize the module activations. Given $\\mathbf{x}$ with $D$ channels, LayerNorm is formulated as:  

$$
\\operatorname{LayerNorm}(\\mathbf{x})={\\frac{\\mathbf{x}-\\operatorname{E}[\\mathbf{x}]}{\\sqrt{\\operatorname{Var}[\\mathbf{x}]+\\epsilon}}}\\odot\\gamma+\\beta.
$$  

Specifically, it first computes the mean $\\operatorname{E}[\\mathbf{x}]$ and the standard-deviation $\\sqrt{\\mathrm{Var}[\\mathbf{x}]}$ pfor normalizing, scaling the vector length to $\\sqrt{D}$ . Then it uses the learnable channel-wise scale $\\gamma$ and bias $\\beta$ for transformation, which helps to adjust the size of activations and balance the relative contributions of specific modules. However, we observe that the trained Conformer using LayerNorm suffers from two problems: 1) It sometimes sets one channel to a large constant value, e.g. 50. We argue that it aims to “defeat” the LayerNorm which fully removes the vector length, functioning as a very large value so that length information could be retained after normalization. 2) Some modules (typically feed-forward or convolution) are “dead” as they have extremely small output values, e.g., $10^{-\\dot{6}}$ .We argue that early in training, the un-trained modules are not useful so they are “turned off” by the LayerNorm scale $\\gamma$ approaching zero. If the scale $\\gamma$ oscillates around zero, the inconsistent sign constantly reverses the gradient directions back-propagating to the modules. Because of the inconsistent gradient sign, the modules never learn anything useful, since this is a bad local optimum which is hard to escape because of the dynamics of stochastic gradient descent-like updates.  

To address above problems, we propose the BiasNorm which is intended to be a simpler replacement of LayerNorm. Specifically, BiasNorm is formulated as:  

$$
B i a s N o r m({\\bf x})=\\frac{{\\bf x}}{\\mathrm{RMS}[{\\bf x}-{\\bf b}]}\\cdot\\exp(\\gamma),
$$  

where $\\mathbf{b}$ is the earnable channel-wise bias, $\\mathrm{RMS}[\\mathbf{x}-\\mathbf{b}]$ is the root-mean-square value taken over channels, and γis a scalar. We first remove the operation of mean subtraction since it is a waste of time unless it follows a non-linearity. The bias bserves as the large constant value which allows to retain the vector length information after normalization. Since the scale $\\exp(\\gamma)$ is always positive, it avoids the gradient oscillation problem.

# 3.4 SWOOSH RAND SWOOSH LACTIVATION FUNCTIONS
Conformer (Gulati et al., 2020) adopts Swish (Ramachandran et al., 2017) activation function with the following formula:  

$$
\\operatorname{Swish}(x)=x\\cdot(1+\\exp(-x))^{-1}.
$$  

In this work, we propose two new activation functions respectively called SwooshR and SwooshL as replacements of Swish:  

$$
\\begin{array}{r l}&{{S w o o s h R}(x)={\\log(1+{\\exp(x-1)})}-0.08x-0.313261687,}\\\\ &{{S w o o s h L}(x)={\\log(1+{\\exp(x-4)})}-0.08x-0.035.}\\end{array}
$$  

In SwooshR , the offset 0.313261687 is to make it pass through the origin; in SwooshL , the offset 0.035 was tuned, which slightly outperformed the value exactly making the curve pass through the origin. We present the curves of Swish, SwooshR , and SwooshL in Appendix Section A.2. SwooshL is roughly a right shifted version of SwooshR . Note that the suffix “L” or “R” represents whether the left or right zero-crossing is at or around $x=0$ . Similar to Swish, SwooshR and SwooshL have lower bounds and are non-monotonic. Compared to Swish, the most striking difference is that SwooshR and SwooshL have non-vanishing slopes for negative inputs, which helps to escape from situations where the input is always negative and prevents the denominator term in Adam-type updates from getting dangerously small. When replacing Swish with SwooshR , we observe that the modules with bypass connections, such as feed-forward and ConvNeXt, tend to learn a large negative bias in the preceding linear layer to learn “normally-off” behavior. Therefore, we use SwooshL for these “normally-off” modules and use SwooshR for convolution modules and the rest of Conv-Embed .",4
