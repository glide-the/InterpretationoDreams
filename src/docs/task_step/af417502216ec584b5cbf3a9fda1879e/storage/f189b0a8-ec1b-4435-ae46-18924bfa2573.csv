角色,内容,分镜
f189b0a8-ec1b-4435-ae46-18924bfa2573,挑战,5>2
f189b0a8-ec1b-4435-ae46-18924bfa2573,"### 问题提出

在探讨大模型中的LayerNorm和RMSNorm的区别时，如何平衡模型性能与计算资源消耗，以及如何解决数据偏见和伦理问题，成为了关键的挑战。具体来说：

1. **模型性能与计算资源消耗的平衡**：
   - LayerNorm和RMSNorm在计算复杂度上存在差异，RMSNorm因其只归一化方差而具有更高的计算效率。然而，LayerNorm在许多任务中表现出色，尤其在NLP领域。如何在选择归一化方法时，既保证模型性能，又有效降低计算资源消耗？
   - 在大规模模型训练中，如何通过优化归一化方法（如选择RMSNorm）来减少计算开销，同时保持或提升模型性能？

2. **数据偏见和伦理问题的解决**：
   - LayerNorm和RMSNorm在处理数据时，是否会对数据偏见产生影响？例如，归一化方法是否可能放大或缩小数据中的某些偏见？
   - 在设计和选择归一化方法时，如何确保其不会引入或加剧数据偏见，从而符合伦理要求？例如，是否可以通过调整归一化参数或引入额外的约束来减少偏见？

这些问题不仅涉及技术层面的优化，还涉及到伦理和公平性的考量，需要在研究和应用中综合考虑。",5>2
f189b0a8-ec1b-4435-ae46-18924bfa2573,"ref_ids: 454845727870837706, chunk_ids: 4, Score: 0.3984, Text: # 2 Problem Formulation

# 2.1 Related Work
Large Language Models (LLMs). The advent of LLMs has led to a paradigm shift in the study of natural language processing (NLP), computer vision, information retrieval, and other domains[Menghani, 2023, Chen et al., 2023, Jiang et al., 2023]. The impressive effectiveness and generalizability of LLMs has come at the price of a drastic increase in LLM sizes [Treviso et al., 2023] and consequent challenges, including huge amounts of computational resources and data required to train, and prohibitive expenses at both training and deployment stages [Bender et al., 2021].  

Efficient Machine Learning (ML) Inference. LLMs belong to a class of models called foundation models [Bommasani et al., 2021] – models that are trained once and can then be used to serve a wide variety of tasks. As such, we expect inference cost to dominate the overall cost of such models and hence focus on works that reduce the cost of ML inference [Menghani, 2023]. The most common approach for efficient ML inference is model compression i.e., replacing a large model with a smaller model of comparable accuracy. Common techniques for model compression include (i) model pruning [Hassibi et al., 1993, LeCun et al., 1989] which drops parts of the model with minimal accuracy loss, (ii) quantization [Jacob et al., 2018, Vanhoucke et al., 2011] which reduces model memory footprints and inference latency by reducing the precision of data representation (e.g., FP32 to INT8), (iii) knowledge distillation [Hinton et al., 2015, Urban et al., 2016] which trains small student models to mimic large teacher models, and (iv) Neural Architecture Search [Elsken et al., 2019, Zoph and Le, 2016] which tunes model architecture to improve model performance, under inference cost constraints. Such static efficiency optimizations typically produce a fixed model with lower inference cost and lower accuracy compared to the large model which may not suffice for foundation models like LLMs, whose core premise is that the same model will serve a range of tasks, each with its own accuracy/cost constraints. This is already manifesting in inference platforms described in Section 1 which need more dynamic optimizations to meet the demands of all users.  

Hybrid ML Inference. Recent works [Kag et al., 2022, Ding et al., 2022] have introduced a new inference paradigm called hybrid inference which uses two models of different sizes instead of a single model for inference. The smaller model (e.g. Llama2 [Touvron et al., 2023]) generally has lower inference cost but also lower accuracy than the larger model (e.g. GPT-4 [OpenAI, 2023]). The key idea is to identify and route easy queries to the small model so that inference cost can be reduced while maintaining response quality. By tuning a threshold on query difficulty we can dynamically trade off quality and cost for the same inference setup. [Kag et al., 2022] study this setup for image classification and propose to train the small model, large model, and router from scratch. However LLM training is expensive and retraining LLMs from scratch for every scenario goes against the very premise of inference with pre-trained foundation models. Moreover text generation [Iqbal and Qureshi, 2022] is often more ambiguous and challenging than image classification due to which novel techniques are required for effective hybrid LLM inference for text generation.  

Inference with Multiple LLMs. Some recent works [Jiang et al., 2023, Chen et al., 2023, Leviathan et al., 2023, Kim et al., 2023] use multiple LLMs for inference but these approaches typically call more than one LLM for a single query that can incur significant computational overheads. Specifically [Jiang et al., 2023] calls an ensemble of LLMs at inference time due to which the inference cost will be proportional to the number of models in the system. [Chen et al., 2023] performs inference using a cascade of LLMs where responses to the query are generated sequentially by the LLMs in the cascade until one of the models has a confidence score higher than a predefined threshold. Our work provides high quality responses while always making a single LLM call for all queries and will thus incur much lower computational cost than both of these works on average. Speculative decoding, introduced in [Leviathan et al., 2023, Kim et al., 2023] speeds up decoding of expensive models by invoking small-and-efficient decoders on the “easy” decoding steps. Instead, in our work we are interested in query routing which assigns “easy” queries to small models to reduce overall inference costs while maintaining high performance. While the two approaches have different goals, an interesting line of future work would be to combine these so that our router assigns queries to the small or large model based on query difficulty and then speculative decoding is applied on top to speed up inference for queries assigned to the large model thereby leading to further cost reduction.",5>2
f189b0a8-ec1b-4435-ae46-18924bfa2573,"ref_ids: 454845727779349442, chunk_ids: 0, Score: 0.2539, Text: # 6 Related Work
Model Quantization Traditional model quantization algorithms mainly focus on the cases where both parameters and activations of the model are quantized ( Lin et al. ,2015 ;Hubara et al. ,2016 ;Tailor et al. ,2021 ;Ni et al. ,2020 ). However, directly quantizing the model will greatly decrease the accuracy of the models, and one important technique to improve the performance is Quantization Aware Training (QAT) ( Jacob et al. ,2018 ), where it simulates the quantization procedure in training to improve the accuracy of the quantized model further. For Transformer based models, the boundary of the compression level has been continuously advanced. For example, 8 -bits quantized transformers as in FullyQT ( Prato et al. ,2019 ) and Q8BERT (Zafrir et al. ,2019 ), 4 -bits quantized BERT in Wu et al. (2023 ) and tenary case as in TernaryBERT (Zhang et al. ,2020 ).  

Model Quantization for LLMs. For quantizing LLMs, due to their prohibitive training expense, we can only use a few training data for calibration. There are two major directions: 1) weight-only quantization, where the weights are quantized into lower bits. In Frantar et al. (2023a ); Yao et al. (2022 ), authors optimize the output error on the calibration set using OBS and gradient descent. 2)  

Activation and weight quantization, where both activations and weights are quantized into lower bits. In this case, the major obstacle is the outliers in activations. LLM.int8() ( Dettmers et al. ,2022 ) addresses this problem by isolating those outliers in fp16/bf16. However, such implementation leads to large latency overhead and is even slower than fp16 inference. Recent studies ( Wei et al. ,2023 ;Xiao et al. ,2023 ) found that the outliers only exist in certain channels, and use the LayerNorm weights ( Wei et al. ,2023 ) and calibrated scales ( Xiao et al. ,2023 )to smooth those channels. Xiao et al. (2023 ) has already proved that we can achieve almost lossless W8A8 quantized LLMs using a few calibration data, without manipulating the original model weights.

# 7 Conclusion and Limitations
In this paper, we propose a data-free fast weightonly quantization algorithm, namely EasyQuant, for LLMs, that potentially improves the quantized model’s performance without using any training data. Our analysis reveals the intrinsic origins of the performance loss when quantizing the model weights into lower bits. We show that by isolating the outliers from quantization, the accuracy of the quantized LLM increases accordingly with decreased reconstruction error. Our experiment proved that EasyQuant significantly outperforms RTN in a data-free setting, and also behaves better than data-dependent algorithms. EasyQuant can finish the quantization for a 176B-sized model within 10 minutes and the overhead of dequantization in EasyQuant is negligible.  

However, we also point out some limitations of our work: The outlier recovery functionality in EasyQuant requires extra CUDA kernels for implementation. Moreover, weight-only quantization can only reduce the memory footprint without any computation cost reduction, hence the latency of our model cannot be minimized. In addition, this outlier isolation will make the weight/activation quantization more challenging because the weight includes numbers under different precision. We have also noticed that EasyQuantcannot outperform the data-dependent methods in all tasks, this motivates us to investigate more effective algorithms in future studies.



# A Appendix
Table 10: Perplexity and zershot results for BLOOM model family   


<html><body><table><tr><td rowspan=""2""></td><td rowspan=""2""></td><td colspan=""3"">Perplexity-based Task</td><td colspan=""4"">Zero-shot Task</td></tr><tr><td>WikiText2</td><td>PTB</td><td>C4</td><td>PIQA</td><td>ARC-easy</td><td>ARC-Challenge StoryCloze</td><td></td></tr><tr><td rowspan=""2"">BLOOM</td><td>fp16</td><td>22.42</td><td>43.69</td><td>26.6</td><td>65.07%</td><td>41.71%</td><td>24.15%</td><td>61.94%</td></tr><tr><td>RTN</td><td>25.90</td><td>51.10</td><td>29.89</td><td>63.11%</td><td>39.40%</td><td>23.89%</td><td>60.15%</td></tr><tr><td rowspan=""2"">560M</td><td>GPTQ</td><td>24.03</td><td>46.97</td><td>28</td><td>64.31%</td><td>40.24%</td><td>23.46%</td><td>61.17%</td></tr><tr><td>EasyQuant</td><td>23.74</td><td>46.86</td><td>28.03</td><td>63.06%</td><td>40.32%</td><td>24.15%</td><td>59.64%</td></tr><tr><td rowspan=""2"">BLOOM</td><td>fp16</td><td>17.69</td><td>57.96</td><td>22.05</td><td>67.14%</td><td>45.41%</td><td>25.68%</td><td>63.27%</td></tr><tr><td>RTN</td><td>22.00</td><td>66.85</td><td>24.44</td><td>65.29%</td><td>42.51%</td><td>23.34%</td><td>60.66%</td></tr><tr><td rowspan=""2"">1.1B</td><td>GPTQ</td><td>19.05</td><td>62.48</td><td>23.25</td><td>66.05%</td><td>44.49%</td><td>25.51%</td><td>62.32%</td></tr><tr><td>EasyQuant</td><td>18.51</td><td>61.83</td><td>22.94</td><td>66.65%</td><td>43.73%</td><td>25.51%</td><td>62.06%</td></tr><tr><td rowspan=""2"">BLOOM</td><td>fp16</td><td>15.39</td><td>30.00</td><td>19.49</td><td>69.97%</td><td>48.11%</td><td>26.79 %</td><td>65.44%</td></tr><tr><td>RTN</td><td>16.97</td><td>33.58</td><td>21.26</td><td>67.74%</td><td>44.70%</td><td>26.45 %</td><td>62.95%</td></tr><tr><td rowspan=""2"">1.7B</td><td>GPTQ</td><td>16.48</td><td>31.84</td><td>20.55</td><td>68.77%</td><td>44.49%</td><td>25.94%</td><td>64.48%</td></tr><tr><td>EasyQuant</td><td>16.01</td><td>31.50</td><td>20.15</td><td>68.99%</td><td>46.89%</td><td>26.19%</td><td>65.37%</td></tr><tr><td rowspan=""2"">BLOOM</td><td>fp16</td><td>13.48</td><td>25.34</td><td>17.49</td><td>70.51%</td><td>53.24%</td><td>30.55 %</td><td>67.79%</td></tr><tr><td>RTN</td><td>14.76</td><td>27.68</td><td>18.76</td><td>69.86%</td><td>51.35%</td><td>29.52%</td><td>67.09%</td></tr><tr><td rowspan=""2"">3B</td><td>GPTQ</td><td>14.2</td><td>26.49</td><td>18.1</td><td>69.42%</td><td>52.82%</td><td>28.92%</td><td>67.22%</td></tr><tr><td>EasyQuant</td><td>14.01</td><td>26.12</td><td>17.96</td><td>69.80%</td><td>50.72%</td><td>28.58%</td><td>67.35%</td></tr><tr><td rowspan=""2"">BLOOM</td><td>fp16</td><td>11.37</td><td>20.83</td><td>15.20</td><td>73.72%</td><td>57.37%</td><td>33.45 %</td><td>71.99%</td></tr><tr><td>RTN</td><td>12.10</td><td>22.42</td><td>16.06</td><td>72.69%</td><td>56.14%</td><td>32.17 %</td><td>70.72%</td></tr><tr><td rowspan=""2"">7.1B</td><td>GPTQ</td><td>11.73</td><td>21.67</td><td>15.6</td><td>72.96%</td><td>56.14%</td><td>32.25%</td><td>71.36%</td></tr><tr><td>EasyQuant</td><td>11.66</td><td>21.47</td><td>15.52</td><td>73.23%</td><td>55.72%</td><td>32.51 %</td><td>71.10%</td></tr><tr><td rowspan=""2"">BLOOM</td><td>fp16</td><td>8.11</td><td>14.59</td><td>11.71</td><td>79.16%</td><td>67.47%</td><td>44.97 %</td><td>76.89%</td></tr><tr><td>RTN</td><td>8.37</td><td>15.00</td><td>12.04</td><td>79.00%</td><td>66.33%</td><td>43.17 %</td><td>76.00%</td></tr><tr><td rowspan=""2"">176B</td><td>GPTQ</td><td>8.21</td><td>14.75</td><td>11.81</td><td>79.00%</td><td>67.42%</td><td>44.10%</td><td>76.32%</td></tr><tr><td>EasyQuant</td><td>8.21</td><td>14.75</td><td></td><td>11.87 79.05%</td><td>67.8%</td><td>44.45%</td><td>77.28%</td></tr></table></body></html>",5>2
f189b0a8-ec1b-4435-ae46-18924bfa2573,"ref_ids: 454895483053685384, chunk_ids: 2, Score: 0.2539, Text: # EasyQuant: An Efficient Data-free Quantization Algorithm for LLMs
Hanlin Tang   

Yifu Sun   

Decheng Wu   

Kai Liu   

Jianchen Zhu   

Zhanhui Kang

# Abstract
Large language models (LLMs) have proven to be very superior to conventional methods in various tasks. However, their expensive computations and high memory requirements are prohibitive for deployment. Model quantization is an effective method for reducing this overhead. The problem is that in most previous works, the quantized model was calibrated using a few samples from the training data, which might affect the generalization of the quantized LLMs to unknown cases and tasks. Hence in this work, we explore an important question: Can we design a data-free quantization method for LLMs to guarantee its generalization performance?  

In this work, we propose EasyQuant, a trainingfree and data-free weight-only quantization algorithm for LLMs. Our observation indicates that two factors: outliers in the weight and quantization ranges, are essential for reducing the quantization error. Therefore, in EasyQuant, we leave the outliers (less than $1\\%$ ) unchanged and optimize the quantization range to reduce the reconstruction error. With these methods, we surprisingly find that EasyQuant achieves comparable performance to the original model. Since EasyQuant does not depend on any training data, the generalization performance of quantized LLMs are safely guaranteed. Moreover, EasyQuant can be implemented in parallel so that the quantized model could be attained in a few minutes even for LLMs over 100B. To our best knowledge, we are the first work that achieves comparable performance with datadependent algorithms under a data-free setting and our algorithm runs over 10 times faster than the data-dependent methods.

# 1 Introduction
Recent work has already proved the superior performance of Transformer ( Vaswani et al. ,2017 )based LLMs ( Workshop ,2023 ;Zhang et al. ,2022 ;Touvron et al. ,2023 ;Brown et al. ,2020 ;Rae et al. ,2021 ;Smith et al. ,2022 ;Chowdhery et al. ,2022 ;Zeng et al. ,2022 ) on various tasks over traditional methods, and has attracted massive interest in how to improve and utilize those LLMs. However, the model size also grows dramatically along with improved performance. Hence the memory footprint and computational cost become the bottleneck for deploying those models. One promising solution to alleviate this overhead is model quantization ( Frantar et al. ,2023a ;Xiao et al. ,2023 ), where we quantize weight only or weight and activation both i order to reduce memory consumption and computational cost.  

Although model quantization is a well-studied area for normal-sized models, such as BERT ( Devlin et al. ,2018 ) and GPT-2 ( Radford et al. ,2019 ), it is still a quite challenging task for LLMs. One major reason is that previous lossless model quantization algorithms require retraining for the quantized model, which is too expensive for models over billions of parameters. Beyond this, previous models are usually designed for specific domain tasks, which means the training data are sampled from limited task domains. However, recent LLMs are usually trained on various domains of data corpus, and they have shown to be quite effective for multi-domain zero-shot tasks. In this case, if we only retrain the quantized LLMs using partial domain corpus, the generalization ability of LLMs might get worse. Therefore both efficiency and generalization guarantees are very important for designing LLMs quantization algorithms. To date, for low-bits weight-only quantization, several posttraining algorithms have been proposed ( Frantar et al. ,2023a ;Yao et al. ,2022 ). However, those methods also require a small calibration set sampled from training data, which still takes at least several hours. Moreover, the use of those calibration data also brings the risk of making the model overfit to the calibration set.  

  
Figure 1: Pipeline of EasyQuant. We first find all the outliers in weight and keep them in full precision (fp32/fp16/bf16). Afterward, we optimize the quantization range (denoted as $q_{r a n g e})$ in order to approximate the normal values more precisely. In the end, the normal values are quantized into lower bits (denoted as $Q[\\cdot])$ with optimized quantization ranges and we set the outliers unchanged in weight.  

Our Contribution: In this work, we propose a novel data-free model quantization algorithm, namely EasyQuant, that potentially improves the performance of low-bits quantized LLMs. The generalization ability of LLMs is inherently guaranteed since EasyQuant does not need any input data. By running EasyQuant for only a few minutes, we can quantize public-available OPT-176B, BLOOM176B, and LLAMA-65B into lower bits without significant loss on various benchmarks. To our best knowledge, this is the first data-free LLM quantization algorithm for LLM quantization without notable system overhead.  

Moreover, our work reveals the essential factors that cause the performance degradation of the quantized LLMs. We show that the outliers in weights are more critical to the model’s performance compared to the normal elements. Beyond this, we propose to use a gradient-based method for optimizing the quantization range. These two strategies can also be used in other scenarios, such as weight-activation quantization and quantizationaware training (QAT).  

Last but not least, we develop efficient CUDA kernels for outlier isolation in dequantization, and proved that hold $1\\%$ outliers in weights unquantized brings negligible (less than $0.1\\%$ ) overhead w.r.t to overall latency. We also propose to implement EasyQuant in parallel for quantizing each weight in the model, which means a 175B-sized model can be quantized into 4 -bits within $10\\;\\mathrm{min}$ -utes.",5>2
