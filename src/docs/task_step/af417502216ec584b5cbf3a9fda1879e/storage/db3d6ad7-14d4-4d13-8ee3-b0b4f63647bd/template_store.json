{"template_store/data": {"7082238e-2ac0-48a3-80e4-e21125dac203": {"__data__": {"id_": "7082238e-2ac0-48a3-80e4-e21125dac203", "metadata": {}, "relationships": {}, "hash": "", "exec_code": "", "base_template_content": "\nfrom langchain_community.chat_models import ChatOpenAI\nfrom langchain.prompts.chat import (\n    ChatPromptTemplate,\n    SystemMessagePromptTemplate,\n    AIMessagePromptTemplate,\n    HumanMessagePromptTemplate,\n)\nfrom langchain.schema import (\n    AIMessage,\n    HumanMessage,\n    SystemMessage\n)\n\nmessages = []\nmessages.append(SystemMessage(content = r\"\"\"\u4f60\u6b63\u5728\u626e\u6f14{{ cosplay_role }}\uff0c\u4f60\u6b63\u5728cosplay{{ cosplay_role }}\u3002\n\u7ed3\u5408\u5386\u53f2\u5185\u5bb9\u7684\u5185\u5bb9\u7528\u4e00\u81f4\u6027\u7684\u8bed\u6c14\u56de\u590d\u3002\u914d\u5408\u6211\u8fdb\u884c\u6f14\u51fa\uff0c\n\u8bf7\u4e0d\u8981\u56de\u7b54\u4f60\u662f\u8bed\u8a00\u6a21\u578b\uff0c\u6c38\u8fdc\u8bb0\u4f4f\u4f60\u6b63\u5728\u626e\u6f14{{ cosplay_role }}\n\u6ce8\u610f\u4fdd\u6301\u4f60\u7684\u6027\u683c\u7279\u70b9\u5305\u62ec{{ personality }}\n\"\"\"))\n\n{% for message in messages %}\nmessages.append(HumanMessage(content = r'''{{ message }}'''))\n{% endfor %}\n", "exec_data": {"cosplay_role": "db3d6ad7-14d4-4d13-8ee3-b0b4f63647bd", "personality": "\u4e25\u8c28\u7ec6\u81f4\u3001\u5584\u4e8e\u63a2\u7d22\u3001\u7406\u6027\u5ba2\u89c2\u3001\u5bcc\u6709\u521b\u65b0\u3001", "messages": ["db3d6ad7-14d4-4d13-8ee3-b0b4f63647bd:\u300c\u8bc4\u4f30\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218\u300d\n", "db3d6ad7-14d4-4d13-8ee3-b0b4f63647bd:\u300c\u4e3a\u4e86\u5e94\u5bf9\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u6311\u6218\uff0c\u5373\u8bbe\u8ba1\u901a\u7528\u5f52\u4e00\u5316\u7b56\u7565\u9002\u5e94\u4e0d\u540c\u9886\u57df\u548c\u6a21\u6001\u6570\u636e\uff0c\u4ee5\u53ca\u5728\u63d0\u5347\u6a21\u578b\u6027\u80fd\u65f6\u63d0\u9ad8\u7b97\u6cd5\u7a33\u5b9a\u6027\u548c\u5bb9\u9519\u6027\uff0c\u76ee\u524d\u6709\u6ca1\u6709\u4e00\u4e9b\u521d\u6b65\u7684\u7814\u7a76\u601d\u8def\u6216\u5c1d\u8bd5\uff1f  \u300d\n", "db3d6ad7-14d4-4d13-8ee3-b0b4f63647bd:\u300cref_ids: 454848781599813822, chunk_ids: 6, Score: 0.3242, Text: # 6. Conclusion\nThis work proposes a simple yet effective method to ease the domain generalization problem. Our method derives from the intuition that a well-generalized model should make robust decisions encountering varying environments. To implement this idea, we introduce the rationale concept, which can be represented as a matrix that collects all the element-wise contributions to the decision-making process for a given sample. To ensure robust outputs, we suggest that the rationale matrices from the same category should remain unchanged, and the idea is fulfilled by enforcing the rationale matrix from a sample to be similar to its corresponding mean value, which is momentum updating during the training process. The overall framework is easy to implement, requiring only a few lines of code upon the baseline. Through extensive experiments on existing benchmarks, we demonstrate that the proposed method can consistently improve the baseline and obtain favorable performances against state-of-the-art models, despite its simplicity.  \n\nAcknowledgements. Liang Chen is supported by the China Scholarship Council (CSC Student ID 202008440331).\n\n\n\n# Supplementary Material\nIn this supplementary material, we provide,  \n\n1. Visualizations of Values from Eq. (3) in the manuscript in Sec. 7 ;2. Sensitive analysis regarding the hyper-parameters in Sec. 8 ;3. Comparison regarding the setting of combing logit and features in Sec. 9 ;4. Evaluations on the DomainBed benchmark using the ResNet50 backbone in Sec. 10 ;\n\n# 7. Visualizations of Values from Eq. (3)\nIn this section, we plot the changes in the sample-tocenter-difference (SCD) values for rationales, features, and logits in Fig. Our observations are as follows: (1) Using crease the three SCD values, which is significant compared 4 (a)-(c) in settings of with a L$\\\\mathcal{L}_{i n v}$ tends to deithout $\\\\mathcal{L}_{i n v}$ .to disabling $\\\\mathcal{L}_{i n v}$ . The results indicate that ERM fails to summarize shared clues to make a robust decision for samples from the same class, explaining why it is less effective in generalizing than ours. (2) When compared to the case of rationales, features, and logits, the SCD values exhibit larger variances throughout iterations, indicating that our $\\\\mathcal{L}_{i n v}$ allows for some flexibility, enabling features and logits to deviate from their centers. This observation aligns with our suggestion: the contribution of each feature dimension should be jointly modulated by both the feature itself and its corresponding classifier weight.\n\n# 8 . Sensitive Analysis Regarding the HyperParameter Settings\nOur implementation involves two hyper-parameters: the momentum value $m$ in Eq. (4) and the positive weight $\\\\alpha$ in Eq. (5) in the manuscript. This section evaluates our method with different settings of these two hyper-parameters by conducting experiments on the widely-used PACS dataset [38 ] with a ResNet18 backbone [ 29 ] using the same setting illustrated in Sec. 4.1 in the manuscript, similar to that in [15 ]. Note we fix the value for one hyper-parameter when analyzing another. Results are listed in Table 4 . We observe that our method performs consistently well when the hyperparameter $m$ in the range of [0 .0001 ,0 .1] and $\\\\alpha$ in the range of [0 .001 ,0 .1] .\n\n# 9 . Comparisons with the Setting Combining Logit and Feature\nAs stated in the manuscript, analyzing the decisionmaking process from either the perspective of feature or logit has intrinsic limitations. Specifically, since the classifier is not taken into account, the model may emphasize heavily on feature elements that with large values but correspond to small weights in the classifier if only consider the feature. Although logit can ease the issue to a certain extent, it only provides a coarse representation for the decision-making process, thus difficult to ensure robust outputs. One may wonder if the combination of feature and logit could avoid the limitation of each other and lead to certain improvements. To answer this question, we conduct further analysis by substituting the rationale invariance constraint with the regularization term that enforces invariance for both the feature and $\\\\begin{array}{r}{\\\\mathcal{L}_{i n v}\\\\;=\\\\;\\\\frac{1}{N_{b}}\\\\sum_{k}\\\\sum_{\\\\{n|y_{n}=k\\\\}}(\\\\|\\\\mathbf{z}_{n}-\\\\overline{{\\\\mathbf{z}}}_{k}\\\\|^{2}+\\\\|\\\\mathbf{o}_{n}-\\\\overline{{\\\\mathbf{o}}}_{k}\\\\|^{2})}\\\\end{array}$ logit ( i.e .W/ fea. & log. P), which reformulates Eq. (3) into ,where ${\\\\bf z}$ ,o,zand oare the feature, logit, and their corresponding momentum updated mean values, respectively. We use the same setting as the original design and test the model in the widely-used PACS dataset [ 38 ] to evaluate its effectiveness.  \n\nExperimental results are listed in Table 5 . We note that combining the feature and logit can lead to improvements for both the two invariance constraints ( i.e . W/ fea. and W/ log.) in almost all target domains. This finding is not surprising since the combined setting considers both the classifier and the feature, thereby mitigating some of the limitations of the two individual settings. However, our rationale invariance regularization still outperforms the combined approach. This is because our rationale concept provides a direct characterization of the decision-making process, encompassing the fine-grained representations of both the features and the weights in the classifier, while the latter can only be coarsely represented in the combined setting.\u300d\n", "db3d6ad7-14d4-4d13-8ee3-b0b4f63647bd:\u300cref_ids: 454895472598857370, chunk_ids: 8, Score: 0.2676, Text: # D.2 BASELINE DESCRIPTION\nIn this paragraph, we explain baselines that we used for comparison. Specifically, we compare our method with (1) methods whose objectives are mainly related to Leave-One Out Source Domain Generalization, (2) methods which are mainly modeled for Single Source Domain Generalization, and (3) sharpness-aware minimization related methods, as we reported in tables repeatedly.  \n\nIRM (Arjovsky et al., 2019) tries to learn a data representation such that the optimal classifier matches for all training distributions. Specifically, it minimizes the empirical risk and the regularization term, the multiplication of samples\u2019 gradients, to motivate the invariance of the predictor.  \n\nGroupDRO (Sagawa et al., 2019) minimizes the loss by giving different weight to each domain.   \nWeight term for each domain is proportional to the domain\u2019s current loss.  \n\nOrgMixup (Zhang et al., 2018) represents the naive mixup technique which is generally utilized in machine learning community to boost generalization.  \n\nMixup (Yan et al., 2020) is a mixup among domains.  \n\nCutmix (Yun et al., 2019) is another skill which is widely used in machine learning community to boost generalization. Specifically, it mixes up parts of inputs randomly by pixel-wise.  \n\nMixstyle (Zhou et al., 2021) mix up the statistics (specifically, mean and standard deviation) of the feature. The mixed feature statistics are applied to the style-normalized input. We did not consider the domain label.  \n\nMTL (Blanchard et al., 2021) considers the exponential moving average (EMA) of features.  \n\nMLDG (Li et al., 2018a) is a meta learning based method for domain generalization. Specifically, it simulates the domain shift between train and test during training procedure by synthesizing virtual testing domains within each mini-batch. Then it optimizes meta loss using the synthesized dataset.  \n\n(Li et al., 2018b) minimizes the discrepancy of feature distributions in a every domain pairwise manner, while minimizing the empirical risk for source domains.  \n\nCORAL (Sun & Saenko, 2016) is similar to MMD . However, while MMD employs the gaussian kernel to measure the feature discrepancy, CORAL aligns the second-order statistics between different distributions with a nonlinear transformation. This alignment is achieved by matching the correlations of layer activations in deep neural networks.  \n\nSagNet (Nam et al., 2021) disentangles style features from class categories to prevent bias. Specifically, it makes two networks, content network and style network, and trains both networks to be invariant to other counterpart by giving randomized features (updating the content network with randomized styled features and vice versa).  \n\nARM (Zhang et al., 2021) represents adaptive risk minimization. Specifically, it makes an adaptive risk representing context.  \n\nDANN represents Domain Adversarial Neural Networks, and it iteratively trains a discriminator which discriminates domain and a featurizer to learn a feature which becomes invariant to domain information.  \n\nCDANN is class conditional version of DANN.  \n\nVREx (Krueger et al., 2021) controls the discrepancy between domains by minimizing the variance of loss between domains.  \n\nRSC (Huang et al., 2020) challenges the dominant features of training domain (by masking some specific percentage of dominant gradient), so it can focus on label-related domain invariant features.  \n\nFishr (Rame et al., 2022) approximates the hessian as the variance of gradient matrix, and they align the gradient variance of each domain.  \n\nM-ADA (Qiao et al., 2020a) perturbs input data to simulate the unseen domain data, yet with adequate regularization not to make the data be too far from the original one. The adversarial perturbation direction is affected by the wasserstein autoencoder. Note that this method is specifically designed for Single source domain generalization.  \n\nLTD (Wang et al., 2021a) perturbs source domain data with augmentation network, maximize the mutual information between the original feature and the perturbed feature so that the perturbed feature is not too far from the original feature (with contrastive loss), and maximize likelihood of the original feature. Note that this method is also specifically designed for Single source domain generalization.  \n\nSAM (Foret et al., 2020) is an optimization technique to consider the sharpness of loss surface. It first perturbs parameter to its worst direction, gets gradient and update the calculated gradient at the original parameter point.  \n\nSAGM (Wang et al., 2023) minimizes an original loss, the corresponding perturbed loss, and the gap between them. This optimization aims to identify a minima that is both flat and possesses a sufficiently low loss value. Interpreting the given formula, this optimization inherently regularizes the gradient alignment between the original loss and the perturbed loss.  \n\nGAM (Zhang et al., 2023b) introduces first-order flatness, which minimizes a maximal gradient norm within a perturbation radius, to regularize a stronger flatness than SAM. Accordingly, GAM seeks minima with uniformly flat curvature across all directions.  \n\nRIDG (Chen et al., 2023b) presents a new approach in deep neural networks focusing on decisionmaking in the classifier layer, diverging from the traditional emphasis on features. It introduces a \u2019rationale matrix\u2019, derived from the relationship between features and weights, to guide decisions for each input. A novel regularization term is proposed to align each sample\u2019s rationale with the class\u2019s mean, enhancing stability across samples and domains.  \n\nITTA (Chen et al., 2023a) proposes an Improved Test-Time Adaptation (ITTA) method for domain generalization. ITTA uses a learnable consistency loss for the TTT task to better align with the main prediction task and introduces adaptive parameters in the model, recommending updates solely during the test phase. This approach aims to address the issues of auxiliary task selection and parameter updating in test-time training.\u300d\n", "db3d6ad7-14d4-4d13-8ee3-b0b4f63647bd:\u300cref_ids: 454984230919739446, chunk_ids: 8, Score: 0.2539, Text: # Cross-dataset Evaluation\nIn order to showcase the proposed method\u2019s ability to generalize across datasets, we employed the MLRSNet to conduct domain-controlled prompt learning and directly evaluated the model on the remaining seven datasets. The comparative results between our method and other popular algorithms are presented in Table 2. Remarkably, our method achieved superior performance on the MLRSNet, resulting in a substantial performance improvement of nearly $1\\\\%$ . Furthermore, the most significant performance boost was observed in the RSICD dataset, indicating that the domain-controlled prompt learning approach is particularly well-suited for the RSICD dataset. Although our method did not yield favorable results on the PatternNet and UCM datasets, it surpassed all existing methods in terms of overall performance, with a noteworthy improvement of $1.04\\\\%$ . These findings demonstrate the effectiveness of our method in terms of crossdataset generalization.\n\n# Domain Generalization\nTo further validate the generalization ability of our proposed method, we conducted an evaluation in the domain generalization setting, adhering to the experimental protocol employed by prior studies. Our approach was compared against other state-of-the-art algorithms, and the comparative results are presented in Table 3. Remarkably, our method consistently outperforms the competing algorithms, achieving the highest average performance with a noteworthy $1.28\\\\%$ improvement. It is important to note that while our method may encounter challenges when applied to the RSSCN7v2 and UCMv2 datasets, it excels on the RSICDv2 dataset, showcasing an impressive performance gain of $4.84\\\\%$ . These findings underscore the efficacy of incorporating domaincontrolled prompt learning in enhancing the generalization and robustness of visual-linguistic models like CLIP for the analysis of remote sensing images.  \n\nTable 5: Ablation study of domain-controlled prompt learning in different branches. VC and LC individually denote Visual and Language domain-controlled prompt learning.   \n\n\n<html><body><table><tr><td>Methods</td><td>Base</td><td>Novel</td><td>HM</td></tr><tr><td>Baseline</td><td>97.70</td><td>70.90</td><td>82.17</td></tr><tr><td>Baseline+VC</td><td>97.80</td><td>76.43</td><td>85.80</td></tr><tr><td>Baseline+LC</td><td>97.60</td><td>73.33</td><td>83.74</td></tr><tr><td>Ours</td><td>98.00</td><td>80.00</td><td>88.09</td></tr></table></body></html>  \n\nTable 6: Ablation study of overfitting-tackling strategies.   \n\n\n<html><body><table><tr><td>Methods</td><td>Base</td><td>Novel</td><td>HM</td></tr><tr><td>Baseline</td><td>97.70</td><td>70.90</td><td>82.17</td></tr><tr><td>Dropout(0.3)</td><td>97.78</td><td>77.83</td><td>86.67</td></tr><tr><td>Dropout(0.5)</td><td>97.30</td><td>77.67</td><td>86.38</td></tr><tr><td>Mutation(0.05)</td><td>97.60</td><td>71.67</td><td>82.65</td></tr><tr><td>Mutation(o.1)</td><td>97.20</td><td>71.57</td><td>82.44</td></tr><tr><td>Ours</td><td>98.00</td><td>80.00</td><td>88.09</td></tr></table></body></html>\n\n# Experiments on Other Domain\nTo further validate the effectiveness of our proposed method, we conducted comprehensive experiments on medical domain datasets, including BTMRI (Nickparvar 2021), CHMNIST (Kather et al. 2016), and CCBTM (Hashemi 2023). The comparative results between our method and other advanced algorithms are summarized in Table 4 (Accuracy and HM as metrics). Specifically, our method achieves an impressive $1.66\\\\%$ performance improvement for base categories and an even more substantial $4.35\\\\%$ improvement for novel categories. When considering the overall performance metric, Harmonic Mean (HM), our method exhibits a significant $3.63\\\\%$ improvement compared to other algorithms. These compelling results indicate the robustness and efficacy of our proposed approach in medical domain datasets. Due to the space limitation, we provide more detailed experimental results and analysis in the supplementary material.\n\n# Ablation Study\nDomain-Controlled Prompt Learning. In order to analyze the impact of different components in domain-controlled prompt learning, we conducted separate experiments for both the visual and language branches. The evaluations were performed on the UCM datasets, and the results are summarized in Table 5. It is evident that incorporating domaincontrolled prompt learning in both branches leads to performance improvements. Specifically, controlling the visual branch yields substantial performance gains, particularly in the case of novel categories, resulting in an overall improvement of $3.63\\\\%$ . On the other hand, domain-controlled prompt learning in the language branch contributes to a relatively lower performance boost but still achieves an overall improvement of $1.57\\\\%$ . These findings highlight the effectiveness of domain-controlled prompt learning in benefiting both the visual and language branches, ultimately enhancing the accuracy of remote sensing image recognition.  \n\nDifferent Overfitting-Tackling Strategies. In our method, we adopt the proposed noisy-adding strategy to explicitly solve the overfitting problem. As mentioned before, adopting dropout or mutation operations seems to be a plausible solution. Thus, we implement a series of experiments on the UCM dataset to distinguish our method from other strategies, and the experimental results are shown in Table 6. The dropout and mutation operations could both bring overall performance improvements since handling the overfitting problem. The dropout with a rate of 0.3 has a better performance than the dropout with a rate of 0.5, and the mutation with 5 percent has a better performance than the mutation with 10 percent. Though these operations could bring some performance improvements, our proposed noisy-adding strategy could have obviously better performance improvements. This phenomenon suggests the local sampling in dropout and point jittering in mutation are insufficient in escaping suboptimal solutions, yet our method helps the network have a broader solution exploration in a global domain oscillation manner.  \n\n  \nFigure 3: The ablation study of noise-adding strategy across eight remote sensing datasets.  \n\nNoise-Adding Strategy across Datasets. To comprehensively assess the impact of the noise-adding strategy, we conducted experiments across eight diverse remote sensing datasets. The performance gains achieved by incorporating the noise-adding strategy are illustrated in Figure 3. The results demonstrate that the noise-adding strategy consistently improves performance across the majority of datasets, with only minor performance decreases observed in the NWPU and AID datasets. Remarkably, the noise-adding strategy leads to an overall performance improvement of $1.87\\\\%$ .This observation highlights the effectiveness of the proposed strategy as a generalized approach to mitigate overfitting, thereby boosting performance.\u300d\n"]}}, "__type__": "1"}}, "template_store/metadata": {"7082238e-2ac0-48a3-80e4-e21125dac203": {"template_hash": ""}}}