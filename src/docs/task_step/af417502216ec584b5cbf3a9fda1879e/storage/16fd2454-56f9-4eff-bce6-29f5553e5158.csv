角色,内容,分镜
16fd2454-56f9-4eff-bce6-29f5553e5158,方法论,0>2
16fd2454-56f9-4eff-bce6-29f5553e5158,"### 问题

在研究方法逐渐从单一模型转向多模型融合，从监督学习扩展到无监督和自监督学习，并强调模型的可解释性和泛化能力的背景下，LayerNorm和RMSNorm这两种归一化方法在多模型融合和无监督学习中的应用有何异同？它们如何影响模型的可解释性和泛化能力？",0>2
16fd2454-56f9-4eff-bce6-29f5553e5158,"ref_ids: 455038427552559154, chunk_ids: 6, Score: 0.1602, Text: # 2 Background and Related Work
We herein discuss background literature from three different perspectives that may be related to our work: model explanation/attribution methods, efforts on attributional robustness (both attacks and defenses), and other recent related work. Attribution Methods. Existing efforts on explainability in DNN models can be broadly categorized as: local and global methods, model-agnostic and model-specific methods, or as post-hoc and ante-hoc (intrinsically interpretable) methods (Molnar 2019; Lecue et al. 2021). Most existing methods in use today – including methods to visualize weights and neurons (Simonyan, Vedaldi, and Zisserman 2014; Zeiler and Fergus 2014), guided backpropagation (Springenberg et al. 2015), CAM (Zhou et al. 2016), GradCAM (Selvaraju et al. 2017), Grad$\\mathrm{CAM++}$ (Chattopadhyay et al. 2018), LIME (Ribeiro, Singh, and Guestrin 2016), DeepLIFT (Shrikumar et al. 2016; Shrikumar, Greenside, and Kundaje 2017), LRP (Bach et al. 2015), Integrated Gradients (Sundararajan, Taly, and Yan 2017), SmoothGrad (Smilkov et al. 2017)), DeepSHAP (Lundberg and Lee 2017) and TCAV (Kim et al. 2018) – are post-hoc methods, which are used on top of a pre-trained DNN model to explain its predictions. We focus on such post-hoc attribution methods in this work. For a more detailed survey of explainability methods for DNN models, please see (Lecue et al. 2021; Molnar 2019; Samek et al. 2019).  

Robustness of Attributions. The growing numbers of attribution methods proposed has also led to efforts on identifying the desirable characteristics of such methods (AlvarezMelis and Jaakkola 2018; Adebayo et al. 2018; Yeh et al. 2019; Chalasani et al. 2020; Tomsett et al. 2020; Boggust et al. 2022; Agarwal et al. 2022). A key desired trait that has been highlighted by many of these efforts is robustness or stability of attributions, i.e., the explanation should not vary significantly within a small local neighborhood of the input (Alvarez-Melis and Jaakkola 2018; Chalasani et al. 2020). Ghorbani, Abid, and Zou (2019) showed that well-known methods such as gradient-based attributions, DeepLIFT (Shrikumar, Greenside, and Kundaje 2017) and Integrated Gradients (IG) (Sundararajan, Taly, and Yan 2017) are vulnerable to such input perturbations, and also provided an algorithm to construct a small imperceptible perturbation which when added to the input results in changes in the attribution. Slack et al. (2020) later showed that methods like LIME (Ribeiro, Singh, and Guestrin 2016) and DeepSHAP (Lundberg and Lee 2017) are also vulnerable to such manipulations. The identification of such vulnerability and potential for attributional attacks has since led to multiple research efforts to make a model’s attributions robust. Chen et al. (2019) proposed a regularization-based approach, where an explicit regularizer term is added to the loss function to maintain the model gradient across input (IG, in particular) while training the DNN model. This was subsequently extended by (Sarkar, Sarkar, and Balasubramanian 2021; Singh et al. 2020; Wang et al. 2020), all of whom provide different training strategies and regularizers to improve attributional robustness of models. Each of these methods including Ghorbani, Abid, and Zou (2019) measures change in attribution before and after input perturbation using the same metrics: top$k$ intersection, and/or rank correlations like Spearman’s $\\rho$ and Kendall’ $\\tau$ . Such metrics have recently, in fact, further been used to understand issues surrounding attributional robustness (Wang and Kong 2022). Other efforts that quantify stability of attributions in tabular data also use Euclidean distance (or its variants) between the original and perturbed attribution maps (Alvarez-Melis and Jaakkola 2018; Yeh et al. 2019; Agarwal et al. 2022). Each of these metrics look for dimension-wise correlation or pixel-level matching between attribution maps before and after perturbation, and thus penalize even a minor change in attribution (say, even by one pixel coordinate location). This results in a false sense of fragility, and could even be misleading. In this work, we highlight the need to revisit such metrics, and propose variants based on locality and diversity that can be easily integrated into existing metrics.  

Other Related Work. In other related efforts that have studied similar properties of attribution-based explanations, (Carvalho, Pereira, and Cardoso 2019; Bhatt, Weller, and Moura 2020) stated that stable explanations should not vary too much between similar input samples, unless the model’s prediction changes drastically. The abovementioned attributional attacks and defense methods (Ghorbani, Abid, and Zou 2019; Sarkar, Sarkar, and Balasubramanian 2021; Singh et al. 2020; Wang et al. 2020) maintain this property, since they focus on input perturbations that change the attribution without changing the model prediction itself. Similarly, Arun et al. (2020) and Fel et al. (2022) introduced the notions of repeatability/reproducibility and generalizability respectively, both of which focus on the desired property that a trustworthy explanation must point to similar evidence across similar input images. In this work, we provide a practical metric to study this notion of similarity by considering locality-sensitive metrics.  

  
Figure 2: From top to bottom, we plot average top$k$ intersection (currently used metric), 3 -LENS-recall $@k$ and 3 -LENS-recall $@k$ -div (proposed metrics) against the $\\ell_{\\infty}$ -norm of attributional attack perturbations for Simple Gradients (SG) (left) and Integrated Gradients (IG) (right) of a SqueezeNet model on Imagenet. We use $k=1000$ and three attributional attack variants proposed by Ghorbani, Abid, and Zou (2019). Evidently, the proposed metrics show more robustness under the same attacks.",0>2
16fd2454-56f9-4eff-bce6-29f5553e5158,"ref_ids: 454984230919739446, chunk_ids: 8, Score: 0.1533, Text: # 2. Related Work

# 2.1. Pansharpening Method
Model-based Approaches. The model-based pansharpening methods can be roughly divided into three categories, i.e., component substitution (CS) approaches, multiresolution analysis (MRA) methods, and variational optimization (VO) techniques. The main idea of the CS approach is to decompose the PAN image and LRMS image first and then fuse the spatial information of the PAN image with the special information of the LRMS image to generate the HRMS image. Representative methods include principal component analysis (PCA) [ 23 ], Brovey method [ 18 ], intensity–hue-saturation (IHA) [ 10 ], and Gram-Schmidt (GS) method [ 24 ]. To further reduce spectral distortion, the MRA approaches reconstruct the HRMS image by injecting the structure information of the PAN image into the upsampled LRMS image. Typical methods include highpass filter (HPF) fusion [ 31 ], indusion method [ 21 ], smoothing filterbased intensity modulation (SFIM) [ 25 ] etc. The VO techniques reformulate the pansharpening task as a variational optimization problem, such as Bayesian methods [ 38 ] and variational approaches [ 12 ,13 ,15 ,16 ].  

Deep Learning Approaches. In the last decade, deep learning (DL) methods have been studied for pansharpening, and this type of method directly learns the mapping from LRMS and PAN to HRMS. Typical DL-based pansharpening methods mainly contain two types of network architecture, i.e., residual structure and two-branch structure. The residual structure adds upsampled LRMS images to the output of the network to obtain the HRMS in the form of regression residuals, such as PanNet [ 45 ], FusionNet [ 14 ], SRPPNN [ 4 ], etc [ 20 ,34 ,41 ,49 ]. Recently, the two-branch structure is becoming more and more popular. This type of method usually conducts feature extraction for PAN and LRMS image, respectively, and fuses their features to reconstruct HRMS image, such as GPPNN [ 43 ], Proximal PanNet [ 5 ], SFIIN [ 50 ], etc [ 2 ,6 ,40 ,44 ,51 ,52 ]. Both types of methods upsample LRMS first and then carry out other operations, implying that upsampling is a vital step for pan-sharpening.

# 2.2. Image Upsampling Method
Classical Methods. Many local interpolation-based upsampling methods are widely used in pansharpening tasks to obtain large-scale MS, especially the bicubic interpolation method [ 9 ]. Besides, there are plenty of similar techniques, such as nearest interpolation [ 35 ], bilinear interpolation [ 35 ], etc [ 27 ,30 ]. However, this type of method suffers from seriously poor adaptability.  

Deep Learning Methods. As deep learning blossoms, many learning-based upsampling methods have been proposed. For instance, transposed convolution [ 17 ] is widely used in many tasks to upsample low-resolution images, which can learn a self-adaptive weight for local interpolation. Following this work, an attention-based image upsampling method [ 22 ] is recently proposed for deep image super-resolution tasks by utilizing the transformer [ 36 ]. However, this method ignores the channel specificity since it uses the same weight for the same position of all channels, which is unsuitable for pansharpening due to the differences among spectral image channels. Additionally, there are also many other upsampling methods, such as Pu-Net [ 46 ], ESPCNN [ 32 ], etc [ 28 ,29 ,39 ]. Among them, ESPCNN is proposed for single-image super-resolution, which enlarges the receptive field by multi-convolution layers.  

However, these upsampling methods suffer from three issues. Firstly, almost all these methods only have a local receptive field, making them unable to explore the global information of LRMS. Secondly, most of the upsampling methods can’t exploit the PAN information as guidance. Thirdly, channel specificity is not considered in these methods.

# 3. Proposed Upsampling Method
In this section, we first introduce our proposed probability-based global cross-modal upsampling (PGCU) method. Then, we design a network architecture to implement the PGCU method.",0>2
16fd2454-56f9-4eff-bce6-29f5553e5158,"ref_ids: 454984283955145766, chunk_ids: 8, Score: 0.1113, Text: # 1 Introduction
The explosive increase in the use of deep neural network (DNN)-based models for applications across domains has resulted in a very strong need to find ways to interpret the decisions made by these models (Gade et al. 2020; Tang et al. 2021; Yap et al. 2021; Oviedo et al. 2022; Oh and Jeong 2020). Interpretability is an important aspect of responsible and trustworthy AI, and model explanation methods (also known as attribution methods) are an important aspect of the community’s efforts towards explaining and debugging real-world AI/ML systems. Attribution methods (Zeiler et al. 2010; Simonyan, Vedaldi, and Zisserman 2014; Bach et al. 2015; Selvaraju et al. 2017; Chattopadhyay et al. 2018; Sundararajan, Taly, and Yan 2017; Shrikumar et al. 2016; Smilkov et al. 2017; Lundberg and Lee 2017) attempt to explain the decisions made by DNN models through inputoutput attributions or saliency maps. (Lipton 2018; Samek et al. 2019; Fan et al. 2021; Zhang et al. 2020) present detailed surveys on these methods. Recently, the growing numbers of attribution methods has led to a concerted focus on studying the robustness of attributions to input perturbations to handle potential security hazards (Chen et al. 2019; Sarkar, Sarkar, and Balasubramanian 2021; Wang and Kong 2022; Agarwal et al. 2022). One could view these efforts as akin to adversarial robustness that focuses on defending against attacks on model predictions, whereas attributional robustness focuses on defending against attacks on model explanations. For example, an explanation for a predicted credit card failure cannot change significantly for a small human-imperceptible change in input features, or the saliency maps explaining the COVID risk prediction from a chest X-ray should not change significantly with a minor human-imperceptible change in the image.  

DNN-based models are known to have a vulnerability to imperceptible adversarial perturbations (Biggio et al. 2013; Szegedy et al. 2014; Goodfellow, Shlens, and Szegedy 2015), which make them misclassify input images. Adversarial training (Madry et al. 2018) is widely understood to provide a reasonable degree of robustness to such perturbation attacks. While adversarial robustness has received significant attention over the last few years (Ozdag 2018; Silva and Najafirad 2020), the need for stable and robust attributions, corresponding explanation methods and their awareness are still in their early stages at this time (Ghorbani, Abid, and $Z_{\\mathrm{ou}}~2019$ ; Chen et al. 2019; Slack et al. 2020; Sarkar, Sarkar, and Balasubramanian 2021; Lakkaraju, Arsov, and Bastani 2020; Slack et al. 2021a,b). In an early effort, (Ghorbani, Abid, and Zou 2019) provided a method to construct a small imperceptible perturbation which when added to an input $x$ results in a change in attribution map of the original map to that of the perturbed image. This is measured through top$k$ intersection, Spearman’s rank-order correlation or Kendall’s rank-order correlation between the two attribution maps (of original and perturbed images). See Figure 1 for an example. Defenses proposed against such attributional attacks (Chen et al. 2019; Singh et al. 2020; Wang et al. 2020; Sarkar, Sarkar, and Balasubramanian 2021) also leverage the same metrics to evaluate the robustness of attribution methods.  

While these efforts have showcased the need and importance of studying the robustness of attribution methods, we note in this work that the metrics used, and hence the methods, can be highly sensitive to minor local changes in attributions (see Fig 1 row 2 ). We, in fact, show (in Appendix B.1) that under existing metrics to evaluate robustness of attributions, a random perturbation can be as strong an attributional attack as existing benchmark methods. This may not be a true indicator of the robustness of a model’s attributions, and can mislead further research efforts in the community. We hence focus our efforts in this work on rethinking metrics and methods to study the robustness of model attributions (in particular, we study image-based attribution methods to have a focused discussion and analysis). Beyond highlighting this important issue, we propose locality-sensitive improvements of the above metrics that incorporate the locality of attributions along with their rank order. We show that such a locality-sensitive distance is upper-bounded by a metric based on symmetric set difference. We also introduce a new measure top$k$ -div that incorporates diversity of a model’s attributions. Our key contributions are summarized below:  

  
Figure 1: Sample images from Flower dataset with Integrated Gradients (IG) before and after attributional attack. The attack used here is the top$k$ attributional attack of Ghorbani, Abid, and Zou (2019) on a ResNet model. Robustness of attribution measured by top$k$ intersection is small, and ranges from 0.04 (first image) to 0.45 (third image) as it penalizes for both local changes in attribution and concentration of top pixels in a small region. Visually, we can observe that such overpenalization leads to a wrong sense of robustness as the changes are within the object of importance.  

• Firstly, we observe that existing robustness metrics for model attributions overpenalize minor drifts in attribution, leading to a false sense of fragility. • In order to address this issue, we propose LocalitysENSitive (LENS) improvements of existing metrics, namely, LENS-top${\\cdot k}$ , LENS-Spearman and LENSKendall, that incorporate the locality of attributions along  

with their rank order. Besides avoiding overpenalizing attribution methods for minor local drifts, we show that our proposed LENS variants are well-motivated by metrics defined on the space of attributions.   
• We subsequently introduce a second measure based on diversity that enriches model attributions by preventing the localized grouping of top model attributions. LENS can be naturally applied to this measure, thereby giving a method to incorporate both diversity and locality in measuring attributional robustness.   
• Our comprehensive empirical results on benchmark datasets and models used in existing work clearly support our aforementioned observations, as well as the need to rethink the evaluation of the robustness of model attributions using locality and diversity.   
• Finally, we also show that existing methods for robust attributions implicitly support such a locality-sensitive metric for evaluating progress in the field.",0>2
