角色,内容,分镜
1cabe3a5-9d12-494f-bccd-63795401d3a1,RMSNorm（Root Mean Square Normalization）,7
1cabe3a5-9d12-494f-bccd-63795401d3a1,"### 问题

在任务步骤层级为7的情况下，结合任务总体描述和任务步骤信息，提出以下问题：

**在深度学习模型的训练过程中，RMSNorm（Root Mean Square Normalization）与LayerNorm（Layer Normalization）相比，主要有哪些优势和劣势？特别是在大规模模型训练中，RMSNorm的计算效率如何影响模型的整体性能？**

### 问题解析

1. **优势**：
   - **计算效率**：RMSNorm只归一化方差，不归一化均值，因此计算量较小，尤其在大规模模型训练中，这种计算效率的提升可能显著减少训练时间。
   - **简化实现**：RMSNorm的实现相对简单，可能更容易集成到现有的深度学习框架中。

2. **劣势**：
   - **归一化效果**：RMSNorm只归一化方差，可能在某些任务中不如LayerNorm有效，尤其是在需要同时归一化均值和方差的任务中。
   - **泛化能力**：RMSNorm的简化归一化方式可能影响模型的泛化能力，特别是在复杂任务中。

3. **大规模模型训练中的影响**：
   - **计算资源**：RMSNorm的计算效率可能减少对计算资源的需求，使得大规模模型训练更加可行。
   - **性能表现**：尽管计算效率高，但RMSNorm在某些任务中的性能表现可能与LayerNorm相当或略优，这需要在实际应用中进行验证。

通过这个问题，可以更深入地理解RMSNorm在大规模模型训练中的应用及其与LayerNorm的对比，从而为选择合适的归一化方法提供依据。",7
1cabe3a5-9d12-494f-bccd-63795401d3a1,"ref_ids: 454848342697342208, chunk_ids: 7, Score: 0.2539, Text: # 1 Introduction
Training modern neural networks can be a challenging problem. They are typically deeper and more heterogeneous in their structure than previous models, containing various types of layers and operations. The complex interactions of these components pose new challenges for optimization. To ensure stable training the magnitude of gradients and activations must be kept in check, avoiding effects such as vanishing gradients, and exploding activations. Efficient training also requires the learning of different components to be balanced. A layer that is updated slowly (e.g., barely changing through the training process) is unlikely to contribute optimally to the final model, wasting compute resources. On the other hand, a rapidly changing layer may cause instability, limiting the maximum stable learning rate and preventing other layers from learning effectively. Over time many methods have been developed to improve the optimization of deep neural networks but are often poorly understood, complicating the application of deep learning.  

Normalization such as Batch Normalization [ 9 ] or Layer Normalization [ 1 ] is one such operation that is ubiquitous in modern networks. Depending on their placement and configuration, normalization can make the output of a network invariant to the scale of certain parameters. An example of this occurs in the ResNet [ 5 ] architecture where convolutional layers are directly followed by batch normalization. In this case, each convolutional filter (corresponding to a single output channel) can be scaled by a positive scalar without changing the output of the batch normalization. We say that weights that have this property are scale-invariant and those who are not are scale-sensitive . The effect of scale-invariant weights in a neural network is fully determined by their direction, as their magnitude does not matter. Hence, it is natural to use the rotation (i.e., angular change) to measure the “effective” size of an update for such weights.  

Optimizing scale-invariant weights with SGD and weight decay can give rise to Spherical Motion Dynamics [ 29 ,4 ]. The gradient of scale-invariant weights is orthogonal to them (see Section 2.1), causing gradient updates to increase the weight magnitude, unlike weight decay which always decreases it. These opposing forces cause the weight norm to converge to a certain value where the effects balance out, a state which we call equilibrium . This convergence does not rely on the loss converging and can happen relatively quickly in training. A key property of equilibrium is that an optimizer step will rotate a scale-invariant weight vector by a fixed angle in expectation. This value is the same for all scale-invariant weight vectors, ensuring that they all get updated at the same “effective” rate regardless of their average gradient magnitude or position in the network. In equilibrium, the Spherical Motion Dynamics can thus help balance the update speed of different layers, preventing them from changing very fast or slowly compared to other layers.  

With standard optimizers, the scale-invariant weights need to converge to equilibrium. The equilibrium weight magnitude generally depends on the learning rate and gradient magnitude and can therefore change over the course of training, causing weights to fall out of equilibrium. Weights are also often initialized with magnitudes that are far from their equilibrium value at the start of training. In typical descent methods, the rotation of a weight has an inverse dependency on the weight magnitude. Weights that are out of equilibrium can thus rotate slower or faster than the equilibrium value. By definition, scale-invariant weights can be scaled without affecting the output of the network, but such scaling still affects the rotation and therefore changes the effect of an update. This leads to learning dynamics that we consider highly arbitrary.  

In this work we propose fixing these dynamics by forcing scale-invariant weights to behave as in equilibrium throughout training. We derive the equilibrium states for AdamW, SGD with momentum (SGDM) and Lion optimizers. Our analysis provides insights into the effects of the hyperparameters of these optimizers and reveals some unintuitive behavioral differences between parameters that are scale-invariant and those which are not. We explain the effect of weight decay in the common case where most scale-sensitive parameters, such as gains and biases, are excluded. We create rotational variants of the optimizers that constrain the weight magnitude of scale-invariant weights and ensure that the average rotational update matches equilibrium. In general, these variants treat the scale-sensitive parameters like the original optimizers. However, we can also apply rotational updates to weights that are not strictly scale-invariant. We show that the RVs can match or exceed the originals, often with a minimal change in the hyperparameters, reducing the need for tuning compared to completely new optimizers. Finally, we relate these variants to existing optimizers like LARS [ 32 ], LAMB [ 33 ] and Nero [ 15 ], explaining some of their observed benefits. Our main contributions can be summarized as:  

•Creating rotational optimizer variants that constrain the Spherical Motion Dynamics (SMD).   
• Deriving the SMD equilibrium for AdamW, SGDM and Lion optimizers.   
• Explaining the effects of optimizer hyperparameters from the SMD perspective.   
• Experimentally showing the viability of training with constrained SMD.   
•Showing that constrained SMD training reduces the need for learning rate warmup and improves optimization of poorly normalized networks.",7
1cabe3a5-9d12-494f-bccd-63795401d3a1,"ref_ids: 454847042436311108, chunk_ids: 4, Score: 0.2451, Text: # 5 I NTUITIONS BEHIND LAYER NORM TUNING
In this section, driven by the empirical success of LayerNorm tuning, we explore the intuitions behind LayerNorm from three perspectives, domain adaptation, expressive power, and gradient variance.  

Table 3: Model performance on different data types. Methods with 80K and Conv.20K suffix are tuned on the full 80K data and the 20K conversational data, respectively.   


<html><body><table><tr><td>Method</td><td>MME</td><td>VQAv2</td><td>MSCOCO</td><td>Flickr30k</td><td>POPE</td></tr><tr><td colspan=""6"">MM-V1CUNA-7B</td></tr><tr><td>Finetune-80K</td><td>625.2/270.7</td><td>15.40</td><td>67.50</td><td>34.61</td><td>73.8/76.5/66.5</td></tr><tr><td>LayerNorm-80K</td><td>723.2/253.2</td><td>17.06</td><td>80.89</td><td>48.01</td><td>76.1/81.1/70.8</td></tr><tr><td>LayerNorm-Conv. 20K</td><td>777.1/231.4</td><td>15.39</td><td>67.30</td><td>40.33</td><td>75.2/79.2/68.8</td></tr><tr><td colspan=""6"">MM-LLAMA2-7B</td></tr><tr><td>Finetune-80K</td><td>661.3/237.1</td><td>16.09</td><td>65.08</td><td>31.64</td><td>56.3/65.0/55.4</td></tr><tr><td>LayerNorm-80K</td><td>583.2/200.7</td><td>16.78</td><td>88.85</td><td>49.24</td><td>66.6/68.5/64.9</td></tr><tr><td>LayerNorm-Conv. 20K</td><td>376.2/157.5</td><td>16.19</td><td>86.80</td><td>44.88</td><td>50.5/50.7/50.3</td></tr><tr><td colspan=""6"">MM-LLAMA2-CHAT-7B</td></tr><tr><td>Finetune-80K</td><td>805.4/234.6</td><td>15.29</td><td>57.40</td><td>26.70</td><td>60.3/69.8/57.9</td></tr><tr><td>LayerNorm-80K</td><td>651.3/219.3</td><td>16.60</td><td>75.34</td><td>43.75</td><td>71.3/72.4/67.8</td></tr><tr><td>LayerNorm-Conv. 20K</td><td>482.9/172.1</td><td>13.88</td><td>66.85</td><td>41.95</td><td>62.7/71.7/61.3</td></tr><tr><td colspan=""6"">MM-LLAMA2-13B</td></tr><tr><td>Finetune-80K</td><td>402.3/199.3</td><td>18.33</td><td>73.88</td><td>45.33</td><td>51.6/51.1/52.2</td></tr><tr><td>LayerNorm-80K</td><td>526.0/177.5</td><td>15.31</td><td>82.92</td><td>48.42</td><td>60.0/69.1/58.9</td></tr><tr><td>LayerNorm-Conv. 20K</td><td>646.0/242.9</td><td>16.01</td><td>76.50</td><td>44.86</td><td>70.0/76.9/68.6</td></tr><tr><td colspan=""6"">MM-LLAMA2-CHAT-13B</td></tr><tr><td>Finetune-80K</td><td>623.3/221.4</td><td>15.17</td><td>64.19</td><td>41.82</td><td>67.6/64.8/64.5</td></tr><tr><td>LayerNorm-80K</td><td>929.3/254.3</td><td>16.10</td><td>74.96</td><td>42.79</td><td>78.9/83.9/74.3</td></tr><tr><td>LayerNorm-Conv. 20K</td><td>769.7/227.5</td><td>15.57</td><td>73.30</td><td>43.08</td><td>68.2/72.8/65.3</td></tr></table></body></html>  

Table 4: Results of models with LayerNorm and/or vision-language Connector activated.   


<html><body><table><tr><td>Method</td><td>MME</td><td>VQAv2</td><td>MSCOCO</td><td>Flickr30k</td><td>POPE</td></tr><tr><td colspan=""6"">MM-LLAMA2-7B</td></tr><tr><td>LayerNorm + Connector</td><td>583.2/200.7</td><td>16.78</td><td>88.85</td><td>49.24</td><td>66.6/68.5/64.9</td></tr><tr><td>Connector</td><td>311.1/105.4</td><td>12.72</td><td>60.43</td><td>35.91</td><td>67.9/73.7/66.9</td></tr><tr><td>LayerNorm</td><td>395.0/191.4</td><td>18.18</td><td>80.13</td><td>41.68</td><td>50.3/51.3/50.2</td></tr><tr><td colspan=""6"">MM-LLAMA2-13B</td></tr><tr><td>LayerNorm + Connector</td><td>526.0/177.5</td><td>15.31</td><td>82.92</td><td>48.42</td><td>60.0/69.1/58.9</td></tr><tr><td>Connector</td><td>507.0/187.9</td><td>15.22</td><td>62.60</td><td>25.13</td><td>60.9/66.8/60.1</td></tr><tr><td>LayerNorm</td><td>405.0/188.6</td><td>16.51</td><td>70.41</td><td>39.86</td><td>50.9/52.7/51.0</td></tr></table></body></html>

# 5.1 LAYER NORM TUNING A DAPTS LLM S TO MULTI -M ODAL
Influence of the Vision-Language Connector The vision-language connector serves as the converter to project features from the vision encoder to the LLM domain. In our previous experiments, we focused on finetuning the LLM component of the MLLMs while keeping the vision-language connector activated by default. To determine which component plays a more important role for domain adaptation of LLM to multi-modal domain, we performed an ablation study by activating the two components separately. Results are presented in table 4 , tuning LayerNorm in attention blocks without activating the vision-language connector resulted in only a $4.2\\%$ and $5.4\\%$ decrease in performance on three traditional multi-modal tasks and the MME benchmark, respectively. This decrease is significantly lower than the $15.6\\%$ and $9.2\\%$ downgrade observed when only activating the Connector on the same tasks. This observation highlights the vital role LayerNorm plays in transforming knowledge from the vision domain to language, indicating LayerNorm as a strong domain adaptor for the LLM architecture.  

  

Figure 3: Layer similarities between different LLM layers in (a) Finetuned and (b) LayerNorm-tuned MM-V ICUNA -7B. The average layer similarity of two models are 0.624 and 0.585, respectively.  

Table 5: Results of models with LL A MA2 Finetuned/LayerNorm-tuned with ViT pre-trained on ImageNet (Deng et al. ,2009 ), which have not been aligned with the language domain.   


<html><body><table><tr><td></td><td>MME</td><td>VQAv2</td><td>MSCOCO</td><td>Flickr30k</td><td>POPE</td></tr><tr><td>Finetune-7B</td><td>406.79/182.5</td><td>15.05</td><td>47.75</td><td>18.97</td><td>50.0/51.6/50.1</td></tr><tr><td>LayerNorm-7B</td><td>301.51/127.14</td><td>15.48</td><td>66.22</td><td>31.73</td><td>50.0/50.1/50.1</td></tr><tr><td>Finetune-13B</td><td>375.41/171.79</td><td>25.38</td><td>51.26</td><td>25.96</td><td>50.3/51.1/51.0</td></tr><tr><td>LayerNorm-13B</td><td>445.98/150.0</td><td>15.59</td><td>64.63</td><td>32.17</td><td>51.2/53.0/50.8</td></tr></table></body></html>  

Switching Visual Features. We employ the ViT encoder from CLIP ( Radford et al. ,2021 ) by default in our previous experiments. CLIP ( Radford et al. ,2021 ) models are trained with image-text contrastive loss, thus its feature space is already aligned with language. Since LayerNorm has shown its effectiveness as a domain adaptor, we are interested in testing whether or not LayerNorm tuning can adapt a LLM to image features that are not pretrained to align with language. The vision encoder is switched to a ViT model that was pretrained on ImageNet (Dosovitskiy et al. ,2021 ;Deng et al. ,2009 ). Results in table 5 demonstrate that both LayerNorm and finetuning approaches can yield high performance. Interestingly, we observe that by LayerNorm tuning with ImageNet trained ViT, which has not been aligned with language, the model is able to achieve comparable performance to full parameter finetuning , i.e ., results show that LayerNorm tuning outperforms finetuning by $12.0\\%$ on captioning tasks, but performs slightly worse by $5.0\\%$ on the MME benchmark. These results again indicates the domain adaptor role of the LayerNorm , hinting the reason behind the empircal success of LayerNorm tuning. Furthermore, it is worth noting that the performance of MLLMs incorporating ViT pretrained on ImageNet is generally inferior to that of CLIP’s vision encoder. This observation provides compelling evidence that, despite differences in tokenizer and training paradigm between CLIP’s text encoder and LL A MA’s, ViT from CLIP has the capacity to learn general patterns of language formulation during pre-training. Thus, significantly enhance MLLM abilities.

# 5.2 LAYER NORM TUNING I MPROVES THE EXPRESSIVE POWER
It is shown in Pires et al. (2023 ) that a Transformer model incorporating anisotropic layer representation can capture a wider range of learning patterns. By computing the cosine similarities between all layers in the LLM of a finetuned MLLM, we aim to investigate whether the improved efficiency is the results of the improved expressive power. In table 6 , we present the average layer similarity of three 7B scale MLLMs, and in fig. 3 we present the visualization of per layer similarity scores of MM-V ICUNA -7B. Our analysis reveals that the transformer layers in the MLLMs with LayerNorm tuning exhibit a clear distinction from one another ( i.e ., an average $10.6\\%$ lower layer similarities comparing finetuning), indicating superior generalization ability and expressive power compared to finetuning. This finding sheds light on why tuning LayerNorm is effective for multi-modal LLM training. For additional visualizations, please refer to the Appendix A.2.1 .  

  
Figure 4: Gradients of the input LayerNorm in the 11th layer of the MM-V ICUNA as training proceeds. LayerNorm-tuned model has lower gradient variance than full parameter finetuning.  

Table 6: Layer representation similarity of LayerNorm and finetuning methods on three 7B MLLMs.   
Lower the similarity is, the better expressive power a model possesses.  

<html><body><table><tr><td>Model</td><td>LayerNorm Sim.</td><td>Finetuning Sim.</td></tr><tr><td>MM-VICUNA</td><td>0.585</td><td>0.624</td></tr><tr><td>MM-LLAMA2</td><td>0.504</td><td>0.591</td></tr><tr><td>MM-LLAMA2-CHAT</td><td>0.550</td><td>0.617</td></tr></table></body></html>",7
1cabe3a5-9d12-494f-bccd-63795401d3a1,"ref_ids: 454846008144214678, chunk_ids: 3, Score: 0.2383, Text: # 7.4 Fine-Tuning Method Selection with LLMem
Table 3 assesses whether LLMem finds the optimal finetuning method to achieve the fastest fine-tuning while avoiding GPU OOM for various models. When measuring the time taken for each method, we applied the maximum batch size that can prevent GPU OOM. LLMem typically selects TP when DP causes GPU OOM. It is challenging for LLMem to choose $\\mathrm{DP+TP}$ because only 4 GPUs were used in the experiment. $\\mathrm{DP+TP}$ allows for more diverse combinations depending on the number of GPUs used and is more likely to be selected. LLMem also suggests CPU offloading when GPU memory is insufficient.",7
