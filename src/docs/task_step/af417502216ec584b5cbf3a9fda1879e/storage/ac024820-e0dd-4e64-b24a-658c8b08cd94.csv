角色,内容,分镜
ac024820-e0dd-4e64-b24a-658c8b08cd94,容错性,4>2
ac024820-e0dd-4e64-b24a-658c8b08cd94,"### 问题

在大规模数据训练中，LayerNorm和RMSNorm在容错性方面有何差异？具体来说，它们在分布式训练中的梯度压缩和模型并行等技术中的表现如何？哪种归一化方法更适合处理大规模数据，尤其是在面对数据分布不均匀或噪声较多的情况下？",4>2
ac024820-e0dd-4e64-b24a-658c8b08cd94,"ref_ids: 454847042436311108, chunk_ids: 4, Score: 0.1982, Text: # 1 I NTRODUCTION
A common assumption in machine learning is that the training and the test data are independent and identically distributed (i.i.d.) samples. In practice, the unseen test data are often sampled from distributions that are different from the one for training. For example, a camel may appear on the grassland during the test time instead of the desert which is their usual habitat (Rosenfeld et al., 2020). However, models such as overparameterized deep neural networks are prone to rely on spurious correlations that are only applicable to the train distribution and fail to generalize under the distribution shifts of test data (Hashimoto et al., 2018; Sagawa et al., 2019; Arjovsky et al., 2019). It is crucial to ensure the generalizability of ML models, especially in safety-critical applications such as autonomous driving and medical diagnosis (Ahmad et al., 2018; Yurtsever et al., 2020).  

Numerous studies have attempted to improve the generalization of deep learning models regarding distribution shifts from various angles (Zhang et al., 2017; 2022; Nam et al., 2020; Koh et al., 2021; Liu et al., 2021a; Yao et al., 2022; Gao et al., 2023). An important line of work addresses the issue based on having the training data partitioned according to their respective domains/environments. For example, cameras are placed around different environments (near water or land) to collect sample photos of birds. Such additional environment information sparks the rapid development of domain generalization (DG) algorithms (Arjovsky et al., 2019; Sagawa et al., 2019; Krueger et al., 2021). The common goal of these approaches is to discover the invariant representations that are optimal in loss functions for all environments to improve generalization by discouraging the learning of spurious correlations that only work for certain subsets of data. However, Gulrajani & Lopez-Paz (2020) and Ye et al. (2022) have shown that no DG algorithms clearly outperform the standard empirical risk minimization (ERM) (Vapnik, 1999). Subsequently, Kirichenko et al. (2022) and Rosenfeld et al. (2022) have argued that ERM may have learned good enough representations. These studies call for understanding whether, when, and why DG algorithms may indeed outperform ERM.  

In this work, we investigate under what conditions DG algorithms are superior to ERM. We demonstrate that the success of some DG algorithms can be ascribed to their robustness to label noise under subpopulation shifts. Theoretically, we prove that when using overparameterized models trained with finite samples under label noise, ERM is more prone to converging to suboptimal solutions that mainly exploit spurious correlations. The analysis is supported by experiments on synthetic datasets. In contrast, the implicit noise robustness of some DG algorithms provides an extra layer of performance guarantee. We trace the origin of the label-noise robustness by analyzing the optimization process of a few exemplary DG algorithms, including IRM (Arjovsky et al., 2019), V-REx (Krueger et al., 2021), and GroupDRO (Sagawa et al., 2020), while algorithms including ERM and Mixup (Zhang et al., 2017) without explicit modification to the objective function, do not offer such a benefit. Empirically, our findings suggest that the noise robustness of DG algorithms can be beneficial in certain synthetic circumstances, where label noise is non-negligible and spurious correlation is severe. However, in general cases with noisy real-world data and pretrained models, there is still no clear evidence that DG algorithms yield better performance at the moment.  

Why should we care about label noise? It can sometimes be argued that label noise is unrealistic in practice, especially when the amount of injected label noise is all but small. However, learning a fully accurate decision boundary from the data may not always be possible (e.g., some critical information is missing from the input). Such kinds of inaccuracy can be alternatively regarded as manifestations of label noise. Furthermore, a common setup for analyzing the failure mode of ERM assumes that the classifier utilizing only the invariant features is only partially predictive (having ${<}100\\%$ accuracy) of the labels (Arjovsky et al., 2019; Sagawa et al., 2020; Shi et al., 2021). These setups are subsumed by our setting where there initially exists a fully predictive invariant classifier for the noise-free data distribution, and some degree of label noise is added afterward. More broadly, we analyze the failure mode of ERM without assuming the spurious features are generated with less variance (hence less noisy) than the invariant features. By looking closer at domain generalization through the lens of noise robustness, we obtain unique understanding of when and why ERM and DG algorithms work.  

As our main contribution, we propose an inclusive theoretical and empirical framework that analyzes the domain generalization performance of algorithms under the effect of label noise:  

1. We theoretically demonstrate that when trained with ERM on finite samples, the tendency of learning spurious correlations rather than invariant features for overparameterized models is jointly determined by both the degrees of spurious correlation and label noise . (Section 4)   
2. We show that several DG algorithms possess the noise-robust property and enable the model to learn invariance rather than spurious correlations despite using data with noisy labels. (Section 5)   
3. We perform extensive experiments to compare DG algorithms across synthetic and real-life datasets injected with label noise but unfortunately find no clear evidence that noise robustness necessarily leads to better performance in general. To address this, we discuss the difficulties of satisfying the theoretical conditions and other potential mitigators in practice. (Section 6, 7)",4>2
ac024820-e0dd-4e64-b24a-658c8b08cd94,"ref_ids: 454845744505973136, chunk_ids: 6, Score: 0.1973, Text: # 6 Conclusion
LLM-based RL algorithms have shown generalization across multiple tasks and games. We argue that this ability comes from implicit memory that fits a large number of parameters to the training data, which is inefficient in terms of model size. In contrast, we propose a new approach inspired by the concept of “working memory” called Decision Transformers with Memory (DT-Mem), which stores training experience explicitly in a content-addressable matrix module for later retrieval and use. Evaluation demonstrates that DT-Mem achieves better generalization on Atari games with only $10\\%$ of the model parameters compared to the state-of-the-art method. Furthermore, we demonstrate that fine-tuning DT-Memwith a small amount of data can produce state-of-the-art results on both Atari games and the Meta-World environment, when compared to MDT [22], PDT [37], and HDT [38].  

Limitations The first limitation of our work is the sample efficiency of memory fine-tuning. The $10\\%$ fine-tuning dataset is still sizeable, and we plan to explore more sample-efficient methods in the future. We could for instance consider a setting with more tasks, each one with less data so that the inter-task generalization would be even more crucial to its performance. Additionally, this work does not propose a control strategy for collecting data on a new task. For future work, we plan to investigate online data collection methods, which includes the design and learning of exploration strategies for an efficient fine-tuning on new tasks. Finally, the approach has been intuitively motivated, but it would be valuable to have a theoretical grounding that would show the structural limits of large models and how equipping them with a memory component overcomes them.  

Societal Impact We do not foresee any significant societal impact resulting from our proposed method. The current algorithm is not designed to interact with humans, nor any realistic environment yet. If one chooses to extend our methods to such situations, caution should be exercised to ensure that any safety and ethical concerns are appropriately addressed. As our work is categorized in the offline-RL domain, it is feasible to supplement its training with a dataset that aligns with human intents and values. However, one must be wary that the way our architecture generalizes across tasks is still not well understood and as a consequence we cannot guarantee the generalization of its desirable features: performance, robustness, fairness, etc. By working towards methods that improve the computational efficiency of large models, we contribute to increase their access and reduce their ecological impact.



# A Implementation Details

# A.1 DT-Mem network architecture
Table 3 summarizes the different model configurations used for evaluation. In this section, we describe these model configurations in detail. While Table 3 provides a summary, we will also provide additional information here. DT-Mem, PDT and HDT are all share the same transformer architectures. However, for task-adaptation, HDT utilizes a pre-trained $2.3\\mathbf{M}$ hyper-network, while DT-Mem introduces 147K LoRA parameters. To compare with MDT, we use the same parameter size as reported in [22].  

Table 3: Detailed Model Sizes   


<html><body><table><tr><td>Model</td><td>Layers</td><td>Hidden size (d)</td><td>Heads</td><td>Params</td><td>Memory Size</td><td>Memory Module Params</td></tr><tr><td>HDT</td><td>4</td><td>512</td><td>8</td><td>13M</td><td>N.A.</td><td>N.A.</td></tr><tr><td>MDT-200M</td><td>10</td><td>1280</td><td>20</td><td>200M</td><td>N.A.</td><td>N.A.</td></tr><tr><td>DT-Mem</td><td>4</td><td>512</td><td>8</td><td>13M</td><td>559K</td><td>7M</td></tr></table></body></html>

# A.2 Hyper-parameters
In this section, we will delve into the specifics of the model parameters. Understanding these parameters is key to understanding the workings of the model. It is worth noting that the source code for this model is publicly available at https://github.com/luciferkonn/DT_Mem/tree/main .This allows for a deeper understanding of the model’s inner workings and may facilitate the replication of its results.  

Table 4: Hyperparameters for DT-Mem training   


<html><body><table><tr><td>Hyperparameters</td><td>Value</td></tr><tr><td>K (length of context)</td><td>28</td></tr><tr><td>dropoutrate</td><td>0.1</td></tr><tr><td>maximum epochs</td><td>1000</td></tr><tr><td>steps for each epoch</td><td>1000</td></tr><tr><td>optimizer learning rate</td><td>1e-4</td></tr><tr><td>weight decay</td><td>1e-4</td></tr><tr><td>gradient norm clip</td><td>1.</td></tr><tr><td>data points for each dataset</td><td>500,000</td></tr><tr><td>batch size</td><td>64</td></tr><tr><td>memory slots</td><td>1290</td></tr><tr><td>activation</td><td>GELU</td></tr><tr><td>optimizer</td><td>Adamw</td></tr><tr><td>scheduler</td><td>LambdaLR</td></tr></table></body></html>

# A.3 Training and fine-tuning algorithm
In this section, we present the pre-training DT-Memin Appendix A.3 and fine-tuning DT-Mem with LoRA in Appendix 5.5.  

We pre-train DT-Mem on multiple offline datasets. Each gradient update of the DT-Memmodel considers information from each training task.  

We fine-tune the memory module to adapt to each downstream task. To achieve this, we fix the pre-trained DT-Mem model parameters and add additional LoRA parameters for the memory module feed-forward neural networks. The fine-tune dataset is used to update these LoRA parameters only.

# Algorithm 1 Pre-train DT-Mem
1: for T episodes do   
2: 3: 4: for Sample trajectories Split trajectories into different segments with length K and calculate return-to-go in the Task $\\mathcal{T}_{i}\\in T^{t r a i n}\\;.$ do $\\tau=(s_{0},a_{0},r_{0},\\cdot\\cdot\\cdot\\,,s_{H},a_{H},r_{H})$ m the dataset $\\mathcal{D}_{i}$ .  
input sequence.   
5: Given $\\hat{\\tau}_{t+1:t+K}$ , compute the sequence embedding $e_{s e q}$ .  
6: Update the working memory and retrieve the relative information as $E_{o u t}$   
7: Given $E_{o u t}$ , predict actions $\\tilde{a}_{t}$ , reward $\\tilde{r}_{t}$ , and return-to-go ${\\tilde{R}}_{t}$ .  
8: Compute the loss according to Eqn. 1.   
9: Update all modules parameters.   
10: end for   
11: end for  

Algorithm 2 Fine-tuning DT-Mem  

$\\hat{B}^{q},\\hat{B}^{k},\\hat{B}^{v},\\hat{A}^{q},\\hat{A}^{k},\\hat{A}^{v},B^{q},A^{q},B^{k},A^{k}$ Require: Fine-tuning dataset $\\mathcal{T}^{i}~\\in~T^{t e s t}$ .dataset $\\mathcal{D}^{i}$ for $\\mathcal{T}^{i}$ .Initialize LoRA parameters 1: for T steps do   
2: Split trajectories into different segments with length $\\mathbf{K}$ and calculate return-to-go in the input sequence.   
3: Given $\\hat{\\tau}_{t+1:t+K}$ , compute the sequence embedding $e_{s e q}$ .  
4: Update working memory using $\\hat{Q}\\,=\\,M(\\hat{W}^{q}+\\hat{B}^{q}\\bar{A}^{q})$ ,$\\hat{K}\\,=\\,M(\\hat{W}^{k}\\,+\\,\\hat{B}^{k}\\hat{A}^{k}),\\hat{V}\\,=$ $M(\\hat{W}^{v}+\\hat{B}^{v}\\hat{A}^{v})$ ,$Q=M(W^{q}+B^{q}A^{q}),K=M(W^{k}+B^{k}A^{k})$   
5: Retrieve the relative information as $E_{o u t}$   
6: Given $E_{o u t}$ , predict actions $\\tilde{a}_{t}$ , reward $\\tilde{r}_{t}$ , and return-to-go ${\\tilde{R}}_{t}$ .  
7: Compute the loss according to Eqn. 1.   
8: Update LoRA parameters only.   
9: end for",4>2
ac024820-e0dd-4e64-b24a-658c8b08cd94,"ref_ids: 455038427552559154, chunk_ids: 6, Score: 0.1875, Text: # 3 Large Scale Results
Here, we present the results for large-scale training to illustrate that our model performance scales with data size. We utilize, LAION115M dataset in addition to the 14M dataset used in the paper. Note that, the downloaded dataset using web urls has an approximate 20% miss rate, leading to overall dataset size of $105M$ for large-scale training.  

We employ the synthetic captions released by BLIP [27] for LAION115M, SBU and Conceptual Caption datasets. We use the Stage-1 model pre-trained with 14M dataset as weight initialization. The LLM alignment stage is trained for an epoch on the large-scale dataset. We experiment with $O P T_{2.7B}$ LLM and $F l a n T5_{X L}$ [8] LLM and present Zero-shot visual question answering results on GQA and OKVQA datasets. $O P T$ is a decoder-only LLM while FlanT 5 is enocder-decoder LLM. Following BLIP-2 [26], for $O P T$ model we train with language modeling loss while FlanT 5 model is trained with prefix language modeling loss i.e., caption is split into two parts: prefix and suffix. The prefix text along with visual representation forms input to LLM encoder and the suffix text is used as generation target for LLM decoder. A random value from start to middle of sentence is picked to divide the caption into two parts.  

Table 10: Zero-shot Visual Question Answering results on GQA and OKVQA datasets with Large scale training. \\* evaluated using official checkpoint   


<html><body><table><tr><td>Method</td><td colspan=""5"">Caption Pre-processing CLIP Caption#Caps/Img Ranking</td><td>GQA OKVQA Acc. Acc.</td></tr><tr><td>Frozen [40]</td><td></td><td></td><td>Dhta</td><td>StageStage2 Data</td><td></td><td>5.9</td></tr><tr><td>VLKD [9]</td><td></td><td></td><td></td><td></td><td></td><td>13.3</td></tr><tr><td>FewVLM [22]</td><td></td><td></td><td></td><td></td><td>29.3</td><td>16.5</td></tr><tr><td>Flamingo3B [2]</td><td></td><td></td><td></td><td></td><td></td><td>41.2</td></tr><tr><td>Flamingo9B [2]</td><td></td><td></td><td></td><td></td><td></td><td>44.7</td></tr><tr><td>Flamingo80B [2]</td><td></td><td></td><td></td><td></td><td></td><td>50.6</td></tr><tr><td>PNP-VQA T03B[38]</td><td></td><td></td><td></td><td></td><td>32.3</td><td>26.6</td></tr><tr><td>PNP-VQA T011B [38]</td><td></td><td></td><td></td><td></td><td>33.4</td><td>30.5</td></tr><tr><td>PNP-VQA UnifiedQAv23B [38]</td><td></td><td></td><td></td><td></td><td>42.3</td><td>34.1</td></tr><tr><td>PNP-VQA UnifiedQAv211B[38]</td><td></td><td></td><td></td><td></td><td>41.9</td><td>35.9</td></tr><tr><td>BLIP-2 OPT2.7B* [26]</td><td></td><td>2</td><td>129M</td><td>129M</td><td>32.5</td><td>31.5</td></tr><tr><td>BLIP-2FlanT5xL*[26]</td><td>√</td><td>2</td><td>129M</td><td>129M</td><td>43.9</td><td>41.2</td></tr><tr><td>BLIP-2 OPT2.7B [26]</td><td>×</td><td>1</td><td>14M</td><td>105M</td><td>32.2</td><td>25</td></tr><tr><td>X-Former(Ours)OPT2.7B</td><td>×</td><td>1</td><td>14M</td><td>105M</td><td>34.3</td><td>27.6</td></tr><tr><td>BLIP-2 FlanT5xL [26]</td><td>×</td><td>1</td><td>14M</td><td>105M</td><td>42.9</td><td>38.2</td></tr><tr><td>X-Former (Ours) FlanT5xL</td><td></td><td>1</td><td>14M</td><td>105M</td><td>44.9</td><td>39.5</td></tr></table></body></html>  

We demonstrate that our model outperforms BLIP-2 [26] at scale in Table 10. Specifically, our model achieves a $2.1\\%$ gain on GQA dataset and $2.6\\%$ gain on OKVQA dataset respectively with $O P T_{2.7B}$ LLM. We show similar gains using $F l a n T5_{X L}$ LLM as well; our approach improves by $2\\%$ on GQA dataset and $1.3\\%$ on OKVQA dataset respectively. Note that PNP-VQA [38] performance relies heavily on QA model specifically UnifiedQAv2 is a task-specific model pretrained for question answering, and OFA [42] trains visual encoders while we keep it frozen hence we do not compare with it.

# 4 Ablation Analysis
Ablation On CLIP Layers As mentioned in Section 3.3 of main text “Leveraging Early Layer CLIP features”, we present additional results by experimenting with different layers from CLIP. We experiment with the following layers $\\{22,24,26,28,30,32,34,36\\}$ as early layer features from CLIP ViT and report performance trend on GQA dataset. As shown in Figure 7, we observe that the best performance is achieved for layer 26 and layer 30, while utilizing features from layers below 26 leads to a drop in performance. Furthermore, using features from layers beyond 30 also results in a decline in performance.  

Our findings demonstrate that the performance using early layer CLIP features is inferior to that of our model, with a $2\\%$ decrease in performance compared to the best layer.  

More Ablations We perform comprehensive ablations studies to analyze the impact of different loss components, effect of Horizontal flip augmentation and effect of Self-Attention. Further, we analyze the impact of X-Former training for LLM alignment and importance of MAE to capture detailed visual information complementing the global semantic representation from CLIP-ViT. Note that for these ablations, we use 8-A100swith batch size of 320/272 for stage 1 and stage 2 respectively.  

We find that ITG significantly affects retrieval more than ITM; without ITC, there is a slight drop in captioning performance. As shown in Table 11, horizontal flip augmentation does not effect overall performance. For comprehensiveness, we analyze the effect of Self-Attention (SA) layer in X-Former as shown in Table 11, row 5. There is a drop in captioning performance when we remove SA layer before the Cross-Attention with MAE.  

Table 11: Ablations Analysis.   


<html><body><table><tr><td>Method</td><td>TR5</td><td>TR10</td><td>IR5</td><td>IR10</td><td>B@4</td><td>C</td></tr><tr><td>w/o ITM</td><td>96.6</td><td>98.8</td><td>93.8</td><td>96.7</td><td>35.9</td><td>120.3</td></tr><tr><td>w/o ITG</td><td>84.9</td><td>93.1</td><td>88.2</td><td>92.9</td><td></td><td></td></tr><tr><td>w/o ITC</td><td></td><td></td><td></td><td></td><td>36.2</td><td>120.7</td></tr><tr><td>w/o HFlip</td><td>93.2</td><td>98.4</td><td>93.9</td><td>97.2</td><td>36.3</td><td>122.4</td></tr><tr><td>w/oSA</td><td>95.5</td><td>99</td><td>93.9</td><td>97.1</td><td>35.6</td><td>120</td></tr><tr><td>X-Former</td><td>(suno) 95.8</td><td>99</td><td>94</td><td>96.7</td><td>37</td><td>123.2</td></tr></table></body></html>  

Table 12: Ablation Analysis. \\*: smaller batch size in both stages   


<html><body><table><tr><td>Method</td><td>GQA</td><td>OKVQA</td></tr><tr><td>CLIP W1 Recon.</td><td>22.5</td><td>8.1</td></tr><tr><td>X-Former Frozen</td><td>25.5</td><td>15.9</td></tr><tr><td>X-Former (Ours)</td><td>31.9</td><td>25.9</td></tr></table></body></html>  

For LLM alignment, we follow BLIP-2 protocol and train X-Former in stage2 along with a Fully Connected layer. To analyze the impact of training XFormer for LLM alignment, we experiment with frozen X-Former in stage-2 and report results in Table 12 row 2. To demonstrate the importance of MAE further, we replace MAE encoder with CLIP-ViT and pass masked image to CLIP-ViT which is then optimized for image reconstruction with MAE decoder. As shown in Table 12 row 1, the performance drops significantly by replacing MAE-ViT encoder with CLIP-ViT on both GQA and OKVQA dataset. Thus demonstrating MAE-ViT encoder plays crucial role in learning detailed visual features.  

  
Fig. 7: Zero-shot visual question answering performance on GQA datasets for different layer features from CLIP.

# 5 Qualitative Results
In this section, we present qualitative results, including cases where our method did not perform as expected. In Figure 8, we present examples that involve comparing the colors of different objects within the image. As you can see in Figure 8 (a), (b), and (c), our method successfully understands the specified objects in the questions, regardless of their positions in the image, and compares their colors accurately. However, Figure 8 (d) shows a more challenging scenario. Here, the pillow and the bed are not clearly distinguishable, which made it difficult for our model to identify the pillow in the image.  

  
Fig. 8: Qualitative Comparison demonstrating ability to compare colors of specified objects.  

Figure 9 showcases the spatial understanding capabilities of our model in comparison to the BLIP-2. A kitchen scene depicted in images 9(a) and (b), our model accurately identifies the refrigerator behind the countertop and the microwave above it, respectively. In contrast, BLIP-2 erroneously predicts a sink and refrigerator for the same questions. Our model correctly discerns the attire of individuals in 9 (c), recognizing that the person to the left of the one wearing glasses is indeed wearing jeans—a detail that BLIP-2 overlooks. However, 9 (d) presents a more challenging scenario for both models. When asked about the color of the computer to the right of the shelf, our model and BLIP-2 both incorrectly identify a silver laptop as black.  

  
Fig. 9: Qualitative Comparison pertaining to question of spatial understanding.  

  
Fig. 10: Qualitative Comparison pertaining to question of relative object understanding in both background and foreground.  

Figure 10 provides insights into the ability of our model to comprehend the relative positioning of objects within an image, both in the background and foreground. Figure 10 (a) and (b), probe the understanding of background elements, our model demonstrates a clear capacity to correctly identify objects, distinguishing trees on a beach and recognizing the tall, green trees beside a double-decker bus. This is in contrast to BLIP-2, which incorrectly identifies grass instead of trees and fails to acknowledge the verdancy and height of the trees. Further, in Figure 10 (c), which shifts the focus to foreground objects, our model accurately discerns the presence of deer in a grassy field. However, in Figure 10 (d), both our model and BLIP-2 inaccurately detect a fence in front of a tennis player, when, in fact, it is behind the player as shown. Overall, our model shows enhanced understanding of object contexts and positioning.  

  
Fig. 11: Qualitative Comparison for questions relating to absolute image position understanding.  

  
Fig. 12: Qualitative Comparison for samples with objects in close proximity in the scene.  

Figure 11 exemplifies the absolute position reasoning capabilities of our model by assessing its ability to identify objects and their locations within an image, whether they are situated at the top or bottom parts of the image. In Figure 11 (a), (b) and (c), our model accurately determines the position of the fried food, the location of the old men, and the placement of the mirror, respectively, demonstrating better understanding of absolute positions within various contexts. However, Figure 11 (d) introduces a more complex situation involving multiple vehicles parked. Our model encounters difficulty here, incorrectly identifying a van as a car due to its close resemblance to car in this image.  

Figure 12 demonstrates the capacity of our model to discern fine details of objects in close proximity within an image. In the living room scene depicted in Figure 12 (a), when questioned about the presence of a girl to the right of a pillow, our model accurately confirms the absence of a girl, whereas BLIP-2 incorrectly asserts a presence. The image of a food plate, shown in Figure 12(b) and (c), further probe the model’s ability to understand foreground and background distinctions. Our model correctly identifies the fruit next to the cake as a strawberry, where as BLIP-2 incorrectly categorizes it as raspberries. Additionally, our model successfully distinguishes the white color of the plate amidst the various food items placed upon it, indicating a enhanced visual perception capabilities. However Figure 12 (d), presents a complex scenario involving an assessment of an onion’s quality, both our model and BLIP-2 fail to correctly evaluate its healthiness. This highlights the challenge in assessing condition/quality of food which is subject to interpretation.  

  
Fig. 13: Query Diversity: Ours (left), BLIP-2 (right)",4>2
