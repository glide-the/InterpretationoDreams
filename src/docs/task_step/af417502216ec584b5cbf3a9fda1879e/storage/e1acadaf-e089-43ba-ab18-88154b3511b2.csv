角色,内容,分镜
e1acadaf-e089-43ba-ab18-88154b3511b2,研究论文中采用的主要框架在不同任务中的应用与变体,1
e1acadaf-e089-43ba-ab18-88154b3511b2,"### 问题

在自然语言处理（NLP）任务中，Transformer及其变体（如BERT、RoBERTa、ALBERT）广泛使用了LayerNorm作为归一化方法。然而，RMSNorm作为一种更简单的归一化方法，在某些情况下表现出相似甚至更优的性能。基于当前任务步骤“研究论文中采用的主要框架在不同任务中的应用与变体”，我们可以提出以下问题：

**在Transformer及其变体（如BERT、RoBERTa、ALBERT）中，RMSNorm是否能够替代LayerNorm，并在文本分类、问答系统等NLP任务中表现出相似的性能？如果可以，RMSNorm在计算效率和模型性能方面有哪些具体优势？**

这个问题旨在探讨RMSNorm在NLP任务中的适用性，以及它是否能够作为一种更高效的归一化方法替代LayerNorm，从而为研究者提供更多的选择。",1
e1acadaf-e089-43ba-ab18-88154b3511b2,"ref_ids: 454845744505973136, chunk_ids: 6, Score: 0.3789, Text: # 2 Related work
Transformer-based Reinforcement Learning methods Transformer [ 34 ] is a powerful architecture designed for sequence modeling. Owing to the capabilities that emerge as model and data size scale up, the Transformer has become a foundational model in several domains, including natural language processing [ 5 ,27 ,33 ] and computer vision [ 12 ]. However, applying Transformers in reinforcement learning settings, such that they generalize to multiple tasks, remains an open problem.  

Recently, Chen et al. [6] and Janner et al. [20] treat the RL problem as a sequence modeling problem and proposed a Transformer-based architecture to solve it with offline RL. These findings inspired researchers to develop more advanced Transformer-based RL methods. Subsequent efforts mainly focus on two aspects: generalization and adaptability. To improve model online adaptability, Zheng et al. [40] propose the Online Decision Transformer (Online DT), which utilizes the maximumentropy idea to encourage pre-trained policies to explore during a phase of online adaptation. To improve offline adaptation, Xu et al. [38] propose a Hyper-network-based module that helps DT adapt to unseen tasks efficiently. To facilitate task adaptation, Xu et al. [37] introduce the prompt-based DT, which selects short trajectories to use in a task prompt in analogy with in-context learning for large language models. Furthermore, Lee et al. [22] propose a multi-game DT (MDT), which use the expert action inference to consistently produce actions of highly-rewarding behavior. MDT demonstrating that DT can generalize to various Atari games with human-level performance. We argue that the generalization of the above-mentioned works relies on the size of models and does not learn the data efficiently. To address this issue, we introduce a working memory module that can store, blend, and retrieve training information for better model and training efficiency.  

Working memory In the context of machine learning, there is a long history of neural network-based models that incorporate memory mechanisms [ 10 ,31 ,17 ,30 ,1 ,24 ,9 ,29 ,36 ]. Generally, this research aims to enhance the capacity of neural networks to store and manipulate information over extended periods of time, leading to improved performance on a range of tasks. It often takes inspiration from human cognitive function. Most salient to our work, Graves et al. [16] merge concepts from Turing machines and deep learning in “Neural Turing Machines” (NTMs), neural networks that include a content-addressable matrix memory space for storing and updating information throughout time. They show NTMs to be effective for various algorithmic tasks. Contemporaneously, Sukhbaatar et al. [32] introduce “memory networks,” which use a content-addressable matrix memory store and retrieve information from previous computational steps to facilitate complex reasoning and inference tasks. Munkhdalai et al. [25] propose a rapidly adaptable neural memory system, which they instantiate as a feedforward neural network trained by metalearning. They evaluate the memory’s effectiveness in a simple RL setting, maze exploration, and on various NLP tasks. This work can be seen as a precursor to our use of LoRA to adapt the working memory module. More recently, Goyal et al. [15] utilize the “global workspace” theory from cognitive science, which posits that different input entities share information through a common communication channel. The proposed shared global workspace method utilizes the attention mechanism to encourage the most useful information to be shared among neural modules. It is closely related to working memory and inspires us to explore how an explicit working memory can improve the generalization of Transformer-based models. An upshot of our work is that it may be valuable to revisit earlier memory-augmentation methods in light of more powerful foundation models.

# 3 Preliminaries

# 3.1 Problem Formulation
the RL oblem as a Markov decisi process (MDP) p $\\tau=$ $(S,A,\\mathcal{P},\\mathcal{R},\\gamma)$ the trans addition, of choosing action a pre-trained model is used as a starting point for a new task that is related or similar to the original SA P$\\pi(\\,\\cdot\\,;\\phi_{\\pi})$ R·: whe el, ∈A $r:S\\times A\\to\\mathbb{R}$ $\\boldsymbol{S}$ S × A → given a state ates a policy $s\\in S$ the reward funct set of states, ∈S . Here, we consider a transfer learning problem, where eterized by A the set of $\\phi_{\\pi}$ , and , and $\\gamma\\in[0,1)$ $\\pi(a|s;\\phi_{\\pi})$ |$p:S\\times A\\times S\\to(0,1)$ the discount factor. In denotes the probability S × A × S → task on which the model was trained. The idea behind transfer learning is to leverage the knowledge learned by the pre-trained model to improve performance on the new task, for which data may be lacking or inaccessible.  

Formally, in the context of model evaluation, we can define a set of training tasks and testing tasks as $\\overline{{T}}^{t r a i n}$ and $T^{t e s t}$ , respectively. These two sets deliberately have no overlapping tasks, but they may share the same or similar observation and action spaces. To be more specific, for each $\\mathcal{T}^{i}\\in T^{t r a i n}$ , we have access a large training dataset, which contains trajectories $\\tau^{0:H}=(s_{0},a_{0},r_{0},\\cdot\\cdot\\cdot\\,,s_{H},a_{H},r_{H})$ · · · , where His the episode length. However, we assume access to only a small amount of data for the testing tasks.  

Our goal is to evaluate the proposed model in two dimensions. First, we want to assess the model’s generalization , which refers to its ability to solve the testing tasks within a finite time with no additional fine-tuning. Second, we want to test the model’s adaptability , which refers to its ability to improve its performance on the testing tasks through fine-tuning on limited data after pre-training on separate tasks.",1
e1acadaf-e089-43ba-ab18-88154b3511b2,"ref_ids: 454846009781566216, chunk_ids: 0, Score: 0.3574, Text: # 3.2 LLMs are state-of-the-art
LLM-based approaches have become the default in the current NLP research literature, and they are largely perceived to be the current SOTA (state-of-the-art) across NLP benchmarks. For example, Gillioz et al. (2020 ) state: “ Models like GPT and BERT relying on this Transformer architecture have fully outperformed the previous state-of-the-art networks. It surpassed the earlier approaches by such a wide margin that all the recent cutting edge models seem to rely on these Transformer-based architectures. ” (p.179).  

The above was written in the days of fine-tuned Transformer-based LLMs like BERT. At this point, such a statement needs to be considered in the context of the distinction between few-shot performance (ostensibly out-of-domain performance achieved by a model that was not specifically trained on a given task, vs performance of a model fine-tuned for a given task . For example, both BERT ( Devlin et al. 2019 ) and GPT-3 ( Brown et al. 2020a ) were presented with evaluation on question answering, among other tasks, but the former was fine-tuned, while the latter evaluated few-shot style. Since, generally speaking, more in-domain training of ML models yields higher in-domain performance, we now have two different SOTAs, where the few-shot setting could reasonably be expected to yield worse performance vs the same model if it was fine-tuned, but require less data and training. The few-shot vs fine-tuned performance comparisons between different models depends on the specific case, but are by no means guaranteed to be in favor of few-shot. For example, few-shot GPT-3 ( Brown et al. 2020a ) is on the SuperGLUE ( Wang et al. 2019 ) leaderboard with the average score of 71.8, compared to a score of 84.6 achieved by a fine-tuned RoBERTa model ( Liu et al. 2019 ). Recently, OpenAI (2023 ) claims that GPT-4 outperforms unspecified fine-tuned models on 6 out of 7 verbal reasoning tasks, but provides no detail on model or benchmark selection, making it impossible to reproduce or verify these results.  

For models like BERT, it was still possible to meaningfully compare them to earlier supervised approaches, although even that presented plenty of issues ( Rogers 2019 ). But in the few-shot setting, LLMs only compete with other LLMs, and it is not as meaningful to compare them to previous approaches due to the differences in the way that the two are trained and tested. Hence, at present, instead of “LLMs are SOTA” in general, we rather have “this LLM is better than other LLMs in few-shot setting”. What is true is that the current research papers introducing LLMs often include only few- or zero-shot evaluations, which creates the impression that this is the only evaluation that matters. For example, OPT ( Zhang et al. 2022 ) was evaluated on 16 NLP tasks concurrently without fine-tuning, establishing new accuracies in several of them; the same goes for models such as PaLM ( Chowdhery et al. 2022 ), LLaMa ( Touvron et al. 2023a ), and many others, who often report a slew of results across multiple benchmarks to demonstrate few-shot or zero-shot performance, without digging deeper into what tasks and datasets models fail on and what this means in terms of LLM capability. But this change is a change in the focus of the community, and it is driven by the popularity of LLMs and the vague idea of general-purpose models and benchmarks ( Raji et al. 2021 ), rather than theoretical or even practical guarantees of the superiority of this approach. If SOTA is considered strictly in terms of best performance — we have not established that any LLM approach is by definition superior to any non-LLM approach for any given NLP task (especially if both models are well-tuned).  

In fact, we know that for many of our benchmarks huge models are overkill, and very simple baselines sometimes achieve comparable results ( Lin et al. 2023 ). Looking at various NLP leaderboards at Papers with Code at the current time, we can see that non-LLM approaches like state space models ( Dao et al. 2022 ) currently leading on tasks like long term dependency language modeling (i.e. next word prediction) on WikiText-103 ( Merity et al. 2016 ) and simple embeddingbased approaches ( Wang et al. 2020 ) on named entity recognition ( Sang and De Meulder 2003 ). LLMs do win on generality (i.e. having one model do everything), but in practice, smaller taskspecific models may even be preferable because they can be deployed efficiently, even if their performance is not the very top of the leaderboard – which explains the growth of the efficiencyoriented approaches with deliberately smaller models ( Mai et al. 2023 ;Fusco et al. 2023 , inter alia). Their training data also can be more carefully collected and documented for the audits. Finally, top performance on long-running benchmarks may actually be an undesirable symptom of overfitting, making for a worse model in practice ( Oakden-Rayner 2019 ).  

Finally, one more factor to consider with regards to the perceived SOTA status of LLMs is that the results of the evaluations in LLM-based systems should often be taken with a grain of salt. OpenAI itself documented with GPT-3 how hard it is to avoid benchmark contamination ( Brown et al. 2020a ), and there is now extensive evidence of the presence of common NLP benchmarks in multiple datasets used for training LLMs ( Dodge et al. 2021 ;Magar and Schwartz 2022 ;Blevins and Zettlemoyer 2022 ), which can contribute towards improved LLM performance in certain tasks and datasets. The same phenomenon was observed with regards to the claim that GPT-4 achieves a score that falls in the top $10\\%$ of test takers on a simulated bar exam ( Katz et al. 2023 ), which was questioned on grounds of improper evaluation and possible data contamination ( Martínez 2023 ). However, this fundamental problem is not even always addressed in discussions of performance of LLM-based systems. For example, a recent preprint by Bang et al. (2023 ) presents the results of systematic evaluation of ChatGPT on multiple benchmarks – but contains no discussion of the possible benchmark contamination, which could render any such results meaningless.",1
e1acadaf-e089-43ba-ab18-88154b3511b2,"ref_ids: 454895409734360760, chunk_ids: 3, Score: 0.3535, Text: # 1 I NTRODUCTION
Transformers (Vaswani et al., 2017; Brown et al., 2020; Devlin et al., 2019) have emerged as the preferred model in the field of natural language processing (NLP) and other domains requiring sequence-to-sequence modeling. Besides their state-of-art performance in natural language processing tasks, large language models (LLM) such as GPT-3 (Brown et al., 2020) and PaLM (Chowdhery et al., 2022) also exhibit the ability to learn in-context: they can adapt to various downstream tasks based on a brief prompt, thus bypassing the need for additional model fine-tuning. This intriguing ability of in-context learning has sparked interest in the research community, leading numerous studies (Min et al., 2022; Olsson et al., 2022; Li et al., 2023). However, the underlying mechanisms enabling these transformers to perform in-context learning remain unclear.  

In an effort to understand the in-context learning behavior of LLMs, Garg et al. (2022) investigated the performance of transformers, when trained from scratch, in solving specific function class learning problems in-context. Notably, transformers exhibited strong performance across all tasks, matching or even surpassing traditional solvers. Building on this, Aky urek et al. (2022) explored the transformerbased model’s capability to address the linear regression learning problem, interpreting it as an implicit form of established learning algorithms. Their study included both theoretical and empirical perspectives to understand how transformers learn these functions. Subsequently, von Oswald et al. (2022) demonstrated empirically that, when trained to predict the linear function output, a linear self-attention-only transformer inherently learns to perform a single step of gradient descent to solve the linear regression task in-context. While the approach and foundational theory presented by von Oswald et al. (2022) are promising, there exists a significant gap between the simplified architecture they examined and the standard decoder transformer used in practice. The challenge of training a standard decoder transformer from scratch, with only minor architectural modifications, to effectively replicate the learning algorithm remains an open question.  

In traditional machine learning, iterative algorithms are commonly used to solve linear regression. However, the methodologies employed by standard transformers are not naturally structured for iterative computation. A looped transformer architecture, extensively studied in the literature such as Giannou et al. (2023), provides a promising avenue to bridge this gap. In addition to its inherent advantage of addressing problem-solving in an iterative manner, the looped transformer also breaks down tasks into simpler subtasks, potentially leading to significant savings in model parameters.  

To illustrate how task breakdown leads to saving parameters, let’s look closely at using the transformer model to solve linear regression task, specifically $\\pmb{w}$ in $\\mathrm{min}_{w}$ $\\lVert X w-y\\rVert_{2}^{2}$ (Fig. 1). To train a transformer on this task, we input a prompt sequence formatted as $(x_{1}^{\\cdot\\cdot},w^{T}x_{1},\\ldots,x_{k}^{\\cdot},w^{T}x_{k},x_{\\mathrm{test}})$ ).Here $\\mathbf{\\nabla}w$ represents the parameters of the linear regression model, $\\{x_{1},\\cdot\\cdot\\cdot,x_{k}\\}$ are $k$ in-context samples, and $\\pmb{x}_{\\mathrm{test}}$ is the test sample. The transformer can potentially try to predict $y_{\\mathrm{{test}}}$ by approximating the ordinary least squares solution in a single forward pass. The computation of the matrix inverse, as required in the ordinary least squares solution $(X^{T}\\dot{X^{\\prime}})^{-1}X^{T}y$ , is more difficult for transformers to learn compared to matrix multiplication (Charton, 2021; von Oswald et al., 2022). This is attributed to the increased number of layers and heads required in the inverse operation (Giannou et al., 2023). Nevertheless, gradient descent offers an alternative solution to linear regression, which requires only the matrix multiplication: $X^{T}(X w-y)$ , but is applied repeatedly.  

  
Figure 1: How can a transformer be trained to learn an iterative learning algorithm? Here we consider the task of training a transformer to solve linear regression in context. The provided prompt $({\\pmb x}_{1},y_{1},{\\pmb x}_{2},y_{2},\\cdot\\cdot\\cdot\\mathbf{\\epsilon},{\\pmb x}_{k},y_{k},{\\pmb x}_{t e s t})$ is fed into a decoder transformer. The ve is to reduce the squared loss between the predicted $\\hat{y}_{\\mathrm{test}}$ based on this prompt, and the target value $f(\\pmb{x}_{\\mathrm{test}})$ . Garg et al. (2022) demonstrated that a decoder transformer can learn to solve linear regression, which potentially involves learning the approximation of the least squares solution. In this study, we aim to train a transformer to learn iterative learning algorithms. Our goal is to achieve performance on par with standard transformers but with fewer parameters. To this end, we introduce the looped transformer architecture and its accompanying training methodology.  

Motivated by this observation, we ask the following question:  

Can looped transformers emulate iterative learning algorithms more efficiently than standard, non-recursive transformers?  

Within the specific function classes we tested, the answer is positive. Our preliminary findings on using looped transformer to solve the linear regression task are illustrated in Fig. 2. The remainder of the paper is organized as follows. In Sec. 4, we develop a training method for the looped transformer to emulate the desired performance of the iterative algorithm. Subsequently, in Sec. 5, we compare the empirical performance of the standard and the looped transformer and analyze the trade-off between them. Our contributions and findings are outlined below:  

Training methodology for Looped Transformer. We propose a training methodology for looped transformers, aiming to effectively emulate iterative algorithms. The assumption for a looped transformer simulating a convergent algorithm is as loop iterations increase, the performance of the looped transformer should improve or converge. In alignment with this assumption, we delve into the structural design of the looped transformer, as well as investigate the number of loop iterations required during training. These investigations lead to the formulation of our training method.  

Performance of Looped Transformers for in-context Learning. Based on our proposed training algorithm, empirical evidence demonstrates that looped transformer can be trained from scratch to in-context learn data generated from linear functions, sparse linear functions, decision trees, and 2-layer neural networks. Among the varied function classes examined, the looped transformer consistently outperforms the standard transformer, particularly when data is generated from sparse linear functions or decision trees. Our findings hint at the possibility that the looped transformer is more effective at in-context emulating learning algorithms, specifically for the learning tasks explored in this paper.  

  
Figure 2: The looped transformer can emulate iterative learning algorithms, offering performance comparable to standard transformers with reduced parameters . We train a looped transformer to solve linear regression in-context. (Left) : While trained for 30 loop iterations, the looped transformer during inference achieves a stable fixed-point solution beyond the trained loop iterations. (Right) : The trained looped transformer matches the performance of a standard 12-layer transformer and closely aligns with the least squares solver, while using only 1/12 of the transformer’s parameters.",1
