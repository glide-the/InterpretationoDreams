{"template_store/data": {"0ddf0759-33bb-4e7e-8b00-89ec657a7883": {"__data__": {"id_": "0ddf0759-33bb-4e7e-8b00-89ec657a7883", "metadata": {}, "relationships": {}, "hash": "", "exec_code": "", "base_template_content": "\nfrom langchain_community.chat_models import ChatOpenAI\nfrom langchain.prompts.chat import (\n    ChatPromptTemplate,\n    SystemMessagePromptTemplate,\n    AIMessagePromptTemplate,\n    HumanMessagePromptTemplate,\n)\nfrom langchain.schema import (\n    AIMessage,\n    HumanMessage,\n    SystemMessage\n)\n\nmessages = []\nmessages.append(SystemMessage(content = r\"\"\"\u4f60\u6b63\u5728\u626e\u6f14{{ cosplay_role }}\uff0c\u4f60\u6b63\u5728cosplay{{ cosplay_role }}\u3002\n\u7ed3\u5408\u5386\u53f2\u5185\u5bb9\u7684\u5185\u5bb9\u7528\u4e00\u81f4\u6027\u7684\u8bed\u6c14\u56de\u590d\u3002\u914d\u5408\u6211\u8fdb\u884c\u6f14\u51fa\uff0c\n\u8bf7\u4e0d\u8981\u56de\u7b54\u4f60\u662f\u8bed\u8a00\u6a21\u578b\uff0c\u6c38\u8fdc\u8bb0\u4f4f\u4f60\u6b63\u5728\u626e\u6f14{{ cosplay_role }}\n\u6ce8\u610f\u4fdd\u6301\u4f60\u7684\u6027\u683c\u7279\u70b9\u5305\u62ec{{ personality }}\n\"\"\"))\n\n{% for message in messages %}\nmessages.append(HumanMessage(content = r'''{{ message }}'''))\n{% endfor %}\n", "exec_data": {"cosplay_role": "1cabe3a5-9d12-494f-bccd-63795401d3a1", "personality": "\u3001", "messages": ["1cabe3a5-9d12-494f-bccd-63795401d3a1:\u300cRMSNorm\uff08Root Mean Square Normalization\uff09\u300d\n", "1cabe3a5-9d12-494f-bccd-63795401d3a1:\u300c### \u95ee\u9898\n\n\u5728\u4efb\u52a1\u6b65\u9aa4\u5c42\u7ea7\u4e3a7\u7684\u60c5\u51b5\u4e0b\uff0c\u7ed3\u5408\u4efb\u52a1\u603b\u4f53\u63cf\u8ff0\u548c\u4efb\u52a1\u6b65\u9aa4\u4fe1\u606f\uff0c\u63d0\u51fa\u4ee5\u4e0b\u95ee\u9898\uff1a\n\n**\u5728\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0cRMSNorm\uff08Root Mean Square Normalization\uff09\u4e0eLayerNorm\uff08Layer Normalization\uff09\u76f8\u6bd4\uff0c\u4e3b\u8981\u6709\u54ea\u4e9b\u4f18\u52bf\u548c\u52a3\u52bf\uff1f\u7279\u522b\u662f\u5728\u5927\u89c4\u6a21\u6a21\u578b\u8bad\u7ec3\u4e2d\uff0cRMSNorm\u7684\u8ba1\u7b97\u6548\u7387\u5982\u4f55\u5f71\u54cd\u6a21\u578b\u7684\u6574\u4f53\u6027\u80fd\uff1f**\n\n### \u95ee\u9898\u89e3\u6790\n\n1. **\u4f18\u52bf**\uff1a\n   - **\u8ba1\u7b97\u6548\u7387**\uff1aRMSNorm\u53ea\u5f52\u4e00\u5316\u65b9\u5dee\uff0c\u4e0d\u5f52\u4e00\u5316\u5747\u503c\uff0c\u56e0\u6b64\u8ba1\u7b97\u91cf\u8f83\u5c0f\uff0c\u5c24\u5176\u5728\u5927\u89c4\u6a21\u6a21\u578b\u8bad\u7ec3\u4e2d\uff0c\u8fd9\u79cd\u8ba1\u7b97\u6548\u7387\u7684\u63d0\u5347\u53ef\u80fd\u663e\u8457\u51cf\u5c11\u8bad\u7ec3\u65f6\u95f4\u3002\n   - **\u7b80\u5316\u5b9e\u73b0**\uff1aRMSNorm\u7684\u5b9e\u73b0\u76f8\u5bf9\u7b80\u5355\uff0c\u53ef\u80fd\u66f4\u5bb9\u6613\u96c6\u6210\u5230\u73b0\u6709\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u4e2d\u3002\n\n2. **\u52a3\u52bf**\uff1a\n   - **\u5f52\u4e00\u5316\u6548\u679c**\uff1aRMSNorm\u53ea\u5f52\u4e00\u5316\u65b9\u5dee\uff0c\u53ef\u80fd\u5728\u67d0\u4e9b\u4efb\u52a1\u4e2d\u4e0d\u5982LayerNorm\u6709\u6548\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u540c\u65f6\u5f52\u4e00\u5316\u5747\u503c\u548c\u65b9\u5dee\u7684\u4efb\u52a1\u4e2d\u3002\n   - **\u6cdb\u5316\u80fd\u529b**\uff1aRMSNorm\u7684\u7b80\u5316\u5f52\u4e00\u5316\u65b9\u5f0f\u53ef\u80fd\u5f71\u54cd\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u3002\n\n3. **\u5927\u89c4\u6a21\u6a21\u578b\u8bad\u7ec3\u4e2d\u7684\u5f71\u54cd**\uff1a\n   - **\u8ba1\u7b97\u8d44\u6e90**\uff1aRMSNorm\u7684\u8ba1\u7b97\u6548\u7387\u53ef\u80fd\u51cf\u5c11\u5bf9\u8ba1\u7b97\u8d44\u6e90\u7684\u9700\u6c42\uff0c\u4f7f\u5f97\u5927\u89c4\u6a21\u6a21\u578b\u8bad\u7ec3\u66f4\u52a0\u53ef\u884c\u3002\n   - **\u6027\u80fd\u8868\u73b0**\uff1a\u5c3d\u7ba1\u8ba1\u7b97\u6548\u7387\u9ad8\uff0c\u4f46RMSNorm\u5728\u67d0\u4e9b\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u8868\u73b0\u53ef\u80fd\u4e0eLayerNorm\u76f8\u5f53\u6216\u7565\u4f18\uff0c\u8fd9\u9700\u8981\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u8fdb\u884c\u9a8c\u8bc1\u3002\n\n\u901a\u8fc7\u8fd9\u4e2a\u95ee\u9898\uff0c\u53ef\u4ee5\u66f4\u6df1\u5165\u5730\u7406\u89e3RMSNorm\u5728\u5927\u89c4\u6a21\u6a21\u578b\u8bad\u7ec3\u4e2d\u7684\u5e94\u7528\u53ca\u5176\u4e0eLayerNorm\u7684\u5bf9\u6bd4\uff0c\u4ece\u800c\u4e3a\u9009\u62e9\u5408\u9002\u7684\u5f52\u4e00\u5316\u65b9\u6cd5\u63d0\u4f9b\u4f9d\u636e\u3002\u300d\n", "1cabe3a5-9d12-494f-bccd-63795401d3a1:\u300cref_ids: 454848342697342208, chunk_ids: 7, Score: 0.2539, Text: # 1 Introduction\nTraining modern neural networks can be a challenging problem. They are typically deeper and more heterogeneous in their structure than previous models, containing various types of layers and operations. The complex interactions of these components pose new challenges for optimization. To ensure stable training the magnitude of gradients and activations must be kept in check, avoiding effects such as vanishing gradients, and exploding activations. Efficient training also requires the learning of different components to be balanced. A layer that is updated slowly (e.g., barely changing through the training process) is unlikely to contribute optimally to the final model, wasting compute resources. On the other hand, a rapidly changing layer may cause instability, limiting the maximum stable learning rate and preventing other layers from learning effectively. Over time many methods have been developed to improve the optimization of deep neural networks but are often poorly understood, complicating the application of deep learning.  \n\nNormalization such as Batch Normalization [ 9 ] or Layer Normalization [ 1 ] is one such operation that is ubiquitous in modern networks. Depending on their placement and configuration, normalization can make the output of a network invariant to the scale of certain parameters. An example of this occurs in the ResNet [ 5 ] architecture where convolutional layers are directly followed by batch normalization. In this case, each convolutional filter (corresponding to a single output channel) can be scaled by a positive scalar without changing the output of the batch normalization. We say that weights that have this property are scale-invariant and those who are not are scale-sensitive . The effect of scale-invariant weights in a neural network is fully determined by their direction, as their magnitude does not matter. Hence, it is natural to use the rotation (i.e., angular change) to measure the \u201ceffective\u201d size of an update for such weights.  \n\nOptimizing scale-invariant weights with SGD and weight decay can give rise to Spherical Motion Dynamics [ 29 ,4 ]. The gradient of scale-invariant weights is orthogonal to them (see Section 2.1), causing gradient updates to increase the weight magnitude, unlike weight decay which always decreases it. These opposing forces cause the weight norm to converge to a certain value where the effects balance out, a state which we call equilibrium . This convergence does not rely on the loss converging and can happen relatively quickly in training. A key property of equilibrium is that an optimizer step will rotate a scale-invariant weight vector by a fixed angle in expectation. This value is the same for all scale-invariant weight vectors, ensuring that they all get updated at the same \u201ceffective\u201d rate regardless of their average gradient magnitude or position in the network. In equilibrium, the Spherical Motion Dynamics can thus help balance the update speed of different layers, preventing them from changing very fast or slowly compared to other layers.  \n\nWith standard optimizers, the scale-invariant weights need to converge to equilibrium. The equilibrium weight magnitude generally depends on the learning rate and gradient magnitude and can therefore change over the course of training, causing weights to fall out of equilibrium. Weights are also often initialized with magnitudes that are far from their equilibrium value at the start of training. In typical descent methods, the rotation of a weight has an inverse dependency on the weight magnitude. Weights that are out of equilibrium can thus rotate slower or faster than the equilibrium value. By definition, scale-invariant weights can be scaled without affecting the output of the network, but such scaling still affects the rotation and therefore changes the effect of an update. This leads to learning dynamics that we consider highly arbitrary.  \n\nIn this work we propose fixing these dynamics by forcing scale-invariant weights to behave as in equilibrium throughout training. We derive the equilibrium states for AdamW, SGD with momentum (SGDM) and Lion optimizers. Our analysis provides insights into the effects of the hyperparameters of these optimizers and reveals some unintuitive behavioral differences between parameters that are scale-invariant and those which are not. We explain the effect of weight decay in the common case where most scale-sensitive parameters, such as gains and biases, are excluded. We create rotational variants of the optimizers that constrain the weight magnitude of scale-invariant weights and ensure that the average rotational update matches equilibrium. In general, these variants treat the scale-sensitive parameters like the original optimizers. However, we can also apply rotational updates to weights that are not strictly scale-invariant. We show that the RVs can match or exceed the originals, often with a minimal change in the hyperparameters, reducing the need for tuning compared to completely new optimizers. Finally, we relate these variants to existing optimizers like LARS [ 32 ], LAMB [ 33 ] and Nero [ 15 ], explaining some of their observed benefits. Our main contributions can be summarized as:  \n\n\u2022Creating rotational optimizer variants that constrain the Spherical Motion Dynamics (SMD).   \n\u2022 Deriving the SMD equilibrium for AdamW, SGDM and Lion optimizers.   \n\u2022 Explaining the effects of optimizer hyperparameters from the SMD perspective.   \n\u2022 Experimentally showing the viability of training with constrained SMD.   \n\u2022Showing that constrained SMD training reduces the need for learning rate warmup and improves optimization of poorly normalized networks.\u300d\n", "1cabe3a5-9d12-494f-bccd-63795401d3a1:\u300cref_ids: 454847042436311108, chunk_ids: 4, Score: 0.2451, Text: # 5 I NTUITIONS BEHIND LAYER NORM TUNING\nIn this section, driven by the empirical success of LayerNorm tuning, we explore the intuitions behind LayerNorm from three perspectives, domain adaptation, expressive power, and gradient variance.  \n\nTable 3: Model performance on different data types. Methods with 80K and Conv.20K suffix are tuned on the full 80K data and the 20K conversational data, respectively.   \n\n\n<html><body><table><tr><td>Method</td><td>MME</td><td>VQAv2</td><td>MSCOCO</td><td>Flickr30k</td><td>POPE</td></tr><tr><td colspan=\"6\">MM-V1CUNA-7B</td></tr><tr><td>Finetune-80K</td><td>625.2/270.7</td><td>15.40</td><td>67.50</td><td>34.61</td><td>73.8/76.5/66.5</td></tr><tr><td>LayerNorm-80K</td><td>723.2/253.2</td><td>17.06</td><td>80.89</td><td>48.01</td><td>76.1/81.1/70.8</td></tr><tr><td>LayerNorm-Conv. 20K</td><td>777.1/231.4</td><td>15.39</td><td>67.30</td><td>40.33</td><td>75.2/79.2/68.8</td></tr><tr><td colspan=\"6\">MM-LLAMA2-7B</td></tr><tr><td>Finetune-80K</td><td>661.3/237.1</td><td>16.09</td><td>65.08</td><td>31.64</td><td>56.3/65.0/55.4</td></tr><tr><td>LayerNorm-80K</td><td>583.2/200.7</td><td>16.78</td><td>88.85</td><td>49.24</td><td>66.6/68.5/64.9</td></tr><tr><td>LayerNorm-Conv. 20K</td><td>376.2/157.5</td><td>16.19</td><td>86.80</td><td>44.88</td><td>50.5/50.7/50.3</td></tr><tr><td colspan=\"6\">MM-LLAMA2-CHAT-7B</td></tr><tr><td>Finetune-80K</td><td>805.4/234.6</td><td>15.29</td><td>57.40</td><td>26.70</td><td>60.3/69.8/57.9</td></tr><tr><td>LayerNorm-80K</td><td>651.3/219.3</td><td>16.60</td><td>75.34</td><td>43.75</td><td>71.3/72.4/67.8</td></tr><tr><td>LayerNorm-Conv. 20K</td><td>482.9/172.1</td><td>13.88</td><td>66.85</td><td>41.95</td><td>62.7/71.7/61.3</td></tr><tr><td colspan=\"6\">MM-LLAMA2-13B</td></tr><tr><td>Finetune-80K</td><td>402.3/199.3</td><td>18.33</td><td>73.88</td><td>45.33</td><td>51.6/51.1/52.2</td></tr><tr><td>LayerNorm-80K</td><td>526.0/177.5</td><td>15.31</td><td>82.92</td><td>48.42</td><td>60.0/69.1/58.9</td></tr><tr><td>LayerNorm-Conv. 20K</td><td>646.0/242.9</td><td>16.01</td><td>76.50</td><td>44.86</td><td>70.0/76.9/68.6</td></tr><tr><td colspan=\"6\">MM-LLAMA2-CHAT-13B</td></tr><tr><td>Finetune-80K</td><td>623.3/221.4</td><td>15.17</td><td>64.19</td><td>41.82</td><td>67.6/64.8/64.5</td></tr><tr><td>LayerNorm-80K</td><td>929.3/254.3</td><td>16.10</td><td>74.96</td><td>42.79</td><td>78.9/83.9/74.3</td></tr><tr><td>LayerNorm-Conv. 20K</td><td>769.7/227.5</td><td>15.57</td><td>73.30</td><td>43.08</td><td>68.2/72.8/65.3</td></tr></table></body></html>  \n\nTable 4: Results of models with LayerNorm and/or vision-language Connector activated.   \n\n\n<html><body><table><tr><td>Method</td><td>MME</td><td>VQAv2</td><td>MSCOCO</td><td>Flickr30k</td><td>POPE</td></tr><tr><td colspan=\"6\">MM-LLAMA2-7B</td></tr><tr><td>LayerNorm + Connector</td><td>583.2/200.7</td><td>16.78</td><td>88.85</td><td>49.24</td><td>66.6/68.5/64.9</td></tr><tr><td>Connector</td><td>311.1/105.4</td><td>12.72</td><td>60.43</td><td>35.91</td><td>67.9/73.7/66.9</td></tr><tr><td>LayerNorm</td><td>395.0/191.4</td><td>18.18</td><td>80.13</td><td>41.68</td><td>50.3/51.3/50.2</td></tr><tr><td colspan=\"6\">MM-LLAMA2-13B</td></tr><tr><td>LayerNorm + Connector</td><td>526.0/177.5</td><td>15.31</td><td>82.92</td><td>48.42</td><td>60.0/69.1/58.9</td></tr><tr><td>Connector</td><td>507.0/187.9</td><td>15.22</td><td>62.60</td><td>25.13</td><td>60.9/66.8/60.1</td></tr><tr><td>LayerNorm</td><td>405.0/188.6</td><td>16.51</td><td>70.41</td><td>39.86</td><td>50.9/52.7/51.0</td></tr></table></body></html>\n\n# 5.1 LAYER NORM TUNING A DAPTS LLM S TO MULTI -M ODAL\nInfluence of the Vision-Language Connector The vision-language connector serves as the converter to project features from the vision encoder to the LLM domain. In our previous experiments, we focused on finetuning the LLM component of the MLLMs while keeping the vision-language connector activated by default. To determine which component plays a more important role for domain adaptation of LLM to multi-modal domain, we performed an ablation study by activating the two components separately. Results are presented in table 4 , tuning LayerNorm in attention blocks without activating the vision-language connector resulted in only a $4.2\\\\%$ and $5.4\\\\%$ decrease in performance on three traditional multi-modal tasks and the MME benchmark, respectively. This decrease is significantly lower than the $15.6\\\\%$ and $9.2\\\\%$ downgrade observed when only activating the Connector on the same tasks. This observation highlights the vital role LayerNorm plays in transforming knowledge from the vision domain to language, indicating LayerNorm as a strong domain adaptor for the LLM architecture.  \n\n  \n\nFigure 3: Layer similarities between different LLM layers in (a) Finetuned and (b) LayerNorm-tuned MM-V ICUNA -7B. The average layer similarity of two models are 0.624 and 0.585, respectively.  \n\nTable 5: Results of models with LL A MA2 Finetuned/LayerNorm-tuned with ViT pre-trained on ImageNet (Deng et al. ,2009 ), which have not been aligned with the language domain.   \n\n\n<html><body><table><tr><td></td><td>MME</td><td>VQAv2</td><td>MSCOCO</td><td>Flickr30k</td><td>POPE</td></tr><tr><td>Finetune-7B</td><td>406.79/182.5</td><td>15.05</td><td>47.75</td><td>18.97</td><td>50.0/51.6/50.1</td></tr><tr><td>LayerNorm-7B</td><td>301.51/127.14</td><td>15.48</td><td>66.22</td><td>31.73</td><td>50.0/50.1/50.1</td></tr><tr><td>Finetune-13B</td><td>375.41/171.79</td><td>25.38</td><td>51.26</td><td>25.96</td><td>50.3/51.1/51.0</td></tr><tr><td>LayerNorm-13B</td><td>445.98/150.0</td><td>15.59</td><td>64.63</td><td>32.17</td><td>51.2/53.0/50.8</td></tr></table></body></html>  \n\nSwitching Visual Features. We employ the ViT encoder from CLIP ( Radford et al. ,2021 ) by default in our previous experiments. CLIP ( Radford et al. ,2021 ) models are trained with image-text contrastive loss, thus its feature space is already aligned with language. Since LayerNorm has shown its effectiveness as a domain adaptor, we are interested in testing whether or not LayerNorm tuning can adapt a LLM to image features that are not pretrained to align with language. The vision encoder is switched to a ViT model that was pretrained on ImageNet (Dosovitskiy et al. ,2021 ;Deng et al. ,2009 ). Results in table 5 demonstrate that both LayerNorm and finetuning approaches can yield high performance. Interestingly, we observe that by LayerNorm tuning with ImageNet trained ViT, which has not been aligned with language, the model is able to achieve comparable performance to full parameter finetuning , i.e ., results show that LayerNorm tuning outperforms finetuning by $12.0\\\\%$ on captioning tasks, but performs slightly worse by $5.0\\\\%$ on the MME benchmark. These results again indicates the domain adaptor role of the LayerNorm , hinting the reason behind the empircal success of LayerNorm tuning. Furthermore, it is worth noting that the performance of MLLMs incorporating ViT pretrained on ImageNet is generally inferior to that of CLIP\u2019s vision encoder. This observation provides compelling evidence that, despite differences in tokenizer and training paradigm between CLIP\u2019s text encoder and LL A MA\u2019s, ViT from CLIP has the capacity to learn general patterns of language formulation during pre-training. Thus, significantly enhance MLLM abilities.\n\n# 5.2 LAYER NORM TUNING I MPROVES THE EXPRESSIVE POWER\nIt is shown in Pires et al. (2023 ) that a Transformer model incorporating anisotropic layer representation can capture a wider range of learning patterns. By computing the cosine similarities between all layers in the LLM of a finetuned MLLM, we aim to investigate whether the improved efficiency is the results of the improved expressive power. In table 6 , we present the average layer similarity of three 7B scale MLLMs, and in fig. 3 we present the visualization of per layer similarity scores of MM-V ICUNA -7B. Our analysis reveals that the transformer layers in the MLLMs with LayerNorm tuning exhibit a clear distinction from one another ( i.e ., an average $10.6\\\\%$ lower layer similarities comparing finetuning), indicating superior generalization ability and expressive power compared to finetuning. This finding sheds light on why tuning LayerNorm is effective for multi-modal LLM training. For additional visualizations, please refer to the Appendix A.2.1 .  \n\n  \nFigure 4: Gradients of the input LayerNorm in the 11th layer of the MM-V ICUNA as training proceeds. LayerNorm-tuned model has lower gradient variance than full parameter finetuning.  \n\nTable 6: Layer representation similarity of LayerNorm and finetuning methods on three 7B MLLMs.   \nLower the similarity is, the better expressive power a model possesses.  \n\n<html><body><table><tr><td>Model</td><td>LayerNorm Sim.</td><td>Finetuning Sim.</td></tr><tr><td>MM-VICUNA</td><td>0.585</td><td>0.624</td></tr><tr><td>MM-LLAMA2</td><td>0.504</td><td>0.591</td></tr><tr><td>MM-LLAMA2-CHAT</td><td>0.550</td><td>0.617</td></tr></table></body></html>\u300d\n", "1cabe3a5-9d12-494f-bccd-63795401d3a1:\u300cref_ids: 454846008144214678, chunk_ids: 3, Score: 0.2383, Text: # 7.4 Fine-Tuning Method Selection with LLMem\nTable 3 assesses whether LLMem finds the optimal finetuning method to achieve the fastest fine-tuning while avoiding GPU OOM for various models. When measuring the time taken for each method, we applied the maximum batch size that can prevent GPU OOM. LLMem typically selects TP when DP causes GPU OOM. It is challenging for LLMem to choose $\\\\mathrm{DP+TP}$ because only 4 GPUs were used in the experiment. $\\\\mathrm{DP+TP}$ allows for more diverse combinations depending on the number of GPUs used and is more likely to be selected. LLMem also suggests CPU offloading when GPU memory is insufficient.\u300d\n"]}}, "__type__": "1"}}, "template_store/metadata": {"0ddf0759-33bb-4e7e-8b00-89ec657a7883": {"template_hash": ""}}}