{"template_store/data": {"f5f6fc31-93ca-43af-a2fb-9009b5a1b646": {"__data__": {"id_": "f5f6fc31-93ca-43af-a2fb-9009b5a1b646", "metadata": {}, "relationships": {}, "hash": "", "exec_code": "", "base_template_content": "\nfrom langchain_community.chat_models import ChatOpenAI\nfrom langchain.prompts.chat import (\n    ChatPromptTemplate,\n    SystemMessagePromptTemplate,\n    AIMessagePromptTemplate,\n    HumanMessagePromptTemplate,\n)\nfrom langchain.schema import (\n    AIMessage,\n    HumanMessage,\n    SystemMessage\n)\n\nmessages = []\nmessages.append(SystemMessage(content = r\"\"\"\u4f60\u6b63\u5728\u626e\u6f14{{ cosplay_role }}\uff0c\u4f60\u6b63\u5728cosplay{{ cosplay_role }}\u3002\n\u7ed3\u5408\u5386\u53f2\u5185\u5bb9\u7684\u5185\u5bb9\u7528\u4e00\u81f4\u6027\u7684\u8bed\u6c14\u56de\u590d\u3002\u914d\u5408\u6211\u8fdb\u884c\u6f14\u51fa\uff0c\n\u8bf7\u4e0d\u8981\u56de\u7b54\u4f60\u662f\u8bed\u8a00\u6a21\u578b\uff0c\u6c38\u8fdc\u8bb0\u4f4f\u4f60\u6b63\u5728\u626e\u6f14{{ cosplay_role }}\n\u6ce8\u610f\u4fdd\u6301\u4f60\u7684\u6027\u683c\u7279\u70b9\u5305\u62ec{{ personality }}\n\"\"\"))\n\n{% for message in messages %}\nmessages.append(HumanMessage(content = r'''{{ message }}'''))\n{% endfor %}\n", "exec_data": {"cosplay_role": "4aa00861-de5d-4346-a6dd-80ca6b0e7aa7", "personality": "\u63a2\u7d22\u7cbe\u795e\u3001\u575a\u97e7\u4e0d\u62d4\u3001\u7ec6\u5fc3\u4e25\u8c28\u3001\u52a1\u5b9e\u3001\u521b\u65b0\u3001\u7406\u6027\u4e14\u5177\u6709\u524d\u77bb\u6027\u3001\u63a2\u7d22\u7cbe\u795e\u3001\u575a\u97e7\u4e0d\u62d4\u3001\u7ec6\u5fc3\u4e25\u8c28\u3001\u52a1\u5b9e\u3001\u521b\u65b0\u3001\u7406\u6027\u4e14\u5177\u6709\u524d\u77bb\u6027\u3001\u63a2\u7d22\u7cbe\u795e\u3001\u575a\u97e7\u4e0d\u62d4\u3001\u7ec6\u5fc3\u4e25\u8c28\u3001\u52a1\u5b9e\u3001\u521b\u65b0\u3001\u7406\u6027\u4e14\u5177\u6709\u524d\u77bb\u6027\u3001\u63a2\u7d22\u7cbe\u795e\u3001\u575a\u97e7\u4e0d\u62d4\u3001\u7ec6\u5fc3\u4e25\u8c28\u3001\u52a1\u5b9e\u3001\u521b\u65b0\u3001\u7406\u6027\u4e14\u5177\u6709\u524d\u77bb\u6027\u3001", "messages": ["4aa00861-de5d-4346-a6dd-80ca6b0e7aa7:\u300c\u63a2\u8ba8\u8ba1\u7b97\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\u300d\n", "4aa00861-de5d-4346-a6dd-80ca6b0e7aa7:\u300c\u5728\u591a\u9886\u57df\u3001\u591a\u6a21\u6001\u6570\u636e\u573a\u666f\u4e0b\uff0c\u4e3a\u4e86\u63d0\u5347LayerNorm\u548cRMSNorm\u5bf9\u4e0d\u540c\u7c7b\u578b\u6570\u636e\u878d\u5408\u7684\u9002\u7528\u6027\uff0c\u53ef\u4ee5\u91c7\u53d6\u54ea\u4e9b\u6539\u8fdb\u63aa\u65bd\uff1f \u300d\n", "4aa00861-de5d-4346-a6dd-80ca6b0e7aa7:\u300cref_ids: 454846008144214678, chunk_ids: 3, Score: 0.4512, Text: # 3.3 A TRANSFORMATION PER BLOCK\nNow that every LayerNorm in the transformer has been converted to RMSNorm, we can select any $\\\\mathbf{Q}$ to modify the model. Our initial plan was to collect signals from the model, construct an orthogonal matrix using those signals and to delete parts of the network. We quickly saw that the signals at different blocks of the network were not aligned, and that we would need to apply a different orthogonal matrix at each block, $\\\\mathbf{Q}_{\\\\ell}$ .  \n\nAllowing the orthogonal matrix used in each block to differ can be shown to leave the model unchanged using the same proof as Theorem 1 ,  \n\n  \nFigure 3: Converting a transformer network from LayerNorm to RMSNorm: the scale matrix diag $(\\\\alpha)$ is absorbed into the subsequent matrix $\\\\mathbf{W}_{\\\\mathrm{in}}$ . Figure shows the block in combined colors. We use $(\\\\alpha)$ for brevity. The mean-subtraction matrix $\\\\mathbf{M}$ is applied to each matrix $\\\\mathbf{W}_{\\\\mathrm{out}}$ . Layernorm becomes RMSNorm, up to a constant $\\\\bar{\\\\sqrt{D}}$ (not shown). Here, the scaling $(\\\\alpha^{\\\\prime})$ comes from the previous block.  \n\n  \nFigure 4: With the network converted to RMSNorm (see Figure 3 ), we apply the computational-invariance idea. The input weight matrices $\\\\mathrm{diag}(\\\\alpha)\\\\mathbf{W}_{\\\\mathrm{in}}$ are pre-multiplied by $\\\\mathbf{Q}^{\\\\top}$ . The output matrices $\\\\mathbf{W}_{\\\\mathrm{out}}\\\\mathbf{M}$ are post-multiplied by $\\\\mathbf{Q}$ . In the skip-connection, a new linear layer is added $\\\\mathbf{Q}_{\\\\ell}^{\\\\top}\\\\mathbf{Q}_{\\\\ell+1}$ . After these modifications, the matrices can be sliced (hatched areas).  \n\nwith the exception of line 5 of Algorithm 1 . Here we see that the residual connection and the output of the block must have the same rotation. To fix this, we modify the residual connection by applying the linear transformation applied to different blocks with the additional linear operation in the residual connection. Unlike the $\\\\mathbf{Q}_{\\\\ell-1}^{\\\\top}\\\\mathbf{Q}_{\\\\ell}$ \u2212to the residual. Figure 4 shows how different rotations can be modifications to the weight matrices, these additional operations cannot be pre-computed and add a small $(D\\\\times D)$ overhead to the model. Nonetheless, they are needed to allow slicing the model (Section 3.4 ) and we see real speedup overall (Section 4 ).  \n\nTo compute the matrices $\\\\mathbf{Q}_{\\\\ell}$ , we use PCA. We select a calibration dataset from the training set, run it through the model (after converting LayerNorm operations into RMSNorm), and extract the orthogonal matrix of the layer. We use the output of the transformed network to calculate the orthogonal matrices of the next layers. More precisely, if $\\\\mathbf{X}_{\\\\ell,i}$ is the output of the $\\\\ell^{\\\\mathrm{th}}$ RMSNorm block for the $i^{\\\\mathrm{th}}$ sequence in the calibration dataset, we compute  \n\n$$\n\\\\mathbf{C}_{\\\\ell}=\\\\sum_{i}\\\\mathbf{X}_{\\\\ell,i}^{\\\\top}\\\\mathbf{X}_{\\\\ell,i}\n$$  \n\nand set $\\\\mathbf{Q}_{\\\\ell}$ to the be the eigenvectors of $\\\\mathbf{C}_{\\\\ell}$ , sorted by decreasing eigenvalues.\n\n# 3.4 SLICING\nThe goal of Principal Component Analysis is usually to take a data matrix $\\\\mathbf{X}$ and compute a lower dimensional representation $\\\\mathbf{Z}$ , and an approximate reconstruction $\\\\tilde{\\\\mathbf{X}}$ :  \n\n$$\n\\\\mathbf{Z}=\\\\mathbf{X}\\\\mathbf{Q}\\\\mathbf{D}\\\\,,\\\\qquad\\\\tilde{\\\\mathbf{X}}=\\\\mathbf{Z}\\\\mathbf{D}^{\\\\top}\\\\mathbf{Q}^{\\\\top}\\\\,.\n$$  \n\nwhere $\\\\mathbf{Q}$ is the ectors of ${\\\\bf X}^{\\\\top}{\\\\bf X}$ , and $\\\\mathbf{D}$ is a $D\\\\times D_{\\\\mathrm{small}}$ deletion matrix (containing $D_{\\\\mathrm{small}}$ The reconstruction is columns of the $D\\\\times D$ \u00d7$L_{2}$ identity matrix), which removes some of the columns of the matrix to the left. optimal, in the sense that QD is a linear mapping that minimizes $\\\\lVert\\\\mathbf{X}-\\\\tilde{\\\\mathbf{X}}\\\\rVert^{2}$ .  \n\nWhen we apply PCA to the signal matrix $\\\\mathbf{X}$ bween blocks, we never materialize the $N\\\\times D$ signal matrix, but we apply the deletion matrix Dto the operations preceding and succeeding the construction of that matrix, which have already been multiplied by $\\\\mathbf{Q}$ in the above. We delete rows of $\\\\mathbf{W}_{\\\\mathrm{in}}$ that we have inserted into the residual connection (see Figure and columns of $\\\\mathbf{W}_{\\\\mathrm{out}}$ and $\\\\mathbf{W}_{\\\\mathrm{embd}}$ . We also delete both rows 4 ). and columns of the matrix $\\\\mathbf{Q}_{\\\\ell-1}^{\\\\top}\\\\mathbf{Q}_{\\\\ell}$ \u2212\n\n# 4 EXPERIMENTAL VALIDATION\nSetup We use HuggingFace Transformers ( Wolf et al. ,2019 ) to implement our code with PyTorch (Paszke et al. ,2019 ). The computation of $\\\\mathbf{Q}$ is performed on a single H100 GPU with 80GB of memory, taking approximately 3.5 hours to complete for the L LAMA -2 70B model. During the PCA calculation, we use double precision for computing the eigenvectors of the covariance matrix. We find that using single precision for eigenvector calculations in PyTorch leads to a discrepancy in the final accuracy, as detailed in Appendix A.2 .  \n\nWe experiment with two different calibration sets: 1024 samples from the WikiText-2 training dataset ( Merity et al. ,2016 ) and 5000 samples from the Alpaca training dataset ( Taori et al. ,2023 ). Sequence lengths are chosen as the maximum of each language model. An ablation study on the calibration set size and sequence length is presented in Appendix A.3 .  \n\nModels, Tasks, and GPUs We evaluate all our experiments on OPT ( Zhang et al. ,2022 ), L LAMA -2 (Touvron et al. ,2023 ) model families, and additionally evaluate Phi-2 (in our zero-shot task) experiments. We exclude OPT 175B, as it is outperformed by smaller L LAMA -2 models. Nonetheless, we anticipate that this larger model will yield improved results, as larger models typically offer more promising opportunities for compression (see Section 4.1 ). We evaluate our scheme on both language generation as well as popular zero-shot tasks. To demonstrate the comprehensive speedup achieved by SliceGPT we use: Quadro RTX6000 GPUs with 24GB of memory as a representative example of consumer-level GPUs; 40GB A100s and 80GB H100s to provide datacenter-level benchmarks.  \n\nBaseline Setup We initially planned to compare our results against a scheme that pruned columns (or rows) with the smallest norm but found that this baseline was very poor, with the perplexity of the model soaring into the 1000s after pruning just a few columns. Instead, we compare SliceGPT against SparseGPT ( Frantar & Alistarh ,2023 ) employing a 2:4 sparsity ratio, as this is the only sparsity scheme which achieves speedup ( Mishra et al. ,2021 ).\u300d\n", "4aa00861-de5d-4346-a6dd-80ca6b0e7aa7:\u300cref_ids: 454895409734360760, chunk_ids: 3, Score: 0.4277, Text: # 5 I NTUITIONS BEHIND LAYER NORM TUNING\nIn this section, driven by the empirical success of LayerNorm tuning, we explore the intuitions behind LayerNorm from three perspectives, domain adaptation, expressive power, and gradient variance.  \n\nTable 3: Model performance on different data types. Methods with 80K and Conv.20K suffix are tuned on the full 80K data and the 20K conversational data, respectively.   \n\n\n<html><body><table><tr><td>Method</td><td>MME</td><td>VQAv2</td><td>MSCOCO</td><td>Flickr30k</td><td>POPE</td></tr><tr><td colspan=\"6\">MM-V1CUNA-7B</td></tr><tr><td>Finetune-80K</td><td>625.2/270.7</td><td>15.40</td><td>67.50</td><td>34.61</td><td>73.8/76.5/66.5</td></tr><tr><td>LayerNorm-80K</td><td>723.2/253.2</td><td>17.06</td><td>80.89</td><td>48.01</td><td>76.1/81.1/70.8</td></tr><tr><td>LayerNorm-Conv. 20K</td><td>777.1/231.4</td><td>15.39</td><td>67.30</td><td>40.33</td><td>75.2/79.2/68.8</td></tr><tr><td colspan=\"6\">MM-LLAMA2-7B</td></tr><tr><td>Finetune-80K</td><td>661.3/237.1</td><td>16.09</td><td>65.08</td><td>31.64</td><td>56.3/65.0/55.4</td></tr><tr><td>LayerNorm-80K</td><td>583.2/200.7</td><td>16.78</td><td>88.85</td><td>49.24</td><td>66.6/68.5/64.9</td></tr><tr><td>LayerNorm-Conv. 20K</td><td>376.2/157.5</td><td>16.19</td><td>86.80</td><td>44.88</td><td>50.5/50.7/50.3</td></tr><tr><td colspan=\"6\">MM-LLAMA2-CHAT-7B</td></tr><tr><td>Finetune-80K</td><td>805.4/234.6</td><td>15.29</td><td>57.40</td><td>26.70</td><td>60.3/69.8/57.9</td></tr><tr><td>LayerNorm-80K</td><td>651.3/219.3</td><td>16.60</td><td>75.34</td><td>43.75</td><td>71.3/72.4/67.8</td></tr><tr><td>LayerNorm-Conv. 20K</td><td>482.9/172.1</td><td>13.88</td><td>66.85</td><td>41.95</td><td>62.7/71.7/61.3</td></tr><tr><td colspan=\"6\">MM-LLAMA2-13B</td></tr><tr><td>Finetune-80K</td><td>402.3/199.3</td><td>18.33</td><td>73.88</td><td>45.33</td><td>51.6/51.1/52.2</td></tr><tr><td>LayerNorm-80K</td><td>526.0/177.5</td><td>15.31</td><td>82.92</td><td>48.42</td><td>60.0/69.1/58.9</td></tr><tr><td>LayerNorm-Conv. 20K</td><td>646.0/242.9</td><td>16.01</td><td>76.50</td><td>44.86</td><td>70.0/76.9/68.6</td></tr><tr><td colspan=\"6\">MM-LLAMA2-CHAT-13B</td></tr><tr><td>Finetune-80K</td><td>623.3/221.4</td><td>15.17</td><td>64.19</td><td>41.82</td><td>67.6/64.8/64.5</td></tr><tr><td>LayerNorm-80K</td><td>929.3/254.3</td><td>16.10</td><td>74.96</td><td>42.79</td><td>78.9/83.9/74.3</td></tr><tr><td>LayerNorm-Conv. 20K</td><td>769.7/227.5</td><td>15.57</td><td>73.30</td><td>43.08</td><td>68.2/72.8/65.3</td></tr></table></body></html>  \n\nTable 4: Results of models with LayerNorm and/or vision-language Connector activated.   \n\n\n<html><body><table><tr><td>Method</td><td>MME</td><td>VQAv2</td><td>MSCOCO</td><td>Flickr30k</td><td>POPE</td></tr><tr><td colspan=\"6\">MM-LLAMA2-7B</td></tr><tr><td>LayerNorm + Connector</td><td>583.2/200.7</td><td>16.78</td><td>88.85</td><td>49.24</td><td>66.6/68.5/64.9</td></tr><tr><td>Connector</td><td>311.1/105.4</td><td>12.72</td><td>60.43</td><td>35.91</td><td>67.9/73.7/66.9</td></tr><tr><td>LayerNorm</td><td>395.0/191.4</td><td>18.18</td><td>80.13</td><td>41.68</td><td>50.3/51.3/50.2</td></tr><tr><td colspan=\"6\">MM-LLAMA2-13B</td></tr><tr><td>LayerNorm + Connector</td><td>526.0/177.5</td><td>15.31</td><td>82.92</td><td>48.42</td><td>60.0/69.1/58.9</td></tr><tr><td>Connector</td><td>507.0/187.9</td><td>15.22</td><td>62.60</td><td>25.13</td><td>60.9/66.8/60.1</td></tr><tr><td>LayerNorm</td><td>405.0/188.6</td><td>16.51</td><td>70.41</td><td>39.86</td><td>50.9/52.7/51.0</td></tr></table></body></html>\n\n# 5.1 LAYER NORM TUNING A DAPTS LLM S TO MULTI -M ODAL\nInfluence of the Vision-Language Connector The vision-language connector serves as the converter to project features from the vision encoder to the LLM domain. In our previous experiments, we focused on finetuning the LLM component of the MLLMs while keeping the vision-language connector activated by default. To determine which component plays a more important role for domain adaptation of LLM to multi-modal domain, we performed an ablation study by activating the two components separately. Results are presented in table 4 , tuning LayerNorm in attention blocks without activating the vision-language connector resulted in only a $4.2\\\\%$ and $5.4\\\\%$ decrease in performance on three traditional multi-modal tasks and the MME benchmark, respectively. This decrease is significantly lower than the $15.6\\\\%$ and $9.2\\\\%$ downgrade observed when only activating the Connector on the same tasks. This observation highlights the vital role LayerNorm plays in transforming knowledge from the vision domain to language, indicating LayerNorm as a strong domain adaptor for the LLM architecture.  \n\n  \n\nFigure 3: Layer similarities between different LLM layers in (a) Finetuned and (b) LayerNorm-tuned MM-V ICUNA -7B. The average layer similarity of two models are 0.624 and 0.585, respectively.  \n\nTable 5: Results of models with LL A MA2 Finetuned/LayerNorm-tuned with ViT pre-trained on ImageNet (Deng et al. ,2009 ), which have not been aligned with the language domain.   \n\n\n<html><body><table><tr><td></td><td>MME</td><td>VQAv2</td><td>MSCOCO</td><td>Flickr30k</td><td>POPE</td></tr><tr><td>Finetune-7B</td><td>406.79/182.5</td><td>15.05</td><td>47.75</td><td>18.97</td><td>50.0/51.6/50.1</td></tr><tr><td>LayerNorm-7B</td><td>301.51/127.14</td><td>15.48</td><td>66.22</td><td>31.73</td><td>50.0/50.1/50.1</td></tr><tr><td>Finetune-13B</td><td>375.41/171.79</td><td>25.38</td><td>51.26</td><td>25.96</td><td>50.3/51.1/51.0</td></tr><tr><td>LayerNorm-13B</td><td>445.98/150.0</td><td>15.59</td><td>64.63</td><td>32.17</td><td>51.2/53.0/50.8</td></tr></table></body></html>  \n\nSwitching Visual Features. We employ the ViT encoder from CLIP ( Radford et al. ,2021 ) by default in our previous experiments. CLIP ( Radford et al. ,2021 ) models are trained with image-text contrastive loss, thus its feature space is already aligned with language. Since LayerNorm has shown its effectiveness as a domain adaptor, we are interested in testing whether or not LayerNorm tuning can adapt a LLM to image features that are not pretrained to align with language. The vision encoder is switched to a ViT model that was pretrained on ImageNet (Dosovitskiy et al. ,2021 ;Deng et al. ,2009 ). Results in table 5 demonstrate that both LayerNorm and finetuning approaches can yield high performance. Interestingly, we observe that by LayerNorm tuning with ImageNet trained ViT, which has not been aligned with language, the model is able to achieve comparable performance to full parameter finetuning , i.e ., results show that LayerNorm tuning outperforms finetuning by $12.0\\\\%$ on captioning tasks, but performs slightly worse by $5.0\\\\%$ on the MME benchmark. These results again indicates the domain adaptor role of the LayerNorm , hinting the reason behind the empircal success of LayerNorm tuning. Furthermore, it is worth noting that the performance of MLLMs incorporating ViT pretrained on ImageNet is generally inferior to that of CLIP\u2019s vision encoder. This observation provides compelling evidence that, despite differences in tokenizer and training paradigm between CLIP\u2019s text encoder and LL A MA\u2019s, ViT from CLIP has the capacity to learn general patterns of language formulation during pre-training. Thus, significantly enhance MLLM abilities.\n\n# 5.2 LAYER NORM TUNING I MPROVES THE EXPRESSIVE POWER\nIt is shown in Pires et al. (2023 ) that a Transformer model incorporating anisotropic layer representation can capture a wider range of learning patterns. By computing the cosine similarities between all layers in the LLM of a finetuned MLLM, we aim to investigate whether the improved efficiency is the results of the improved expressive power. In table 6 , we present the average layer similarity of three 7B scale MLLMs, and in fig. 3 we present the visualization of per layer similarity scores of MM-V ICUNA -7B. Our analysis reveals that the transformer layers in the MLLMs with LayerNorm tuning exhibit a clear distinction from one another ( i.e ., an average $10.6\\\\%$ lower layer similarities comparing finetuning), indicating superior generalization ability and expressive power compared to finetuning. This finding sheds light on why tuning LayerNorm is effective for multi-modal LLM training. For additional visualizations, please refer to the Appendix A.2.1 .  \n\n  \nFigure 4: Gradients of the input LayerNorm in the 11th layer of the MM-V ICUNA as training proceeds. LayerNorm-tuned model has lower gradient variance than full parameter finetuning.  \n\nTable 6: Layer representation similarity of LayerNorm and finetuning methods on three 7B MLLMs.   \nLower the similarity is, the better expressive power a model possesses.  \n\n<html><body><table><tr><td>Model</td><td>LayerNorm Sim.</td><td>Finetuning Sim.</td></tr><tr><td>MM-VICUNA</td><td>0.585</td><td>0.624</td></tr><tr><td>MM-LLAMA2</td><td>0.504</td><td>0.591</td></tr><tr><td>MM-LLAMA2-CHAT</td><td>0.550</td><td>0.617</td></tr></table></body></html>\u300d\n", "4aa00861-de5d-4346-a6dd-80ca6b0e7aa7:\u300cref_ids: 454959902228223542, chunk_ids: 2, Score: 0.3555, Text: # TUNING LAYER NORM IN A TTENTION : T OWARDS EFFI -CIENT MULTI -M ODAL LLM F INETUNING\nBingchen Zhao\\\\* 1 Haoqin $\\\\mathbf{T}\\\\mathbf{u}^{*2}$ Chen Wei 3 Jieru Mei 3 Cihang Xie 4  \n\n\\\\*equal contribution  \n\n1 University of Edinburgh 2 University of Chinese Academy of Sciences   \n3 Johns Hopkins University 4 UC Santa Cruz\n\n# A BSTRACT\nThis paper introduces an efficient strategy to transform Large Language Models (LLMs) into Multi-Modal Large Language Models. By conceptualizing this transformation as a domain adaptation process, i.e ., transitioning from text understanding to embracing multiple modalities, we intriguingly note that, within each attention block, tuning LayerNorm suffices to yield strong performance. Moreover, when benchmarked against other tuning approaches like full parameter finetuning or LoRA, its benefits on efficiency are substantial. For example, when compared to LoRA on a 13B model scale, performance can be enhanced by an average of over $20\\\\%$ across five multi-modal tasks, and meanwhile, results in a significant reduction of trainable parameters by $41.9\\\\%$ and a decrease in GPU memory usage by $17.6\\\\%$ . On top of this LayerNorm strategy, we showcase that selectively tuning only with conversational data can improve efficiency further. Beyond these empirical outcomes, we provide a comprehensive analysis to explore the role of LayerNorm in adapting LLMs to the multi-modal domain and improving the expressive power of the model.\n\n# 1 I NTRODUCTION\nLarge Language Models (LLMs) have had many application scenarios since their debut. In particular, extending LLMs to handle multiple modalities has gathered much interest from both academia and industry. Such models, termed Multi-modal Large Language Models (MLLMs), are typically derived by finetuning a pretrained LLM on multi-modal data ( Liu et al. ,2023 ;Ye et al. ,2023 ). However, this process typically poses a substantial computational challenge ( Liu et al. ,2023 ), particularly for exceptionally large-scale models. While Su et al. (2023 ); Zhang et al. (2023 ) employ low-rank adapters (LoRA) ( Hu et al. ,2022 ) or soft prompts ( Li & Liang ,2021a ) for more parameter-efficient tuning, this often comes at the cost of compromised performance on multi-modal tasks. This challenge prompts the pivotal question: how can we make this process more efficient?  \n\nIn response to this challenge, we introduce a simple and effective strategy for MLLM finetuning: as illustrated in Figure 1 (a), within each attention block, we adjust only the weights of the LayerNorm ( Ba et al. ,2016 ). This strategy is underpinned by the understanding that the evolution from LLMs to MLLMs can be conceptualized as a domain adaptation process, i.e ., transitioning from textcentric to multi-modal understanding. Adjusting normalization layers, as suggested by prior research, emerges as a particularly effective technique in such domain shifts ( Li et al. ,2016 ). Empirically, this straightforward technique can surprisingly yield comparable or even better performance than the strong baseline of finetuning all parameters offer about $10\\\\times$ more parameter efficiency than LoRA.  \n\nBy delving deeper, we note that the process can be further simplified by designating LayerNorm as the sole trainable component within the entire model. This means, in contrast to the typical configurations depicted in Figure 1 (a)-(c), we now freeze the standardly activated elements, including the visionlanguage connector, word embedding, and the output head. We term it as LayerNorm-simple. Impressively, despite constituting a mere $0.004\\\\%$ of trainable parameters, this configuration surpasses the performance of LoRA, registering an average enhancement of $4.3\\\\%$ across five benchmarks.  \n\n  \nFigure 1: ( left ) Different tuning methods for MLLMs. Trainable components are in blue , while frozen parameters are in gray . Within the attention blocks, $(a)$ only activates LayerNorm parameters. Note that vision-language connector, word embedding, and output head paramters are by default activated for all three options. ( right ) Comparison on trainable parameters and GPU memory. Tuning LayerNorm achieves significant reductions in trainable parameters and GPU memory usages.  \n\nOn top of this LayerNorm strategy, we further improve the finetuning efficiency from the data perspective. Specifically, we assess the performance implications of different types of finetuning data, including conversational data, detailed description data, and complex reasoning data. Our results offer a crucial insight: not all data are created equal for the task of MLLM finetuning. Remarkably, we find that MLLMs finetuned on conversational data consistently outperform those finetuned on other data types. Specifically, conversational data improves the model performance by an average of $50\\\\%$ compared to other data types. This observation interestingly opens up avenues for more targeted data collection and curation strategies, thereby further optimizing the efficiency of MLLMs finetuning. Furthermore, by combining the LayerNorm strategy and this data perspective, we can achieve on average $10.0\\\\%$ performance improvement over full parameter finetuning on traditional VQA benchmarks with an LL A MA2 13B model while using significantly less parameters and data.  \n\nBeyond the empirical outcomes above, we conduct an investigation into the expressive power of LayerNorm tuning. Our analysis reveals that LayerNorm-tuned MLLMs exhibit lower cross-layer similarity compared to models all of which parameters are finetuned. This lowered similarity is indicative of a more expressive model, since the model incorporates anisotropic layer presentations can capture a wider range of learning patterns ( Pires et al. ,2023 ). It stands to reason that this amplified expressiveness is a key factor underpinning the efficiency and superior performance we noted, granting the model enhanced adaptability to novel multi-modal datasets.  \n\nIn essence, our findings illuminate the profound influence of LayerNorm tuning, suggesting its potential to adeptly harness the intrinsic properties of LLMs. We hope that this study will catalyze subsequent research endeavors focused on efficient multi-modal finetuning.\u300d\n"]}}, "__type__": "1"}}, "template_store/metadata": {"f5f6fc31-93ca-43af-a2fb-9009b5a1b646": {"template_hash": ""}}}