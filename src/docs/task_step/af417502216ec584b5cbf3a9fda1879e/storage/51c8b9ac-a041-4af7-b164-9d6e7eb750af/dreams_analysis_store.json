{"analysis_store/data": {"3abda052-4400-4e81-99f5-a68a76357d1e": {"__data__": {"id_": "3abda052-4400-4e81-99f5-a68a76357d1e", "metadata": {}, "relationships": {}, "hash": "", "story_scenario_context": "### Step by Step Decomposition\n\n#### Step 1: \u7406\u89e3\u4efb\u52a1\u80cc\u666f\n- **\u4efb\u52a1\u80cc\u666f**\uff1a\u4f5c\u4e3a\u4e00\u4e2a\u793e\u4f1a\u5b66\u7814\u7a76\u5b66\u8005\uff0c\u60a8\u5df2\u7ecf\u67e5\u9605\u4e86\u5362\u66fc\u7684\u300a\u4f5c\u4e3a\u6fc0\u60c5\u7684\u7231\u60c5\u300b\u4e00\u4e66\uff0c\u5e76\u5c1d\u8bd5\u901a\u8fc7\u53c2\u8003\u6587\u732e\u4e2d\u5b9a\u4e49\u7684\u7231\u60c5\u8bed\u4e49\u5b66\uff0c\u4ece\u6587\u672c\u4e2d\u603b\u7ed3\u4e0b\u65b9\u7247\u6bb5\u3002\n- **\u4e3b\u8981\u4efb\u52a1**\uff1a\u7814\u7a76\u4ea4\u6d41\u5a92\u4ecb\u9886\u57df\u7684\u8bed\u4e49\u4fe1\u606f\uff0c\u6fc0\u60c5\u7684\u975e\u7406\u6027\u4e0e\u98ce\u96c5\u60c5\u672f\u7684\u5076\u7136\u6027\uff0c\u81ea\u8eab\u7684\u5feb\u611f\u662f\u5426\u8f6c\u79fb\u5230\u793e\u4f1a\u884c\u4e3a\u4e0a\uff0c\u8bed\u4e49\u4fe1\u606f\u7684\u56fa\u5b9a\u5f62\u5f0f\u4e0e\u9884\u671f\u843d\u7a7a\u56e0\u679c\u6027\uff0c\u662f\u5426\u5b58\u5728\u53ef\u6fc0\u53d1\u6027\u62d3\u5c55\u5230\u5426\u5b9a\u7269\u4e4b\u4e2d\u3002\n\n#### Step 2: \u5206\u6790\u6587\u672c\u5185\u5bb9\n- **\u6587\u672c\u5185\u5bb9**\uff1a\u89d2\u8272\u3001\u5185\u5bb9\u3001\u5206\u955c\u3002\n- **\u5206\u6790**\uff1a\u6587\u672c\u5185\u5bb9\u8f83\u4e3a\u7b80\u6d01\uff0c\u4ec5\u5305\u542b\u89d2\u8272\u3001\u5185\u5bb9\u548c\u5206\u955c\u4e09\u4e2a\u90e8\u5206\uff0c\u6ca1\u6709\u5177\u4f53\u63cf\u8ff0\u3002\n\n#### Step 3: \u7814\u7a76\u7a81\u51fa\u7279\u70b9\n- **\u793e\u4ea4\u5a92\u4f53\u4e2d\u7684\u7231\u60c5\u8868\u73b0**\uff1a\u5efa\u8bae\u7814\u7a76\u793e\u4ea4\u5a92\u4f53\u4e2d\u7684\u7231\u60c5\u8868\u73b0\uff0c\u63a2\u8ba8\u5362\u66fc\u7684\u7231\u60c5\u8bed\u4e49\u5b66\u5728\u793e\u4ea4\u5a92\u4f53\u4e2d\u7684\u5e94\u7528\u3002\n- **\u5362\u66fc\u7684\u7231\u60c5\u8bed\u4e49\u5b66\u5728\u7535\u89c6\u548c\u7535\u5f71\u4e2d\u7684\u5e94\u7528**\uff1a\u63d0\u8bae\u7814\u7a76\u5362\u66fc\u7684\u7231\u60c5\u8bed\u4e49\u5b66\u5728\u7535\u89c6\u548c\u7535\u5f71\u4e2d\u7684\u5e94\u7528\uff0c\u63a2\u8ba8\u5176\u5728\u5a92\u4f53\u4e2d\u7684\u8bed\u4e49\u4fe1\u606f\u3002\n- **\u5362\u66fc\u7406\u8bba\u5728\u793e\u4f1a\u5b66\u7814\u7a76\u4e2d\u7684\u6f5c\u5728\u5e94\u7528**\uff1a\u63a8\u8350\u6df1\u5165\u7814\u7a76\u5362\u66fc\u7684\u7406\u8bba\uff0c\u4e86\u89e3\u5176\u4e0e\u793e\u4f1a\u5b66\u7814\u7a76\u4e2d\u7684\u60c5\u611f\u56e0\u7d20\u7684\u5bc6\u5207\u5173\u7cfb\u3002\n- **\u5362\u66fc\u7406\u8bba\u4e0e\u793e\u4f1a\u5b66\u7814\u7a76\u7684\u9760\u8fd1\u5ea6**\uff1a\u9700\u8981\u8003\u8651\u5362\u66fc\u7406\u8bba\u548c\u793e\u4f1a\u5b66\u7814\u7a76\u7684\u9760\u8fd1\u5ea6\uff0c\u63a2\u8ba8\u5176\u5982\u4f55\u63a5\u8fd1\u793e\u4ea4\u5a92\u4f53\u4e2d\u7684\u7231\u60c5\u8868\u73b0\u3002\n- **\u5362\u66fc\u7406\u8bba\u548c\u793e\u4f1a\u5b66\u7684\u4ea4\u53c9\u70b9**\uff1a\u53ef\u4ee5\u63a2\u8ba8\u5362\u66fc\u7406\u8bba\u548c\u793e\u4f1a\u5b66\u7684\u4ea4\u53c9\u70b9\uff0c\u4e86\u89e3\u5176\u5728\u793e\u4ea4\u5a92\u4f53\u4e2d\u7684\u7231\u60c5\u8868\u73b0\u4e0e\u5362\u66fc\u7406\u8bba\u7684\u91cd\u53e0\u3002\n\n#### Step 4: \u603b\u7ed3\u7814\u7a76\u8981\u70b9\n1. **\u793e\u4ea4\u5a92\u4f53\u4e2d\u7684\u7231\u60c5\u8868\u73b0**\uff1a\u7814\u7a76\u793e\u4ea4\u5a92\u4f53\u4e2d\u7684\u7231\u60c5\u8868\u73b0\uff0c\u63a2\u8ba8\u5362\u66fc\u7684\u7231\u60c5\u8bed\u4e49\u5b66\u5728\u793e\u4ea4\u5a92\u4f53\u4e2d\u7684\u5e94\u7528\u3002\n2. **\u5362\u66fc\u7684\u7231\u60c5\u8bed\u4e49\u5b66\u5728\u7535\u89c6\u548c\u7535\u5f71\u4e2d\u7684\u5e94\u7528**\uff1a\u7814\u7a76\u5362\u66fc\u7684\u7231\u60c5\u8bed\u4e49\u5b66\u5728\u7535\u89c6\u548c\u7535\u5f71\u4e2d\u7684\u5e94\u7528\uff0c\u63a2\u8ba8\u5176\u5728\u5a92\u4f53\u4e2d\u7684\u8bed\u4e49\u4fe1\u606f\u3002\n3. **\u5362\u66fc\u7406\u8bba\u5728\u793e\u4f1a\u5b66\u7814\u7a76\u4e2d\u7684\u6f5c\u5728\u5e94\u7528**\uff1a\u6df1\u5165\u7814\u7a76\u5362\u66fc\u7684\u7406\u8bba\uff0c\u4e86\u89e3\u5176\u4e0e\u793e\u4f1a\u5b66\u7814\u7a76\u4e2d\u7684\u60c5\u611f\u56e0\u7d20\u7684\u5bc6\u5207\u5173\u7cfb\u3002\n4. **\u5362\u66fc\u7406\u8bba\u4e0e\u793e\u4f1a\u5b66\u7814\u7a76\u7684\u9760\u8fd1\u5ea6**\uff1a\u8003\u8651\u5362\u66fc\u7406\u8bba\u548c\u793e\u4f1a\u5b66\u7814\u7a76\u7684\u9760\u8fd1\u5ea6\uff0c\u63a2\u8ba8\u5176\u5982\u4f55\u63a5\u8fd1\u793e\u4ea4\u5a92\u4f53\u4e2d\u7684\u7231\u60c5\u8868\u73b0\u3002\n5. **\u5362\u66fc\u7406\u8bba\u548c\u793e\u4f1a\u5b66\u7684\u4ea4\u53c9\u70b9**\uff1a\u63a2\u8ba8\u5362\u66fc\u7406\u8bba\u548c\u793e\u4f1a\u5b66\u7684\u4ea4\u53c9\u70b9\uff0c\u4e86\u89e3\u5176\u5728\u793e\u4ea4\u5a92\u4f53\u4e2d\u7684\u7231\u60c5\u8868\u73b0\u4e0e\u5362\u66fc\u7406\u8bba\u7684\u91cd\u53e0\u3002\n\n#### Step 5: \u6700\u7ec8\u7b54\u6848\n\u6839\u636e\u4e0a\u8ff0\u5206\u6790\uff0c\u7814\u7a76\u5e94\u91cd\u70b9\u5173\u6ce8\u4ee5\u4e0b\u51e0\u4e2a\u65b9\u9762\uff1a\n1. **\u793e\u4ea4\u5a92\u4f53\u4e2d\u7684\u7231\u60c5\u8868\u73b0**\uff1a\u63a2\u8ba8\u5362\u66fc\u7684\u7231\u60c5\u8bed\u4e49\u5b66\u5728\u793e\u4ea4\u5a92\u4f53\u4e2d\u7684\u5e94\u7528\u3002\n2. **\u5362\u66fc\u7684\u7231\u60c5\u8bed\u4e49\u5b66\u5728\u7535\u89c6\u548c\u7535\u5f71\u4e2d\u7684\u5e94\u7528**\uff1a\u7814\u7a76\u5176\u5728\u5a92\u4f53\u4e2d\u7684\u8bed\u4e49\u4fe1\u606f\u3002\n3. **\u5362\u66fc\u7406\u8bba\u5728\u793e\u4f1a\u5b66\u7814\u7a76\u4e2d\u7684\u6f5c\u5728\u5e94\u7528**\uff1a\u6df1\u5165\u4e86\u89e3\u5176\u4e0e\u793e\u4f1a\u5b66\u7814\u7a76\u4e2d\u7684\u60c5\u611f\u56e0\u7d20\u7684\u5bc6\u5207\u5173\u7cfb\u3002\n4. **\u5362\u66fc\u7406\u8bba\u4e0e\u793e\u4f1a\u5b66\u7814\u7a76\u7684\u9760\u8fd1\u5ea6**\uff1a\u63a2\u8ba8\u5176\u5982\u4f55\u63a5\u8fd1\u793e\u4ea4\u5a92\u4f53\u4e2d\u7684\u7231\u60c5\u8868\u73b0\u3002\n5. **\u5362\u66fc\u7406\u8bba\u548c\u793e\u4f1a\u5b66\u7684\u4ea4\u53c9\u70b9**\uff1a\u4e86\u89e3\u5176\u5728\u793e\u4ea4\u5a92\u4f53\u4e2d\u7684\u7231\u60c5\u8868\u73b0\u4e0e\u5362\u66fc\u7406\u8bba\u7684\u91cd\u53e0\u3002\n\n\u901a\u8fc7\u8fd9\u4e9b\u7814\u7a76\uff0c\u53ef\u4ee5\u66f4\u5168\u9762\u5730\u7406\u89e3\u5362\u66fc\u7684\u7231\u60c5\u8bed\u4e49\u5b66\u5728\u793e\u4f1a\u5b66\u4e2d\u7684\u5e94\u7528\u53ca\u5176\u5728\u5a92\u4f53\u4e2d\u7684\u8868\u73b0\u3002", "scene_monologue_context": "**\u72ec\u767d\u4fe1\u606f\uff1a**\n\n\u4eca\u5929\uff0c\u6211\u6df1\u5165\u7814\u7a76\u4e86\u5728\u5927\u6a21\u578b\u4e2dLayerNorm\u548cRMSNorm\u8fd9\u4e24\u79cd\u5f52\u4e00\u5316\u65b9\u6cd5\u7684\u6280\u672f\u8fdb\u6b65\u3002\u7279\u522b\u662f\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u548c\u590d\u6742\u4efb\u52a1\u4e2d\uff0c\u8fd9\u4e24\u79cd\u65b9\u6cd5\u5982\u4f55\u5f71\u54cd\u6a21\u578b\u7684\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002\u6211\u67e5\u9605\u4e86\u76f8\u5173\u7684\u6587\u732e\u548c\u8d44\u6599\uff0c\u53d1\u73b0LayerNorm\u548cRMSNorm\u5728\u63d0\u5347\u6a21\u578b\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u6709\u7740\u663e\u8457\u7684\u4f5c\u7528\u3002LayerNorm\u901a\u8fc7\u5bf9\u6bcf\u4e00\u5c42\u7684\u8f93\u51fa\u8fdb\u884c\u5f52\u4e00\u5316\uff0c\u51cf\u5c11\u4e86\u5185\u90e8\u534f\u53d8\u91cf\u504f\u79fb\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002\u800cRMSNorm\u5219\u901a\u8fc7\u8ba1\u7b97\u5747\u65b9\u6839\u6765\u8fdb\u884c\u5f52\u4e00\u5316\uff0c\u8fdb\u4e00\u6b65\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u7279\u522b\u9002\u5408\u5927\u89c4\u6a21\u6570\u636e\u96c6\u548c\u590d\u6742\u4efb\u52a1\u3002\n\n\u6b64\u5916\uff0c\u6211\u8fd8\u4e86\u89e3\u5230\uff0c\u5728\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\uff0c\u6a21\u578b\u538b\u7f29\u548c\u9ad8\u6548\u63a8\u7406\u662f\u5f53\u524d\u7814\u7a76\u7684\u70ed\u70b9\u3002\u6a21\u578b\u538b\u7f29\u6280\u672f\u5982\u526a\u679d\u3001\u91cf\u5316\u548c\u77e5\u8bc6\u84b8\u998f\u7b49\uff0c\u53ef\u4ee5\u6709\u6548\u51cf\u5c11\u6a21\u578b\u7684\u8ba1\u7b97\u8d44\u6e90\u548c\u5185\u5b58\u5360\u7528\uff0c\u4ece\u800c\u63d0\u9ad8\u63a8\u7406\u6548\u7387\u3002\u800c\u6df7\u5408\u63a8\u7406\u548c\u591a\u6a21\u578b\u63a8\u7406\u5219\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u6a21\u578b\u5927\u5c0f\u548c\u8def\u7531\u7b56\u7565\uff0c\u8fdb\u4e00\u6b65\u4f18\u5316\u4e86\u63a8\u7406\u6210\u672c\u548c\u54cd\u5e94\u8d28\u91cf\u3002\n\n\u5728\u7814\u7a76\u8fc7\u7a0b\u4e2d\uff0c\u6211\u8fd8\u53d1\u73b0\u4e86\u4e00\u4e9b\u65b0\u7684\u7814\u7a76\u65b9\u5411\uff0c\u5982\u7ed3\u5408\u6df7\u5408\u63a8\u7406\u548c\u63a8\u6d4b\u89e3\u7801\uff0c\u4ee5\u8fdb\u4e00\u6b65\u964d\u4f4e\u63a8\u7406\u6210\u672c\u3002\u8fd9\u4e9b\u6280\u672f\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u6027\u80fd\uff0c\u8fd8\u51cf\u5c11\u4e86\u8ba1\u7b97\u8d44\u6e90\u7684\u6d88\u8017\uff0c\u5bf9\u4e8e\u63a8\u52a8\u5927\u6a21\u578b\u7684\u5b9e\u9645\u5e94\u7528\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002\n\n\u603b\u7684\u6765\u8bf4\uff0c\u4eca\u5929\u7684\u63a2\u7d22\u8ba9\u6211\u5bf9LayerNorm\u548cRMSNorm\u7684\u6280\u672f\u8fdb\u6b65\u6709\u4e86\u66f4\u6df1\u5165\u7684\u7406\u89e3\uff0c\u4e5f\u8ba9\u6211\u770b\u5230\u4e86\u5927\u6a21\u578b\u5728\u9ad8\u6548\u63a8\u7406\u548c\u6a21\u578b\u538b\u7f29\u65b9\u9762\u7684\u5e7f\u9614\u524d\u666f\u3002\u672a\u6765\uff0c\u6211\u5e0c\u671b\u80fd\u7ee7\u7eed\u6df1\u5165\u7814\u7a76\u8fd9\u4e9b\u6280\u672f\uff0c\u63a2\u7d22\u66f4\u591a\u4f18\u5316\u6a21\u578b\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u7684\u65b9\u6cd5\u3002", "user_id": "\u6b64\u6765\u8bbf\u8005", "scene_content": "\u89d2\u8272    \u5185\u5bb9    \u5206\u955c\n", "story_board_summary_context": "51c8b9ac-a041-4af7-b164-9d6e7eb750af:\u300c\u6280\u672f\u8fdb\u6b65\u300d\n51c8b9ac-a041-4af7-b164-9d6e7eb750af:\u300c### \u95ee\u9898\n\n\u5728\u5927\u6a21\u578b\u4e2d\uff0cLayerNorm\u548cRMSNorm\u4f5c\u4e3a\u4e24\u79cd\u5e38\u7528\u7684\u5f52\u4e00\u5316\u65b9\u6cd5\uff0c\u5b83\u4eec\u5728\u63d0\u5347\u6a21\u578b\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u6709\u54ea\u4e9b\u5177\u4f53\u7684\u6280\u672f\u8fdb\u6b65\uff1f\u7279\u522b\u662f\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u548c\u590d\u6742\u4efb\u52a1\u4e2d\uff0c\u8fd9\u4e24\u79cd\u65b9\u6cd5\u5982\u4f55\u5f71\u54cd\u6a21\u578b\u7684\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff1f\u300d\n51c8b9ac-a041-4af7-b164-9d6e7eb750af:\u300cref_ids: 454847042436311108, chunk_ids: 4, Score: 0.3359, Text: # 2 Problem Formulation\n\n# 2.1 Related Work\nLarge Language Models (LLMs). The advent of LLMs has led to a paradigm shift in the study of natural language processing (NLP), computer vision, information retrieval, and other domains[Menghani, 2023, Chen et al., 2023, Jiang et al., 2023]. The impressive effectiveness and generalizability of LLMs has come at the price of a drastic increase in LLM sizes [Treviso et al., 2023] and consequent challenges, including huge amounts of computational resources and data required to train, and prohibitive expenses at both training and deployment stages [Bender et al., 2021].  \n\nEfficient Machine Learning (ML) Inference. LLMs belong to a class of models called foundation models [Bommasani et al., 2021] \u2013 models that are trained once and can then be used to serve a wide variety of tasks. As such, we expect inference cost to dominate the overall cost of such models and hence focus on works that reduce the cost of ML inference [Menghani, 2023]. The most common approach for efficient ML inference is model compression i.e., replacing a large model with a smaller model of comparable accuracy. Common techniques for model compression include (i) model pruning [Hassibi et al., 1993, LeCun et al., 1989] which drops parts of the model with minimal accuracy loss, (ii) quantization [Jacob et al., 2018, Vanhoucke et al., 2011] which reduces model memory footprints and inference latency by reducing the precision of data representation (e.g., FP32 to INT8), (iii) knowledge distillation [Hinton et al., 2015, Urban et al., 2016] which trains small student models to mimic large teacher models, and (iv) Neural Architecture Search [Elsken et al., 2019, Zoph and Le, 2016] which tunes model architecture to improve model performance, under inference cost constraints. Such static efficiency optimizations typically produce a fixed model with lower inference cost and lower accuracy compared to the large model which may not suffice for foundation models like LLMs, whose core premise is that the same model will serve a range of tasks, each with its own accuracy/cost constraints. This is already manifesting in inference platforms described in Section 1 which need more dynamic optimizations to meet the demands of all users.  \n\nHybrid ML Inference. Recent works [Kag et al., 2022, Ding et al., 2022] have introduced a new inference paradigm called hybrid inference which uses two models of different sizes instead of a single model for inference. The smaller model (e.g. Llama2 [Touvron et al., 2023]) generally has lower inference cost but also lower accuracy than the larger model (e.g. GPT-4 [OpenAI, 2023]). The key idea is to identify and route easy queries to the small model so that inference cost can be reduced while maintaining response quality. By tuning a threshold on query difficulty we can dynamically trade off quality and cost for the same inference setup. [Kag et al., 2022] study this setup for image classification and propose to train the small model, large model, and router from scratch. However LLM training is expensive and retraining LLMs from scratch for every scenario goes against the very premise of inference with pre-trained foundation models. Moreover text generation [Iqbal and Qureshi, 2022] is often more ambiguous and challenging than image classification due to which novel techniques are required for effective hybrid LLM inference for text generation.  \n\nInference with Multiple LLMs. Some recent works [Jiang et al., 2023, Chen et al., 2023, Leviathan et al., 2023, Kim et al., 2023] use multiple LLMs for inference but these approaches typically call more than one LLM for a single query that can incur significant computational overheads. Specifically [Jiang et al., 2023] calls an ensemble of LLMs at inference time due to which the inference cost will be proportional to the number of models in the system. [Chen et al., 2023] performs inference using a cascade of LLMs where responses to the query are generated sequentially by the LLMs in the cascade until one of the models has a confidence score higher than a predefined threshold. Our work provides high quality responses while always making a single LLM call for all queries and will thus incur much lower computational cost than both of these works on average. Speculative decoding, introduced in [Leviathan et al., 2023, Kim et al., 2023] speeds up decoding of expensive models by invoking small-and-efficient decoders on the \u201ceasy\u201d decoding steps. Instead, in our work we are interested in query routing which assigns \u201ceasy\u201d queries to small models to reduce overall inference costs while maintaining high performance. While the two approaches have different goals, an interesting line of future work would be to combine these so that our router assigns queries to the small or large model based on query difficulty and then speculative decoding is applied on top to speed up inference for queries assigned to the large model thereby leading to further cost reduction.\u300d\n51c8b9ac-a041-4af7-b164-9d6e7eb750af:\u300cref_ids: 454845744505973136, chunk_ids: 6, Score: 0.3340, Text: # 6 Conclusion\nLLM-based RL algorithms have shown generalization across multiple tasks and games. We argue that this ability comes from implicit memory that fits a large number of parameters to the training data, which is inefficient in terms of model size. In contrast, we propose a new approach inspired by the concept of \u201cworking memory\u201d called Decision Transformers with Memory (DT-Mem), which stores training experience explicitly in a content-addressable matrix module for later retrieval and use. Evaluation demonstrates that DT-Mem achieves better generalization on Atari games with only $10\\\\%$ of the model parameters compared to the state-of-the-art method. Furthermore, we demonstrate that fine-tuning DT-Memwith a small amount of data can produce state-of-the-art results on both Atari games and the Meta-World environment, when compared to MDT [22], PDT [37], and HDT [38].  \n\nLimitations The first limitation of our work is the sample efficiency of memory fine-tuning. The $10\\\\%$ fine-tuning dataset is still sizeable, and we plan to explore more sample-efficient methods in the future. We could for instance consider a setting with more tasks, each one with less data so that the inter-task generalization would be even more crucial to its performance. Additionally, this work does not propose a control strategy for collecting data on a new task. For future work, we plan to investigate online data collection methods, which includes the design and learning of exploration strategies for an efficient fine-tuning on new tasks. Finally, the approach has been intuitively motivated, but it would be valuable to have a theoretical grounding that would show the structural limits of large models and how equipping them with a memory component overcomes them.  \n\nSocietal Impact We do not foresee any significant societal impact resulting from our proposed method. The current algorithm is not designed to interact with humans, nor any realistic environment yet. If one chooses to extend our methods to such situations, caution should be exercised to ensure that any safety and ethical concerns are appropriately addressed. As our work is categorized in the offline-RL domain, it is feasible to supplement its training with a dataset that aligns with human intents and values. However, one must be wary that the way our architecture generalizes across tasks is still not well understood and as a consequence we cannot guarantee the generalization of its desirable features: performance, robustness, fairness, etc. By working towards methods that improve the computational efficiency of large models, we contribute to increase their access and reduce their ecological impact.\n\n\n\n# A Implementation Details\n\n# A.1 DT-Mem network architecture\nTable 3 summarizes the different model configurations used for evaluation. In this section, we describe these model configurations in detail. While Table 3 provides a summary, we will also provide additional information here. DT-Mem, PDT and HDT are all share the same transformer architectures. However, for task-adaptation, HDT utilizes a pre-trained $2.3\\\\mathbf{M}$ hyper-network, while DT-Mem introduces 147K LoRA parameters. To compare with MDT, we use the same parameter size as reported in [22].  \n\nTable 3: Detailed Model Sizes   \n\n\n<html><body><table><tr><td>Model</td><td>Layers</td><td>Hidden size (d)</td><td>Heads</td><td>Params</td><td>Memory Size</td><td>Memory Module Params</td></tr><tr><td>HDT</td><td>4</td><td>512</td><td>8</td><td>13M</td><td>N.A.</td><td>N.A.</td></tr><tr><td>MDT-200M</td><td>10</td><td>1280</td><td>20</td><td>200M</td><td>N.A.</td><td>N.A.</td></tr><tr><td>DT-Mem</td><td>4</td><td>512</td><td>8</td><td>13M</td><td>559K</td><td>7M</td></tr></table></body></html>\n\n# A.2 Hyper-parameters\nIn this section, we will delve into the specifics of the model parameters. Understanding these parameters is key to understanding the workings of the model. It is worth noting that the source code for this model is publicly available at https://github.com/luciferkonn/DT_Mem/tree/main .This allows for a deeper understanding of the model\u2019s inner workings and may facilitate the replication of its results.  \n\nTable 4: Hyperparameters for DT-Mem training   \n\n\n<html><body><table><tr><td>Hyperparameters</td><td>Value</td></tr><tr><td>K (length of context)</td><td>28</td></tr><tr><td>dropoutrate</td><td>0.1</td></tr><tr><td>maximum epochs</td><td>1000</td></tr><tr><td>steps for each epoch</td><td>1000</td></tr><tr><td>optimizer learning rate</td><td>1e-4</td></tr><tr><td>weight decay</td><td>1e-4</td></tr><tr><td>gradient norm clip</td><td>1.</td></tr><tr><td>data points for each dataset</td><td>500,000</td></tr><tr><td>batch size</td><td>64</td></tr><tr><td>memory slots</td><td>1290</td></tr><tr><td>activation</td><td>GELU</td></tr><tr><td>optimizer</td><td>Adamw</td></tr><tr><td>scheduler</td><td>LambdaLR</td></tr></table></body></html>\n\n# A.3 Training and fine-tuning algorithm\nIn this section, we present the pre-training DT-Memin Appendix A.3 and fine-tuning DT-Mem with LoRA in Appendix 5.5.  \n\nWe pre-train DT-Mem on multiple offline datasets. Each gradient update of the DT-Memmodel considers information from each training task.  \n\nWe fine-tune the memory module to adapt to each downstream task. To achieve this, we fix the pre-trained DT-Mem model parameters and add additional LoRA parameters for the memory module feed-forward neural networks. The fine-tune dataset is used to update these LoRA parameters only.\n\n# Algorithm 1 Pre-train DT-Mem\n1: for T episodes do   \n2: 3: 4: for Sample trajectories Split trajectories into different segments with length K and calculate return-to-go in the Task $\\\\mathcal{T}_{i}\\\\in T^{t r a i n}\\\\;.$ do $\\\\tau=(s_{0},a_{0},r_{0},\\\\cdot\\\\cdot\\\\cdot\\\\,,s_{H},a_{H},r_{H})$ m the dataset $\\\\mathcal{D}_{i}$ .  \ninput sequence.   \n5: Given $\\\\hat{\\\\tau}_{t+1:t+K}$ , compute the sequence embedding $e_{s e q}$ .  \n6: Update the working memory and retrieve the relative information as $E_{o u t}$   \n7: Given $E_{o u t}$ , predict actions $\\\\tilde{a}_{t}$ , reward $\\\\tilde{r}_{t}$ , and return-to-go ${\\\\tilde{R}}_{t}$ .  \n8: Compute the loss according to Eqn. 1.   \n9: Update all modules parameters.   \n10: end for   \n11: end for  \n\nAlgorithm 2 Fine-tuning DT-Mem  \n\n$\\\\hat{B}^{q},\\\\hat{B}^{k},\\\\hat{B}^{v},\\\\hat{A}^{q},\\\\hat{A}^{k},\\\\hat{A}^{v},B^{q},A^{q},B^{k},A^{k}$ Require: Fine-tuning dataset $\\\\mathcal{T}^{i}~\\\\in~T^{t e s t}$ .dataset $\\\\mathcal{D}^{i}$ for $\\\\mathcal{T}^{i}$ .Initialize LoRA parameters 1: for T steps do   \n2: Split trajectories into different segments with length $\\\\mathbf{K}$ and calculate return-to-go in the input sequence.   \n3: Given $\\\\hat{\\\\tau}_{t+1:t+K}$ , compute the sequence embedding $e_{s e q}$ .  \n4: Update working memory using $\\\\hat{Q}\\\\,=\\\\,M(\\\\hat{W}^{q}+\\\\hat{B}^{q}\\\\bar{A}^{q})$ ,$\\\\hat{K}\\\\,=\\\\,M(\\\\hat{W}^{k}\\\\,+\\\\,\\\\hat{B}^{k}\\\\hat{A}^{k}),\\\\hat{V}\\\\,=$ $M(\\\\hat{W}^{v}+\\\\hat{B}^{v}\\\\hat{A}^{v})$ ,$Q=M(W^{q}+B^{q}A^{q}),K=M(W^{k}+B^{k}A^{k})$   \n5: Retrieve the relative information as $E_{o u t}$   \n6: Given $E_{o u t}$ , predict actions $\\\\tilde{a}_{t}$ , reward $\\\\tilde{r}_{t}$ , and return-to-go ${\\\\tilde{R}}_{t}$ .  \n7: Compute the loss according to Eqn. 1.   \n8: Update LoRA parameters only.   \n9: end for\u300d\n51c8b9ac-a041-4af7-b164-9d6e7eb750af:\u300cref_ids: 455038427524247598, chunk_ids: 4, Score: 0.2129, Text: # 8 Discussion\nThe worst group performance of a model is affected by two factors: the quality of the representation of the core features produced by the feature extractor and the weight assigned to the core features in the last classification layer. In contrast to prior work, we consider the quality of the feature extractor in isolation, focusing on realistic datasets and large-scale models. We find that many of the popular group robustness methods improve the worst group performance primarily by learning a better last layer and not by learning a better feature representation. Similarly, regularization techniques such as early stopping and strong weight decay can improve the worst group accuracy by learning a better last layer, but do not lead to a consistent improvement in terms of the quality of the learned feature representations. On the other hand, the base model architecture and pre-training strategy have a major effect on the quality of the feature representations.  \n\nOur observations suggest an important open question: is it possible to significantly improve upon standard ERM in terms of the quality of the learned representations for a given base model? In future work, it would be interesting to evaluate methods such as Rich Feature Construction [ 99 ], gradient starvation [ 68 ], ensembling [ 47 ] and other techniques for increasing feature diversity [e.g. 48 ]. We hope that our work will also inspire new group robustness methods targeted specifically at improving the quality of the core feature representations.\n\n\n\n# Appendix Outline\nThis appendix is structured as follows. In Section A we describe the datasets, augmentation policies and models used in this paper. In Section Bwe provide details on the methods, implementations and hyper-parameters, as well as detailed results for the experiments in Section 5 . In Section Cwe provide additional details on the experiments in Section 6 . In Section Dwe provide additional details and results for the experiments in Section 7 . In Section Ewe provide additional results on the MultiNLI dataset. Finally, in Section Fwe describe the limitation, broader impact, compute and licenses.  \n\nTools and packages. During the work on this paper, we used the following tools and packages: NumPy[24],SciPy[87],PyTorch[67],TorchVision[55],Jupyter notebooks[42],Matplotlib[33],Pandas [57 ], Weights&Biases [7 ], timm [90 ], transformers [92 ], vissl [21 ].\n\n# A Data and Models\nIn this section, we describe the datasets, data augmentation policies and models used throughout the paper.\u300d\n", "dreams_guidance_context": "### Step 1: \u786e\u5b9a\u7814\u7a76\u7684\u5173\u952e\u95ee\u9898\u548c\u76ee\u6807\n\u9996\u5148\uff0c\u6211\u4eec\u9700\u8981\u660e\u786e\u7814\u7a76\u7684\u6838\u5fc3\u95ee\u9898\u548c\u76ee\u6807\u3002\u6839\u636e\u7814\u7a76\u4e3b\u9898\u6897\u6982\uff0c\u6838\u5fc3\u95ee\u9898\u5728\u4e8e\uff1a\n- **LayerNorm\u548cRMSNorm\u5728\u5927\u6a21\u578b\u4e2d\u7684\u6280\u672f\u8fdb\u6b65**\uff1a\u8fd9\u4e24\u79cd\u5f52\u4e00\u5316\u65b9\u6cd5\u5982\u4f55\u63d0\u5347\u6a21\u578b\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\uff1f\u7279\u522b\u662f\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u548c\u590d\u6742\u4efb\u52a1\u4e2d\uff0c\u5b83\u4eec\u5982\u4f55\u5f71\u54cd\u6a21\u578b\u7684\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff1f\n\n\u76ee\u6807\u662f\u901a\u8fc7\u5206\u6790\u6587\u732e\uff0c\u63d0\u70bc\u51fa\u8fd9\u4e24\u79cd\u65b9\u6cd5\u7684\u6280\u672f\u8fdb\u6b65\u3001\u5e94\u7528\u573a\u666f\u53ca\u5176\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5177\u4f53\u5f71\u54cd\u3002\n\n### Step 2: \u68b3\u7406\u7b97\u6cd5\u548c\u65b9\u6cd5\n\u63a5\u4e0b\u6765\uff0c\u6211\u4eec\u9700\u8981\u5206\u6790\u6587\u732e\u4e2d\u4f7f\u7528\u7684\u7b97\u6cd5\u548c\u65b9\u6cd5\u3002\u5177\u4f53\u95ee\u9898\u5305\u62ec\uff1a\n- **LayerNorm\u548cRMSNorm\u7684\u5177\u4f53\u5b9e\u73b0**\uff1a\u8fd9\u4e24\u79cd\u5f52\u4e00\u5316\u65b9\u6cd5\u7684\u5177\u4f53\u5b9e\u73b0\u7ec6\u8282\u662f\u4ec0\u4e48\uff1f\u5b83\u4eec\u4e0e\u4f20\u7edf\u7684\u5f52\u4e00\u5316\u65b9\u6cd5\uff08\u5982BatchNorm\uff09\u6709\u4f55\u4e0d\u540c\uff1f\n- **\u6a21\u578b\u538b\u7f29\u548c\u9ad8\u6548\u63a8\u7406\u6280\u672f**\uff1a\u6587\u732e\u4e2d\u63d0\u5230\u7684\u6a21\u578b\u538b\u7f29\u6280\u672f\uff08\u5982\u526a\u679d\u3001\u91cf\u5316\u3001\u77e5\u8bc6\u84b8\u998f\uff09\u548c\u9ad8\u6548\u63a8\u7406\u6280\u672f\uff08\u5982\u6df7\u5408\u63a8\u7406\u3001\u591a\u6a21\u578b\u63a8\u7406\uff09\u662f\u5982\u4f55\u4e0eLayerNorm\u548cRMSNorm\u7ed3\u5408\u7684\uff1f\n- **\u521b\u65b0\u70b9**\uff1a\u662f\u5426\u6709\u65b0\u7684\u4f18\u5316\u6280\u5de7\u6216\u7b97\u6cd5\u521b\u65b0\u88ab\u5f15\u5165\uff1f\u4f8b\u5982\uff0c\u7ed3\u5408\u6df7\u5408\u63a8\u7406\u548c\u63a8\u6d4b\u89e3\u7801\u7684\u6280\u672f\u3002\n\n### Step 3: \u5206\u6790\u5b9e\u9a8c\u8bbe\u8ba1\u548c\u7ed3\u679c\n\u6df1\u5165\u63a2\u8ba8\u5b9e\u9a8c\u90e8\u5206\uff0c\u6211\u4eec\u9700\u8981\u56de\u7b54\u4ee5\u4e0b\u95ee\u9898\uff1a\n- **\u5b9e\u9a8c\u8bbe\u8ba1**\uff1a\u6587\u732e\u4e2d\u7684\u5b9e\u9a8c\u8bbe\u8ba1\u5982\u4f55\u652f\u6301\u8bba\u6587\u4e2d\u7684\u5047\u8bbe\uff1f\u662f\u5426\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u548c\u590d\u6742\u4efb\u52a1\u4e2d\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff1f\n- **\u5b9e\u9a8c\u7ed3\u679c**\uff1a\u5b9e\u9a8c\u7ed3\u679c\u662f\u5426\u8868\u660eLayerNorm\u548cRMSNorm\u5728\u63d0\u5347\u6a21\u578b\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u6709\u663e\u8457\u6548\u679c\uff1f\u662f\u5426\u5bf9\u5b9e\u9a8c\u7ed3\u679c\u8fdb\u884c\u4e86\u5145\u5206\u9a8c\u8bc1\uff1f\n- **\u5bf9\u6bd4\u5206\u6790**\uff1a\u4e0e\u4f20\u7edf\u7684\u5f52\u4e00\u5316\u65b9\u6cd5\u76f8\u6bd4\uff0cLayerNorm\u548cRMSNorm\u5728\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u6709\u4f55\u4f18\u52bf\uff1f\n\n### Step 4: \u8bc4\u4f30\u7814\u7a76\u7684\u5c40\u9650\u6027\u548c\u672a\u6765\u65b9\u5411\n\u6700\u540e\uff0c\u6211\u4eec\u9700\u8981\u5173\u6ce8\u6587\u732e\u4e2d\u7684\u5c40\u9650\u6027\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002\u5177\u4f53\u95ee\u9898\u5305\u62ec\uff1a\n- **\u5c40\u9650\u6027**\uff1a\u4f5c\u8005\u662f\u5426\u63d0\u5230\u76ee\u524d\u7814\u7a76\u7684\u5c40\u9650\u6027\uff1f\u4f8b\u5982\uff0cLayerNorm\u548cRMSNorm\u5728\u67d0\u4e9b\u7279\u5b9a\u4efb\u52a1\u6216\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u662f\u5426\u6709\u9650\uff1f\n- **\u672a\u6765\u65b9\u5411**\uff1a\u4f5c\u8005\u662f\u5426\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u5efa\u8bae\u6216\u62d3\u5c55\u65b9\u5411\uff1f\u4f8b\u5982\uff0c\u7ed3\u5408\u6df7\u5408\u63a8\u7406\u548c\u63a8\u6d4b\u89e3\u7801\u7684\u6280\u672f\u662f\u5426\u5177\u6709\u8fdb\u4e00\u6b65\u4f18\u5316\u7684\u6f5c\u529b\uff1f\n- **\u9886\u57df\u63a8\u52a8**\uff1a\u8fd9\u4e9b\u7814\u7a76\u65b9\u5411\u662f\u5426\u80fd\u63a8\u52a8\u5927\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\uff1f\u4f8b\u5982\uff0c\u5982\u4f55\u901a\u8fc7\u4f18\u5316\u5f52\u4e00\u5316\u65b9\u6cd5\u6765\u964d\u4f4e\u8ba1\u7b97\u8d44\u6e90\u7684\u6d88\u8017\uff1f\n\n### Step 5: \u603b\u7ed3\u63d0\u70bc\u76f8\u5173\u4fe1\u606f\n\u901a\u8fc7\u4e0a\u8ff0\u6b65\u9aa4\u7684\u5206\u6790\uff0c\u6211\u4eec\u53ef\u4ee5\u63d0\u70bc\u51fa\u4ee5\u4e0b\u5173\u952e\u4fe1\u606f\uff1a\n1. **LayerNorm\u548cRMSNorm\u7684\u6280\u672f\u8fdb\u6b65**\uff1a\u8fd9\u4e24\u79cd\u5f52\u4e00\u5316\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u548c\u590d\u6742\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002\n2. **\u6a21\u578b\u538b\u7f29\u548c\u9ad8\u6548\u63a8\u7406\u6280\u672f**\uff1a\u526a\u679d\u3001\u91cf\u5316\u3001\u77e5\u8bc6\u84b8\u998f\u7b49\u6a21\u578b\u538b\u7f29\u6280\u672f\uff0c\u4ee5\u53ca\u6df7\u5408\u63a8\u7406\u3001\u591a\u6a21\u578b\u63a8\u7406\u7b49\u9ad8\u6548\u63a8\u7406\u6280\u672f\uff0c\u4e0eLayerNorm\u548cRMSNorm\u7ed3\u5408\uff0c\u8fdb\u4e00\u6b65\u4f18\u5316\u4e86\u6a21\u578b\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u3002\n3. **\u5b9e\u9a8c\u9a8c\u8bc1**\uff1a\u6587\u732e\u4e2d\u7684\u5b9e\u9a8c\u8bbe\u8ba1\u5145\u5206\u652f\u6301\u4e86\u8bba\u6587\u4e2d\u7684\u5047\u8bbe\uff0c\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86LayerNorm\u548cRMSNorm\u5728\u63d0\u5347\u6a21\u578b\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u7684\u6709\u6548\u6027\u3002\n4. **\u5c40\u9650\u6027\u548c\u672a\u6765\u65b9\u5411**\uff1a\u5f53\u524d\u7814\u7a76\u5b58\u5728\u4e00\u5b9a\u7684\u5c40\u9650\u6027\uff0c\u5982\u5728\u67d0\u4e9b\u7279\u5b9a\u4efb\u52a1\u6216\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u6709\u9650\u3002\u672a\u6765\u7814\u7a76\u65b9\u5411\u5305\u62ec\u7ed3\u5408\u6df7\u5408\u63a8\u7406\u548c\u63a8\u6d4b\u89e3\u7801\uff0c\u4ee5\u8fdb\u4e00\u6b65\u964d\u4f4e\u63a8\u7406\u6210\u672c\u3002\n\n### Step 6: \u6700\u7ec8\u7b54\u6848\n\u901a\u8fc7\u7cfb\u7edf\u5730\u5206\u6790\u6587\u732e\uff0c\u6211\u4eec\u53ef\u4ee5\u5f97\u51fa\u4ee5\u4e0b\u7ed3\u8bba\uff1a\n- **LayerNorm\u548cRMSNorm\u5728\u5927\u6a21\u578b\u4e2d\u7684\u6280\u672f\u8fdb\u6b65**\uff1a\u8fd9\u4e24\u79cd\u5f52\u4e00\u5316\u65b9\u6cd5\u901a\u8fc7\u51cf\u5c11\u5185\u90e8\u534f\u53d8\u91cf\u504f\u79fb\u548c\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002\n- **\u6a21\u578b\u538b\u7f29\u548c\u9ad8\u6548\u63a8\u7406\u6280\u672f**\uff1a\u526a\u679d\u3001\u91cf\u5316\u3001\u77e5\u8bc6\u84b8\u998f\u7b49\u6a21\u578b\u538b\u7f29\u6280\u672f\uff0c\u4ee5\u53ca\u6df7\u5408\u63a8\u7406\u3001\u591a\u6a21\u578b\u63a8\u7406\u7b49\u9ad8\u6548\u63a8\u7406\u6280\u672f\uff0c\u4e0eLayerNorm\u548cRMSNorm\u7ed3\u5408\uff0c\u8fdb\u4e00\u6b65\u4f18\u5316\u4e86\u6a21\u578b\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u3002\n- **\u5b9e\u9a8c\u9a8c\u8bc1**\uff1a\u6587\u732e\u4e2d\u7684\u5b9e\u9a8c\u8bbe\u8ba1\u5145\u5206\u652f\u6301\u4e86\u8bba\u6587\u4e2d\u7684\u5047\u8bbe\uff0c\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86LayerNorm\u548cRMSNorm\u5728\u63d0\u5347\u6a21\u578b\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u7684\u6709\u6548\u6027\u3002\n- **\u5c40\u9650\u6027\u548c\u672a\u6765\u65b9\u5411**\uff1a\u5f53\u524d\u7814\u7a76\u5b58\u5728\u4e00\u5b9a\u7684\u5c40\u9650\u6027\uff0c\u672a\u6765\u7814\u7a76\u65b9\u5411\u5305\u62ec\u7ed3\u5408\u6df7\u5408\u63a8\u7406\u548c\u63a8\u6d4b\u89e3\u7801\uff0c\u4ee5\u8fdb\u4e00\u6b65\u964d\u4f4e\u63a8\u7406\u6210\u672c\u3002\n\n\u901a\u8fc7\u8fd9\u4e9b\u7814\u7a76\uff0c\u6211\u4eec\u53ef\u4ee5\u66f4\u5168\u9762\u5730\u7406\u89e3LayerNorm\u548cRMSNorm\u5728\u5927\u6a21\u578b\u4e2d\u7684\u5e94\u7528\u53ca\u5176\u5728\u9ad8\u6548\u63a8\u7406\u548c\u6a21\u578b\u538b\u7f29\u65b9\u9762\u7684\u5e7f\u9614\u524d\u666f\u3002", "evolutionary_step": "### Step by Step Decomposition\n\n#### 1. \u7814\u7a76\u8ba1\u7b97\u673a\u7f51\u7edc\u4e0e\u901a\u4fe1\u5a92\u4ecb\u4e2d\u7684\u8bed\u4e49\u4fe1\u606f\n\u5bf9\u8bdd\u4e2d\u5e76\u672a\u76f4\u63a5\u6d89\u53ca\u8ba1\u7b97\u673a\u7f51\u7edc\u4e0e\u901a\u4fe1\u5a92\u4ecb\u4e2d\u7684\u8bed\u4e49\u4fe1\u606f\u3002\u7136\u800c\uff0c\u5bf9\u8bdd\u4e2d\u63d0\u5230\u7684\u201c\u5927\u6a21\u578b\u201d\uff08\u5982LLMs\uff09\u548c\u201c\u63a8\u7406\u4f18\u5316\u201d\u6280\u672f\uff08\u5982\u6a21\u578b\u538b\u7f29\u3001\u91cf\u5316\u3001\u77e5\u8bc6\u84b8\u998f\u7b49\uff09\u53ef\u4ee5\u95f4\u63a5\u5f71\u54cd\u7f51\u7edc\u901a\u4fe1\u7684\u6548\u7387\u3002\u4f8b\u5982\uff0c\u6a21\u578b\u538b\u7f29\u548c\u91cf\u5316\u53ef\u4ee5\u51cf\u5c11\u6a21\u578b\u7684\u5927\u5c0f\u548c\u8ba1\u7b97\u9700\u6c42\uff0c\u4ece\u800c\u964d\u4f4e\u7f51\u7edc\u4f20\u8f93\u7684\u8d1f\u62c5\u3002\n\n#### 2. \u7814\u7a76\u8ba1\u7b97\u673a\u7b97\u6cd5\u7684\u975e\u7406\u6027\u8868\u73b0\u4e0e\u4f18\u5316\u6280\u672f\u4e2d\u7684\u5076\u7136\u6027\n\u5bf9\u8bdd\u4e2d\u63d0\u5230\u7684\u201cLayerNorm\u201d\u548c\u201cRMSNorm\u201d\u4f5c\u4e3a\u5f52\u4e00\u5316\u65b9\u6cd5\uff0c\u5176\u9009\u62e9\u548c\u5e94\u7528\u53ef\u80fd\u4f1a\u5f71\u54cd\u6a21\u578b\u7684\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002\u8fd9\u4e9b\u65b9\u6cd5\u5728\u4f18\u5316\u8fc7\u7a0b\u4e2d\u53ef\u80fd\u4f1a\u8868\u73b0\u51fa\u975e\u7406\u6027\u884c\u4e3a\uff0c\u4f8b\u5982\u5728\u67d0\u4e9b\u6570\u636e\u96c6\u6216\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u800c\u5728\u5176\u4ed6\u60c5\u51b5\u4e0b\u8868\u73b0\u4e0d\u4f73\u3002\u6b64\u5916\uff0c\u5bf9\u8bdd\u4e2d\u63d0\u5230\u7684\u201c\u6df7\u5408\u63a8\u7406\u201d\u548c\u201c\u591a\u6a21\u578b\u63a8\u7406\u201d\u6280\u672f\u4e5f\u6d89\u53ca\u5230\u7b97\u6cd5\u4f18\u5316\u4e2d\u7684\u5076\u7136\u6027\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u6280\u672f\u9700\u8981\u6839\u636e\u67e5\u8be2\u7684\u96be\u5ea6\u52a8\u6001\u8c03\u6574\u6a21\u578b\u7684\u8c03\u7528\uff0c\u8fd9\u53ef\u80fd\u4f1a\u5bfc\u81f4\u4e0d\u540c\u7684\u6027\u80fd\u8868\u73b0\u3002\n\n#### 3. \u7814\u7a76\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u4e2d\u7684\u53cd\u9988\u673a\u5236\u4e0e\u5b9e\u9645\u5e94\u7528\u7684\u8f6c\u5316\u5173\u7cfb\n\u5bf9\u8bdd\u4e2d\u63d0\u5230\u7684\u201c\u53cd\u9988\u673a\u5236\u201d\u4e3b\u8981\u4f53\u73b0\u5728\u6a21\u578b\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u8fc7\u7a0b\u4e2d\u3002\u4f8b\u5982\uff0c\u6a21\u578b\u538b\u7f29\u548c\u91cf\u5316\u6280\u672f\u901a\u8fc7\u51cf\u5c11\u6a21\u578b\u7684\u5927\u5c0f\u548c\u8ba1\u7b97\u9700\u6c42\u6765\u4f18\u5316\u63a8\u7406\u6548\u7387\uff0c\u8fd9\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u53ef\u4ee5\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002\u6b64\u5916\uff0c\u5bf9\u8bdd\u4e2d\u63d0\u5230\u7684\u201c\u6df7\u5408\u63a8\u7406\u201d\u6280\u672f\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u6a21\u578b\u7684\u8c03\u7528\uff0c\u53ef\u4ee5\u5728\u4fdd\u6301\u54cd\u5e94\u8d28\u91cf\u7684\u540c\u65f6\u964d\u4f4e\u63a8\u7406\u6210\u672c\uff0c\u8fd9\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5177\u6709\u91cd\u8981\u7684\u8f6c\u5316\u4ef7\u503c\u3002\n\n#### 4. \u7814\u7a76\u7b97\u6cd5\u548c\u6570\u636e\u7ed3\u6784\u7684\u56fa\u5b9a\u5f62\u5f0f\u4e0e\u9884\u671f\u7ed3\u679c\u7684\u56e0\u679c\u6027\uff0c\u662f\u5426\u5b58\u5728\u53ef\u62d3\u5c55\u6027\u548c\u53cd\u5411\u63a8\u7406\u7684\u6f5c\u529b\n\u5bf9\u8bdd\u4e2d\u63d0\u5230\u7684\u201cLayerNorm\u201d\u548c\u201cRMSNorm\u201d\u4f5c\u4e3a\u5f52\u4e00\u5316\u65b9\u6cd5\uff0c\u5176\u56fa\u5b9a\u5f62\u5f0f\u4e0e\u9884\u671f\u7ed3\u679c\u4e4b\u95f4\u5b58\u5728\u56e0\u679c\u6027\u3002\u4f8b\u5982\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5728\u63d0\u5347\u6a21\u578b\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u5177\u6709\u660e\u786e\u7684\u6548\u679c\u3002\u6b64\u5916\uff0c\u5bf9\u8bdd\u4e2d\u63d0\u5230\u7684\u201c\u6df7\u5408\u63a8\u7406\u201d\u548c\u201c\u591a\u6a21\u578b\u63a8\u7406\u201d\u6280\u672f\u5c55\u793a\u4e86\u7b97\u6cd5\u548c\u6570\u636e\u7ed3\u6784\u7684\u53ef\u62d3\u5c55\u6027\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u6280\u672f\u53ef\u4ee5\u6839\u636e\u4e0d\u540c\u7684\u4efb\u52a1\u548c\u67e5\u8be2\u96be\u5ea6\u52a8\u6001\u8c03\u6574\u6a21\u578b\u7684\u8c03\u7528\u3002\u53cd\u5411\u63a8\u7406\u7684\u6f5c\u529b\u4f53\u73b0\u5728\u8fd9\u4e9b\u6280\u672f\u53ef\u4ee5\u901a\u8fc7\u5206\u6790\u67e5\u8be2\u7684\u96be\u5ea6\u548c\u6a21\u578b\u7684\u6027\u80fd\uff0c\u8fdb\u4e00\u6b65\u4f18\u5316\u6a21\u578b\u7684\u8c03\u7528\u7b56\u7565\u3002\n\n### \u603b\u7ed3\n\u5bf9\u8bdd\u4e3b\u8981\u56f4\u7ed5\u5927\u6a21\u578b\u4e2d\u7684\u5f52\u4e00\u5316\u65b9\u6cd5\uff08LayerNorm\u548cRMSNorm\uff09\u4ee5\u53ca\u63a8\u7406\u4f18\u5316\u6280\u672f\uff08\u5982\u6a21\u578b\u538b\u7f29\u3001\u91cf\u5316\u3001\u6df7\u5408\u63a8\u7406\u7b49\uff09\u5c55\u5f00\u3002\u8fd9\u4e9b\u6280\u672f\u5728\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3001\u8ba1\u7b97\u6548\u7387\u548c\u5b9e\u9645\u5e94\u7528\u8f6c\u5316\u65b9\u9762\u5177\u6709\u91cd\u8981\u4f5c\u7528\u3002\u901a\u8fc7\u7814\u7a76\u8fd9\u4e9b\u6280\u672f\u7684\u56fa\u5b9a\u5f62\u5f0f\u4e0e\u9884\u671f\u7ed3\u679c\u7684\u56e0\u679c\u6027\uff0c\u53ef\u4ee5\u8fdb\u4e00\u6b65\u62d3\u5c55\u5176\u5e94\u7528\u6f5c\u529b\uff0c\u5e76\u901a\u8fc7\u53cd\u5411\u63a8\u7406\u4f18\u5316\u6a21\u578b\u7684\u8c03\u7528\u7b56\u7565\u3002", "dreams_personality_context": "### Step by Step Decomposition\n\n#### 1. \u7814\u7a76\u8ba1\u7b97\u673a\u7f51\u7edc\u4e0e\u901a\u4fe1\u5a92\u4ecb\u4e2d\u7684\u8bed\u4e49\u4fe1\u606f\n\u5bf9\u8bdd\u4e2d\u5e76\u672a\u76f4\u63a5\u6d89\u53ca\u8ba1\u7b97\u673a\u7f51\u7edc\u4e0e\u901a\u4fe1\u5a92\u4ecb\u4e2d\u7684\u8bed\u4e49\u4fe1\u606f\u3002\u7136\u800c\uff0c\u5bf9\u8bdd\u4e2d\u63d0\u5230\u7684\u201c\u5927\u6a21\u578b\u201d\uff08\u5982LLMs\uff09\u548c\u201c\u63a8\u7406\u4f18\u5316\u201d\u6280\u672f\uff08\u5982\u6a21\u578b\u538b\u7f29\u3001\u91cf\u5316\u3001\u77e5\u8bc6\u84b8\u998f\u7b49\uff09\u53ef\u4ee5\u95f4\u63a5\u5f71\u54cd\u7f51\u7edc\u901a\u4fe1\u7684\u6548\u7387\u3002\u4f8b\u5982\uff0c\u6a21\u578b\u538b\u7f29\u548c\u91cf\u5316\u53ef\u4ee5\u51cf\u5c11\u6a21\u578b\u7684\u5927\u5c0f\u548c\u8ba1\u7b97\u9700\u6c42\uff0c\u4ece\u800c\u964d\u4f4e\u7f51\u7edc\u4f20\u8f93\u7684\u8d1f\u62c5\u3002\n\n#### 2. \u7814\u7a76\u8ba1\u7b97\u673a\u7b97\u6cd5\u7684\u975e\u7406\u6027\u8868\u73b0\u4e0e\u4f18\u5316\u6280\u672f\u4e2d\u7684\u5076\u7136\u6027\n\u5bf9\u8bdd\u4e2d\u63d0\u5230\u7684\u201cLayerNorm\u201d\u548c\u201cRMSNorm\u201d\u4f5c\u4e3a\u5f52\u4e00\u5316\u65b9\u6cd5\uff0c\u5176\u9009\u62e9\u548c\u5e94\u7528\u53ef\u80fd\u4f1a\u5f71\u54cd\u6a21\u578b\u7684\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002\u8fd9\u4e9b\u65b9\u6cd5\u5728\u4f18\u5316\u8fc7\u7a0b\u4e2d\u53ef\u80fd\u4f1a\u8868\u73b0\u51fa\u975e\u7406\u6027\u884c\u4e3a\uff0c\u4f8b\u5982\u5728\u67d0\u4e9b\u6570\u636e\u96c6\u6216\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u800c\u5728\u5176\u4ed6\u60c5\u51b5\u4e0b\u8868\u73b0\u4e0d\u4f73\u3002\u6b64\u5916\uff0c\u5bf9\u8bdd\u4e2d\u63d0\u5230\u7684\u201c\u6df7\u5408\u63a8\u7406\u201d\u548c\u201c\u591a\u6a21\u578b\u63a8\u7406\u201d\u6280\u672f\u4e5f\u6d89\u53ca\u5230\u7b97\u6cd5\u4f18\u5316\u4e2d\u7684\u5076\u7136\u6027\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u6280\u672f\u9700\u8981\u6839\u636e\u67e5\u8be2\u7684\u96be\u5ea6\u52a8\u6001\u8c03\u6574\u6a21\u578b\u7684\u8c03\u7528\uff0c\u8fd9\u53ef\u80fd\u4f1a\u5bfc\u81f4\u4e0d\u540c\u7684\u6027\u80fd\u8868\u73b0\u3002\n\n#### 3. \u7814\u7a76\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u4e2d\u7684\u53cd\u9988\u673a\u5236\u4e0e\u5b9e\u9645\u5e94\u7528\u7684\u8f6c\u5316\u5173\u7cfb\n\u5bf9\u8bdd\u4e2d\u63d0\u5230\u7684\u201c\u53cd\u9988\u673a\u5236\u201d\u4e3b\u8981\u4f53\u73b0\u5728\u6a21\u578b\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u8fc7\u7a0b\u4e2d\u3002\u4f8b\u5982\uff0c\u6a21\u578b\u538b\u7f29\u548c\u91cf\u5316\u6280\u672f\u901a\u8fc7\u51cf\u5c11\u6a21\u578b\u7684\u5927\u5c0f\u548c\u8ba1\u7b97\u9700\u6c42\u6765\u4f18\u5316\u63a8\u7406\u6548\u7387\uff0c\u8fd9\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u53ef\u4ee5\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002\u6b64\u5916\uff0c\u5bf9\u8bdd\u4e2d\u63d0\u5230\u7684\u201c\u6df7\u5408\u63a8\u7406\u201d\u6280\u672f\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u6a21\u578b\u7684\u8c03\u7528\uff0c\u53ef\u4ee5\u5728\u4fdd\u6301\u54cd\u5e94\u8d28\u91cf\u7684\u540c\u65f6\u964d\u4f4e\u63a8\u7406\u6210\u672c\uff0c\u8fd9\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5177\u6709\u91cd\u8981\u7684\u8f6c\u5316\u4ef7\u503c\u3002\n\n#### 4. \u7814\u7a76\u7b97\u6cd5\u548c\u6570\u636e\u7ed3\u6784\u7684\u56fa\u5b9a\u5f62\u5f0f\u4e0e\u9884\u671f\u7ed3\u679c\u7684\u56e0\u679c\u6027\uff0c\u662f\u5426\u5b58\u5728\u53ef\u62d3\u5c55\u6027\u548c\u53cd\u5411\u63a8\u7406\u7684\u6f5c\u529b\n\u5bf9\u8bdd\u4e2d\u63d0\u5230\u7684\u201cLayerNorm\u201d\u548c\u201cRMSNorm\u201d\u4f5c\u4e3a\u5f52\u4e00\u5316\u65b9\u6cd5\uff0c\u5176\u56fa\u5b9a\u5f62\u5f0f\u4e0e\u9884\u671f\u7ed3\u679c\u4e4b\u95f4\u5b58\u5728\u56e0\u679c\u6027\u3002\u4f8b\u5982\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5728\u63d0\u5347\u6a21\u578b\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u5177\u6709\u660e\u786e\u7684\u6548\u679c\u3002\u6b64\u5916\uff0c\u5bf9\u8bdd\u4e2d\u63d0\u5230\u7684\u201c\u6df7\u5408\u63a8\u7406\u201d\u548c\u201c\u591a\u6a21\u578b\u63a8\u7406\u201d\u6280\u672f\u5c55\u793a\u4e86\u7b97\u6cd5\u548c\u6570\u636e\u7ed3\u6784\u7684\u53ef\u62d3\u5c55\u6027\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u6280\u672f\u53ef\u4ee5\u6839\u636e\u4e0d\u540c\u7684\u4efb\u52a1\u548c\u67e5\u8be2\u96be\u5ea6\u52a8\u6001\u8c03\u6574\u6a21\u578b\u7684\u8c03\u7528\u3002\u53cd\u5411\u63a8\u7406\u7684\u6f5c\u529b\u4f53\u73b0\u5728\u8fd9\u4e9b\u6280\u672f\u53ef\u4ee5\u901a\u8fc7\u5206\u6790\u67e5\u8be2\u7684\u96be\u5ea6\u548c\u6a21\u578b\u7684\u6027\u80fd\uff0c\u8fdb\u4e00\u6b65\u4f18\u5316\u6a21\u578b\u7684\u8c03\u7528\u7b56\u7565\u3002\n\n### \u603b\u7ed3\n\u5bf9\u8bdd\u4e3b\u8981\u56f4\u7ed5\u5927\u6a21\u578b\u4e2d\u7684\u5f52\u4e00\u5316\u65b9\u6cd5\uff08LayerNorm\u548cRMSNorm\uff09\u4ee5\u53ca\u63a8\u7406\u4f18\u5316\u6280\u672f\uff08\u5982\u6a21\u578b\u538b\u7f29\u3001\u91cf\u5316\u3001\u6df7\u5408\u63a8\u7406\u7b49\uff09\u5c55\u5f00\u3002\u8fd9\u4e9b\u6280\u672f\u5728\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3001\u8ba1\u7b97\u6548\u7387\u548c\u5b9e\u9645\u5e94\u7528\u8f6c\u5316\u65b9\u9762\u5177\u6709\u91cd\u8981\u4f5c\u7528\u3002\u901a\u8fc7\u7814\u7a76\u8fd9\u4e9b\u6280\u672f\u7684\u56fa\u5b9a\u5f62\u5f0f\u4e0e\u9884\u671f\u7ed3\u679c\u7684\u56e0\u679c\u6027\uff0c\u53ef\u4ee5\u8fdb\u4e00\u6b65\u62d3\u5c55\u5176\u5e94\u7528\u6f5c\u529b\uff0c\u5e76\u901a\u8fc7\u53cd\u5411\u63a8\u7406\u4f18\u5316\u6a21\u578b\u7684\u8c03\u7528\u7b56\u7565\u3002", "ref_analysis_id": ""}, "__type__": "dreams_node"}}, "analysis_store/ref_analysis_info": {"": {"node_ids": ["3abda052-4400-4e81-99f5-a68a76357d1e"], "metadata": {}}}, "analysis_store/metadata": {"3abda052-4400-4e81-99f5-a68a76357d1e": {"analysis_hash": "", "ref_analysis_id": ""}}}