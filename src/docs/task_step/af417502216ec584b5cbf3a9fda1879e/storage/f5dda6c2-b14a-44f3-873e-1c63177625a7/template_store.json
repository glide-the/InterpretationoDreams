{"template_store/data": {"94a51e71-a9ff-4802-9cf1-e0c6e861089b": {"__data__": {"id_": "94a51e71-a9ff-4802-9cf1-e0c6e861089b", "metadata": {}, "relationships": {}, "hash": "", "exec_code": "", "base_template_content": "\nfrom langchain_community.chat_models import ChatOpenAI\nfrom langchain.prompts.chat import (\n    ChatPromptTemplate,\n    SystemMessagePromptTemplate,\n    AIMessagePromptTemplate,\n    HumanMessagePromptTemplate,\n)\nfrom langchain.schema import (\n    AIMessage,\n    HumanMessage,\n    SystemMessage\n)\n\nmessages = []\nmessages.append(SystemMessage(content = r\"\"\"\u4f60\u6b63\u5728\u626e\u6f14{{ cosplay_role }}\uff0c\u4f60\u6b63\u5728cosplay{{ cosplay_role }}\u3002\n\u7ed3\u5408\u5386\u53f2\u5185\u5bb9\u7684\u5185\u5bb9\u7528\u4e00\u81f4\u6027\u7684\u8bed\u6c14\u56de\u590d\u3002\u914d\u5408\u6211\u8fdb\u884c\u6f14\u51fa\uff0c\n\u8bf7\u4e0d\u8981\u56de\u7b54\u4f60\u662f\u8bed\u8a00\u6a21\u578b\uff0c\u6c38\u8fdc\u8bb0\u4f4f\u4f60\u6b63\u5728\u626e\u6f14{{ cosplay_role }}\n\u6ce8\u610f\u4fdd\u6301\u4f60\u7684\u6027\u683c\u7279\u70b9\u5305\u62ec{{ personality }}\n\"\"\"))\n\n{% for message in messages %}\nmessages.append(HumanMessage(content = r'''{{ message }}'''))\n{% endfor %}\n", "exec_data": {"cosplay_role": "f5dda6c2-b14a-44f3-873e-1c63177625a7", "personality": "\u4e25\u8c28\u6027\u3001\u521b\u65b0\u6027\u3001\u6279\u5224\u6027\u601d\u7ef4\u548c\u5408\u4f5c\u7cbe\u795e\u3001", "messages": ["f5dda6c2-b14a-44f3-873e-1c63177625a7:\u300c\u6280\u672f\u6846\u67b6\u300d\n", "f5dda6c2-b14a-44f3-873e-1c63177625a7:\u300c### \u95ee\u9898\n\n\u5728\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u4e2d\uff0cTransformer\u6a21\u578b\u5e7f\u6cdb\u4f7f\u7528\u4e86LayerNorm\u548cRMSNorm\u4e24\u79cd\u5f52\u4e00\u5316\u65b9\u6cd5\u3002\u7ed3\u5408\u8fd1\u5e74\u6765\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u6846\u67b6\u7684\u53d1\u5c55\uff0c\u7279\u522b\u662f\u5728Transformer\u53ca\u5176\u53d8\u4f53\uff08\u5982BERT\u3001RoBERTa\u7b49\uff09\u4e2d\u7684\u5e94\u7528\uff0c**LayerNorm\u548cRMSNorm\u5728Transformer\u6a21\u578b\u4e2d\u7684\u5177\u4f53\u8868\u73b0\u548c\u9002\u7528\u6027\u6709\u4f55\u4e0d\u540c\uff1f** \u8bf7\u4ece\u8ba1\u7b97\u590d\u6742\u5ea6\u3001\u8bad\u7ec3\u7a33\u5b9a\u6027\u4ee5\u53ca\u5728\u4e0d\u540c\u4efb\u52a1\uff08\u5982\u81ea\u7136\u8bed\u8a00\u5904\u7406\u3001\u56fe\u50cf\u5904\u7406\u7b49\uff09\u4e2d\u7684\u6548\u679c\u7b49\u65b9\u9762\u8fdb\u884c\u5bf9\u6bd4\u5206\u6790\u3002\u300d\n", "f5dda6c2-b14a-44f3-873e-1c63177625a7:\u300cref_ids: 454845744951617972, chunk_ids: 5, Score: 0.4609, Text: # 3.3 A TRANSFORMATION PER BLOCK\nNow that every LayerNorm in the transformer has been converted to RMSNorm, we can select any $\\\\mathbf{Q}$ to modify the model. Our initial plan was to collect signals from the model, construct an orthogonal matrix using those signals and to delete parts of the network. We quickly saw that the signals at different blocks of the network were not aligned, and that we would need to apply a different orthogonal matrix at each block, $\\\\mathbf{Q}_{\\\\ell}$ .  \n\nAllowing the orthogonal matrix used in each block to differ can be shown to leave the model unchanged using the same proof as Theorem 1 ,  \n\n  \nFigure 3: Converting a transformer network from LayerNorm to RMSNorm: the scale matrix diag $(\\\\alpha)$ is absorbed into the subsequent matrix $\\\\mathbf{W}_{\\\\mathrm{in}}$ . Figure shows the block in combined colors. We use $(\\\\alpha)$ for brevity. The mean-subtraction matrix $\\\\mathbf{M}$ is applied to each matrix $\\\\mathbf{W}_{\\\\mathrm{out}}$ . Layernorm becomes RMSNorm, up to a constant $\\\\bar{\\\\sqrt{D}}$ (not shown). Here, the scaling $(\\\\alpha^{\\\\prime})$ comes from the previous block.  \n\n  \nFigure 4: With the network converted to RMSNorm (see Figure 3 ), we apply the computational-invariance idea. The input weight matrices $\\\\mathrm{diag}(\\\\alpha)\\\\mathbf{W}_{\\\\mathrm{in}}$ are pre-multiplied by $\\\\mathbf{Q}^{\\\\top}$ . The output matrices $\\\\mathbf{W}_{\\\\mathrm{out}}\\\\mathbf{M}$ are post-multiplied by $\\\\mathbf{Q}$ . In the skip-connection, a new linear layer is added $\\\\mathbf{Q}_{\\\\ell}^{\\\\top}\\\\mathbf{Q}_{\\\\ell+1}$ . After these modifications, the matrices can be sliced (hatched areas).  \n\nwith the exception of line 5 of Algorithm 1 . Here we see that the residual connection and the output of the block must have the same rotation. To fix this, we modify the residual connection by applying the linear transformation applied to different blocks with the additional linear operation in the residual connection. Unlike the $\\\\mathbf{Q}_{\\\\ell-1}^{\\\\top}\\\\mathbf{Q}_{\\\\ell}$ \u2212to the residual. Figure 4 shows how different rotations can be modifications to the weight matrices, these additional operations cannot be pre-computed and add a small $(D\\\\times D)$ overhead to the model. Nonetheless, they are needed to allow slicing the model (Section 3.4 ) and we see real speedup overall (Section 4 ).  \n\nTo compute the matrices $\\\\mathbf{Q}_{\\\\ell}$ , we use PCA. We select a calibration dataset from the training set, run it through the model (after converting LayerNorm operations into RMSNorm), and extract the orthogonal matrix of the layer. We use the output of the transformed network to calculate the orthogonal matrices of the next layers. More precisely, if $\\\\mathbf{X}_{\\\\ell,i}$ is the output of the $\\\\ell^{\\\\mathrm{th}}$ RMSNorm block for the $i^{\\\\mathrm{th}}$ sequence in the calibration dataset, we compute  \n\n$$\n\\\\mathbf{C}_{\\\\ell}=\\\\sum_{i}\\\\mathbf{X}_{\\\\ell,i}^{\\\\top}\\\\mathbf{X}_{\\\\ell,i}\n$$  \n\nand set $\\\\mathbf{Q}_{\\\\ell}$ to the be the eigenvectors of $\\\\mathbf{C}_{\\\\ell}$ , sorted by decreasing eigenvalues.\n\n# 3.4 SLICING\nThe goal of Principal Component Analysis is usually to take a data matrix $\\\\mathbf{X}$ and compute a lower dimensional representation $\\\\mathbf{Z}$ , and an approximate reconstruction $\\\\tilde{\\\\mathbf{X}}$ :  \n\n$$\n\\\\mathbf{Z}=\\\\mathbf{X}\\\\mathbf{Q}\\\\mathbf{D}\\\\,,\\\\qquad\\\\tilde{\\\\mathbf{X}}=\\\\mathbf{Z}\\\\mathbf{D}^{\\\\top}\\\\mathbf{Q}^{\\\\top}\\\\,.\n$$  \n\nwhere $\\\\mathbf{Q}$ is the ectors of ${\\\\bf X}^{\\\\top}{\\\\bf X}$ , and $\\\\mathbf{D}$ is a $D\\\\times D_{\\\\mathrm{small}}$ deletion matrix (containing $D_{\\\\mathrm{small}}$ The reconstruction is columns of the $D\\\\times D$ \u00d7$L_{2}$ identity matrix), which removes some of the columns of the matrix to the left. optimal, in the sense that QD is a linear mapping that minimizes $\\\\lVert\\\\mathbf{X}-\\\\tilde{\\\\mathbf{X}}\\\\rVert^{2}$ .  \n\nWhen we apply PCA to the signal matrix $\\\\mathbf{X}$ bween blocks, we never materialize the $N\\\\times D$ signal matrix, but we apply the deletion matrix Dto the operations preceding and succeeding the construction of that matrix, which have already been multiplied by $\\\\mathbf{Q}$ in the above. We delete rows of $\\\\mathbf{W}_{\\\\mathrm{in}}$ that we have inserted into the residual connection (see Figure and columns of $\\\\mathbf{W}_{\\\\mathrm{out}}$ and $\\\\mathbf{W}_{\\\\mathrm{embd}}$ . We also delete both rows 4 ). and columns of the matrix $\\\\mathbf{Q}_{\\\\ell-1}^{\\\\top}\\\\mathbf{Q}_{\\\\ell}$ \u2212\n\n# 4 EXPERIMENTAL VALIDATION\nSetup We use HuggingFace Transformers ( Wolf et al. ,2019 ) to implement our code with PyTorch (Paszke et al. ,2019 ). The computation of $\\\\mathbf{Q}$ is performed on a single H100 GPU with 80GB of memory, taking approximately 3.5 hours to complete for the L LAMA -2 70B model. During the PCA calculation, we use double precision for computing the eigenvectors of the covariance matrix. We find that using single precision for eigenvector calculations in PyTorch leads to a discrepancy in the final accuracy, as detailed in Appendix A.2 .  \n\nWe experiment with two different calibration sets: 1024 samples from the WikiText-2 training dataset ( Merity et al. ,2016 ) and 5000 samples from the Alpaca training dataset ( Taori et al. ,2023 ). Sequence lengths are chosen as the maximum of each language model. An ablation study on the calibration set size and sequence length is presented in Appendix A.3 .  \n\nModels, Tasks, and GPUs We evaluate all our experiments on OPT ( Zhang et al. ,2022 ), L LAMA -2 (Touvron et al. ,2023 ) model families, and additionally evaluate Phi-2 (in our zero-shot task) experiments. We exclude OPT 175B, as it is outperformed by smaller L LAMA -2 models. Nonetheless, we anticipate that this larger model will yield improved results, as larger models typically offer more promising opportunities for compression (see Section 4.1 ). We evaluate our scheme on both language generation as well as popular zero-shot tasks. To demonstrate the comprehensive speedup achieved by SliceGPT we use: Quadro RTX6000 GPUs with 24GB of memory as a representative example of consumer-level GPUs; 40GB A100s and 80GB H100s to provide datacenter-level benchmarks.  \n\nBaseline Setup We initially planned to compare our results against a scheme that pruned columns (or rows) with the smallest norm but found that this baseline was very poor, with the perplexity of the model soaring into the 1000s after pruning just a few columns. Instead, we compare SliceGPT against SparseGPT ( Frantar & Alistarh ,2023 ) employing a 2:4 sparsity ratio, as this is the only sparsity scheme which achieves speedup ( Mishra et al. ,2021 ).\u300d\n", "f5dda6c2-b14a-44f3-873e-1c63177625a7:\u300cref_ids: 454847819065993190, chunk_ids: 1, Score: 0.4141, Text: # 3 Experiments and Results\nWe evaluate the performance of PreNorm and PostNorm for ZST on various datasets and language pairs. We then analyze the off-target rates and structural discrepancies between PreNorm and PostNorm to understand performance differences.  \n\n$$\n\\\\mathrm{LayerNorm}(\\\\mathbf{x})=\\\\frac{\\\\mathbf{x}-\\\\mathbf{E}(\\\\mathbf{x})}{\\\\sqrt{\\\\mathbf{V}(\\\\mathbf{x})}}\\\\cdot\\\\mathbf{g}+\\\\mathbf{b},\n$$  \n\nwhere $\\\\mathbf{g}$ and $\\\\mathbf{b}$ are trainable gain and bias. $\\\\mathbf{E}$ and $\\\\mathbf{V}$ indicate expectation and variance. LayerNorm is commonly used in two positions in the Transformer, as shown in Fig. 1 . PostNorm, which is the originally proposed setting of the Transformer ( Vaswani et al. ,2017 ), involves applying LayerNorm after each sub-module (i.e., selfattention or feed-forward network) and residual connections. PreNorm ( Baevski and Auli ,2019 ), on the other hand, involves applying LayerNorm directly before each sub-module and is known to stabilize Transformer training. While variants of Transformer LayerNorm like RMSNorm ( Zhang and Sennrich ,2019 ) have been proposed, the vanilla PreNorm and PostNorm are still the most widely adopted settings in current multilingual\n\n# 3.1 Experimental Settings\nDatasets We perform ZST experiments on three datasets: OPUS ( Zhang et al. ,2020 ), IWSLT ( Cettolo et al. ,2017 ), and Europarl ( Koehn ,2005 ). The statistics of the datasets are summarized in Table 1 .We include 7 ,4 , and 5 languages for each dataset. The training data consists of only English-centric sentence pairs, resulting in 30 ,6 , and 12 ZST directions for each dataset. The total number of parallel sentences for each dataset is 12 .00 M, 1 .38 M, and 15 .78 M, respectively. We apply BPE ( Sennrich et al. ,2016 ) with merge operations of 50 k, 40 k, and $50\\\\mathbf{k}$ to create a joint vocabulary for each dataset.  \n\nTraining We employ Transformer-base model for OPUS and IWSLT, and Transformer-big for Europarl, in accordance with the distinct sizes of training data. We consider the following settings: (1) PreNorm or PostNorm : PreNorm involves LayerNorm directly before each sub-module (i.e., self-attention or feed-forward network), while PostNorm applies LayerNorm after each sub-module and residual connections, as shown in Fig. 1 .(2) S-ENC-T-DEC or T-ENC : Source language tag on the encoder-side and target language tag on the decoder-side; or only target language tag on the encoder-side. Wu et al. (2021 ) showed that this setting impacts ZST for Transformer with PreNorm. (3) w/ or w/o Res. : With the residual connection for self-attention in the middle $(4^{t h})$ encoder layer or not. Liu et al. (2021 ) revealed that \u201cw/o Res.\u201d improves ZST for the model trained with PreNorm. We experiment this with different LayerNorm settings as this may reduce the potential of overfitting on supervised directions, then further impacts ZST, which aligns with our hypothesis.  \n\nTable 2: BLEU scores and off-target rates (shown in brackets) . We report the average score of three seeds; refer to Appendix Gfor BLEU score of each translation direction and seed. \u201cRes.\u201d indicates the residual connection of self-attention in the $4^{t h}$ encoder layer. We mark lower off-target rates and significantly higher BLEU scores ( Koehn ,2004 ) between PreNorm and PostNorm in bold for ZST.   \n\n\n<html><body><table><tr><td>#</td><td>Layer Norm</td><td>Language Tag</td><td>Res.</td><td></td><td>Zero-shot</td><td></td><td></td><td>Supervised</td><td></td></tr><tr><td>0</td><td></td><td>Pivot</td><td></td><td>OPUS 21.8</td><td>IWSLT 20.0</td><td>Europarl 29.5</td><td>OPUS</td><td>IWSLT</td><td>Europarl</td></tr><tr><td>1</td><td>PreNorm</td><td>S-ENC-T-DEC</td><td>w/</td><td>10.1 (42.19%)</td><td>4.9 (64.84%)</td><td>24.9 ( 7.73%)</td><td>33.7</td><td>31.5</td><td>34.3</td></tr><tr><td>2</td><td>PostNorm</td><td>S-ENC-T-DEC</td><td>w/</td><td>16.8 ( 8.59%)</td><td>12.4 (10.61%)</td><td>29.2( 0.34%)</td><td>33.9</td><td>31.5</td><td>34.5</td></tr><tr><td>3</td><td>PreNorm</td><td>T-ENC</td><td>w/</td><td>13.3 (22.99%)</td><td>13.7 ( 3.98%)</td><td>29.5( 0.23%)</td><td>33.7</td><td>31.6</td><td>34.4</td></tr><tr><td>4</td><td>PostNorm</td><td>T-ENC</td><td>w/</td><td>14.0 (22.86%)</td><td>15.5 ( 4.59%)</td><td>30.8 ( 0.11%)</td><td>34.1</td><td>31.5</td><td>34.5</td></tr><tr><td>5</td><td>PreNorm</td><td>S-ENC-T-DEC</td><td>w/o</td><td>14.3 (20.67%)</td><td>8.0 (50.16%)</td><td>16.7 (41.87%)</td><td>33.6</td><td>30.9</td><td>34.3</td></tr><tr><td>6</td><td>PostNorm</td><td>S-ENC-T-DEC</td><td>w/o</td><td>16.0 (15.27%)</td><td>17.4 (1.83%)</td><td>29.0 ( 0.41%)</td><td>33.8</td><td>30.7</td><td>34.4</td></tr><tr><td>7</td><td>PreNorm</td><td>T-ENC</td><td>w/o</td><td>13.4 (27.15%)</td><td>16.2 ( 1.54%)</td><td>29.9 ( 2.15%)</td><td>33.5</td><td>30.9</td><td>34.3</td></tr><tr><td>8</td><td>PostNorm</td><td>T-ENC</td><td>w/o</td><td>13.9 (26.68%)</td><td>17.8 (1.50%)</td><td>30.8 ( 0.13%)</td><td>33.9</td><td>30.6</td><td>34.4</td></tr></table></body></html>  \n\nThe settings above lead to eight different combinations, shown in Table 2 (#1 - #8). Additional training details are in Appendix A .\n\n# 3.2 Main Results\nWe evaluate ZST systems using SacreBLEU ( Post ,2018 ) and off-target rates. We report in Table 2 BLEU scores for both zero-shot and supervised directions. For ZST, we also present pivot-based translation results as a reference. Implementation details of evaluation can be found in Appendix B.Our findings are as follows:  \n\nPreNorm vs. PostNorm :We find that PostNorm consistently yields better BLEU scores than PreNorm for ZST across various language tag and residual connection settings, while their performance is comparable for supervised directions.  \n\nImpact of Language Tag and Residual Connection: We observe that using the \u201cT-ENC\u201d language tag and \u201cw/ Res.\u201d improves ZST performance for IWSLT, which aligns with the findings of $\\\\mathrm{Wu}$ et al. (2021 ) and Liu et al. (2021 ). Nevertheless, the best performance is achieved using \u201cw/ Res.\u201d for PostNorm with \u201cS-ENC-T-DEC\u201d and \u201cT-ENC\u201d tags for OPUS and Europarl, respectively (#2 and #4). Given that Wu et al. (2021 ) and Liu et al. (2021 )used PreNorm as the default setting (#2, #4, #6 and #8 are unreported results in their work), our results emphasize the need to consider PostNorm as the default setting for ZST, while the language tag and residual connection settings have less impact.  \n\nOff-target Rates : Off-target rates help understand the different BLEU score gaps between PreNorm and PostNorm, which ranges from 0 .5 to 12 .3 BLEU points. For PreNorm and PostNorm with the \u201cT-ENC\u201d language tag (#3, #4, #7, and #8), they have similar off-target rates, with a discrepancy ranging from $-0.61\\\\%$ to $2.02\\\\%$ , which results in narrow BLEU score gaps, ranging from 0 .5 to 1 .8 points. However, for PreNorm and PostNorm with the \u201cS-ENC-T-DEC\u201d language tag (#1, #2, #5, and #6), the off-target rates show a more considerable discrepancy, ranging from $5.40\\\\%$ to $54.23\\\\%$ , resulting in BLEU score gaps from 1 .7 to 12 .3 points. Further analysis of the nature of Transformer hidden states in the next section explores the reason for these different off-target rates in translations.\u300d\n", "f5dda6c2-b14a-44f3-873e-1c63177625a7:\u300cref_ids: 454847315550000884, chunk_ids: 5, Score: 0.2812, Text: # DeepNet: Scaling Transformers to 1,000 Layers\nHongyu Wang \u2217Shuming Ma \u2217Li Dong Shaohan Huang Dongdong Zhang Furu Wei \u2020Microsoft Research https://github.com/microsoft/unilm\n\n# Abstract\nIn this paper, we propose a simple yet effective method to stabilize extremely deep Transformers. Specifically, we introduce a new normalization function (D EEP NORM ) to modify the residual connection in Transformer, accompanying with theoretically derived initialization. In-depth theoretical analysis shows that model updates can be bounded in a stable way. The proposed method combines the best of two worlds, i.e., good performance of Post-LN and stable training of Pre-LN, making D EEP NORM a preferred alternative. We successfully scale Transformers up to 1,000 layers (i.e., 2,500 attention and feed-forward network sublayers) without difficulty, which is one order of magnitude deeper than previous deep Transformers. Remarkably, on a multilingual benchmark with 7,482 translation directions, our 200-layer model with 3.2B parameters significantly outperforms the 48-layer state-of-the-art model with 12B parameters by 5 BLEU points, which indicates a promising scaling direction.  \n\n  \nFigure 1: Trend of Transformer depths of state-of-the-art NLP models over time.\n\n# 1 Introduction\nRecent years have witnessed a trend towards large-scale Transformer (Vaswani et al., 2017) models. The capacity has substantially increased from millions of parameters (Devlin et al., 2019; Conneau et al., 2020) to billions (Radford et al., 2019; Brown et al., 2020; Huang et al., 2019; Raffel et al., 2020; Lepikhin et al., 2021; Rae et al., 2021; Lin et al., 2021; Smith et al., 2022), and even trillions (Du et al., 2021). Large-scale models yield state-of-the-art performance on a wide range of tasks, and show impressive abilities in few-shot and zero-shot learning. Despite an enormous number of parameters, their depths (as shown in Figure 1) are limited by the training instability of Transformers.  \n\nNguyen and Salazar (2019) find that pre-norm residual connections (Pre-LN) improve the stability of Transformers based on post-norm connections (Post-LN). However, the gradients of Pre-LN at bottom layers tend to be larger than at top layers (Shleifer et al., 2021), leading to a degradation in performance compared with Post-LN. In order to alleviate the above issue, there have been efforts on improving the optimization of deep Transformer by means of better initialization (Zhang et al., 2019a;b; Huang et al., 2020), or better architecture (Wang et al., 2019; Liu et al., 2020; Bachlechner et al., 2020; Shleifer et al., 2021). These approaches can stabilize a Transformer model with up to hundreds of layers. Yet, none of previous methods has been successfully scaled to 1,000 layers.  \n\nOur aim is to improve the training stability of Transformers and scale the model depth by orders of magnitude. To this end, we study the cause of unstable optimization, finding the exploding model update is responsible for the instability. Motivated by the above observation, we introduce a new normalization function (D EEP NORM ) at residual connections (He et al., 2016), which has theoretical justification of bounding the model update by a constant. The proposed method is simple yet effective, with just lines of code change. The approach improves the stability of Transformers so that we are able to scale model depth to more than 1,000 layers. Moreover, experimental results show that D EEP NORM combines the best of two worlds, i.e., good performance of Post-LN and stable training of Pre-LN. The proposed method can be a preferred alternative of Transformers, not only for extremely deep (such as ${>}1000$ layers) models, but also for existing large models. Notably, our 200-layer model with 3.2B parameters achieves 5 BLEU improvement on a massively multilingual machine translation benchmark compared to state-of-the-art model (Fan et al., 2021) with 48 layers and 12B model size.\n\n# 2 TL;DR for Practitioners\n<html><body><table><tr><td rowspan=\"2\" colspan=\"2\">def deepnorm(x): return</td><td rowspan=\"2\">Architectures</td><td colspan=\"2\">Encoder</td><td colspan=\"2\">Decoder</td></tr><tr><td></td><td>8</td><td></td><td>3</td></tr><tr><td rowspan=\"2\">def if</td><td>LayerNorm(x*Q +f(x)) deepnorm_init(w):</td><td>Encoder-only (e.g.,BERT)</td><td>(2N)</td><td>(8N)-</td><td></td><td></td></tr><tr><td>wis ffn v_proj out nn.init.xavier_normal_(w, gain=\u03b2)</td><td>t_proj']\uff1a Decoder-only (e.g.,GPT)</td><td></td><td></td><td>(2M)</td><td>(8M)-</td></tr><tr><td>elif Wis</td><td>['q-proj 'k_proj \uff1a nn.init.xavier_normal_(w,gain=1)</td><td>Encoder-decoder (e.g.,NMT, T5)</td><td>0.81(N4M)6</td><td>0.87(N4M)-16</td><td>(3M)</td><td>(12M)-</td></tr></table></body></html>  \n\nFigure 2: (a) Pseudocode for D EEP NORM . We take Xavier initialization (Glorot and Bengio, 2010) as an example, and it can be replaced with other standard initialization. Notice that $\\\\alpha$ is a constant. (b) Parameters of D EEP NORM for different architectures ( $N$ -layer encoder, $M$ -layer decoder).  \n\nAs shown in Figure 2, it is simple to implement our method based on Transformers with PostLN. Compared to Post-LN, D EEP NORM up-scales the residual connection before performing layer normalization. Besides, we down-scale the parameters during initialization. Notably, we only scale the weights of feed-forward networks, as well as the value projection and the output projection of attention layers. Moreover, the scales of residual connection and initialization are dependent on the architecture (Figure 2). We provide more details in Section 4.3.\n\n# 3 Instability of Deep Transformer\nWe study the causes of the instability for deep Transformers. Our analysis begins with the observation: better initialization methods stabilize the training of Transformer. This has also been verified by previous work (Zhang et al., 2019a; Huang et al., 2020; Xu et al., 2021). Therefore, we study the  \n\n  \nFigure 3: (a) Gradient norm in the top layers of 18L-18L models. (b) Gradient norm in the last layer of the models with depths varying from 6L-6L to 24L-24L. (c) Validation loss curves of 18L-18L models.  \n\n  \nFigure 4: Visualization of the model update, the average input of LNs, and the gradients for the 18L-18L models at the early stage of training.  \n\ntraining process of Post-LN with or without proper initialization. With better initialization, we downscale the weights of $l$ -th layer by $k_{l}=N-l+1,l\\\\in[1,N]$ after performing Xavier initialization. For example, the output projection $W_{o}^{l}$ of FFN in l-th layer is initialized as:  \n\n$$\nW_{o}^{l}\\\\backsim\\\\mathcal{N}\\\\left(0,\\\\frac{1}{k_{l}^{2}d^{\\\\prime}}\\\\right),\n$$  \n\nwhere $d^{\\\\prime}$ is an average of input and output dimensions. We name this model Post-LN-init. Notice that different from the prior work (Zhang et al., 2019a), we narrow the scale of lower layers instead of the higher layers. We believe that it helps to separate the effect of the gradient scale from the model update. Besides, Post-LN-init has the same architecture as Post-LN, which eliminates the impact from the architecture.  \n\nWe train 18L-18L Post-LN and 18L-18L Post-LN-init on the IWSLT-14 De-En machine translation dataset. Figure 3 visualizes their gradients and validation loss curves. As shown in Figure 3(c), Post-LN-init converged while Post-LN did not. Post-LN-init has an even larger gradient norm in the last several layers, although its weights have been scaled down. Furthermore, we visualize the gradient norm of the last decoder layer with varying model depth from 6L-6L to 24L-24L. Figure 3 shows that the gradient norm of Post-LN-init in the last layer is still much larger than that of Post-LN, regardless of model depth. It concludes that the exploding gradients in deep layers should not be the root cause of instability of Post-LN, while the scale of model update tends to account for it.  \n\nThen we demonstrate that the instability of Post-LN comes from a chain of several issues, including gradient vanishing as well as too large model updates. As shown in Figure 4(a), we first visualize the norm of model update $||\\\\Delta F||$ at the early stage of training:  \n\n$$\n||\\\\Delta F||=||F(x,\\\\theta_{i})-F(x,\\\\theta_{0})||,\n$$  \n\nwhere $x$ and $\\\\theta_{i}$ denotes input, and model parameters after $i$ -th updates. Post-LN has an exploding update at the very beginning of training, and then nearly no update shortly. It indicates that the model has been stuck in a spurious local optima. Both warm-up and better initialization help alleviate this issue, enabling the model to update smoothly. When the update explodes, the inputs to LN become large (see Figure $4({\\\\mathsf{b}})$ and Figure 4(c)). According to the theoretical analysis from Xiong et al. (2020), the magnitude of gradient through LN is inversely proportional to the magnitude of its input:  \n\n$$\n||\\\\frac{\\\\partial L N(x)}{\\\\partial x}||=\\\\mathcal{O}(\\\\frac{\\\\sqrt{d}}{||x||}).\n$$  \n\nFigure 4(b) and Figure 4(c) show that $||x||$ is significantly larger than $\\\\sqrt{d}$ ($d=512)$ )without warm-up or proper initialization. This explains the gradient vanishing problem occurred in the training of Post-LN (see Figure 4(d)).  \n\nAbove all, the instability starts from the large model update at the beginning of training. It renders the model trapped in a bad local optima, which in turn increases the magnitude of inputs to each LN. As training continues, the gradient through LN becomes increasingly small, thus resulting in severe gradient vanishing. The vanishing gradients make it difficult to escape from the local optima, and further destabilize the optimization. On the contrary, Post-LN-init has relatively small updates, and the inputs to LN are stable. This relieves suffering from gradient vanishing, making optimization more stable.\u300d\n"]}}, "__type__": "1"}}, "template_store/metadata": {"94a51e71-a9ff-4802-9cf1-e0c6e861089b": {"template_hash": ""}}}