角色,内容,分镜
30b2ef30-75d2-40c5-b100-d2fefb6cb330,研究成果、方法的创新性与应用价值,6
30b2ef30-75d2-40c5-b100-d2fefb6cb330,LayerNorm 和 RMSNorm 的研究成果为后续研究提供了哪些具体的重要参考？它们在不同应用领域提升模型性能和训练效率的具体表现和作用机制是什么？ ,6
30b2ef30-75d2-40c5-b100-d2fefb6cb330,"ref_ids: 454847819065993190, chunk_ids: 1, Score: 0.4863, Text: # 4 EXPERIMENTAL RESULTS

# 4.1 TUNING LAYER NORM
Tuning LayerNorm in Attention Blocks. In table 1 , it is noteworthy that activating only the LayerNorm yields the least activated parameters, yet the model performances are surprisingly impressive when compared to tuning other modules. Specifically, in two captioning tasks, the VQAv2 task, and the challenging hallucination benchmark POPE , models with only the LayerNorm activated consistently outperform all other competitors by at least $8.2\\%$ . On the comprehensively evaluated benchmark MME , while tuning LayerNorm outperforms finetuning the intact language model by an average of $6.6\\%$ on the Perception aspect, it lags behind finetuning by an average of $6.3\\%$ on the Cognition score. It is vital to note, however, that the LayerNorm only accounts for approximately $2.5\\%$ of the training parameters in the whole model.  

In addition to tuning modules, another observation is that MLLMs incorporating human-aligned LLMs (such as LL A MA2- CHAT ) exhibit superior performance in complex and demanding tasks such as POPE and MME compared to their unaligned counterparts. This underscores the importance of utilizing aligned LLMs to construct a more powerful MLLMs.  

Tuning LayerNorm and Only LayerNorm. As the above LayerNorm method finetunes (1) visionlanguage connector, (2) word embedding, (3) output head, and (4) LayerNorm component in the LLM simultaneously, a pertinent question arises: Is it possible for (4) LayerNorm alone to generalize effectively in training MLLMs? To address this query, we take a step further and solely finetune LayerNorm in MLLMs, which is denoted as LayerNorm-simp. in table 1 . The results are intriguing, demonstrating that even with a mere $0.004\\%$ parameter finetuning in the whole model, LayerNormsimp. surpasses full parameter finetuning on three conventional vision-language tasks (i.e., two captioning and one VQA tasks) by $10\\%$ , and only lags behind full finetuning by $7.9\\%$ on the MME benchmark. This intriguing discovery suggests that the transition from LLM to MLLMs probably involves a domain adaptation process as the LayerNorm takes the most credits in tuning a wellbehaved MLLMs. The LayerNorm alone may be also capable of integrating vision information with language tokens seamlessly.  

Memory Consumption and Parameter Efficiency. In table 2 , we present the total memory consumption and the percentage of trainable parameters of each MLLMs finetuning method across 7B and 13B scales. Methods like full parameter finetuning and finetuning MLPs in attention modules face out-of-memory (OOM) issue even on a high-capacity 80GB A100 GPU, while LayerNorm based methods stand out for their efficiency. Specifically, LayerNorm tuning requires only $24.2\\,\\mathrm{GB}$ and $38.3\\,\\mathrm{GB}$ memory at 7B and 13B scales respectively. Remarkably, LayerNorm-simp. further reduces the memory to $18.9\\,\\mathrm{GB}$ and 31.7 GB. In terms of trainable parameters, LayerNorm based methods also show remarkable efficiency, LayerNorm utilizes only $3.78\\%$ and $2.50\\%$ of the total parameters at the 7B and 13B scales, and LayerNorm-simp. takes efficiency to an extreme, involving only $0.004\\%$ and $0.003\\%$ of the parameters at these scales. These results demonstrate the efficiency advantage of LayerNorm tuning, compared with existing methods like LoRA or full parameter finetuning.  

  
Figure 2: Performances of models that are finetuned on different datasets on four multi-modal benchmarks. The MME score is the sum of both Cognition and Perception scores on the benchmark.

# 4.2 ‘L ESS IS MORE ’ON BOTH DATA AND PARAMETER SIDES
Efficiency in training can also be improved by considering the data used in LLMs and MLLMs ( Zhou et al. ,2023 ;Wei et al. ,2023 ). To this end, we conducted experiments using LL A MA2-7B and LL A MA2-7BCHAT , where we divided the training data into three categories, each comprising 20K data points: image-grounded conversation, image detail descriptions, and image-based complex reasoning, as previously deployed in Liu et al. (2023 ). Based on the results presented in fig. 2 , we observe that the image-grounded conversation data is the most effective in enhancing the multi-modal capabilities of the model, with an average improvement of over $50\\%$ compared to other data types. This highlights the potential benefits of a targeted approach that leverages the strengths of specific data types to facilitate more nuanced and effective multi-modal tuning for MLLMs.  

To validate ‘Less is More’ on both the data and parameter sides, we present results of MLLMs with LayerNorm activated in LLM and tuned on 20k conversational data in table 3 . Our experimental results indicate that even with a smaller dataset and the use of LayerNorm tuning, the model outperforms the full parameter finetuning approach on the full 80K dataset by $18.4\\%$ on two captioning tasks, and only falls short in MME by a tolerable $2.5\\%$ . It is noteworthy that LayerNorm with 20K data is only $7.6\\%$ and $7.4\\%$ behind LayerNorm on the full 80K dataset for two captioning tasks and MME task, respectively. These findings demonstrate that ‘Less is More’ for both the parameter and data perspectives beyond language domain Zhou et al. (2023 ), but for multi-modal tuning.",6
30b2ef30-75d2-40c5-b100-d2fefb6cb330,"ref_ids: 454846008172788376, chunk_ids: 4, Score: 0.4277, Text: # 5.3 LAYER NORM TUNING HAS SMALLER GRADIENT VARIANCE
A well accepted view about LayerNorm is that, as the neural network goes deeper, the mean of LayerNorm gradients should goes to zero as the LayerNorm itself is designed to normalize all training parameters. In the meantime, the variance of LayerNorm gradients should be small to ensure a better generalization ability of the model ( Xu et al. ,2019 ) (See the proof in Appendix A.2.2 ). As we presented in fig. 4 , MLLM with LayerNorm tuning method has a more concentrated LayerNorm gradients than fine-tuning during the training process. This result gives another view on the effectiveness of LayerNorm from the optimization perspective. More visualizations are listed in Appendix A.2.2 .

# 6 CONCLUSION AND DISCUSSIONS
LayerNorm is effective and sufficient built upon MLLM pre-training. MLLM training typically involves pre-training on image-text pairs followed by finetuning on visual instruction data. While the second stage of training receives more attention, it is worth noting that the function of the first stage pre-training is non-negligible for training a competent MLLM. We have presented in the paper only a small portion of parameter activation is sufficient to tune a well-behaved MLLM. However, other models such as I NSTRUCT BLIP ( Dai et al. ,2023 ) and M INI GPT4 ( Zhu et al. ,2023 ) only tune the vision-language connector, leaving the LLM untouched during the second stage of training. These models have yielded strong performances when given a large-scale finetuning dataset. In Sec. 5.1 , we demonstrate that tuning LayerNorm may be a more effective means for the second stage training, especially when compared to existing parameter-efficient methods for training MLLMs.  

Limitations. One shortcoming of these parameter-efficient finetuning methods is that they are more sensitive to hyper-parameters ( e.g ., learning rate, training epoch) than finetuning. Since the number of trainable parameters of LayerNorm is small, the model performance of LayerNorm method also varies when twitching the training hyper-parameters. This drawback calls for potential future investigations on the LayerNorm tuning method. In the Appendix A.1 , we give a hint for the grid search range of learning rate on both 7B and 13B scaled models using LayerNorm tuning based on our experimental results.  

Conclusion. Our studies demonstrate LayerNorm tuning as a simple yet effective tuning method for adapting LLMs comprehend multi-modal content across various model variants. Compared to LoRA tuning or full parameter finetuning, LayerNorm tuning reduces the trainable parameters by a significant $41.9\\%$ , enabling efficient finetuning of MLLMs on consumer-grade GPUs. Moreover, we demonstrate that MLLMs can achieve exceptional performance with minimal “right” data and parameters, showcasing the potential of LayerNorm tuning method in real-world applications. Given the empirical success of LayerNorm tuning, we revisited the MLLM finetuning from a domain adaptation perspective and showed that LayerNorm plays a critical role in adapting LLMs to the multi-modal domain. Additionally, our research illustrates the expressive power and optimization potential of LayerNorm tuning from layer similarities and the gradient variance. We hope that our work could inspire future works on designing improved PEFT methods that enable more diverse application scenarios for MLLMs.



# A A PPENDIX

# A.1 TRAINING DETAILS
For the first stage, we set the learning rate to 2e-3 for all variants. During the second stage, we search learning the learning rate from [2e-3, 1e-3, 6e-4, 3e-4, 1e-4, 5e-5, 2e-5, 1e-5, 6e-6, 1e-6, 1e-7] for all models and pick the best learning rate based on their performances on the CIDEr score on the Flickr $30\\,\\mathrm{k}$ task.  

According to our tryouts based on Flickr $30\\,\\mathrm{k}$ results in Table A1 , the recommended learning rate for 7B scale is between 6e-4 to 2e-3, while on the 13B, the learning rate should be searched in the range of 3e-6 to 6e-5.  

Table A1: Performance of MLLMs (LayerNorm-simp.) trained with different learning rates and scales on the Flickr30k task.   


<html><body><table><tr><td>Learning Rate</td><td>3e-6</td><td>1e-5</td><td>3e-5</td><td>6e-5</td></tr><tr><td>MM-LLAMA2 7B</td><td>21.42</td><td>32.45</td><td>43.04</td><td>28.24</td></tr><tr><td>Learning Rate</td><td>6e-4</td><td>1e-3</td><td>2e-3</td><td></td></tr><tr><td>MM-LLAMA213B</td><td>37.35</td><td>46.88</td><td>44.15</td><td></td></tr></table></body></html>

# A.2INSIGHTS OFLAYERNORMTUNING

# A.2.1 VISUALIZATION EXAMPLES OF LAYER SIMILARITIES
Lower similarities between different layers of the transformer indicates more expressive power ( Pires et al. ,2023 ). In section 5.2 , we have shown the computed cosine similarity between layers on a Vicuna model, here we show the layer similarities between layers on LL A MA2 and LL A MA2 CHAT models in fig. A1 and fig. A2 . It is clear that, LayerNorm tuning again allows the model to learn dissimilar layer representations, improving the expressive power of the model.

# A.2.2 GRADIENTS OF LAYER NORM
Visualization examples of LayerNorm gradients. In fig. A3 and fig. A4 , we present the gradients of the LayerNorm parameters during the training process. Similar to the one we have shown in the main text, LayerNorm tuning demonstrates a smaller gradient variance which is important for converging to a better local minimum ( Xu et al. ,2019 ).  

Proof of smaller variance in LayerNorm . As stated in Sec. 5.3 , deeper the network is, the variance of LayerNorm in the model should be naturally smaller ( $\\mathrm{\\DeltaXu}$ et al. ,2019 ). We first let $\\mathbf{y}\\,=\\,(y_{1},y_{2},...,y_{N})$ be the normalized vector, meaning the mean and variance of $\\mathbf{y}$ is 0 and 1 ,respectively. We can then formulate the standard LayerNorm as follow:  

$$
{\\mathbf{y}}={\\frac{\\mathbf{x}-{\\boldsymbol{\\mu}}}{\\sigma}},\\quad{\\boldsymbol{\\mu}}={\\frac{1}{N}}\\sum_{i=1}^{N}x_{i},\\quad\\sigma={\\sqrt{{\\frac{1}{N}}\\sum_{i=1}^{N}\\left(x_{i}-{\\boldsymbol{\\mu}}\\right)^{2}}},
$$  

where $\\mathbf{x}=(x_{1},x_{2},...,x_{N})$ is the input vector and $N$ is the dimension of $\\mathbf{x}$ .$\\mu$ and $\\sigma$ are the mean and standard deviation of $\\mathbf{x}$ .  

We first define $\\mathbf{1}_{N}=\\underbrace{(1,1,...,1)^{\\intercal}}_{N}$ . For calculating the gradients of the normalized vector $\\mathbf{y}$ , we first simulate the backward propagation regarding the loss {z }$\\ell$ :  

$$
{\\frac{\\partial\\ell}{\\partial\\mathbf{x}}}=\\left({\\frac{\\partial\\mathbf{y}}{\\partial\\mathbf{x}}}+{\\frac{\\partial\\mu}{\\partial\\mathbf{x}}}{\\frac{\\partial\\mathbf{y}}{\\partial\\mu}}+{\\frac{\\partial\\sigma}{\\partial\\mathbf{x}}}{\\frac{\\partial\\mathbf{y}}{\\partial\\sigma}}\\right){\\frac{\\partial\\ell}{\\partial\\mathbf{y}}}={\\frac{1}{\\sigma}}\\left(I-{\\frac{\\mathbf{y}\\mathbf{y}^{\\intercal}}{N}}-{\\frac{\\mathbf{1}_{N}\\mathbf{1}_{N}^{\\intercal}}{N}}\\right){\\frac{\\partial\\ell}{\\partial\\mathbf{y}}}.
$$  

  
Figure A1: Layer similarities between different LLM layers in (a) Finetuned and (b) LayerNormtuned MM-LL A MA2-7B.  

Here we define $\\begin{array}{r l r}{\\frac{\\partial\\ell}{\\partial\\mathbf x}}&{{}\\!\\!=}&{\\!\\!(a_{1},a_{2},...,a_{N})}\\end{array}$ with mean $\\bar{a}$ and standard deviation $D_{a}$ , and $\\begin{array}{r l}{\\frac{\\partial\\ell}{\\partial\\mathbf{y}}}&{{}=}\\end{array}$ $(b_{1},b_{2},...,b_{N})$ with mean $\\bar{b}$ and standard deviation $D_{b}$ . We set $\\begin{array}{r}{W_{1}\\;=\\;I\\,-\\,\\frac{{\\bf y}{\\bf y}^{\\intercal}}{N}\\,-\\,\\frac{{\\bf1}_{N}{\\bf1}_{N}^{\\intercal}}{N}}\\end{array}$ −, we can verify that:  

$$
\\lfloor\\mathbf{\\Pi}_{N}^{\\mathsf{T}}W_{1}=\\mathbf{1}_{N}^{\\mathsf{T}}{\\frac{1}{\\sigma}}\\left(I-{\\frac{\\mathbf{1}_{N}\\mathbf{1}_{N}^{\\mathsf{T}}+\\mathbf{y}\\mathbf{y}^{\\mathsf{T}}}{N}}\\right)={\\frac{1}{\\sigma}}\\left(\\mathbf{1}_{N}-{\\frac{\\mathbf{1}_{N}^{\\mathsf{T}}\\mathbf{1}_{N}}{N}}\\mathbf{1}_{N}^{\\mathsf{T}}-{\\frac{\\mathbf{1}_{N}^{\\mathsf{T}}\\mathbf{y}}{N}}\\mathbf{y}^{\\mathsf{T}}\\right)={\\frac{\\mathbf{1}_{N}-\\mathbf{1}_{N}-0}{\\sigma}}=0
$$  

Therefore, we can easily proof that $N\\bar{a}\\propto{\\bf1}_{N}^{\\top}W_{1}\\bar{b}=0$ , which means the mean of $\\frac{\\partial\\ell}{\\partial\\mathbf{x}}$ should be zero. Then we dive into proofing the variance of LayerNorm gradients should be small when the number of network parameters $N$ becomes large.  

$$
\\begin{array}{l}{{\\displaystyle{D_{a}=\\sum_{i=1}^{N}(a_{i}-\\bar{a})^{2}/N=\\sum_{i=1}^{N}a_{i}^{2}/N}\\ ~}}\\\\ {{\\displaystyle{=\\left\\|{(a_{1},a_{2},\\ldots,a_{N})^{\\top}}\\right\\|^{2}/N}\\ ~}}\\\\ {{\\displaystyle{=\\left\\|{W_{1}\\left(b_{1},b_{2},\\ldots,b_{N}\\right)^{\\top}}\\right\\|^{2}/N}\\ ~}}\\\\ {{\\displaystyle{=\\left\\|{W_{1}\\left(b_{1}-\\bar{b},b_{2}-\\bar{b},\\ldots,b_{N}-\\bar{b}\\right)^{\\top}+W_{1}\\bar{b}{\\bf1}_{N}}\\right\\|^{2}/N}\\ }}\\\\ {{\\displaystyle{=\\left\\|{W_{1}\\left(g_{1}-\\bar{b},g_{2}-\\bar{b},\\ldots,g_{N}-\\bar{b}\\right)^{\\top}}\\right\\|^{2}/N}\\ ~}}\\\\ {{\\displaystyle{\\leq W_{1}^{2}\\sum_{i=1}^{N}(b_{i}-\\bar{b})^{2}/N}\\ }}\\end{array}
$$  

Since the projection matrix $W_{1}$ is idempotent, we have $W_{1}^{2}=W_{1}$ . That is to say, when $N$ is large enough, there stands the network parameter $\\begin{array}{r}{D_{a}\\le\\big(I-\\frac{\\mathbf{y}\\mathbf{y}^{\\top}+\\mathbf{1}_{N}\\mathbf{1}_{N}^{\\top}}{N}\\big)\\sum_{i=1}^{N}(b_{i}-\\bar{b_{}})^{2}/N\\propto1/N^{2}}\\end{array}$ Nis large, the gradient variance of LayerNorm should be small. P−. As a consequence, when  

  
Figure A2: Layer similarities between different LLM layers in (a) Finetuned and (b) LayerNormtuned MM-LL A MA2-7B CHAT .  

  
Figure A3: The gradients of both input and post LayerNorm in 21st layer of the MM-V ICUNA as the training proceeds.  

  
Figure A4: The gradients of both input and post LayerNorm in 11th layer of the MM-V ICUNA as the training proceeds.",6
30b2ef30-75d2-40c5-b100-d2fefb6cb330,"ref_ids: 454895361066540518, chunk_ids: 1, Score: 0.4180, Text: # B.2 MODEL LAYERS
In this section, we give the formal definition of LayerNorm $\\operatorname{LN}(\\cdot)$ and RMS Norm ${\\mathrm{RMS}}\\left(\\cdot\\right)$ .  

Definition 1 (LayerNorm) .LayerNorm $L N(\\cdot;\\mu,\\beta,\\epsilon)$ of dimension $D$ is defined as:  

$$
L N(\\mathbf{x};\\pmb{\\mu},\\beta,\\epsilon)=\\frac{\\mathbf{x}-\\mathbb{E}[\\mathbf{x}]}{\\sqrt{\\mathrm{Var}[\\mathbf{x}]+\\epsilon}}\\odot\\pmb{\\mu}+\\beta,
$$  

where $\\mathbf{x},\\pmb{\\mu},\\beta\\in\\mathbb{R}^{D}$ .  

Definition 2 (RMSNorm) .RMS Norm $R M S(\\cdot;\\mu,\\epsilon)$ of dimension $D$ is defined as:  

$$
R M S(\\mathbf{x};\\pmb{\\mu},\\epsilon)=\\frac{\\mathbf{x}}{\\sqrt{\\frac{1}{D}\\sum_{i=1}^{D}(\\mathbf{x}[i])^{2}+\\epsilon}}\\odot\\pmb{\\mu},
$$  

where x,$\\pmb{\\mu}\\in\\mathbb{R}^{D}$ .  

Remark. In neural networks, inputs of normalization layers are usually high dimension tensors. In this case, LayerNorm and RMSNorm normally apply to the last dimension separately.

# B.3 LOSSLESS EXPANSION IN VECTOR SPACE
In this section, we first give the general definition of lossless expansion in vector space.  

dimensions satisfy dim it is invertible. Definition 3 (Lossless $(\\bar{\\mathcal{T}})\\geq d i m(S)$ T≥S, a vector space expansion ector space) .Given $\\boldsymbol{S}$ and V$\\tau$ $\\mathcal{V}:\\mathcal{S}\\rightarrow\\mathcal{T}$ S →T is said to be lossless if ector spaces where the  

Remark. Note that the identity function Id is lossless with its inverse being itself.  

Then we give a few examples of lossless vector space expansions. These examples will also be used in LEMON.  

Example B.3.1 (Vector average expansion $\\mathcal{V}_{\\mathrm{avg.}}$ ).Let $\\mathbf{\\widetilde{x}}\\in\\mathbb{R}^{D_{S}}$ be a vector of dimension $D_{S}$ and its average $\\begin{array}{r}{\\lambda_{V}g(\\mathbf{x})=\\mathbb{E}[\\mathbf{x}]=\\frac{1}{D_{S}}\\sum_{i}^{D_{S}}\\mathbf{x}[i]}\\end{array}$ P].$\\mathbf{x}_{a\\nu g}^{*}$ is called the average expanded xof dimension $D_{T}$  

with $D_{T}\\geq D_{S}$ if  

$$
\\mathbf{x}_{a v g}^{*}=\\mathcal{V}_{a v g}(\\mathbf{x})=C o n c a t\\left[\\underbrace{\\mathbf{x}^{\\mathsf{T}},\\cdots,\\mathbf{x}^{\\mathsf{T}}}_{\\lfloor D_{T}/D s\\rfloor},\\underbrace{A v g(\\mathbf{x}),\\cdots,A v g(\\mathbf{x})}_{D_{T}\\mathrm{~mod~}D_{S}}\\right]^{\\mathsf{T}}\\in\\mathbb{R}^{D_{T}}.
$$  

Example B.3.2 (Vector z o expansion $\\mathcal{V}_{\\mathrm{zero.}}$ ).Le $\\mathbf{x}\\in\\mathbb{R}^{D_{S}}$ be a vector of dimension $D_{S}$ .$\\mathbf{x}_{z e r o}^{*}$ is called the zero expanded xof dimension $D_{T}$ with $D_{T}\\geq D_{S}$ ≥if  

$$
\\begin{array}{r}{\\mathbf{x}_{z e r o}^{*}=\\mathcal{V}_{z e r o}(\\mathbf{x})=C o n c a t\\left[\\underbrace{\\mathbf{x^{\\mathsf{T}}},\\cdots,\\mathbf{x^{\\mathsf{T}}}}_{\\lfloor D_{T}/D_{S}\\rfloor},\\underbrace{0,\\cdots,0}_{D_{T}\\mathrm{~mod~}D_{S}}\\right]^{\\mathsf{T}}\\in\\mathbb{R}^{D_{T}}.}\\end{array}
$$  

Example B.3.3 (Vector circula expansion $\\mathcal{V}_{\\mathrm{circ}})$ Let $\\mathbf{x}\\in\\mathbb{R}^{D_{S}}$ a vector of dimension $D_{S}$ .${\\bf x}_{c i r c}^{*}$ is called the circular expanded xof dimension $D_{T}$ with $D_{T}\\geq D_{S}$ ≥if  

$$
\\begin{array}{r}{\\mathbf{x}_{c i r c}^{*}=\\mathcal{V}_{c i r c}(\\mathbf{x})=C o n c a t\\underbrace{\\left[\\mathbf{x}^{\\mathsf{T}},\\cdots,\\mathbf{x}^{\\mathsf{T}},\\mathbf{x}^{\\mathsf{T}}[\\colon D_{T}\\bmod D_{S}]\\right]^{\\mathsf{T}}\\in\\mathbb{R}^{D_{T}}}_{[D_{T}/D_{S}]}.}\\end{array}
$$  

Example B.3.4 (Vector random expansion $\\mathcal{V}_{\\mathrm{rand.}}$ Let $\\mathbf{\\Deltax}\\in\\mathbb{R}^{D_{S}}$ a vector of dimension $D_{S}$ .${\\bf x}_{r a n d}^{*}$ is called the random expanded xof dimension $D_{T}$ with $D_{T}\\geq D_{S}$ ≥if  

$$
\\begin{array}{r}{\\mathbf{x}_{r a n d}^{*}=\\mathcal{V}_{r a n d}(\\mathbf{x};\\zeta)=C o n c a t\\left[\\underbrace{\\mathbf{x^{\\intercal}},\\cdots,\\mathbf{x^{\\intercal}}}_{\\lfloor D_{T}/D_{S}\\rfloor},\\zeta^{\\intercal}\\right]^{\\intercal}\\in\\mathbb{R}^{D_{T}},}\\end{array}
$$  

where $\\zeta\\in\\mathbb{R}^{D_{T}}$ mod $D_{S}$ is an arbitrary vector.  

Remark. (1) All vector expansion examples above follow the same pattern. Specifically, when $D_{T}$ expanding from di mod s by $D_{S}$ entries differently. (2) The random vector ating $\\textbf{x}\\lfloor D_{T}/D_{S}\\rfloor D_{S}$ ⌊$D_{S}$ ⌋$D_{T}$ number of times. , all vector expansion methods pad first $\\zeta$ in vector random expansion is arbitrary, Each method deals with the remaining $\\lfloor D_{T}/D_{S}\\rfloor D_{S}$ enso $\\mathcal{V}_{a\\nu g}$ ,$\\mathcal{V}_{z e r o}$ ,$\\mathcal{V}_{c i r c}\\subset\\mathcal{V}_{r a n d}$ . (3) Here all three examples are expansion methods for vectors. In practice, neural networks like Transformers are dealing high dimensional tensors. These tensors can essentially be thought of as collections of vectors. In such scenarios, we can apply the expansion methods separately to the last dimension of these tensors.  

In the following claim, we show that vectors expanded by these operators are lossless.  

$\\mathcal{V}_{c i r c}$ V, and vector random expansion m 1. Vector average expansio V$\\gamma_{r a n d}$ $\\mathcal{V}_{a\\nu g},$ are all lossless expansion for vectors. , vector zero expansion $\\mathcal{V}_{z e r o}$ , vector circular expansion Proof. The inverse function $\\mathcal{V}^{-1}:\\mathbb{R}^{D_{T}}\\rightarrow\\mathbb{R}^{D_{S}}$ of these vector expansion methods is  

$$
\\nu^{-1}({\\bf x})={\\bf x}[:D_{S}].
$$  

Remark. In practice, we want inverse mapping of expansion methods to be easily computed just like the example above.

# B.4LOSSLESS EXPANSION FOR OPERATORS
We then give the definition of lossless expansion for operators. These operators apply on tensors, hence our definition of lossless operator expansion is based on lossless expansion in vector space. These operators can be different layers used in Transformer architectures, including LayerNorm, convolutional layers, and fully-connected layers, etc.  

Definit ansio der vector spaces $S^{i n},S^{o u t},\\mathcal{T}^{i n}$ and $\\mathcal{T}^{o u t}$ such that with $g(\\cdot):S^{i n}\\rightarrow S^{o u t}$ ·$n(S^{i n})\\leq d i m(T^{i n})$ S→S or space e T. We say the ope and dim $d i m\\big(S^{\\bar{o}u t}\\big)\\leq d i m\\big(T^{o u t}\\big)$ S$\\mathcal{V}_{i n}:S^{i\\bar{n}}\\to\\mathcal{T}^{i n}$ ≤TEMo is $(\\mathcal{V}_{i n},\\mathcal{V}_{o u t})$ VVess output vector space expansion ppose the op -lossless for $g(\\cdot)$ ·or is denoted if there exist $\\mathcal{V}_{o u t}:S^{o u t}\\to\\mathcal{T}^{o u t}$ VS→T such that V$\\mathcal{V}_{o u t}(g(\\mathbf{x}))=\\mathcal{E}[g](\\mathcal{V}_{i n}(\\mathbf{x})),\\forall\\mathbf{x}\\in S^{i n}$ EV∀∈S .  

$(\\mathcal{V}_{i n},\\mathcal{V}_{o u t})$ Remark. losslessly expanded input, the output of the to be invertible, we do not have restrictions on the operator expansion VV(1) Intuitively, a lossless operator -lossless for the origina $g(\\cdot)$ ·tput. (2) For conciseness, we use ‘ ’ interchangeably. (3) We only require the v Eexpanded oper pansion can be understood a $^{\\cdot}\\mathcal{E}[g]$ Eis a is $(\\mathcal{V}_{i n},\\mathcal{V}_{o u t})$ EVtor expansions .V$\\nu_{o u t}$ ows: when using losslessly expa -lossles V$\\mathcal{V}_{i n}$ and ‘ and $\\mathcal{E}$ V$\\nu_{o u t}$ $\\mathcal{V}_{i n}$ ed",6
