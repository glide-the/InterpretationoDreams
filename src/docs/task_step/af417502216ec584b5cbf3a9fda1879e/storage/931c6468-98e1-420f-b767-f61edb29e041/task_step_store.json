{"task_step_store/data": {"84f82b99-b788-4ad1-b84c-6105c1054610": {"__data__": {"id_": "84f82b99-b788-4ad1-b84c-6105c1054610", "metadata": {}, "relationships": {}, "hash": "", "start_task_context": "\u5927\u6a21\u578b\u4e2d\u7684LayerNorm\u548cRMSNorm\u6709\u4ec0\u4e48\u533a\u522b\uff1f", "aemo_representation_context": "### Step-by-Step Decomposition of Computer Science Research Content\n\n#### 1. \u5206\u6790\u8fd1\u51e0\u5e74\u7814\u7a76\u9886\u57df\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba\n- **\u6280\u672f\u6846\u67b6**\uff1a\u8fd1\u5e74\u6765\uff0c\u6df1\u5ea6\u5b66\u4e60\u3001\u5f3a\u5316\u5b66\u4e60\u3001\u56fe\u795e\u7ecf\u7f51\u7edc\u7b49\u6846\u67b6\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u9886\u57df\u5360\u636e\u4e3b\u5bfc\u5730\u4f4d\u3002\u5177\u4f53\u6846\u67b6\u5982Transformer\u3001BERT\u3001GNN\u7b49\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002\n- **\u65b9\u6cd5\u8bba**\uff1a\u7814\u7a76\u65b9\u6cd5\u9010\u6e10\u4ece\u5355\u4e00\u6a21\u578b\u8f6c\u5411\u591a\u6a21\u578b\u878d\u5408\uff0c\u4ece\u76d1\u7763\u5b66\u4e60\u6269\u5c55\u5230\u65e0\u76d1\u7763\u548c\u81ea\u76d1\u7763\u5b66\u4e60\uff0c\u5f3a\u8c03\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002\n\n#### 2. \u7814\u7a76\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u4e3b\u8981\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0e\u53d8\u4f53\n- **\u5e94\u7528**\uff1a\u4f8b\u5982\uff0cTransformer\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u4e2d\u88ab\u5e7f\u6cdb\u5e94\u7528\uff0cBERT\u53d8\u4f53\u5982RoBERTa\u3001ALBERT\u5728\u6587\u672c\u5206\u7c7b\u3001\u95ee\u7b54\u7cfb\u7edf\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff1bGNN\u5728\u63a8\u8350\u7cfb\u7edf\u3001\u77e5\u8bc6\u56fe\u8c31\u7b49\u9886\u57df\u6709\u663e\u8457\u5e94\u7528\u3002\n- **\u53d8\u4f53**\uff1a\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\uff0c\u7814\u7a76\u8005\u63d0\u51fa\u4e86\u591a\u79cd\u53d8\u4f53\uff0c\u5982\u9488\u5bf9\u957f\u5e8f\u5217\u5904\u7406\u7684Transformer-XL\uff0c\u9488\u5bf9\u591a\u6a21\u6001\u6570\u636e\u7684ViLBERT\u7b49\u3002\n\n#### 3. \u8bc4\u4f30\u5b66\u672f\u754c\u7684\u6280\u672f\u8fdb\u6b65\u4e0e\u5c40\u9650\u6027\n- **\u6280\u672f\u8fdb\u6b65**\uff1a\u6a21\u578b\u6027\u80fd\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\uff0c\u8ba1\u7b97\u6548\u7387\u4e5f\u6709\u6240\u63d0\u9ad8\uff0c\u5982EfficientNet\u5728\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u8868\u73b0\u3002\n- **\u5c40\u9650\u6027**\uff1a\u4f9d\u7136\u5b58\u5728\u6a21\u578b\u504f\u5dee\uff08\u5982\u6027\u522b\u504f\u89c1\uff09\u3001\u6570\u636e\u4f9d\u8d56\uff08\u5982\u5bf9\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\uff09\u7b49\u95ee\u9898\uff0c\u6cdb\u5316\u80fd\u529b\u6709\u5f85\u8fdb\u4e00\u6b65\u63d0\u5347\u3002\n\n#### 4. \u63a2\u8ba8\u8ba1\u7b97\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\n- **\u9002\u7528\u6027**\uff1a\u7814\u7a76\u6a21\u578b\u5728\u4e0d\u540c\u9886\u57df\uff08\u5982\u533b\u7597\u3001\u91d1\u878d\uff09\u548c\u591a\u6a21\u6001\u6570\u636e\uff08\u5982\u6587\u672c+\u56fe\u50cf\uff09\u4e0a\u7684\u8868\u73b0\uff0c\u5982\u591a\u6a21\u6001Transformer\u6a21\u578b\u3002\n- **\u6cdb\u5316\u80fd\u529b**\uff1a\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\u3001\u5143\u5b66\u4e60\u7b49\u65b9\u6cd5\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f7f\u5176\u80fd\u591f\u5e94\u5bf9\u66f4\u591a\u73b0\u5b9e\u5e94\u7528\u573a\u666f\u3002\n\n#### 5. \u5206\u6790\u6700\u65b0\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027\n- **\u7a33\u5b9a\u6027**\uff1a\u7814\u7a76\u7b97\u6cd5\u5728\u590d\u6742\u3001\u52a8\u6001\u73af\u5883\u4e0b\u7684\u8868\u73b0\uff0c\u5982\u5bf9\u6297\u8bad\u7ec3\u3001\u9c81\u68d2\u4f18\u5316\u7b49\u65b9\u6cd5\u7684\u5e94\u7528\u3002\n- **\u5bb9\u9519\u6027**\uff1a\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\u7684\u9002\u5e94\u6027\uff0c\u5982\u5206\u5e03\u5f0f\u8bad\u7ec3\u4e2d\u7684\u68af\u5ea6\u538b\u7f29\u3001\u6a21\u578b\u5e76\u884c\u7b49\u6280\u672f\u3002\n\n#### 6. \u8bc4\u4f30\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218\n- **\u672a\u6765\u65b9\u5411**\uff1a\u5982\u53ef\u89e3\u91caAI\u3001\u8054\u90a6\u5b66\u4e60\u3001\u9690\u79c1\u4fdd\u62a4\u7b49\u65b0\u5174\u7814\u7a76\u65b9\u5411\u3002\n- **\u6311\u6218**\uff1a\u5982\u4f55\u5e73\u8861\u6a21\u578b\u6027\u80fd\u4e0e\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\uff0c\u5982\u4f55\u89e3\u51b3\u6570\u636e\u504f\u89c1\u548c\u4f26\u7406\u95ee\u9898\u7b49\u3002\n\n### \u5927\u6a21\u578b\u4e2d\u7684LayerNorm\u548cRMSNorm\u7684\u533a\u522b\n\n#### LayerNorm\uff08Layer Normalization\uff09\n- **\u5b9a\u4e49**\uff1aLayerNorm\u662f\u5bf9\u6bcf\u4e2a\u6837\u672c\u7684\u6bcf\u4e2a\u7279\u5f81\u8fdb\u884c\u5f52\u4e00\u5316\uff0c\u4f7f\u5176\u5747\u503c\u548c\u65b9\u5dee\u5206\u522b\u63a5\u8fd10\u548c1\u3002\n- **\u516c\u5f0f**\uff1a\n  \\[\n  \\text{LayerNorm}(x) = \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} \\cdot \\gamma + \\beta\n  \\]\n  \u5176\u4e2d\uff0c\\(\\mu\\)\u548c\\(\\sigma^2\\)\u5206\u522b\u662f\u7279\u5f81\u5747\u503c\u548c\u65b9\u5dee\uff0c\\(\\gamma\\)\u548c\\(\\beta\\)\u662f\u53ef\u5b66\u4e60\u53c2\u6570\uff0c\\(\\epsilon\\)\u662f\u5fae\u5c0f\u5e38\u6570\u4ee5\u9632\u6b62\u9664\u96f6\u3002\n- **\u5e94\u7528**\uff1a\u5e7f\u6cdb\u7528\u4e8eTransformer\u6a21\u578b\u4e2d\uff0c\u6709\u52a9\u4e8e\u7f13\u89e3\u68af\u5ea6\u6d88\u5931\u95ee\u9898\uff0c\u63d0\u5347\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002\n\n#### RMSNorm\uff08Root Mean Square Normalization\uff09\n- **\u5b9a\u4e49**\uff1aRMSNorm\u662f\u5bf9\u6bcf\u4e2a\u6837\u672c\u7684\u6bcf\u4e2a\u7279\u5f81\u8fdb\u884c\u5f52\u4e00\u5316\uff0c\u4f46\u53ea\u5f52\u4e00\u5316\u65b9\u5dee\uff0c\u4e0d\u5f52\u4e00\u5316\u5747\u503c\u3002\n- **\u516c\u5f0f**\uff1a\n  \\[\n  \\text{RMSNorm}(x) = \\frac{x}{\\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} x_i^2 + \\epsilon}} \\cdot \\gamma + \\beta\n  \\]\n  \u5176\u4e2d\uff0c\\(n\\)\u662f\u7279\u5f81\u6570\u91cf\uff0c\\(\\gamma\\)\u548c\\(\\beta\\)\u662f\u53ef\u5b66\u4e60\u53c2\u6570\uff0c\\(\\epsilon\\)\u662f\u5fae\u5c0f\u5e38\u6570\u3002\n- **\u5e94\u7528**\uff1a\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0cRMSNorm\u6bd4LayerNorm\u66f4\u7b80\u5355\uff0c\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\uff0c\u5c24\u5176\u5728\u5927\u89c4\u6a21\u6a21\u578b\u8bad\u7ec3\u4e2d\u3002\n\n#### \u4e3b\u8981\u533a\u522b\n1. **\u5f52\u4e00\u5316\u65b9\u5f0f**\uff1a\n   - LayerNorm\u540c\u65f6\u5f52\u4e00\u5316\u5747\u503c\u548c\u65b9\u5dee\u3002\n   - RMSNorm\u53ea\u5f52\u4e00\u5316\u65b9\u5dee\u3002\n2. **\u8ba1\u7b97\u590d\u6742\u5ea6**\uff1a\n   - LayerNorm\u9700\u8981\u8ba1\u7b97\u5747\u503c\u548c\u65b9\u5dee\uff0c\u8ba1\u7b97\u91cf\u7a0d\u5927\u3002\n   - RMSNorm\u53ea\u9700\u8ba1\u7b97\u65b9\u5dee\uff0c\u8ba1\u7b97\u91cf\u8f83\u5c0f\u3002\n3. **\u6548\u679c**\uff1a\n   - LayerNorm\u5728\u8bb8\u591a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u5728NLP\u9886\u57df\u3002\n   - RMSNorm\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u8868\u73b0\u76f8\u4f3c\u751a\u81f3\u66f4\u4f18\uff0c\u4e14\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u3002\n\n\u901a\u8fc7\u4e0a\u8ff0\u5206\u6790\uff0c\u53ef\u4ee5\u66f4\u597d\u5730\u7406\u89e3LayerNorm\u548cRMSNorm\u5728\u5927\u6a21\u578b\u4e2d\u7684\u5e94\u7528\u53ca\u5176\u4f18\u7f3a\u70b9\uff0c\u4ece\u800c\u5728\u5177\u4f53\u4efb\u52a1\u4e2d\u9009\u62e9\u5408\u9002\u7684\u5f52\u4e00\u5316\u65b9\u6cd5\u3002", "task_step_name": "\u5206\u6790\u8fd1\u51e0\u5e74\u7814\u7a76\u9886\u57df\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba", "task_step_description": "\u8fd1\u5e74\u6765\uff0c\u6df1\u5ea6\u5b66\u4e60\u3001\u5f3a\u5316\u5b66\u4e60\u3001\u56fe\u795e\u7ecf\u7f51\u7edc\u7b49\u6846\u67b6\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u9886\u57df\u5360\u636e\u4e3b\u5bfc\u5730\u4f4d\u3002\u5177\u4f53\u6846\u67b6\u5982Transformer\u3001BERT\u3001GNN\u7b49\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002\u7814\u7a76\u65b9\u6cd5\u9010\u6e10\u4ece\u5355\u4e00\u6a21\u578b\u8f6c\u5411\u591a\u6a21\u578b\u878d\u5408\uff0c\u4ece\u76d1\u7763\u5b66\u4e60\u6269\u5c55\u5230\u65e0\u76d1\u7763\u548c\u81ea\u76d1\u7763\u5b66\u4e60\uff0c\u5f3a\u8c03\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "task_step_level": "0", "task_step_question": "### \u95ee\u9898\n\n\u5728\u5206\u6790\u8fd1\u51e0\u5e74\u7814\u7a76\u9886\u57df\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba\u65f6\uff0c\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u5f52\u4e00\u5316\u6280\u672f\uff08\u5982LayerNorm\u548cRMSNorm\uff09\u5728Transformer\u7b49\u4e3b\u6d41\u6846\u67b6\u4e2d\u626e\u6f14\u4e86\u91cd\u8981\u89d2\u8272\u3002\u7ed3\u5408\u5f53\u524d\u4efb\u52a1\u6b65\u9aa4\u7684\u63cf\u8ff0\uff0c**LayerNorm\u548cRMSNorm\u5728Transformer\u6846\u67b6\u4e2d\u7684\u5e94\u7528\u6709\u54ea\u4e9b\u5177\u4f53\u5dee\u5f02\uff1f\u8fd9\u4e9b\u5dee\u5f02\u5982\u4f55\u5f71\u54cd\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff1f**", "task_step_question_context": [{"ref_id": "454845529630447100", "chunk_id": "0", "score": 0.404296875, "text": "# 3.3 A TRANSFORMATION PER BLOCK\nNow that every LayerNorm in the transformer has been converted to RMSNorm, we can select any $\\mathbf{Q}$ to modify the model. Our initial plan was to collect signals from the model, construct an orthogonal matrix using those signals and to delete parts of the network. We quickly saw that the signals at different blocks of the network were not aligned, and that we would need to apply a different orthogonal matrix at each block, $\\mathbf{Q}_{\\ell}$ .  \n\nAllowing the orthogonal matrix used in each block to differ can be shown to leave the model unchanged using the same proof as Theorem 1 ,  \n\n  \nFigure 3: Converting a transformer network from LayerNorm to RMSNorm: the scale matrix diag $(\\alpha)$ is absorbed into the subsequent matrix $\\mathbf{W}_{\\mathrm{in}}$ . Figure shows the block in combined colors. We use $(\\alpha)$ for brevity. The mean-subtraction matrix $\\mathbf{M}$ is applied to each matrix $\\mathbf{W}_{\\mathrm{out}}$ . Layernorm becomes RMSNorm, up to a constant $\\bar{\\sqrt{D}}$ (not shown). Here, the scaling $(\\alpha^{\\prime})$ comes from the previous block.  \n\n  \nFigure 4: With the network converted to RMSNorm (see Figure 3 ), we apply the computational-invariance idea. The input weight matrices $\\mathrm{diag}(\\alpha)\\mathbf{W}_{\\mathrm{in}}$ are pre-multiplied by $\\mathbf{Q}^{\\top}$ . The output matrices $\\mathbf{W}_{\\mathrm{out}}\\mathbf{M}$ are post-multiplied by $\\mathbf{Q}$ . In the skip-connection, a new linear layer is added $\\mathbf{Q}_{\\ell}^{\\top}\\mathbf{Q}_{\\ell+1}$ . After these modifications, the matrices can be sliced (hatched areas).  \n\nwith the exception of line 5 of Algorithm 1 . Here we see that the residual connection and the output of the block must have the same rotation. To fix this, we modify the residual connection by applying the linear transformation applied to different blocks with the additional linear operation in the residual connection. Unlike the $\\mathbf{Q}_{\\ell-1}^{\\top}\\mathbf{Q}_{\\ell}$ \u2212to the residual. Figure 4 shows how different rotations can be modifications to the weight matrices, these additional operations cannot be pre-computed and add a small $(D\\times D)$ overhead to the model. Nonetheless, they are needed to allow slicing the model (Section 3.4 ) and we see real speedup overall (Section 4 ).  \n\nTo compute the matrices $\\mathbf{Q}_{\\ell}$ , we use PCA. We select a calibration dataset from the training set, run it through the model (after converting LayerNorm operations into RMSNorm), and extract the orthogonal matrix of the layer. We use the output of the transformed network to calculate the orthogonal matrices of the next layers. More precisely, if $\\mathbf{X}_{\\ell,i}$ is the output of the $\\ell^{\\mathrm{th}}$ RMSNorm block for the $i^{\\mathrm{th}}$ sequence in the calibration dataset, we compute  \n\n$$\n\\mathbf{C}_{\\ell}=\\sum_{i}\\mathbf{X}_{\\ell,i}^{\\top}\\mathbf{X}_{\\ell,i}\n$$  \n\nand set $\\mathbf{Q}_{\\ell}$ to the be the eigenvectors of $\\mathbf{C}_{\\ell}$ , sorted by decreasing eigenvalues.\n\n# 3.4 SLICING\nThe goal of Principal Component Analysis is usually to take a data matrix $\\mathbf{X}$ and compute a lower dimensional representation $\\mathbf{Z}$ , and an approximate reconstruction $\\tilde{\\mathbf{X}}$ :  \n\n$$\n\\mathbf{Z}=\\mathbf{X}\\mathbf{Q}\\mathbf{D}\\,,\\qquad\\tilde{\\mathbf{X}}=\\mathbf{Z}\\mathbf{D}^{\\top}\\mathbf{Q}^{\\top}\\,.\n$$  \n\nwhere $\\mathbf{Q}$ is the ectors of ${\\bf X}^{\\top}{\\bf X}$ , and $\\mathbf{D}$ is a $D\\times D_{\\mathrm{small}}$ deletion matrix (containing $D_{\\mathrm{small}}$ The reconstruction is columns of the $D\\times D$ \u00d7$L_{2}$ identity matrix), which removes some of the columns of the matrix to the left. optimal, in the sense that QD is a linear mapping that minimizes $\\lVert\\mathbf{X}-\\tilde{\\mathbf{X}}\\rVert^{2}$ .  \n\nWhen we apply PCA to the signal matrix $\\mathbf{X}$ bween blocks, we never materialize the $N\\times D$ signal matrix, but we apply the deletion matrix Dto the operations preceding and succeeding the construction of that matrix, which have already been multiplied by $\\mathbf{Q}$ in the above. We delete rows of $\\mathbf{W}_{\\mathrm{in}}$ that we have inserted into the residual connection (see Figure and columns of $\\mathbf{W}_{\\mathrm{out}}$ and $\\mathbf{W}_{\\mathrm{embd}}$ . We also delete both rows 4 ). and columns of the matrix $\\mathbf{Q}_{\\ell-1}^{\\top}\\mathbf{Q}_{\\ell}$ \u2212\n\n# 4 EXPERIMENTAL VALIDATION\nSetup We use HuggingFace Transformers ( Wolf et al. ,2019 ) to implement our code with PyTorch (Paszke et al. ,2019 ). The computation of $\\mathbf{Q}$ is performed on a single H100 GPU with 80GB of memory, taking approximately 3.5 hours to complete for the L LAMA -2 70B model. During the PCA calculation, we use double precision for computing the eigenvectors of the covariance matrix. We find that using single precision for eigenvector calculations in PyTorch leads to a discrepancy in the final accuracy, as detailed in Appendix A.2 .  \n\nWe experiment with two different calibration sets: 1024 samples from the WikiText-2 training dataset ( Merity et al. ,2016 ) and 5000 samples from the Alpaca training dataset ( Taori et al. ,2023 ). Sequence lengths are chosen as the maximum of each language model. An ablation study on the calibration set size and sequence length is presented in Appendix A.3 .  \n\nModels, Tasks, and GPUs We evaluate all our experiments on OPT ( Zhang et al. ,2022 ), L LAMA -2 (Touvron et al. ,2023 ) model families, and additionally evaluate Phi-2 (in our zero-shot task) experiments. We exclude OPT 175B, as it is outperformed by smaller L LAMA -2 models. Nonetheless, we anticipate that this larger model will yield improved results, as larger models typically offer more promising opportunities for compression (see Section 4.1 ). We evaluate our scheme on both language generation as well as popular zero-shot tasks. To demonstrate the comprehensive speedup achieved by SliceGPT we use: Quadro RTX6000 GPUs with 24GB of memory as a representative example of consumer-level GPUs; 40GB A100s and 80GB H100s to provide datacenter-level benchmarks.  \n\nBaseline Setup We initially planned to compare our results against a scheme that pruned columns (or rows) with the smallest norm but found that this baseline was very poor, with the perplexity of the model soaring into the 1000s after pruning just a few columns. Instead, we compare SliceGPT against SparseGPT ( Frantar & Alistarh ,2023 ) employing a 2:4 sparsity ratio, as this is the only sparsity scheme which achieves speedup ( Mishra et al. ,2021 )."}, {"ref_id": "454847042436311108", "chunk_id": "4", "score": 0.279296875, "text": "# 3 Experiments and Results\nWe evaluate the performance of PreNorm and PostNorm for ZST on various datasets and language pairs. We then analyze the off-target rates and structural discrepancies between PreNorm and PostNorm to understand performance differences.  \n\n$$\n\\mathrm{LayerNorm}(\\mathbf{x})=\\frac{\\mathbf{x}-\\mathbf{E}(\\mathbf{x})}{\\sqrt{\\mathbf{V}(\\mathbf{x})}}\\cdot\\mathbf{g}+\\mathbf{b},\n$$  \n\nwhere $\\mathbf{g}$ and $\\mathbf{b}$ are trainable gain and bias. $\\mathbf{E}$ and $\\mathbf{V}$ indicate expectation and variance. LayerNorm is commonly used in two positions in the Transformer, as shown in Fig. 1 . PostNorm, which is the originally proposed setting of the Transformer ( Vaswani et al. ,2017 ), involves applying LayerNorm after each sub-module (i.e., selfattention or feed-forward network) and residual connections. PreNorm ( Baevski and Auli ,2019 ), on the other hand, involves applying LayerNorm directly before each sub-module and is known to stabilize Transformer training. While variants of Transformer LayerNorm like RMSNorm ( Zhang and Sennrich ,2019 ) have been proposed, the vanilla PreNorm and PostNorm are still the most widely adopted settings in current multilingual\n\n# 3.1 Experimental Settings\nDatasets We perform ZST experiments on three datasets: OPUS ( Zhang et al. ,2020 ), IWSLT ( Cettolo et al. ,2017 ), and Europarl ( Koehn ,2005 ). The statistics of the datasets are summarized in Table 1 .We include 7 ,4 , and 5 languages for each dataset. The training data consists of only English-centric sentence pairs, resulting in 30 ,6 , and 12 ZST directions for each dataset. The total number of parallel sentences for each dataset is 12 .00 M, 1 .38 M, and 15 .78 M, respectively. We apply BPE ( Sennrich et al. ,2016 ) with merge operations of 50 k, 40 k, and $50\\mathbf{k}$ to create a joint vocabulary for each dataset.  \n\nTraining We employ Transformer-base model for OPUS and IWSLT, and Transformer-big for Europarl, in accordance with the distinct sizes of training data. We consider the following settings: (1) PreNorm or PostNorm : PreNorm involves LayerNorm directly before each sub-module (i.e., self-attention or feed-forward network), while PostNorm applies LayerNorm after each sub-module and residual connections, as shown in Fig. 1 .(2) S-ENC-T-DEC or T-ENC : Source language tag on the encoder-side and target language tag on the decoder-side; or only target language tag on the encoder-side. Wu et al. (2021 ) showed that this setting impacts ZST for Transformer with PreNorm. (3) w/ or w/o Res. : With the residual connection for self-attention in the middle $(4^{t h})$ encoder layer or not. Liu et al. (2021 ) revealed that \u201cw/o Res.\u201d improves ZST for the model trained with PreNorm. We experiment this with different LayerNorm settings as this may reduce the potential of overfitting on supervised directions, then further impacts ZST, which aligns with our hypothesis.  \n\nTable 2: BLEU scores and off-target rates (shown in brackets) . We report the average score of three seeds; refer to Appendix Gfor BLEU score of each translation direction and seed. \u201cRes.\u201d indicates the residual connection of self-attention in the $4^{t h}$ encoder layer. We mark lower off-target rates and significantly higher BLEU scores ( Koehn ,2004 ) between PreNorm and PostNorm in bold for ZST.   \n\n\n<html><body><table><tr><td>#</td><td>Layer Norm</td><td>Language Tag</td><td>Res.</td><td></td><td>Zero-shot</td><td></td><td></td><td>Supervised</td><td></td></tr><tr><td>0</td><td></td><td>Pivot</td><td></td><td>OPUS 21.8</td><td>IWSLT 20.0</td><td>Europarl 29.5</td><td>OPUS</td><td>IWSLT</td><td>Europarl</td></tr><tr><td>1</td><td>PreNorm</td><td>S-ENC-T-DEC</td><td>w/</td><td>10.1 (42.19%)</td><td>4.9 (64.84%)</td><td>24.9 ( 7.73%)</td><td>33.7</td><td>31.5</td><td>34.3</td></tr><tr><td>2</td><td>PostNorm</td><td>S-ENC-T-DEC</td><td>w/</td><td>16.8 ( 8.59%)</td><td>12.4 (10.61%)</td><td>29.2( 0.34%)</td><td>33.9</td><td>31.5</td><td>34.5</td></tr><tr><td>3</td><td>PreNorm</td><td>T-ENC</td><td>w/</td><td>13.3 (22.99%)</td><td>13.7 ( 3.98%)</td><td>29.5( 0.23%)</td><td>33.7</td><td>31.6</td><td>34.4</td></tr><tr><td>4</td><td>PostNorm</td><td>T-ENC</td><td>w/</td><td>14.0 (22.86%)</td><td>15.5 ( 4.59%)</td><td>30.8 ( 0.11%)</td><td>34.1</td><td>31.5</td><td>34.5</td></tr><tr><td>5</td><td>PreNorm</td><td>S-ENC-T-DEC</td><td>w/o</td><td>14.3 (20.67%)</td><td>8.0 (50.16%)</td><td>16.7 (41.87%)</td><td>33.6</td><td>30.9</td><td>34.3</td></tr><tr><td>6</td><td>PostNorm</td><td>S-ENC-T-DEC</td><td>w/o</td><td>16.0 (15.27%)</td><td>17.4 (1.83%)</td><td>29.0 ( 0.41%)</td><td>33.8</td><td>30.7</td><td>34.4</td></tr><tr><td>7</td><td>PreNorm</td><td>T-ENC</td><td>w/o</td><td>13.4 (27.15%)</td><td>16.2 ( 1.54%)</td><td>29.9 ( 2.15%)</td><td>33.5</td><td>30.9</td><td>34.3</td></tr><tr><td>8</td><td>PostNorm</td><td>T-ENC</td><td>w/o</td><td>13.9 (26.68%)</td><td>17.8 (1.50%)</td><td>30.8 ( 0.13%)</td><td>33.9</td><td>30.6</td><td>34.4</td></tr></table></body></html>  \n\nThe settings above lead to eight different combinations, shown in Table 2 (#1 - #8). Additional training details are in Appendix A .\n\n# 3.2 Main Results\nWe evaluate ZST systems using SacreBLEU ( Post ,2018 ) and off-target rates. We report in Table 2 BLEU scores for both zero-shot and supervised directions. For ZST, we also present pivot-based translation results as a reference. Implementation details of evaluation can be found in Appendix B.Our findings are as follows:  \n\nPreNorm vs. PostNorm :We find that PostNorm consistently yields better BLEU scores than PreNorm for ZST across various language tag and residual connection settings, while their performance is comparable for supervised directions.  \n\nImpact of Language Tag and Residual Connection: We observe that using the \u201cT-ENC\u201d language tag and \u201cw/ Res.\u201d improves ZST performance for IWSLT, which aligns with the findings of $\\mathrm{Wu}$ et al. (2021 ) and Liu et al. (2021 ). Nevertheless, the best performance is achieved using \u201cw/ Res.\u201d for PostNorm with \u201cS-ENC-T-DEC\u201d and \u201cT-ENC\u201d tags for OPUS and Europarl, respectively (#2 and #4). Given that Wu et al. (2021 ) and Liu et al. (2021 )used PreNorm as the default setting (#2, #4, #6 and #8 are unreported results in their work), our results emphasize the need to consider PostNorm as the default setting for ZST, while the language tag and residual connection settings have less impact.  \n\nOff-target Rates : Off-target rates help understand the different BLEU score gaps between PreNorm and PostNorm, which ranges from 0 .5 to 12 .3 BLEU points. For PreNorm and PostNorm with the \u201cT-ENC\u201d language tag (#3, #4, #7, and #8), they have similar off-target rates, with a discrepancy ranging from $-0.61\\%$ to $2.02\\%$ , which results in narrow BLEU score gaps, ranging from 0 .5 to 1 .8 points. However, for PreNorm and PostNorm with the \u201cS-ENC-T-DEC\u201d language tag (#1, #2, #5, and #6), the off-target rates show a more considerable discrepancy, ranging from $5.40\\%$ to $54.23\\%$ , resulting in BLEU score gaps from 1 .7 to 12 .3 points. Further analysis of the nature of Transformer hidden states in the next section explores the reason for these different off-target rates in translations."}, {"ref_id": "454895409734360760", "chunk_id": "3", "score": 0.271484375, "text": "# 5 I NTUITIONS BEHIND LAYER NORM TUNING\nIn this section, driven by the empirical success of LayerNorm tuning, we explore the intuitions behind LayerNorm from three perspectives, domain adaptation, expressive power, and gradient variance.  \n\nTable 3: Model performance on different data types. Methods with 80K and Conv.20K suffix are tuned on the full 80K data and the 20K conversational data, respectively.   \n\n\n<html><body><table><tr><td>Method</td><td>MME</td><td>VQAv2</td><td>MSCOCO</td><td>Flickr30k</td><td>POPE</td></tr><tr><td colspan=\"6\">MM-V1CUNA-7B</td></tr><tr><td>Finetune-80K</td><td>625.2/270.7</td><td>15.40</td><td>67.50</td><td>34.61</td><td>73.8/76.5/66.5</td></tr><tr><td>LayerNorm-80K</td><td>723.2/253.2</td><td>17.06</td><td>80.89</td><td>48.01</td><td>76.1/81.1/70.8</td></tr><tr><td>LayerNorm-Conv. 20K</td><td>777.1/231.4</td><td>15.39</td><td>67.30</td><td>40.33</td><td>75.2/79.2/68.8</td></tr><tr><td colspan=\"6\">MM-LLAMA2-7B</td></tr><tr><td>Finetune-80K</td><td>661.3/237.1</td><td>16.09</td><td>65.08</td><td>31.64</td><td>56.3/65.0/55.4</td></tr><tr><td>LayerNorm-80K</td><td>583.2/200.7</td><td>16.78</td><td>88.85</td><td>49.24</td><td>66.6/68.5/64.9</td></tr><tr><td>LayerNorm-Conv. 20K</td><td>376.2/157.5</td><td>16.19</td><td>86.80</td><td>44.88</td><td>50.5/50.7/50.3</td></tr><tr><td colspan=\"6\">MM-LLAMA2-CHAT-7B</td></tr><tr><td>Finetune-80K</td><td>805.4/234.6</td><td>15.29</td><td>57.40</td><td>26.70</td><td>60.3/69.8/57.9</td></tr><tr><td>LayerNorm-80K</td><td>651.3/219.3</td><td>16.60</td><td>75.34</td><td>43.75</td><td>71.3/72.4/67.8</td></tr><tr><td>LayerNorm-Conv. 20K</td><td>482.9/172.1</td><td>13.88</td><td>66.85</td><td>41.95</td><td>62.7/71.7/61.3</td></tr><tr><td colspan=\"6\">MM-LLAMA2-13B</td></tr><tr><td>Finetune-80K</td><td>402.3/199.3</td><td>18.33</td><td>73.88</td><td>45.33</td><td>51.6/51.1/52.2</td></tr><tr><td>LayerNorm-80K</td><td>526.0/177.5</td><td>15.31</td><td>82.92</td><td>48.42</td><td>60.0/69.1/58.9</td></tr><tr><td>LayerNorm-Conv. 20K</td><td>646.0/242.9</td><td>16.01</td><td>76.50</td><td>44.86</td><td>70.0/76.9/68.6</td></tr><tr><td colspan=\"6\">MM-LLAMA2-CHAT-13B</td></tr><tr><td>Finetune-80K</td><td>623.3/221.4</td><td>15.17</td><td>64.19</td><td>41.82</td><td>67.6/64.8/64.5</td></tr><tr><td>LayerNorm-80K</td><td>929.3/254.3</td><td>16.10</td><td>74.96</td><td>42.79</td><td>78.9/83.9/74.3</td></tr><tr><td>LayerNorm-Conv. 20K</td><td>769.7/227.5</td><td>15.57</td><td>73.30</td><td>43.08</td><td>68.2/72.8/65.3</td></tr></table></body></html>  \n\nTable 4: Results of models with LayerNorm and/or vision-language Connector activated.   \n\n\n<html><body><table><tr><td>Method</td><td>MME</td><td>VQAv2</td><td>MSCOCO</td><td>Flickr30k</td><td>POPE</td></tr><tr><td colspan=\"6\">MM-LLAMA2-7B</td></tr><tr><td>LayerNorm + Connector</td><td>583.2/200.7</td><td>16.78</td><td>88.85</td><td>49.24</td><td>66.6/68.5/64.9</td></tr><tr><td>Connector</td><td>311.1/105.4</td><td>12.72</td><td>60.43</td><td>35.91</td><td>67.9/73.7/66.9</td></tr><tr><td>LayerNorm</td><td>395.0/191.4</td><td>18.18</td><td>80.13</td><td>41.68</td><td>50.3/51.3/50.2</td></tr><tr><td colspan=\"6\">MM-LLAMA2-13B</td></tr><tr><td>LayerNorm + Connector</td><td>526.0/177.5</td><td>15.31</td><td>82.92</td><td>48.42</td><td>60.0/69.1/58.9</td></tr><tr><td>Connector</td><td>507.0/187.9</td><td>15.22</td><td>62.60</td><td>25.13</td><td>60.9/66.8/60.1</td></tr><tr><td>LayerNorm</td><td>405.0/188.6</td><td>16.51</td><td>70.41</td><td>39.86</td><td>50.9/52.7/51.0</td></tr></table></body></html>\n\n# 5.1 LAYER NORM TUNING A DAPTS LLM S TO MULTI -M ODAL\nInfluence of the Vision-Language Connector The vision-language connector serves as the converter to project features from the vision encoder to the LLM domain. In our previous experiments, we focused on finetuning the LLM component of the MLLMs while keeping the vision-language connector activated by default. To determine which component plays a more important role for domain adaptation of LLM to multi-modal domain, we performed an ablation study by activating the two components separately. Results are presented in table 4 , tuning LayerNorm in attention blocks without activating the vision-language connector resulted in only a $4.2\\%$ and $5.4\\%$ decrease in performance on three traditional multi-modal tasks and the MME benchmark, respectively. This decrease is significantly lower than the $15.6\\%$ and $9.2\\%$ downgrade observed when only activating the Connector on the same tasks. This observation highlights the vital role LayerNorm plays in transforming knowledge from the vision domain to language, indicating LayerNorm as a strong domain adaptor for the LLM architecture.  \n\n  \n\nFigure 3: Layer similarities between different LLM layers in (a) Finetuned and (b) LayerNorm-tuned MM-V ICUNA -7B. The average layer similarity of two models are 0.624 and 0.585, respectively.  \n\nTable 5: Results of models with LL A MA2 Finetuned/LayerNorm-tuned with ViT pre-trained on ImageNet (Deng et al. ,2009 ), which have not been aligned with the language domain.   \n\n\n<html><body><table><tr><td></td><td>MME</td><td>VQAv2</td><td>MSCOCO</td><td>Flickr30k</td><td>POPE</td></tr><tr><td>Finetune-7B</td><td>406.79/182.5</td><td>15.05</td><td>47.75</td><td>18.97</td><td>50.0/51.6/50.1</td></tr><tr><td>LayerNorm-7B</td><td>301.51/127.14</td><td>15.48</td><td>66.22</td><td>31.73</td><td>50.0/50.1/50.1</td></tr><tr><td>Finetune-13B</td><td>375.41/171.79</td><td>25.38</td><td>51.26</td><td>25.96</td><td>50.3/51.1/51.0</td></tr><tr><td>LayerNorm-13B</td><td>445.98/150.0</td><td>15.59</td><td>64.63</td><td>32.17</td><td>51.2/53.0/50.8</td></tr></table></body></html>  \n\nSwitching Visual Features. We employ the ViT encoder from CLIP ( Radford et al. ,2021 ) by default in our previous experiments. CLIP ( Radford et al. ,2021 ) models are trained with image-text contrastive loss, thus its feature space is already aligned with language. Since LayerNorm has shown its effectiveness as a domain adaptor, we are interested in testing whether or not LayerNorm tuning can adapt a LLM to image features that are not pretrained to align with language. The vision encoder is switched to a ViT model that was pretrained on ImageNet (Dosovitskiy et al. ,2021 ;Deng et al. ,2009 ). Results in table 5 demonstrate that both LayerNorm and finetuning approaches can yield high performance. Interestingly, we observe that by LayerNorm tuning with ImageNet trained ViT, which has not been aligned with language, the model is able to achieve comparable performance to full parameter finetuning , i.e ., results show that LayerNorm tuning outperforms finetuning by $12.0\\%$ on captioning tasks, but performs slightly worse by $5.0\\%$ on the MME benchmark. These results again indicates the domain adaptor role of the LayerNorm , hinting the reason behind the empircal success of LayerNorm tuning. Furthermore, it is worth noting that the performance of MLLMs incorporating ViT pretrained on ImageNet is generally inferior to that of CLIP\u2019s vision encoder. This observation provides compelling evidence that, despite differences in tokenizer and training paradigm between CLIP\u2019s text encoder and LL A MA\u2019s, ViT from CLIP has the capacity to learn general patterns of language formulation during pre-training. Thus, significantly enhance MLLM abilities.\n\n# 5.2 LAYER NORM TUNING I MPROVES THE EXPRESSIVE POWER\nIt is shown in Pires et al. (2023 ) that a Transformer model incorporating anisotropic layer representation can capture a wider range of learning patterns. By computing the cosine similarities between all layers in the LLM of a finetuned MLLM, we aim to investigate whether the improved efficiency is the results of the improved expressive power. In table 6 , we present the average layer similarity of three 7B scale MLLMs, and in fig. 3 we present the visualization of per layer similarity scores of MM-V ICUNA -7B. Our analysis reveals that the transformer layers in the MLLMs with LayerNorm tuning exhibit a clear distinction from one another ( i.e ., an average $10.6\\%$ lower layer similarities comparing finetuning), indicating superior generalization ability and expressive power compared to finetuning. This finding sheds light on why tuning LayerNorm is effective for multi-modal LLM training. For additional visualizations, please refer to the Appendix A.2.1 .  \n\n  \nFigure 4: Gradients of the input LayerNorm in the 11th layer of the MM-V ICUNA as training proceeds. LayerNorm-tuned model has lower gradient variance than full parameter finetuning.  \n\nTable 6: Layer representation similarity of LayerNorm and finetuning methods on three 7B MLLMs.   \nLower the similarity is, the better expressive power a model possesses.  \n\n<html><body><table><tr><td>Model</td><td>LayerNorm Sim.</td><td>Finetuning Sim.</td></tr><tr><td>MM-VICUNA</td><td>0.585</td><td>0.624</td></tr><tr><td>MM-LLAMA2</td><td>0.504</td><td>0.591</td></tr><tr><td>MM-LLAMA2-CHAT</td><td>0.550</td><td>0.617</td></tr></table></body></html>"}], "task_step_question_answer": "\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\uff0c\u901a\u4fe1\u6548\u7387\u7684\u63d0\u5347\u662f\u4e00\u4e2a\u5173\u952e\u95ee\u9898\uff0c\u7814\u7a76\u8005\u4eec\u901a\u8fc7\u6a21\u578b\u538b\u7f29\u3001\u68af\u5ea6\u91cf\u5316\u548c\u5f02\u6b65\u66f4\u65b0\u7b49\u6280\u672f\u6765\u51cf\u5c11\u901a\u4fe1\u5f00\u9500\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7684\u6027\u80fd\u3002\u5dee\u5206\u9690\u79c1\u6280\u672f\u901a\u8fc7\u5728\u6570\u636e\u6216\u6a21\u578b\u4e2d\u6dfb\u52a0\u566a\u58f0\uff0c\u4fdd\u62a4\u4e2a\u4f53\u9690\u79c1\uff0c\u540c\u65f6\u5c3d\u91cf\u4e0d\u5f71\u54cd\u6a21\u578b\u7684\u6574\u4f53\u6027\u80fd\u3002\u8fd9\u4e9b\u6280\u672f\u5728\u533b\u7597\u3001\u91d1\u878d\u7b49\u5bf9\u9690\u79c1\u8981\u6c42\u6781\u9ad8\u7684\u9886\u57df\u5c55\u73b0\u4e86\u5de8\u5927\u7684\u5e94\u7528\u6f5c\u529b\u3002\u6b64\u5916\uff0cTransformer\u6846\u67b6\u4e2d\u7684\u5f52\u4e00\u5316\u6280\u672f\uff0c\u5982Layer Normalization\uff0c\u5bf9BERT\u7684\u9884\u8bad\u7ec3\u6548\u679c\u6709\u663e\u8457\u5f71\u54cd\uff0c\u5b83\u901a\u8fc7\u7a33\u5b9a\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u52a0\u901f\u6536\u655b\uff0c\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002GNN\u4e2d\u7684\u56fe\u5377\u79ef\u64cd\u4f5c\u4e0eTransformer\u7684\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7684\u7ed3\u5408\uff0c\u53ef\u4ee5\u66f4\u597d\u5730\u6355\u6349\u56fe\u7ed3\u6784\u4e2d\u7684\u5168\u5c40\u548c\u5c40\u90e8\u4fe1\u606f\uff0c\u63d0\u5347\u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u56fe\u6570\u636e\u65f6\u7684\u8868\u73b0\u3002\u8fd9\u4e9b\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u4e4b\u95f4\u7684\u4ea4\u53c9\u5f71\u54cd\uff0c\u4e0d\u4ec5\u63a8\u52a8\u4e86\u5404\u81ea\u9886\u57df\u7684\u53d1\u5c55\uff0c\u4e5f\u4e3a\u89e3\u51b3\u66f4\u590d\u6742\u7684\u5b9e\u9645\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u548c\u5de5\u5177\u3002", "ref_task_step_id": ""}, "__type__": "task_step_node"}, "f5dda6c2-b14a-44f3-873e-1c63177625a7": {"__data__": {"id_": "f5dda6c2-b14a-44f3-873e-1c63177625a7", "metadata": {}, "relationships": {}, "hash": "", "start_task_context": "\u5927\u6a21\u578b\u4e2d\u7684LayerNorm\u548cRMSNorm\u6709\u4ec0\u4e48\u533a\u522b\uff1f", "aemo_representation_context": "### Step-by-Step Decomposition of Computer Science Research Content\n\n#### 1. \u5206\u6790\u8fd1\u51e0\u5e74\u7814\u7a76\u9886\u57df\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba\n- **\u6280\u672f\u6846\u67b6**\uff1a\u8fd1\u5e74\u6765\uff0c\u6df1\u5ea6\u5b66\u4e60\u3001\u5f3a\u5316\u5b66\u4e60\u3001\u56fe\u795e\u7ecf\u7f51\u7edc\u7b49\u6846\u67b6\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u9886\u57df\u5360\u636e\u4e3b\u5bfc\u5730\u4f4d\u3002\u5177\u4f53\u6846\u67b6\u5982Transformer\u3001BERT\u3001GNN\u7b49\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002\n- **\u65b9\u6cd5\u8bba**\uff1a\u7814\u7a76\u65b9\u6cd5\u9010\u6e10\u4ece\u5355\u4e00\u6a21\u578b\u8f6c\u5411\u591a\u6a21\u578b\u878d\u5408\uff0c\u4ece\u76d1\u7763\u5b66\u4e60\u6269\u5c55\u5230\u65e0\u76d1\u7763\u548c\u81ea\u76d1\u7763\u5b66\u4e60\uff0c\u5f3a\u8c03\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002\n\n#### 2. \u7814\u7a76\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u4e3b\u8981\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0e\u53d8\u4f53\n- **\u5e94\u7528**\uff1a\u4f8b\u5982\uff0cTransformer\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u4e2d\u88ab\u5e7f\u6cdb\u5e94\u7528\uff0cBERT\u53d8\u4f53\u5982RoBERTa\u3001ALBERT\u5728\u6587\u672c\u5206\u7c7b\u3001\u95ee\u7b54\u7cfb\u7edf\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff1bGNN\u5728\u63a8\u8350\u7cfb\u7edf\u3001\u77e5\u8bc6\u56fe\u8c31\u7b49\u9886\u57df\u6709\u663e\u8457\u5e94\u7528\u3002\n- **\u53d8\u4f53**\uff1a\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\uff0c\u7814\u7a76\u8005\u63d0\u51fa\u4e86\u591a\u79cd\u53d8\u4f53\uff0c\u5982\u9488\u5bf9\u957f\u5e8f\u5217\u5904\u7406\u7684Transformer-XL\uff0c\u9488\u5bf9\u591a\u6a21\u6001\u6570\u636e\u7684ViLBERT\u7b49\u3002\n\n#### 3. \u8bc4\u4f30\u5b66\u672f\u754c\u7684\u6280\u672f\u8fdb\u6b65\u4e0e\u5c40\u9650\u6027\n- **\u6280\u672f\u8fdb\u6b65**\uff1a\u6a21\u578b\u6027\u80fd\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\uff0c\u8ba1\u7b97\u6548\u7387\u4e5f\u6709\u6240\u63d0\u9ad8\uff0c\u5982EfficientNet\u5728\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u8868\u73b0\u3002\n- **\u5c40\u9650\u6027**\uff1a\u4f9d\u7136\u5b58\u5728\u6a21\u578b\u504f\u5dee\uff08\u5982\u6027\u522b\u504f\u89c1\uff09\u3001\u6570\u636e\u4f9d\u8d56\uff08\u5982\u5bf9\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\uff09\u7b49\u95ee\u9898\uff0c\u6cdb\u5316\u80fd\u529b\u6709\u5f85\u8fdb\u4e00\u6b65\u63d0\u5347\u3002\n\n#### 4. \u63a2\u8ba8\u8ba1\u7b97\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\n- **\u9002\u7528\u6027**\uff1a\u7814\u7a76\u6a21\u578b\u5728\u4e0d\u540c\u9886\u57df\uff08\u5982\u533b\u7597\u3001\u91d1\u878d\uff09\u548c\u591a\u6a21\u6001\u6570\u636e\uff08\u5982\u6587\u672c+\u56fe\u50cf\uff09\u4e0a\u7684\u8868\u73b0\uff0c\u5982\u591a\u6a21\u6001Transformer\u6a21\u578b\u3002\n- **\u6cdb\u5316\u80fd\u529b**\uff1a\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\u3001\u5143\u5b66\u4e60\u7b49\u65b9\u6cd5\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f7f\u5176\u80fd\u591f\u5e94\u5bf9\u66f4\u591a\u73b0\u5b9e\u5e94\u7528\u573a\u666f\u3002\n\n#### 5. \u5206\u6790\u6700\u65b0\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027\n- **\u7a33\u5b9a\u6027**\uff1a\u7814\u7a76\u7b97\u6cd5\u5728\u590d\u6742\u3001\u52a8\u6001\u73af\u5883\u4e0b\u7684\u8868\u73b0\uff0c\u5982\u5bf9\u6297\u8bad\u7ec3\u3001\u9c81\u68d2\u4f18\u5316\u7b49\u65b9\u6cd5\u7684\u5e94\u7528\u3002\n- **\u5bb9\u9519\u6027**\uff1a\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\u7684\u9002\u5e94\u6027\uff0c\u5982\u5206\u5e03\u5f0f\u8bad\u7ec3\u4e2d\u7684\u68af\u5ea6\u538b\u7f29\u3001\u6a21\u578b\u5e76\u884c\u7b49\u6280\u672f\u3002\n\n#### 6. \u8bc4\u4f30\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218\n- **\u672a\u6765\u65b9\u5411**\uff1a\u5982\u53ef\u89e3\u91caAI\u3001\u8054\u90a6\u5b66\u4e60\u3001\u9690\u79c1\u4fdd\u62a4\u7b49\u65b0\u5174\u7814\u7a76\u65b9\u5411\u3002\n- **\u6311\u6218**\uff1a\u5982\u4f55\u5e73\u8861\u6a21\u578b\u6027\u80fd\u4e0e\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\uff0c\u5982\u4f55\u89e3\u51b3\u6570\u636e\u504f\u89c1\u548c\u4f26\u7406\u95ee\u9898\u7b49\u3002\n\n### \u5927\u6a21\u578b\u4e2d\u7684LayerNorm\u548cRMSNorm\u7684\u533a\u522b\n\n#### LayerNorm\uff08Layer Normalization\uff09\n- **\u5b9a\u4e49**\uff1aLayerNorm\u662f\u5bf9\u6bcf\u4e2a\u6837\u672c\u7684\u6bcf\u4e2a\u7279\u5f81\u8fdb\u884c\u5f52\u4e00\u5316\uff0c\u4f7f\u5176\u5747\u503c\u548c\u65b9\u5dee\u5206\u522b\u63a5\u8fd10\u548c1\u3002\n- **\u516c\u5f0f**\uff1a\n  \\[\n  \\text{LayerNorm}(x) = \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} \\cdot \\gamma + \\beta\n  \\]\n  \u5176\u4e2d\uff0c\\(\\mu\\)\u548c\\(\\sigma^2\\)\u5206\u522b\u662f\u7279\u5f81\u5747\u503c\u548c\u65b9\u5dee\uff0c\\(\\gamma\\)\u548c\\(\\beta\\)\u662f\u53ef\u5b66\u4e60\u53c2\u6570\uff0c\\(\\epsilon\\)\u662f\u5fae\u5c0f\u5e38\u6570\u4ee5\u9632\u6b62\u9664\u96f6\u3002\n- **\u5e94\u7528**\uff1a\u5e7f\u6cdb\u7528\u4e8eTransformer\u6a21\u578b\u4e2d\uff0c\u6709\u52a9\u4e8e\u7f13\u89e3\u68af\u5ea6\u6d88\u5931\u95ee\u9898\uff0c\u63d0\u5347\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002\n\n#### RMSNorm\uff08Root Mean Square Normalization\uff09\n- **\u5b9a\u4e49**\uff1aRMSNorm\u662f\u5bf9\u6bcf\u4e2a\u6837\u672c\u7684\u6bcf\u4e2a\u7279\u5f81\u8fdb\u884c\u5f52\u4e00\u5316\uff0c\u4f46\u53ea\u5f52\u4e00\u5316\u65b9\u5dee\uff0c\u4e0d\u5f52\u4e00\u5316\u5747\u503c\u3002\n- **\u516c\u5f0f**\uff1a\n  \\[\n  \\text{RMSNorm}(x) = \\frac{x}{\\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} x_i^2 + \\epsilon}} \\cdot \\gamma + \\beta\n  \\]\n  \u5176\u4e2d\uff0c\\(n\\)\u662f\u7279\u5f81\u6570\u91cf\uff0c\\(\\gamma\\)\u548c\\(\\beta\\)\u662f\u53ef\u5b66\u4e60\u53c2\u6570\uff0c\\(\\epsilon\\)\u662f\u5fae\u5c0f\u5e38\u6570\u3002\n- **\u5e94\u7528**\uff1a\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0cRMSNorm\u6bd4LayerNorm\u66f4\u7b80\u5355\uff0c\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\uff0c\u5c24\u5176\u5728\u5927\u89c4\u6a21\u6a21\u578b\u8bad\u7ec3\u4e2d\u3002\n\n#### \u4e3b\u8981\u533a\u522b\n1. **\u5f52\u4e00\u5316\u65b9\u5f0f**\uff1a\n   - LayerNorm\u540c\u65f6\u5f52\u4e00\u5316\u5747\u503c\u548c\u65b9\u5dee\u3002\n   - RMSNorm\u53ea\u5f52\u4e00\u5316\u65b9\u5dee\u3002\n2. **\u8ba1\u7b97\u590d\u6742\u5ea6**\uff1a\n   - LayerNorm\u9700\u8981\u8ba1\u7b97\u5747\u503c\u548c\u65b9\u5dee\uff0c\u8ba1\u7b97\u91cf\u7a0d\u5927\u3002\n   - RMSNorm\u53ea\u9700\u8ba1\u7b97\u65b9\u5dee\uff0c\u8ba1\u7b97\u91cf\u8f83\u5c0f\u3002\n3. **\u6548\u679c**\uff1a\n   - LayerNorm\u5728\u8bb8\u591a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u5728NLP\u9886\u57df\u3002\n   - RMSNorm\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u8868\u73b0\u76f8\u4f3c\u751a\u81f3\u66f4\u4f18\uff0c\u4e14\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u3002\n\n\u901a\u8fc7\u4e0a\u8ff0\u5206\u6790\uff0c\u53ef\u4ee5\u66f4\u597d\u5730\u7406\u89e3LayerNorm\u548cRMSNorm\u5728\u5927\u6a21\u578b\u4e2d\u7684\u5e94\u7528\u53ca\u5176\u4f18\u7f3a\u70b9\uff0c\u4ece\u800c\u5728\u5177\u4f53\u4efb\u52a1\u4e2d\u9009\u62e9\u5408\u9002\u7684\u5f52\u4e00\u5316\u65b9\u6cd5\u3002", "task_step_name": "\u6280\u672f\u6846\u67b6", "task_step_description": "\u8fd1\u5e74\u6765\uff0c\u6df1\u5ea6\u5b66\u4e60\u3001\u5f3a\u5316\u5b66\u4e60\u3001\u56fe\u795e\u7ecf\u7f51\u7edc\u7b49\u6846\u67b6\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u9886\u57df\u5360\u636e\u4e3b\u5bfc\u5730\u4f4d\u3002\u5177\u4f53\u6846\u67b6\u5982Transformer\u3001BERT\u3001GNN\u7b49\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "task_step_level": "0>1", "task_step_question": "### \u95ee\u9898\n\n\u5728\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u4e2d\uff0cTransformer\u6a21\u578b\u5e7f\u6cdb\u4f7f\u7528\u4e86LayerNorm\u548cRMSNorm\u4e24\u79cd\u5f52\u4e00\u5316\u65b9\u6cd5\u3002\u7ed3\u5408\u8fd1\u5e74\u6765\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u6846\u67b6\u7684\u53d1\u5c55\uff0c\u7279\u522b\u662f\u5728Transformer\u53ca\u5176\u53d8\u4f53\uff08\u5982BERT\u3001RoBERTa\u7b49\uff09\u4e2d\u7684\u5e94\u7528\uff0c**LayerNorm\u548cRMSNorm\u5728Transformer\u6a21\u578b\u4e2d\u7684\u5177\u4f53\u8868\u73b0\u548c\u9002\u7528\u6027\u6709\u4f55\u4e0d\u540c\uff1f** \u8bf7\u4ece\u8ba1\u7b97\u590d\u6742\u5ea6\u3001\u8bad\u7ec3\u7a33\u5b9a\u6027\u4ee5\u53ca\u5728\u4e0d\u540c\u4efb\u52a1\uff08\u5982\u81ea\u7136\u8bed\u8a00\u5904\u7406\u3001\u56fe\u50cf\u5904\u7406\u7b49\uff09\u4e2d\u7684\u6548\u679c\u7b49\u65b9\u9762\u8fdb\u884c\u5bf9\u6bd4\u5206\u6790\u3002", "task_step_question_context": [{"ref_id": "454845744951617972", "chunk_id": "5", "score": 0.4609375, "text": "# 3.3 A TRANSFORMATION PER BLOCK\nNow that every LayerNorm in the transformer has been converted to RMSNorm, we can select any $\\mathbf{Q}$ to modify the model. Our initial plan was to collect signals from the model, construct an orthogonal matrix using those signals and to delete parts of the network. We quickly saw that the signals at different blocks of the network were not aligned, and that we would need to apply a different orthogonal matrix at each block, $\\mathbf{Q}_{\\ell}$ .  \n\nAllowing the orthogonal matrix used in each block to differ can be shown to leave the model unchanged using the same proof as Theorem 1 ,  \n\n  \nFigure 3: Converting a transformer network from LayerNorm to RMSNorm: the scale matrix diag $(\\alpha)$ is absorbed into the subsequent matrix $\\mathbf{W}_{\\mathrm{in}}$ . Figure shows the block in combined colors. We use $(\\alpha)$ for brevity. The mean-subtraction matrix $\\mathbf{M}$ is applied to each matrix $\\mathbf{W}_{\\mathrm{out}}$ . Layernorm becomes RMSNorm, up to a constant $\\bar{\\sqrt{D}}$ (not shown). Here, the scaling $(\\alpha^{\\prime})$ comes from the previous block.  \n\n  \nFigure 4: With the network converted to RMSNorm (see Figure 3 ), we apply the computational-invariance idea. The input weight matrices $\\mathrm{diag}(\\alpha)\\mathbf{W}_{\\mathrm{in}}$ are pre-multiplied by $\\mathbf{Q}^{\\top}$ . The output matrices $\\mathbf{W}_{\\mathrm{out}}\\mathbf{M}$ are post-multiplied by $\\mathbf{Q}$ . In the skip-connection, a new linear layer is added $\\mathbf{Q}_{\\ell}^{\\top}\\mathbf{Q}_{\\ell+1}$ . After these modifications, the matrices can be sliced (hatched areas).  \n\nwith the exception of line 5 of Algorithm 1 . Here we see that the residual connection and the output of the block must have the same rotation. To fix this, we modify the residual connection by applying the linear transformation applied to different blocks with the additional linear operation in the residual connection. Unlike the $\\mathbf{Q}_{\\ell-1}^{\\top}\\mathbf{Q}_{\\ell}$ \u2212to the residual. Figure 4 shows how different rotations can be modifications to the weight matrices, these additional operations cannot be pre-computed and add a small $(D\\times D)$ overhead to the model. Nonetheless, they are needed to allow slicing the model (Section 3.4 ) and we see real speedup overall (Section 4 ).  \n\nTo compute the matrices $\\mathbf{Q}_{\\ell}$ , we use PCA. We select a calibration dataset from the training set, run it through the model (after converting LayerNorm operations into RMSNorm), and extract the orthogonal matrix of the layer. We use the output of the transformed network to calculate the orthogonal matrices of the next layers. More precisely, if $\\mathbf{X}_{\\ell,i}$ is the output of the $\\ell^{\\mathrm{th}}$ RMSNorm block for the $i^{\\mathrm{th}}$ sequence in the calibration dataset, we compute  \n\n$$\n\\mathbf{C}_{\\ell}=\\sum_{i}\\mathbf{X}_{\\ell,i}^{\\top}\\mathbf{X}_{\\ell,i}\n$$  \n\nand set $\\mathbf{Q}_{\\ell}$ to the be the eigenvectors of $\\mathbf{C}_{\\ell}$ , sorted by decreasing eigenvalues.\n\n# 3.4 SLICING\nThe goal of Principal Component Analysis is usually to take a data matrix $\\mathbf{X}$ and compute a lower dimensional representation $\\mathbf{Z}$ , and an approximate reconstruction $\\tilde{\\mathbf{X}}$ :  \n\n$$\n\\mathbf{Z}=\\mathbf{X}\\mathbf{Q}\\mathbf{D}\\,,\\qquad\\tilde{\\mathbf{X}}=\\mathbf{Z}\\mathbf{D}^{\\top}\\mathbf{Q}^{\\top}\\,.\n$$  \n\nwhere $\\mathbf{Q}$ is the ectors of ${\\bf X}^{\\top}{\\bf X}$ , and $\\mathbf{D}$ is a $D\\times D_{\\mathrm{small}}$ deletion matrix (containing $D_{\\mathrm{small}}$ The reconstruction is columns of the $D\\times D$ \u00d7$L_{2}$ identity matrix), which removes some of the columns of the matrix to the left. optimal, in the sense that QD is a linear mapping that minimizes $\\lVert\\mathbf{X}-\\tilde{\\mathbf{X}}\\rVert^{2}$ .  \n\nWhen we apply PCA to the signal matrix $\\mathbf{X}$ bween blocks, we never materialize the $N\\times D$ signal matrix, but we apply the deletion matrix Dto the operations preceding and succeeding the construction of that matrix, which have already been multiplied by $\\mathbf{Q}$ in the above. We delete rows of $\\mathbf{W}_{\\mathrm{in}}$ that we have inserted into the residual connection (see Figure and columns of $\\mathbf{W}_{\\mathrm{out}}$ and $\\mathbf{W}_{\\mathrm{embd}}$ . We also delete both rows 4 ). and columns of the matrix $\\mathbf{Q}_{\\ell-1}^{\\top}\\mathbf{Q}_{\\ell}$ \u2212\n\n# 4 EXPERIMENTAL VALIDATION\nSetup We use HuggingFace Transformers ( Wolf et al. ,2019 ) to implement our code with PyTorch (Paszke et al. ,2019 ). The computation of $\\mathbf{Q}$ is performed on a single H100 GPU with 80GB of memory, taking approximately 3.5 hours to complete for the L LAMA -2 70B model. During the PCA calculation, we use double precision for computing the eigenvectors of the covariance matrix. We find that using single precision for eigenvector calculations in PyTorch leads to a discrepancy in the final accuracy, as detailed in Appendix A.2 .  \n\nWe experiment with two different calibration sets: 1024 samples from the WikiText-2 training dataset ( Merity et al. ,2016 ) and 5000 samples from the Alpaca training dataset ( Taori et al. ,2023 ). Sequence lengths are chosen as the maximum of each language model. An ablation study on the calibration set size and sequence length is presented in Appendix A.3 .  \n\nModels, Tasks, and GPUs We evaluate all our experiments on OPT ( Zhang et al. ,2022 ), L LAMA -2 (Touvron et al. ,2023 ) model families, and additionally evaluate Phi-2 (in our zero-shot task) experiments. We exclude OPT 175B, as it is outperformed by smaller L LAMA -2 models. Nonetheless, we anticipate that this larger model will yield improved results, as larger models typically offer more promising opportunities for compression (see Section 4.1 ). We evaluate our scheme on both language generation as well as popular zero-shot tasks. To demonstrate the comprehensive speedup achieved by SliceGPT we use: Quadro RTX6000 GPUs with 24GB of memory as a representative example of consumer-level GPUs; 40GB A100s and 80GB H100s to provide datacenter-level benchmarks.  \n\nBaseline Setup We initially planned to compare our results against a scheme that pruned columns (or rows) with the smallest norm but found that this baseline was very poor, with the perplexity of the model soaring into the 1000s after pruning just a few columns. Instead, we compare SliceGPT against SparseGPT ( Frantar & Alistarh ,2023 ) employing a 2:4 sparsity ratio, as this is the only sparsity scheme which achieves speedup ( Mishra et al. ,2021 )."}, {"ref_id": "454847819065993190", "chunk_id": "1", "score": 0.4140625, "text": "# 3 Experiments and Results\nWe evaluate the performance of PreNorm and PostNorm for ZST on various datasets and language pairs. We then analyze the off-target rates and structural discrepancies between PreNorm and PostNorm to understand performance differences.  \n\n$$\n\\mathrm{LayerNorm}(\\mathbf{x})=\\frac{\\mathbf{x}-\\mathbf{E}(\\mathbf{x})}{\\sqrt{\\mathbf{V}(\\mathbf{x})}}\\cdot\\mathbf{g}+\\mathbf{b},\n$$  \n\nwhere $\\mathbf{g}$ and $\\mathbf{b}$ are trainable gain and bias. $\\mathbf{E}$ and $\\mathbf{V}$ indicate expectation and variance. LayerNorm is commonly used in two positions in the Transformer, as shown in Fig. 1 . PostNorm, which is the originally proposed setting of the Transformer ( Vaswani et al. ,2017 ), involves applying LayerNorm after each sub-module (i.e., selfattention or feed-forward network) and residual connections. PreNorm ( Baevski and Auli ,2019 ), on the other hand, involves applying LayerNorm directly before each sub-module and is known to stabilize Transformer training. While variants of Transformer LayerNorm like RMSNorm ( Zhang and Sennrich ,2019 ) have been proposed, the vanilla PreNorm and PostNorm are still the most widely adopted settings in current multilingual\n\n# 3.1 Experimental Settings\nDatasets We perform ZST experiments on three datasets: OPUS ( Zhang et al. ,2020 ), IWSLT ( Cettolo et al. ,2017 ), and Europarl ( Koehn ,2005 ). The statistics of the datasets are summarized in Table 1 .We include 7 ,4 , and 5 languages for each dataset. The training data consists of only English-centric sentence pairs, resulting in 30 ,6 , and 12 ZST directions for each dataset. The total number of parallel sentences for each dataset is 12 .00 M, 1 .38 M, and 15 .78 M, respectively. We apply BPE ( Sennrich et al. ,2016 ) with merge operations of 50 k, 40 k, and $50\\mathbf{k}$ to create a joint vocabulary for each dataset.  \n\nTraining We employ Transformer-base model for OPUS and IWSLT, and Transformer-big for Europarl, in accordance with the distinct sizes of training data. We consider the following settings: (1) PreNorm or PostNorm : PreNorm involves LayerNorm directly before each sub-module (i.e., self-attention or feed-forward network), while PostNorm applies LayerNorm after each sub-module and residual connections, as shown in Fig. 1 .(2) S-ENC-T-DEC or T-ENC : Source language tag on the encoder-side and target language tag on the decoder-side; or only target language tag on the encoder-side. Wu et al. (2021 ) showed that this setting impacts ZST for Transformer with PreNorm. (3) w/ or w/o Res. : With the residual connection for self-attention in the middle $(4^{t h})$ encoder layer or not. Liu et al. (2021 ) revealed that \u201cw/o Res.\u201d improves ZST for the model trained with PreNorm. We experiment this with different LayerNorm settings as this may reduce the potential of overfitting on supervised directions, then further impacts ZST, which aligns with our hypothesis.  \n\nTable 2: BLEU scores and off-target rates (shown in brackets) . We report the average score of three seeds; refer to Appendix Gfor BLEU score of each translation direction and seed. \u201cRes.\u201d indicates the residual connection of self-attention in the $4^{t h}$ encoder layer. We mark lower off-target rates and significantly higher BLEU scores ( Koehn ,2004 ) between PreNorm and PostNorm in bold for ZST.   \n\n\n<html><body><table><tr><td>#</td><td>Layer Norm</td><td>Language Tag</td><td>Res.</td><td></td><td>Zero-shot</td><td></td><td></td><td>Supervised</td><td></td></tr><tr><td>0</td><td></td><td>Pivot</td><td></td><td>OPUS 21.8</td><td>IWSLT 20.0</td><td>Europarl 29.5</td><td>OPUS</td><td>IWSLT</td><td>Europarl</td></tr><tr><td>1</td><td>PreNorm</td><td>S-ENC-T-DEC</td><td>w/</td><td>10.1 (42.19%)</td><td>4.9 (64.84%)</td><td>24.9 ( 7.73%)</td><td>33.7</td><td>31.5</td><td>34.3</td></tr><tr><td>2</td><td>PostNorm</td><td>S-ENC-T-DEC</td><td>w/</td><td>16.8 ( 8.59%)</td><td>12.4 (10.61%)</td><td>29.2( 0.34%)</td><td>33.9</td><td>31.5</td><td>34.5</td></tr><tr><td>3</td><td>PreNorm</td><td>T-ENC</td><td>w/</td><td>13.3 (22.99%)</td><td>13.7 ( 3.98%)</td><td>29.5( 0.23%)</td><td>33.7</td><td>31.6</td><td>34.4</td></tr><tr><td>4</td><td>PostNorm</td><td>T-ENC</td><td>w/</td><td>14.0 (22.86%)</td><td>15.5 ( 4.59%)</td><td>30.8 ( 0.11%)</td><td>34.1</td><td>31.5</td><td>34.5</td></tr><tr><td>5</td><td>PreNorm</td><td>S-ENC-T-DEC</td><td>w/o</td><td>14.3 (20.67%)</td><td>8.0 (50.16%)</td><td>16.7 (41.87%)</td><td>33.6</td><td>30.9</td><td>34.3</td></tr><tr><td>6</td><td>PostNorm</td><td>S-ENC-T-DEC</td><td>w/o</td><td>16.0 (15.27%)</td><td>17.4 (1.83%)</td><td>29.0 ( 0.41%)</td><td>33.8</td><td>30.7</td><td>34.4</td></tr><tr><td>7</td><td>PreNorm</td><td>T-ENC</td><td>w/o</td><td>13.4 (27.15%)</td><td>16.2 ( 1.54%)</td><td>29.9 ( 2.15%)</td><td>33.5</td><td>30.9</td><td>34.3</td></tr><tr><td>8</td><td>PostNorm</td><td>T-ENC</td><td>w/o</td><td>13.9 (26.68%)</td><td>17.8 (1.50%)</td><td>30.8 ( 0.13%)</td><td>33.9</td><td>30.6</td><td>34.4</td></tr></table></body></html>  \n\nThe settings above lead to eight different combinations, shown in Table 2 (#1 - #8). Additional training details are in Appendix A .\n\n# 3.2 Main Results\nWe evaluate ZST systems using SacreBLEU ( Post ,2018 ) and off-target rates. We report in Table 2 BLEU scores for both zero-shot and supervised directions. For ZST, we also present pivot-based translation results as a reference. Implementation details of evaluation can be found in Appendix B.Our findings are as follows:  \n\nPreNorm vs. PostNorm :We find that PostNorm consistently yields better BLEU scores than PreNorm for ZST across various language tag and residual connection settings, while their performance is comparable for supervised directions.  \n\nImpact of Language Tag and Residual Connection: We observe that using the \u201cT-ENC\u201d language tag and \u201cw/ Res.\u201d improves ZST performance for IWSLT, which aligns with the findings of $\\mathrm{Wu}$ et al. (2021 ) and Liu et al. (2021 ). Nevertheless, the best performance is achieved using \u201cw/ Res.\u201d for PostNorm with \u201cS-ENC-T-DEC\u201d and \u201cT-ENC\u201d tags for OPUS and Europarl, respectively (#2 and #4). Given that Wu et al. (2021 ) and Liu et al. (2021 )used PreNorm as the default setting (#2, #4, #6 and #8 are unreported results in their work), our results emphasize the need to consider PostNorm as the default setting for ZST, while the language tag and residual connection settings have less impact.  \n\nOff-target Rates : Off-target rates help understand the different BLEU score gaps between PreNorm and PostNorm, which ranges from 0 .5 to 12 .3 BLEU points. For PreNorm and PostNorm with the \u201cT-ENC\u201d language tag (#3, #4, #7, and #8), they have similar off-target rates, with a discrepancy ranging from $-0.61\\%$ to $2.02\\%$ , which results in narrow BLEU score gaps, ranging from 0 .5 to 1 .8 points. However, for PreNorm and PostNorm with the \u201cS-ENC-T-DEC\u201d language tag (#1, #2, #5, and #6), the off-target rates show a more considerable discrepancy, ranging from $5.40\\%$ to $54.23\\%$ , resulting in BLEU score gaps from 1 .7 to 12 .3 points. Further analysis of the nature of Transformer hidden states in the next section explores the reason for these different off-target rates in translations."}, {"ref_id": "454847315550000884", "chunk_id": "5", "score": 0.28125, "text": "# DeepNet: Scaling Transformers to 1,000 Layers\nHongyu Wang \u2217Shuming Ma \u2217Li Dong Shaohan Huang Dongdong Zhang Furu Wei \u2020Microsoft Research https://github.com/microsoft/unilm\n\n# Abstract\nIn this paper, we propose a simple yet effective method to stabilize extremely deep Transformers. Specifically, we introduce a new normalization function (D EEP NORM ) to modify the residual connection in Transformer, accompanying with theoretically derived initialization. In-depth theoretical analysis shows that model updates can be bounded in a stable way. The proposed method combines the best of two worlds, i.e., good performance of Post-LN and stable training of Pre-LN, making D EEP NORM a preferred alternative. We successfully scale Transformers up to 1,000 layers (i.e., 2,500 attention and feed-forward network sublayers) without difficulty, which is one order of magnitude deeper than previous deep Transformers. Remarkably, on a multilingual benchmark with 7,482 translation directions, our 200-layer model with 3.2B parameters significantly outperforms the 48-layer state-of-the-art model with 12B parameters by 5 BLEU points, which indicates a promising scaling direction.  \n\n  \nFigure 1: Trend of Transformer depths of state-of-the-art NLP models over time.\n\n# 1 Introduction\nRecent years have witnessed a trend towards large-scale Transformer (Vaswani et al., 2017) models. The capacity has substantially increased from millions of parameters (Devlin et al., 2019; Conneau et al., 2020) to billions (Radford et al., 2019; Brown et al., 2020; Huang et al., 2019; Raffel et al., 2020; Lepikhin et al., 2021; Rae et al., 2021; Lin et al., 2021; Smith et al., 2022), and even trillions (Du et al., 2021). Large-scale models yield state-of-the-art performance on a wide range of tasks, and show impressive abilities in few-shot and zero-shot learning. Despite an enormous number of parameters, their depths (as shown in Figure 1) are limited by the training instability of Transformers.  \n\nNguyen and Salazar (2019) find that pre-norm residual connections (Pre-LN) improve the stability of Transformers based on post-norm connections (Post-LN). However, the gradients of Pre-LN at bottom layers tend to be larger than at top layers (Shleifer et al., 2021), leading to a degradation in performance compared with Post-LN. In order to alleviate the above issue, there have been efforts on improving the optimization of deep Transformer by means of better initialization (Zhang et al., 2019a;b; Huang et al., 2020), or better architecture (Wang et al., 2019; Liu et al., 2020; Bachlechner et al., 2020; Shleifer et al., 2021). These approaches can stabilize a Transformer model with up to hundreds of layers. Yet, none of previous methods has been successfully scaled to 1,000 layers.  \n\nOur aim is to improve the training stability of Transformers and scale the model depth by orders of magnitude. To this end, we study the cause of unstable optimization, finding the exploding model update is responsible for the instability. Motivated by the above observation, we introduce a new normalization function (D EEP NORM ) at residual connections (He et al., 2016), which has theoretical justification of bounding the model update by a constant. The proposed method is simple yet effective, with just lines of code change. The approach improves the stability of Transformers so that we are able to scale model depth to more than 1,000 layers. Moreover, experimental results show that D EEP NORM combines the best of two worlds, i.e., good performance of Post-LN and stable training of Pre-LN. The proposed method can be a preferred alternative of Transformers, not only for extremely deep (such as ${>}1000$ layers) models, but also for existing large models. Notably, our 200-layer model with 3.2B parameters achieves 5 BLEU improvement on a massively multilingual machine translation benchmark compared to state-of-the-art model (Fan et al., 2021) with 48 layers and 12B model size.\n\n# 2 TL;DR for Practitioners\n<html><body><table><tr><td rowspan=\"2\" colspan=\"2\">def deepnorm(x): return</td><td rowspan=\"2\">Architectures</td><td colspan=\"2\">Encoder</td><td colspan=\"2\">Decoder</td></tr><tr><td></td><td>8</td><td></td><td>3</td></tr><tr><td rowspan=\"2\">def if</td><td>LayerNorm(x*Q +f(x)) deepnorm_init(w):</td><td>Encoder-only (e.g.,BERT)</td><td>(2N)</td><td>(8N)-</td><td></td><td></td></tr><tr><td>wis ffn v_proj out nn.init.xavier_normal_(w, gain=\u03b2)</td><td>t_proj']\uff1a Decoder-only (e.g.,GPT)</td><td></td><td></td><td>(2M)</td><td>(8M)-</td></tr><tr><td>elif Wis</td><td>['q-proj 'k_proj \uff1a nn.init.xavier_normal_(w,gain=1)</td><td>Encoder-decoder (e.g.,NMT, T5)</td><td>0.81(N4M)6</td><td>0.87(N4M)-16</td><td>(3M)</td><td>(12M)-</td></tr></table></body></html>  \n\nFigure 2: (a) Pseudocode for D EEP NORM . We take Xavier initialization (Glorot and Bengio, 2010) as an example, and it can be replaced with other standard initialization. Notice that $\\alpha$ is a constant. (b) Parameters of D EEP NORM for different architectures ( $N$ -layer encoder, $M$ -layer decoder).  \n\nAs shown in Figure 2, it is simple to implement our method based on Transformers with PostLN. Compared to Post-LN, D EEP NORM up-scales the residual connection before performing layer normalization. Besides, we down-scale the parameters during initialization. Notably, we only scale the weights of feed-forward networks, as well as the value projection and the output projection of attention layers. Moreover, the scales of residual connection and initialization are dependent on the architecture (Figure 2). We provide more details in Section 4.3.\n\n# 3 Instability of Deep Transformer\nWe study the causes of the instability for deep Transformers. Our analysis begins with the observation: better initialization methods stabilize the training of Transformer. This has also been verified by previous work (Zhang et al., 2019a; Huang et al., 2020; Xu et al., 2021). Therefore, we study the  \n\n  \nFigure 3: (a) Gradient norm in the top layers of 18L-18L models. (b) Gradient norm in the last layer of the models with depths varying from 6L-6L to 24L-24L. (c) Validation loss curves of 18L-18L models.  \n\n  \nFigure 4: Visualization of the model update, the average input of LNs, and the gradients for the 18L-18L models at the early stage of training.  \n\ntraining process of Post-LN with or without proper initialization. With better initialization, we downscale the weights of $l$ -th layer by $k_{l}=N-l+1,l\\in[1,N]$ after performing Xavier initialization. For example, the output projection $W_{o}^{l}$ of FFN in l-th layer is initialized as:  \n\n$$\nW_{o}^{l}\\backsim\\mathcal{N}\\left(0,\\frac{1}{k_{l}^{2}d^{\\prime}}\\right),\n$$  \n\nwhere $d^{\\prime}$ is an average of input and output dimensions. We name this model Post-LN-init. Notice that different from the prior work (Zhang et al., 2019a), we narrow the scale of lower layers instead of the higher layers. We believe that it helps to separate the effect of the gradient scale from the model update. Besides, Post-LN-init has the same architecture as Post-LN, which eliminates the impact from the architecture.  \n\nWe train 18L-18L Post-LN and 18L-18L Post-LN-init on the IWSLT-14 De-En machine translation dataset. Figure 3 visualizes their gradients and validation loss curves. As shown in Figure 3(c), Post-LN-init converged while Post-LN did not. Post-LN-init has an even larger gradient norm in the last several layers, although its weights have been scaled down. Furthermore, we visualize the gradient norm of the last decoder layer with varying model depth from 6L-6L to 24L-24L. Figure 3 shows that the gradient norm of Post-LN-init in the last layer is still much larger than that of Post-LN, regardless of model depth. It concludes that the exploding gradients in deep layers should not be the root cause of instability of Post-LN, while the scale of model update tends to account for it.  \n\nThen we demonstrate that the instability of Post-LN comes from a chain of several issues, including gradient vanishing as well as too large model updates. As shown in Figure 4(a), we first visualize the norm of model update $||\\Delta F||$ at the early stage of training:  \n\n$$\n||\\Delta F||=||F(x,\\theta_{i})-F(x,\\theta_{0})||,\n$$  \n\nwhere $x$ and $\\theta_{i}$ denotes input, and model parameters after $i$ -th updates. Post-LN has an exploding update at the very beginning of training, and then nearly no update shortly. It indicates that the model has been stuck in a spurious local optima. Both warm-up and better initialization help alleviate this issue, enabling the model to update smoothly. When the update explodes, the inputs to LN become large (see Figure $4({\\mathsf{b}})$ and Figure 4(c)). According to the theoretical analysis from Xiong et al. (2020), the magnitude of gradient through LN is inversely proportional to the magnitude of its input:  \n\n$$\n||\\frac{\\partial L N(x)}{\\partial x}||=\\mathcal{O}(\\frac{\\sqrt{d}}{||x||}).\n$$  \n\nFigure 4(b) and Figure 4(c) show that $||x||$ is significantly larger than $\\sqrt{d}$ ($d=512)$ )without warm-up or proper initialization. This explains the gradient vanishing problem occurred in the training of Post-LN (see Figure 4(d)).  \n\nAbove all, the instability starts from the large model update at the beginning of training. It renders the model trapped in a bad local optima, which in turn increases the magnitude of inputs to each LN. As training continues, the gradient through LN becomes increasingly small, thus resulting in severe gradient vanishing. The vanishing gradients make it difficult to escape from the local optima, and further destabilize the optimization. On the contrary, Post-LN-init has relatively small updates, and the inputs to LN are stable. This relieves suffering from gradient vanishing, making optimization more stable."}], "task_step_question_answer": "\u5728\u8de8\u6a21\u6001\u5b66\u4e60\u4e2d\uff0c\u591a\u6a21\u6001Transformer\u6846\u67b6\u53ef\u4ee5\u901a\u8fc7\u81ea\u6ce8\u610f\u529b\u673a\u5236\u6355\u6349\u4e0d\u540c\u6a21\u6001\u4e4b\u95f4\u7684\u4ea4\u4e92\u5173\u7cfb\uff0c\u4ece\u800c\u63d0\u5347\u6a21\u578b\u7684\u8868\u793a\u80fd\u529b\u3002\u4f8b\u5982\uff0c\u5728\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u4e2d\uff0c\u53ef\u4ee5\u901a\u8fc7\u591a\u6a21\u6001Transformer\u5c06\u56fe\u50cf\u548c\u6587\u672c\u4fe1\u606f\u8054\u5408\u5efa\u6a21\uff0c\u4ece\u800c\u63d0\u5347\u95ee\u7b54\u7684\u51c6\u786e\u6027\u3002\u5728\u591a\u4efb\u52a1\u5b66\u4e60\u4e2d\uff0c\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\u53ef\u4ee5\u901a\u8fc7\u5171\u4eab\u5e95\u5c42\u8868\u793a\u548c\u4efb\u52a1\u7279\u5b9a\u7684\u4e0a\u5c42\u8868\u793a\uff0c\u5e73\u8861\u591a\u4e2a\u4efb\u52a1\u4e4b\u95f4\u7684\u51b2\u7a81\u3002\u4f8b\u5982\uff0c\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u4e2d\uff0c\u53ef\u4ee5\u901a\u8fc7\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\u540c\u65f6\u8fdb\u884c\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u548c\u60c5\u611f\u5206\u6790\uff0c\u4ece\u800c\u63d0\u5347\u6a21\u578b\u7684\u6574\u4f53\u6027\u80fd\u3002\u6280\u672f\u6846\u67b6\u7684\u4f18\u5316\u53ef\u4ee5\u901a\u8fc7\u6a21\u578b\u67b6\u6784\u641c\u7d22\u3001\u8d85\u53c2\u6570\u4f18\u5316\u548c\u81ea\u52a8\u5316\u673a\u5668\u5b66\u4e60\u6280\u672f\u6765\u5b9e\u73b0\u3002\u4f8b\u5982\uff0c\u5728\u6a21\u578b\u67b6\u6784\u641c\u7d22\u4e2d\uff0c\u53ef\u4ee5\u901a\u8fc7\u795e\u7ecf\u67b6\u6784\u641c\u7d22\uff08NAS\uff09\u81ea\u52a8\u641c\u7d22\u6700\u4f18\u7684\u6a21\u578b\u67b6\u6784\uff0c\u4ece\u800c\u63d0\u5347\u6a21\u578b\u7684\u6027\u80fd\u3002\u5728\u8d85\u53c2\u6570\u4f18\u5316\u4e2d\uff0c\u53ef\u4ee5\u901a\u8fc7\u8d1d\u53f6\u65af\u4f18\u5316\u6216\u7f51\u683c\u641c\u7d22\u627e\u5230\u6700\u4f18\u7684\u8d85\u53c2\u6570\u7ec4\u5408\uff0c\u4ece\u800c\u63d0\u5347\u6a21\u578b\u7684\u8bad\u7ec3\u6548\u7387\u3002\u5728\u81ea\u52a8\u5316\u673a\u5668\u5b66\u4e60\u4e2d\uff0c\u53ef\u4ee5\u901a\u8fc7AutoML\u6846\u67b6\u81ea\u52a8\u5b8c\u6210\u7279\u5f81\u5de5\u7a0b\u3001\u6a21\u578b\u9009\u62e9\u548c\u8d85\u53c2\u6570\u4f18\u5316\uff0c\u4ece\u800c\u63d0\u5347\u6a21\u578b\u7684\u6574\u4f53\u6027\u80fd\u3002", "ref_task_step_id": ""}, "__type__": "task_step_node"}, "16fd2454-56f9-4eff-bce6-29f5553e5158": {"__data__": {"id_": "16fd2454-56f9-4eff-bce6-29f5553e5158", "metadata": {}, "relationships": {}, "hash": "", "start_task_context": "\u5927\u6a21\u578b\u4e2d\u7684LayerNorm\u548cRMSNorm\u6709\u4ec0\u4e48\u533a\u522b\uff1f", "aemo_representation_context": "### Step-by-Step Decomposition of Computer Science Research Content\n\n#### 1. \u5206\u6790\u8fd1\u51e0\u5e74\u7814\u7a76\u9886\u57df\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba\n- **\u6280\u672f\u6846\u67b6**\uff1a\u8fd1\u5e74\u6765\uff0c\u6df1\u5ea6\u5b66\u4e60\u3001\u5f3a\u5316\u5b66\u4e60\u3001\u56fe\u795e\u7ecf\u7f51\u7edc\u7b49\u6846\u67b6\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u9886\u57df\u5360\u636e\u4e3b\u5bfc\u5730\u4f4d\u3002\u5177\u4f53\u6846\u67b6\u5982Transformer\u3001BERT\u3001GNN\u7b49\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002\n- **\u65b9\u6cd5\u8bba**\uff1a\u7814\u7a76\u65b9\u6cd5\u9010\u6e10\u4ece\u5355\u4e00\u6a21\u578b\u8f6c\u5411\u591a\u6a21\u578b\u878d\u5408\uff0c\u4ece\u76d1\u7763\u5b66\u4e60\u6269\u5c55\u5230\u65e0\u76d1\u7763\u548c\u81ea\u76d1\u7763\u5b66\u4e60\uff0c\u5f3a\u8c03\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002\n\n#### 2. \u7814\u7a76\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u4e3b\u8981\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0e\u53d8\u4f53\n- **\u5e94\u7528**\uff1a\u4f8b\u5982\uff0cTransformer\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u4e2d\u88ab\u5e7f\u6cdb\u5e94\u7528\uff0cBERT\u53d8\u4f53\u5982RoBERTa\u3001ALBERT\u5728\u6587\u672c\u5206\u7c7b\u3001\u95ee\u7b54\u7cfb\u7edf\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff1bGNN\u5728\u63a8\u8350\u7cfb\u7edf\u3001\u77e5\u8bc6\u56fe\u8c31\u7b49\u9886\u57df\u6709\u663e\u8457\u5e94\u7528\u3002\n- **\u53d8\u4f53**\uff1a\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\uff0c\u7814\u7a76\u8005\u63d0\u51fa\u4e86\u591a\u79cd\u53d8\u4f53\uff0c\u5982\u9488\u5bf9\u957f\u5e8f\u5217\u5904\u7406\u7684Transformer-XL\uff0c\u9488\u5bf9\u591a\u6a21\u6001\u6570\u636e\u7684ViLBERT\u7b49\u3002\n\n#### 3. \u8bc4\u4f30\u5b66\u672f\u754c\u7684\u6280\u672f\u8fdb\u6b65\u4e0e\u5c40\u9650\u6027\n- **\u6280\u672f\u8fdb\u6b65**\uff1a\u6a21\u578b\u6027\u80fd\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\uff0c\u8ba1\u7b97\u6548\u7387\u4e5f\u6709\u6240\u63d0\u9ad8\uff0c\u5982EfficientNet\u5728\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u8868\u73b0\u3002\n- **\u5c40\u9650\u6027**\uff1a\u4f9d\u7136\u5b58\u5728\u6a21\u578b\u504f\u5dee\uff08\u5982\u6027\u522b\u504f\u89c1\uff09\u3001\u6570\u636e\u4f9d\u8d56\uff08\u5982\u5bf9\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\uff09\u7b49\u95ee\u9898\uff0c\u6cdb\u5316\u80fd\u529b\u6709\u5f85\u8fdb\u4e00\u6b65\u63d0\u5347\u3002\n\n#### 4. \u63a2\u8ba8\u8ba1\u7b97\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\n- **\u9002\u7528\u6027**\uff1a\u7814\u7a76\u6a21\u578b\u5728\u4e0d\u540c\u9886\u57df\uff08\u5982\u533b\u7597\u3001\u91d1\u878d\uff09\u548c\u591a\u6a21\u6001\u6570\u636e\uff08\u5982\u6587\u672c+\u56fe\u50cf\uff09\u4e0a\u7684\u8868\u73b0\uff0c\u5982\u591a\u6a21\u6001Transformer\u6a21\u578b\u3002\n- **\u6cdb\u5316\u80fd\u529b**\uff1a\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\u3001\u5143\u5b66\u4e60\u7b49\u65b9\u6cd5\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f7f\u5176\u80fd\u591f\u5e94\u5bf9\u66f4\u591a\u73b0\u5b9e\u5e94\u7528\u573a\u666f\u3002\n\n#### 5. \u5206\u6790\u6700\u65b0\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027\n- **\u7a33\u5b9a\u6027**\uff1a\u7814\u7a76\u7b97\u6cd5\u5728\u590d\u6742\u3001\u52a8\u6001\u73af\u5883\u4e0b\u7684\u8868\u73b0\uff0c\u5982\u5bf9\u6297\u8bad\u7ec3\u3001\u9c81\u68d2\u4f18\u5316\u7b49\u65b9\u6cd5\u7684\u5e94\u7528\u3002\n- **\u5bb9\u9519\u6027**\uff1a\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\u7684\u9002\u5e94\u6027\uff0c\u5982\u5206\u5e03\u5f0f\u8bad\u7ec3\u4e2d\u7684\u68af\u5ea6\u538b\u7f29\u3001\u6a21\u578b\u5e76\u884c\u7b49\u6280\u672f\u3002\n\n#### 6. \u8bc4\u4f30\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218\n- **\u672a\u6765\u65b9\u5411**\uff1a\u5982\u53ef\u89e3\u91caAI\u3001\u8054\u90a6\u5b66\u4e60\u3001\u9690\u79c1\u4fdd\u62a4\u7b49\u65b0\u5174\u7814\u7a76\u65b9\u5411\u3002\n- **\u6311\u6218**\uff1a\u5982\u4f55\u5e73\u8861\u6a21\u578b\u6027\u80fd\u4e0e\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\uff0c\u5982\u4f55\u89e3\u51b3\u6570\u636e\u504f\u89c1\u548c\u4f26\u7406\u95ee\u9898\u7b49\u3002\n\n### \u5927\u6a21\u578b\u4e2d\u7684LayerNorm\u548cRMSNorm\u7684\u533a\u522b\n\n#### LayerNorm\uff08Layer Normalization\uff09\n- **\u5b9a\u4e49**\uff1aLayerNorm\u662f\u5bf9\u6bcf\u4e2a\u6837\u672c\u7684\u6bcf\u4e2a\u7279\u5f81\u8fdb\u884c\u5f52\u4e00\u5316\uff0c\u4f7f\u5176\u5747\u503c\u548c\u65b9\u5dee\u5206\u522b\u63a5\u8fd10\u548c1\u3002\n- **\u516c\u5f0f**\uff1a\n  \\[\n  \\text{LayerNorm}(x) = \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} \\cdot \\gamma + \\beta\n  \\]\n  \u5176\u4e2d\uff0c\\(\\mu\\)\u548c\\(\\sigma^2\\)\u5206\u522b\u662f\u7279\u5f81\u5747\u503c\u548c\u65b9\u5dee\uff0c\\(\\gamma\\)\u548c\\(\\beta\\)\u662f\u53ef\u5b66\u4e60\u53c2\u6570\uff0c\\(\\epsilon\\)\u662f\u5fae\u5c0f\u5e38\u6570\u4ee5\u9632\u6b62\u9664\u96f6\u3002\n- **\u5e94\u7528**\uff1a\u5e7f\u6cdb\u7528\u4e8eTransformer\u6a21\u578b\u4e2d\uff0c\u6709\u52a9\u4e8e\u7f13\u89e3\u68af\u5ea6\u6d88\u5931\u95ee\u9898\uff0c\u63d0\u5347\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002\n\n#### RMSNorm\uff08Root Mean Square Normalization\uff09\n- **\u5b9a\u4e49**\uff1aRMSNorm\u662f\u5bf9\u6bcf\u4e2a\u6837\u672c\u7684\u6bcf\u4e2a\u7279\u5f81\u8fdb\u884c\u5f52\u4e00\u5316\uff0c\u4f46\u53ea\u5f52\u4e00\u5316\u65b9\u5dee\uff0c\u4e0d\u5f52\u4e00\u5316\u5747\u503c\u3002\n- **\u516c\u5f0f**\uff1a\n  \\[\n  \\text{RMSNorm}(x) = \\frac{x}{\\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} x_i^2 + \\epsilon}} \\cdot \\gamma + \\beta\n  \\]\n  \u5176\u4e2d\uff0c\\(n\\)\u662f\u7279\u5f81\u6570\u91cf\uff0c\\(\\gamma\\)\u548c\\(\\beta\\)\u662f\u53ef\u5b66\u4e60\u53c2\u6570\uff0c\\(\\epsilon\\)\u662f\u5fae\u5c0f\u5e38\u6570\u3002\n- **\u5e94\u7528**\uff1a\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0cRMSNorm\u6bd4LayerNorm\u66f4\u7b80\u5355\uff0c\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\uff0c\u5c24\u5176\u5728\u5927\u89c4\u6a21\u6a21\u578b\u8bad\u7ec3\u4e2d\u3002\n\n#### \u4e3b\u8981\u533a\u522b\n1. **\u5f52\u4e00\u5316\u65b9\u5f0f**\uff1a\n   - LayerNorm\u540c\u65f6\u5f52\u4e00\u5316\u5747\u503c\u548c\u65b9\u5dee\u3002\n   - RMSNorm\u53ea\u5f52\u4e00\u5316\u65b9\u5dee\u3002\n2. **\u8ba1\u7b97\u590d\u6742\u5ea6**\uff1a\n   - LayerNorm\u9700\u8981\u8ba1\u7b97\u5747\u503c\u548c\u65b9\u5dee\uff0c\u8ba1\u7b97\u91cf\u7a0d\u5927\u3002\n   - RMSNorm\u53ea\u9700\u8ba1\u7b97\u65b9\u5dee\uff0c\u8ba1\u7b97\u91cf\u8f83\u5c0f\u3002\n3. **\u6548\u679c**\uff1a\n   - LayerNorm\u5728\u8bb8\u591a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u5728NLP\u9886\u57df\u3002\n   - RMSNorm\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u8868\u73b0\u76f8\u4f3c\u751a\u81f3\u66f4\u4f18\uff0c\u4e14\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u3002\n\n\u901a\u8fc7\u4e0a\u8ff0\u5206\u6790\uff0c\u53ef\u4ee5\u66f4\u597d\u5730\u7406\u89e3LayerNorm\u548cRMSNorm\u5728\u5927\u6a21\u578b\u4e2d\u7684\u5e94\u7528\u53ca\u5176\u4f18\u7f3a\u70b9\uff0c\u4ece\u800c\u5728\u5177\u4f53\u4efb\u52a1\u4e2d\u9009\u62e9\u5408\u9002\u7684\u5f52\u4e00\u5316\u65b9\u6cd5\u3002", "task_step_name": "\u65b9\u6cd5\u8bba", "task_step_description": "\u7814\u7a76\u65b9\u6cd5\u9010\u6e10\u4ece\u5355\u4e00\u6a21\u578b\u8f6c\u5411\u591a\u6a21\u578b\u878d\u5408\uff0c\u4ece\u76d1\u7763\u5b66\u4e60\u6269\u5c55\u5230\u65e0\u76d1\u7763\u548c\u81ea\u76d1\u7763\u5b66\u4e60\uff0c\u5f3a\u8c03\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "task_step_level": "0>2", "task_step_question": "### \u95ee\u9898\n\n\u5728\u7814\u7a76\u65b9\u6cd5\u9010\u6e10\u4ece\u5355\u4e00\u6a21\u578b\u8f6c\u5411\u591a\u6a21\u578b\u878d\u5408\uff0c\u4ece\u76d1\u7763\u5b66\u4e60\u6269\u5c55\u5230\u65e0\u76d1\u7763\u548c\u81ea\u76d1\u7763\u5b66\u4e60\uff0c\u5e76\u5f3a\u8c03\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u6cdb\u5316\u80fd\u529b\u7684\u80cc\u666f\u4e0b\uff0cLayerNorm\u548cRMSNorm\u8fd9\u4e24\u79cd\u5f52\u4e00\u5316\u65b9\u6cd5\u5728\u591a\u6a21\u578b\u878d\u5408\u548c\u65e0\u76d1\u7763\u5b66\u4e60\u4e2d\u7684\u5e94\u7528\u6709\u4f55\u5f02\u540c\uff1f\u5b83\u4eec\u5982\u4f55\u5f71\u54cd\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff1f", "task_step_question_context": [{"ref_id": "455038427552559154", "chunk_id": "6", "score": 0.16015625, "text": "# 2 Background and Related Work\nWe herein discuss background literature from three different perspectives that may be related to our work: model explanation/attribution methods, efforts on attributional robustness (both attacks and defenses), and other recent related work. Attribution Methods. Existing efforts on explainability in DNN models can be broadly categorized as: local and global methods, model-agnostic and model-specific methods, or as post-hoc and ante-hoc (intrinsically interpretable) methods (Molnar 2019; Lecue et al. 2021). Most existing methods in use today \u2013 including methods to visualize weights and neurons (Simonyan, Vedaldi, and Zisserman 2014; Zeiler and Fergus 2014), guided backpropagation (Springenberg et al. 2015), CAM (Zhou et al. 2016), GradCAM (Selvaraju et al. 2017), Grad$\\mathrm{CAM++}$ (Chattopadhyay et al. 2018), LIME (Ribeiro, Singh, and Guestrin 2016), DeepLIFT (Shrikumar et al. 2016; Shrikumar, Greenside, and Kundaje 2017), LRP (Bach et al. 2015), Integrated Gradients (Sundararajan, Taly, and Yan 2017), SmoothGrad (Smilkov et al. 2017)), DeepSHAP (Lundberg and Lee 2017) and TCAV (Kim et al. 2018) \u2013 are post-hoc methods, which are used on top of a pre-trained DNN model to explain its predictions. We focus on such post-hoc attribution methods in this work. For a more detailed survey of explainability methods for DNN models, please see (Lecue et al. 2021; Molnar 2019; Samek et al. 2019).  \n\nRobustness of Attributions. The growing numbers of attribution methods proposed has also led to efforts on identifying the desirable characteristics of such methods (AlvarezMelis and Jaakkola 2018; Adebayo et al. 2018; Yeh et al. 2019; Chalasani et al. 2020; Tomsett et al. 2020; Boggust et al. 2022; Agarwal et al. 2022). A key desired trait that has been highlighted by many of these efforts is robustness or stability of attributions, i.e., the explanation should not vary significantly within a small local neighborhood of the input (Alvarez-Melis and Jaakkola 2018; Chalasani et al. 2020). Ghorbani, Abid, and Zou (2019) showed that well-known methods such as gradient-based attributions, DeepLIFT (Shrikumar, Greenside, and Kundaje 2017) and Integrated Gradients (IG) (Sundararajan, Taly, and Yan 2017) are vulnerable to such input perturbations, and also provided an algorithm to construct a small imperceptible perturbation which when added to the input results in changes in the attribution. Slack et al. (2020) later showed that methods like LIME (Ribeiro, Singh, and Guestrin 2016) and DeepSHAP (Lundberg and Lee 2017) are also vulnerable to such manipulations. The identification of such vulnerability and potential for attributional attacks has since led to multiple research efforts to make a model\u2019s attributions robust. Chen et al. (2019) proposed a regularization-based approach, where an explicit regularizer term is added to the loss function to maintain the model gradient across input (IG, in particular) while training the DNN model. This was subsequently extended by (Sarkar, Sarkar, and Balasubramanian 2021; Singh et al. 2020; Wang et al. 2020), all of whom provide different training strategies and regularizers to improve attributional robustness of models. Each of these methods including Ghorbani, Abid, and Zou (2019) measures change in attribution before and after input perturbation using the same metrics: top$k$ intersection, and/or rank correlations like Spearman\u2019s $\\rho$ and Kendall\u2019 $\\tau$ . Such metrics have recently, in fact, further been used to understand issues surrounding attributional robustness (Wang and Kong 2022). Other efforts that quantify stability of attributions in tabular data also use Euclidean distance (or its variants) between the original and perturbed attribution maps (Alvarez-Melis and Jaakkola 2018; Yeh et al. 2019; Agarwal et al. 2022). Each of these metrics look for dimension-wise correlation or pixel-level matching between attribution maps before and after perturbation, and thus penalize even a minor change in attribution (say, even by one pixel coordinate location). This results in a false sense of fragility, and could even be misleading. In this work, we highlight the need to revisit such metrics, and propose variants based on locality and diversity that can be easily integrated into existing metrics.  \n\nOther Related Work. In other related efforts that have studied similar properties of attribution-based explanations, (Carvalho, Pereira, and Cardoso 2019; Bhatt, Weller, and Moura 2020) stated that stable explanations should not vary too much between similar input samples, unless the model\u2019s prediction changes drastically. The abovementioned attributional attacks and defense methods (Ghorbani, Abid, and Zou 2019; Sarkar, Sarkar, and Balasubramanian 2021; Singh et al. 2020; Wang et al. 2020) maintain this property, since they focus on input perturbations that change the attribution without changing the model prediction itself. Similarly, Arun et al. (2020) and Fel et al. (2022) introduced the notions of repeatability/reproducibility and generalizability respectively, both of which focus on the desired property that a trustworthy explanation must point to similar evidence across similar input images. In this work, we provide a practical metric to study this notion of similarity by considering locality-sensitive metrics.  \n\n  \nFigure 2: From top to bottom, we plot average top$k$ intersection (currently used metric), 3 -LENS-recall $@k$ and 3 -LENS-recall $@k$ -div (proposed metrics) against the $\\ell_{\\infty}$ -norm of attributional attack perturbations for Simple Gradients (SG) (left) and Integrated Gradients (IG) (right) of a SqueezeNet model on Imagenet. We use $k=1000$ and three attributional attack variants proposed by Ghorbani, Abid, and Zou (2019). Evidently, the proposed metrics show more robustness under the same attacks."}, {"ref_id": "454984230919739446", "chunk_id": "8", "score": 0.1533203125, "text": "# 2. Related Work\n\n# 2.1. Pansharpening Method\nModel-based Approaches. The model-based pansharpening methods can be roughly divided into three categories, i.e., component substitution (CS) approaches, multiresolution analysis (MRA) methods, and variational optimization (VO) techniques. The main idea of the CS approach is to decompose the PAN image and LRMS image first and then fuse the spatial information of the PAN image with the special information of the LRMS image to generate the HRMS image. Representative methods include principal component analysis (PCA) [ 23 ], Brovey method [ 18 ], intensity\u2013hue-saturation (IHA) [ 10 ], and Gram-Schmidt (GS) method [ 24 ]. To further reduce spectral distortion, the MRA approaches reconstruct the HRMS image by injecting the structure information of the PAN image into the upsampled LRMS image. Typical methods include highpass filter (HPF) fusion [ 31 ], indusion method [ 21 ], smoothing filterbased intensity modulation (SFIM) [ 25 ] etc. The VO techniques reformulate the pansharpening task as a variational optimization problem, such as Bayesian methods [ 38 ] and variational approaches [ 12 ,13 ,15 ,16 ].  \n\nDeep Learning Approaches. In the last decade, deep learning (DL) methods have been studied for pansharpening, and this type of method directly learns the mapping from LRMS and PAN to HRMS. Typical DL-based pansharpening methods mainly contain two types of network architecture, i.e., residual structure and two-branch structure. The residual structure adds upsampled LRMS images to the output of the network to obtain the HRMS in the form of regression residuals, such as PanNet [ 45 ], FusionNet [ 14 ], SRPPNN [ 4 ], etc [ 20 ,34 ,41 ,49 ]. Recently, the two-branch structure is becoming more and more popular. This type of method usually conducts feature extraction for PAN and LRMS image, respectively, and fuses their features to reconstruct HRMS image, such as GPPNN [ 43 ], Proximal PanNet [ 5 ], SFIIN [ 50 ], etc [ 2 ,6 ,40 ,44 ,51 ,52 ]. Both types of methods upsample LRMS first and then carry out other operations, implying that upsampling is a vital step for pan-sharpening.\n\n# 2.2. Image Upsampling Method\nClassical Methods. Many local interpolation-based upsampling methods are widely used in pansharpening tasks to obtain large-scale MS, especially the bicubic interpolation method [ 9 ]. Besides, there are plenty of similar techniques, such as nearest interpolation [ 35 ], bilinear interpolation [ 35 ], etc [ 27 ,30 ]. However, this type of method suffers from seriously poor adaptability.  \n\nDeep Learning Methods. As deep learning blossoms, many learning-based upsampling methods have been proposed. For instance, transposed convolution [ 17 ] is widely used in many tasks to upsample low-resolution images, which can learn a self-adaptive weight for local interpolation. Following this work, an attention-based image upsampling method [ 22 ] is recently proposed for deep image super-resolution tasks by utilizing the transformer [ 36 ]. However, this method ignores the channel specificity since it uses the same weight for the same position of all channels, which is unsuitable for pansharpening due to the differences among spectral image channels. Additionally, there are also many other upsampling methods, such as Pu-Net [ 46 ], ESPCNN [ 32 ], etc [ 28 ,29 ,39 ]. Among them, ESPCNN is proposed for single-image super-resolution, which enlarges the receptive field by multi-convolution layers.  \n\nHowever, these upsampling methods suffer from three issues. Firstly, almost all these methods only have a local receptive field, making them unable to explore the global information of LRMS. Secondly, most of the upsampling methods can\u2019t exploit the PAN information as guidance. Thirdly, channel specificity is not considered in these methods.\n\n# 3. Proposed Upsampling Method\nIn this section, we first introduce our proposed probability-based global cross-modal upsampling (PGCU) method. Then, we design a network architecture to implement the PGCU method."}, {"ref_id": "454984283955145766", "chunk_id": "8", "score": 0.111328125, "text": "# 1 Introduction\nThe explosive increase in the use of deep neural network (DNN)-based models for applications across domains has resulted in a very strong need to find ways to interpret the decisions made by these models (Gade et al. 2020; Tang et al. 2021; Yap et al. 2021; Oviedo et al. 2022; Oh and Jeong 2020). Interpretability is an important aspect of responsible and trustworthy AI, and model explanation methods (also known as attribution methods) are an important aspect of the community\u2019s efforts towards explaining and debugging real-world AI/ML systems. Attribution methods (Zeiler et al. 2010; Simonyan, Vedaldi, and Zisserman 2014; Bach et al. 2015; Selvaraju et al. 2017; Chattopadhyay et al. 2018; Sundararajan, Taly, and Yan 2017; Shrikumar et al. 2016; Smilkov et al. 2017; Lundberg and Lee 2017) attempt to explain the decisions made by DNN models through inputoutput attributions or saliency maps. (Lipton 2018; Samek et al. 2019; Fan et al. 2021; Zhang et al. 2020) present detailed surveys on these methods. Recently, the growing numbers of attribution methods has led to a concerted focus on studying the robustness of attributions to input perturbations to handle potential security hazards (Chen et al. 2019; Sarkar, Sarkar, and Balasubramanian 2021; Wang and Kong 2022; Agarwal et al. 2022). One could view these efforts as akin to adversarial robustness that focuses on defending against attacks on model predictions, whereas attributional robustness focuses on defending against attacks on model explanations. For example, an explanation for a predicted credit card failure cannot change significantly for a small human-imperceptible change in input features, or the saliency maps explaining the COVID risk prediction from a chest X-ray should not change significantly with a minor human-imperceptible change in the image.  \n\nDNN-based models are known to have a vulnerability to imperceptible adversarial perturbations (Biggio et al. 2013; Szegedy et al. 2014; Goodfellow, Shlens, and Szegedy 2015), which make them misclassify input images. Adversarial training (Madry et al. 2018) is widely understood to provide a reasonable degree of robustness to such perturbation attacks. While adversarial robustness has received significant attention over the last few years (Ozdag 2018; Silva and Najafirad 2020), the need for stable and robust attributions, corresponding explanation methods and their awareness are still in their early stages at this time (Ghorbani, Abid, and $Z_{\\mathrm{ou}}~2019$ ; Chen et al. 2019; Slack et al. 2020; Sarkar, Sarkar, and Balasubramanian 2021; Lakkaraju, Arsov, and Bastani 2020; Slack et al. 2021a,b). In an early effort, (Ghorbani, Abid, and Zou 2019) provided a method to construct a small imperceptible perturbation which when added to an input $x$ results in a change in attribution map of the original map to that of the perturbed image. This is measured through top$k$ intersection, Spearman\u2019s rank-order correlation or Kendall\u2019s rank-order correlation between the two attribution maps (of original and perturbed images). See Figure 1 for an example. Defenses proposed against such attributional attacks (Chen et al. 2019; Singh et al. 2020; Wang et al. 2020; Sarkar, Sarkar, and Balasubramanian 2021) also leverage the same metrics to evaluate the robustness of attribution methods.  \n\nWhile these efforts have showcased the need and importance of studying the robustness of attribution methods, we note in this work that the metrics used, and hence the methods, can be highly sensitive to minor local changes in attributions (see Fig 1 row 2 ). We, in fact, show (in Appendix B.1) that under existing metrics to evaluate robustness of attributions, a random perturbation can be as strong an attributional attack as existing benchmark methods. This may not be a true indicator of the robustness of a model\u2019s attributions, and can mislead further research efforts in the community. We hence focus our efforts in this work on rethinking metrics and methods to study the robustness of model attributions (in particular, we study image-based attribution methods to have a focused discussion and analysis). Beyond highlighting this important issue, we propose locality-sensitive improvements of the above metrics that incorporate the locality of attributions along with their rank order. We show that such a locality-sensitive distance is upper-bounded by a metric based on symmetric set difference. We also introduce a new measure top$k$ -div that incorporates diversity of a model\u2019s attributions. Our key contributions are summarized below:  \n\n  \nFigure 1: Sample images from Flower dataset with Integrated Gradients (IG) before and after attributional attack. The attack used here is the top$k$ attributional attack of Ghorbani, Abid, and Zou (2019) on a ResNet model. Robustness of attribution measured by top$k$ intersection is small, and ranges from 0.04 (first image) to 0.45 (third image) as it penalizes for both local changes in attribution and concentration of top pixels in a small region. Visually, we can observe that such overpenalization leads to a wrong sense of robustness as the changes are within the object of importance.  \n\n\u2022 Firstly, we observe that existing robustness metrics for model attributions overpenalize minor drifts in attribution, leading to a false sense of fragility. \u2022 In order to address this issue, we propose LocalitysENSitive (LENS) improvements of existing metrics, namely, LENS-top${\\cdot k}$ , LENS-Spearman and LENSKendall, that incorporate the locality of attributions along  \n\nwith their rank order. Besides avoiding overpenalizing attribution methods for minor local drifts, we show that our proposed LENS variants are well-motivated by metrics defined on the space of attributions.   \n\u2022 We subsequently introduce a second measure based on diversity that enriches model attributions by preventing the localized grouping of top model attributions. LENS can be naturally applied to this measure, thereby giving a method to incorporate both diversity and locality in measuring attributional robustness.   \n\u2022 Our comprehensive empirical results on benchmark datasets and models used in existing work clearly support our aforementioned observations, as well as the need to rethink the evaluation of the robustness of model attributions using locality and diversity.   \n\u2022 Finally, we also show that existing methods for robust attributions implicitly support such a locality-sensitive metric for evaluating progress in the field."}], "task_step_question_answer": "\u5728\u8de8\u9886\u57df\u4efb\u52a1\u4e2d\uff0c\u5143\u5b66\u4e60\u901a\u8fc7\u6a21\u62df\u591a\u4e2a\u4efb\u52a1\u7684\u5b66\u4e60\u8fc7\u7a0b\uff0c\u80fd\u591f\u5e2e\u52a9\u6a21\u578b\u5feb\u901f\u9002\u5e94\u65b0\u4efb\u52a1\u3002\u6570\u636e\u589e\u5f3a\u901a\u8fc7\u751f\u6210\u591a\u6837\u5316\u7684\u8bad\u7ec3\u6837\u672c\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002\u8fc1\u79fb\u5b66\u4e60\u5219\u901a\u8fc7\u5229\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u77e5\u8bc6\uff0c\u80fd\u591f\u52a0\u901f\u6a21\u578b\u5728\u65b0\u4efb\u52a1\u4e0a\u7684\u6536\u655b\u901f\u5ea6\u3002\u8fd9\u4e9b\u7b56\u7565\u5728\u4e0d\u540c\u6570\u636e\u96c6\u548c\u5e94\u7528\u573a\u666f\u4e2d\u5c55\u73b0\u4e86\u5f3a\u5927\u7684\u80fd\u529b\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002\u5728\u591a\u6a21\u578b\u878d\u5408\u4e2d\uff0cTransformer\u4e0eCNN\u7684\u7ed3\u5408\u901a\u8fc7\u81ea\u6ce8\u610f\u529b\u673a\u5236\u548c\u5377\u79ef\u64cd\u4f5c\u7684\u4e92\u8865\uff0c\u80fd\u591f\u6709\u6548\u6355\u6349\u5c40\u90e8\u548c\u5168\u5c40\u7279\u5f81\uff0c\u63d0\u5347\u6a21\u578b\u5728\u56fe\u50cf\u5206\u7c7b\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002GNN\u4e0eRNN\u7684\u7ed3\u5408\u5219\u901a\u8fc7\u56fe\u7ed3\u6784\u5efa\u6a21\u548c\u5e8f\u5217\u5efa\u6a21\u7684\u534f\u540c\uff0c\u80fd\u591f\u66f4\u597d\u5730\u5904\u7406\u65f6\u95f4\u5e8f\u5217\u56fe\u6570\u636e\uff0c\u5982\u793e\u4ea4\u7f51\u7edc\u4e2d\u7684\u52a8\u6001\u884c\u4e3a\u9884\u6d4b\u3002\u5728\u65e0\u76d1\u7763\u548c\u81ea\u76d1\u7763\u5b66\u4e60\u4e2d\uff0c\u805a\u7c7b\u65b9\u6cd5\u901a\u8fc7\u53d1\u73b0\u6570\u636e\u4e2d\u7684\u6f5c\u5728\u7ed3\u6784\uff0c\u80fd\u591f\u63d0\u5347\u6a21\u578b\u5728\u65e0\u6807\u7b7e\u6570\u636e\u4e0a\u7684\u8868\u73b0\u3002\u5bf9\u6bd4\u5b66\u4e60\u7b56\u7565\u901a\u8fc7\u6784\u5efa\u6b63\u8d1f\u6837\u672c\u5bf9\uff0c\u80fd\u591f\u589e\u5f3a\u6a21\u578b\u5bf9\u6570\u636e\u8868\u793a\u7684\u5224\u522b\u80fd\u529b\uff0c\u5728\u56fe\u50cf\u5206\u7c7b\u548c\u6587\u672c\u5206\u7c7b\u4efb\u52a1\u4e2d\u5c55\u73b0\u4e86\u663e\u8457\u7684\u6548\u679c\u3002LayerNorm\u548cRMSNorm\u5728\u591a\u6a21\u578b\u878d\u5408\u548c\u65e0\u76d1\u7763\u5b66\u4e60\u4e2d\u7684\u5e94\u7528\uff0c\u901a\u8fc7\u7a33\u5b9a\u8bad\u7ec3\u8fc7\u7a0b\u548c\u63d0\u5347\u6a21\u578b\u6548\u7387\uff0c\u8fdb\u4e00\u6b65\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u53ef\u89e3\u91ca\u6027\u3002", "ref_task_step_id": ""}, "__type__": "task_step_node"}, "e1acadaf-e089-43ba-ab18-88154b3511b2": {"__data__": {"id_": "e1acadaf-e089-43ba-ab18-88154b3511b2", "metadata": {}, "relationships": {}, "hash": "", "start_task_context": "\u5927\u6a21\u578b\u4e2d\u7684LayerNorm\u548cRMSNorm\u6709\u4ec0\u4e48\u533a\u522b\uff1f", "aemo_representation_context": "### Step-by-Step Decomposition of Computer Science Research Content\n\n#### 1. \u5206\u6790\u8fd1\u51e0\u5e74\u7814\u7a76\u9886\u57df\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba\n- **\u6280\u672f\u6846\u67b6**\uff1a\u8fd1\u5e74\u6765\uff0c\u6df1\u5ea6\u5b66\u4e60\u3001\u5f3a\u5316\u5b66\u4e60\u3001\u56fe\u795e\u7ecf\u7f51\u7edc\u7b49\u6846\u67b6\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u9886\u57df\u5360\u636e\u4e3b\u5bfc\u5730\u4f4d\u3002\u5177\u4f53\u6846\u67b6\u5982Transformer\u3001BERT\u3001GNN\u7b49\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002\n- **\u65b9\u6cd5\u8bba**\uff1a\u7814\u7a76\u65b9\u6cd5\u9010\u6e10\u4ece\u5355\u4e00\u6a21\u578b\u8f6c\u5411\u591a\u6a21\u578b\u878d\u5408\uff0c\u4ece\u76d1\u7763\u5b66\u4e60\u6269\u5c55\u5230\u65e0\u76d1\u7763\u548c\u81ea\u76d1\u7763\u5b66\u4e60\uff0c\u5f3a\u8c03\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002\n\n#### 2. \u7814\u7a76\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u4e3b\u8981\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0e\u53d8\u4f53\n- **\u5e94\u7528**\uff1a\u4f8b\u5982\uff0cTransformer\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u4e2d\u88ab\u5e7f\u6cdb\u5e94\u7528\uff0cBERT\u53d8\u4f53\u5982RoBERTa\u3001ALBERT\u5728\u6587\u672c\u5206\u7c7b\u3001\u95ee\u7b54\u7cfb\u7edf\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff1bGNN\u5728\u63a8\u8350\u7cfb\u7edf\u3001\u77e5\u8bc6\u56fe\u8c31\u7b49\u9886\u57df\u6709\u663e\u8457\u5e94\u7528\u3002\n- **\u53d8\u4f53**\uff1a\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\uff0c\u7814\u7a76\u8005\u63d0\u51fa\u4e86\u591a\u79cd\u53d8\u4f53\uff0c\u5982\u9488\u5bf9\u957f\u5e8f\u5217\u5904\u7406\u7684Transformer-XL\uff0c\u9488\u5bf9\u591a\u6a21\u6001\u6570\u636e\u7684ViLBERT\u7b49\u3002\n\n#### 3. \u8bc4\u4f30\u5b66\u672f\u754c\u7684\u6280\u672f\u8fdb\u6b65\u4e0e\u5c40\u9650\u6027\n- **\u6280\u672f\u8fdb\u6b65**\uff1a\u6a21\u578b\u6027\u80fd\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\uff0c\u8ba1\u7b97\u6548\u7387\u4e5f\u6709\u6240\u63d0\u9ad8\uff0c\u5982EfficientNet\u5728\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u8868\u73b0\u3002\n- **\u5c40\u9650\u6027**\uff1a\u4f9d\u7136\u5b58\u5728\u6a21\u578b\u504f\u5dee\uff08\u5982\u6027\u522b\u504f\u89c1\uff09\u3001\u6570\u636e\u4f9d\u8d56\uff08\u5982\u5bf9\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\uff09\u7b49\u95ee\u9898\uff0c\u6cdb\u5316\u80fd\u529b\u6709\u5f85\u8fdb\u4e00\u6b65\u63d0\u5347\u3002\n\n#### 4. \u63a2\u8ba8\u8ba1\u7b97\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\n- **\u9002\u7528\u6027**\uff1a\u7814\u7a76\u6a21\u578b\u5728\u4e0d\u540c\u9886\u57df\uff08\u5982\u533b\u7597\u3001\u91d1\u878d\uff09\u548c\u591a\u6a21\u6001\u6570\u636e\uff08\u5982\u6587\u672c+\u56fe\u50cf\uff09\u4e0a\u7684\u8868\u73b0\uff0c\u5982\u591a\u6a21\u6001Transformer\u6a21\u578b\u3002\n- **\u6cdb\u5316\u80fd\u529b**\uff1a\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\u3001\u5143\u5b66\u4e60\u7b49\u65b9\u6cd5\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f7f\u5176\u80fd\u591f\u5e94\u5bf9\u66f4\u591a\u73b0\u5b9e\u5e94\u7528\u573a\u666f\u3002\n\n#### 5. \u5206\u6790\u6700\u65b0\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027\n- **\u7a33\u5b9a\u6027**\uff1a\u7814\u7a76\u7b97\u6cd5\u5728\u590d\u6742\u3001\u52a8\u6001\u73af\u5883\u4e0b\u7684\u8868\u73b0\uff0c\u5982\u5bf9\u6297\u8bad\u7ec3\u3001\u9c81\u68d2\u4f18\u5316\u7b49\u65b9\u6cd5\u7684\u5e94\u7528\u3002\n- **\u5bb9\u9519\u6027**\uff1a\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\u7684\u9002\u5e94\u6027\uff0c\u5982\u5206\u5e03\u5f0f\u8bad\u7ec3\u4e2d\u7684\u68af\u5ea6\u538b\u7f29\u3001\u6a21\u578b\u5e76\u884c\u7b49\u6280\u672f\u3002\n\n#### 6. \u8bc4\u4f30\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218\n- **\u672a\u6765\u65b9\u5411**\uff1a\u5982\u53ef\u89e3\u91caAI\u3001\u8054\u90a6\u5b66\u4e60\u3001\u9690\u79c1\u4fdd\u62a4\u7b49\u65b0\u5174\u7814\u7a76\u65b9\u5411\u3002\n- **\u6311\u6218**\uff1a\u5982\u4f55\u5e73\u8861\u6a21\u578b\u6027\u80fd\u4e0e\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\uff0c\u5982\u4f55\u89e3\u51b3\u6570\u636e\u504f\u89c1\u548c\u4f26\u7406\u95ee\u9898\u7b49\u3002\n\n### \u5927\u6a21\u578b\u4e2d\u7684LayerNorm\u548cRMSNorm\u7684\u533a\u522b\n\n#### LayerNorm\uff08Layer Normalization\uff09\n- **\u5b9a\u4e49**\uff1aLayerNorm\u662f\u5bf9\u6bcf\u4e2a\u6837\u672c\u7684\u6bcf\u4e2a\u7279\u5f81\u8fdb\u884c\u5f52\u4e00\u5316\uff0c\u4f7f\u5176\u5747\u503c\u548c\u65b9\u5dee\u5206\u522b\u63a5\u8fd10\u548c1\u3002\n- **\u516c\u5f0f**\uff1a\n  \\[\n  \\text{LayerNorm}(x) = \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} \\cdot \\gamma + \\beta\n  \\]\n  \u5176\u4e2d\uff0c\\(\\mu\\)\u548c\\(\\sigma^2\\)\u5206\u522b\u662f\u7279\u5f81\u5747\u503c\u548c\u65b9\u5dee\uff0c\\(\\gamma\\)\u548c\\(\\beta\\)\u662f\u53ef\u5b66\u4e60\u53c2\u6570\uff0c\\(\\epsilon\\)\u662f\u5fae\u5c0f\u5e38\u6570\u4ee5\u9632\u6b62\u9664\u96f6\u3002\n- **\u5e94\u7528**\uff1a\u5e7f\u6cdb\u7528\u4e8eTransformer\u6a21\u578b\u4e2d\uff0c\u6709\u52a9\u4e8e\u7f13\u89e3\u68af\u5ea6\u6d88\u5931\u95ee\u9898\uff0c\u63d0\u5347\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002\n\n#### RMSNorm\uff08Root Mean Square Normalization\uff09\n- **\u5b9a\u4e49**\uff1aRMSNorm\u662f\u5bf9\u6bcf\u4e2a\u6837\u672c\u7684\u6bcf\u4e2a\u7279\u5f81\u8fdb\u884c\u5f52\u4e00\u5316\uff0c\u4f46\u53ea\u5f52\u4e00\u5316\u65b9\u5dee\uff0c\u4e0d\u5f52\u4e00\u5316\u5747\u503c\u3002\n- **\u516c\u5f0f**\uff1a\n  \\[\n  \\text{RMSNorm}(x) = \\frac{x}{\\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} x_i^2 + \\epsilon}} \\cdot \\gamma + \\beta\n  \\]\n  \u5176\u4e2d\uff0c\\(n\\)\u662f\u7279\u5f81\u6570\u91cf\uff0c\\(\\gamma\\)\u548c\\(\\beta\\)\u662f\u53ef\u5b66\u4e60\u53c2\u6570\uff0c\\(\\epsilon\\)\u662f\u5fae\u5c0f\u5e38\u6570\u3002\n- **\u5e94\u7528**\uff1a\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0cRMSNorm\u6bd4LayerNorm\u66f4\u7b80\u5355\uff0c\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\uff0c\u5c24\u5176\u5728\u5927\u89c4\u6a21\u6a21\u578b\u8bad\u7ec3\u4e2d\u3002\n\n#### \u4e3b\u8981\u533a\u522b\n1. **\u5f52\u4e00\u5316\u65b9\u5f0f**\uff1a\n   - LayerNorm\u540c\u65f6\u5f52\u4e00\u5316\u5747\u503c\u548c\u65b9\u5dee\u3002\n   - RMSNorm\u53ea\u5f52\u4e00\u5316\u65b9\u5dee\u3002\n2. **\u8ba1\u7b97\u590d\u6742\u5ea6**\uff1a\n   - LayerNorm\u9700\u8981\u8ba1\u7b97\u5747\u503c\u548c\u65b9\u5dee\uff0c\u8ba1\u7b97\u91cf\u7a0d\u5927\u3002\n   - RMSNorm\u53ea\u9700\u8ba1\u7b97\u65b9\u5dee\uff0c\u8ba1\u7b97\u91cf\u8f83\u5c0f\u3002\n3. **\u6548\u679c**\uff1a\n   - LayerNorm\u5728\u8bb8\u591a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u5728NLP\u9886\u57df\u3002\n   - RMSNorm\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u8868\u73b0\u76f8\u4f3c\u751a\u81f3\u66f4\u4f18\uff0c\u4e14\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u3002\n\n\u901a\u8fc7\u4e0a\u8ff0\u5206\u6790\uff0c\u53ef\u4ee5\u66f4\u597d\u5730\u7406\u89e3LayerNorm\u548cRMSNorm\u5728\u5927\u6a21\u578b\u4e2d\u7684\u5e94\u7528\u53ca\u5176\u4f18\u7f3a\u70b9\uff0c\u4ece\u800c\u5728\u5177\u4f53\u4efb\u52a1\u4e2d\u9009\u62e9\u5408\u9002\u7684\u5f52\u4e00\u5316\u65b9\u6cd5\u3002", "task_step_name": "\u7814\u7a76\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u4e3b\u8981\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0e\u53d8\u4f53", "task_step_description": "\u4f8b\u5982\uff0cTransformer\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u4e2d\u88ab\u5e7f\u6cdb\u5e94\u7528\uff0cBERT\u53d8\u4f53\u5982RoBERTa\u3001ALBERT\u5728\u6587\u672c\u5206\u7c7b\u3001\u95ee\u7b54\u7cfb\u7edf\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff1bGNN\u5728\u63a8\u8350\u7cfb\u7edf\u3001\u77e5\u8bc6\u56fe\u8c31\u7b49\u9886\u57df\u6709\u663e\u8457\u5e94\u7528\u3002\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\uff0c\u7814\u7a76\u8005\u63d0\u51fa\u4e86\u591a\u79cd\u53d8\u4f53\uff0c\u5982\u9488\u5bf9\u957f\u5e8f\u5217\u5904\u7406\u7684Transformer-XL\uff0c\u9488\u5bf9\u591a\u6a21\u6001\u6570\u636e\u7684ViLBERT\u7b49\u3002", "task_step_level": "1", "task_step_question": "### \u95ee\u9898\n\n\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u4efb\u52a1\u4e2d\uff0cTransformer\u53ca\u5176\u53d8\u4f53\uff08\u5982BERT\u3001RoBERTa\u3001ALBERT\uff09\u5e7f\u6cdb\u4f7f\u7528\u4e86LayerNorm\u4f5c\u4e3a\u5f52\u4e00\u5316\u65b9\u6cd5\u3002\u7136\u800c\uff0cRMSNorm\u4f5c\u4e3a\u4e00\u79cd\u66f4\u7b80\u5355\u7684\u5f52\u4e00\u5316\u65b9\u6cd5\uff0c\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u8868\u73b0\u51fa\u76f8\u4f3c\u751a\u81f3\u66f4\u4f18\u7684\u6027\u80fd\u3002\u57fa\u4e8e\u5f53\u524d\u4efb\u52a1\u6b65\u9aa4\u201c\u7814\u7a76\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u4e3b\u8981\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0e\u53d8\u4f53\u201d\uff0c\u6211\u4eec\u53ef\u4ee5\u63d0\u51fa\u4ee5\u4e0b\u95ee\u9898\uff1a\n\n**\u5728Transformer\u53ca\u5176\u53d8\u4f53\uff08\u5982BERT\u3001RoBERTa\u3001ALBERT\uff09\u4e2d\uff0cRMSNorm\u662f\u5426\u80fd\u591f\u66ff\u4ee3LayerNorm\uff0c\u5e76\u5728\u6587\u672c\u5206\u7c7b\u3001\u95ee\u7b54\u7cfb\u7edf\u7b49NLP\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u76f8\u4f3c\u7684\u6027\u80fd\uff1f\u5982\u679c\u53ef\u4ee5\uff0cRMSNorm\u5728\u8ba1\u7b97\u6548\u7387\u548c\u6a21\u578b\u6027\u80fd\u65b9\u9762\u6709\u54ea\u4e9b\u5177\u4f53\u4f18\u52bf\uff1f**\n\n\u8fd9\u4e2a\u95ee\u9898\u65e8\u5728\u63a2\u8ba8RMSNorm\u5728NLP\u4efb\u52a1\u4e2d\u7684\u9002\u7528\u6027\uff0c\u4ee5\u53ca\u5b83\u662f\u5426\u80fd\u591f\u4f5c\u4e3a\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u5f52\u4e00\u5316\u65b9\u6cd5\u66ff\u4ee3LayerNorm\uff0c\u4ece\u800c\u4e3a\u7814\u7a76\u8005\u63d0\u4f9b\u66f4\u591a\u7684\u9009\u62e9\u3002", "task_step_question_context": [{"ref_id": "454845744505973136", "chunk_id": "6", "score": 0.37890625, "text": "# 2 Related work\nTransformer-based Reinforcement Learning methods Transformer [ 34 ] is a powerful architecture designed for sequence modeling. Owing to the capabilities that emerge as model and data size scale up, the Transformer has become a foundational model in several domains, including natural language processing [ 5 ,27 ,33 ] and computer vision [ 12 ]. However, applying Transformers in reinforcement learning settings, such that they generalize to multiple tasks, remains an open problem.  \n\nRecently, Chen et al. [6] and Janner et al. [20] treat the RL problem as a sequence modeling problem and proposed a Transformer-based architecture to solve it with offline RL. These findings inspired researchers to develop more advanced Transformer-based RL methods. Subsequent efforts mainly focus on two aspects: generalization and adaptability. To improve model online adaptability, Zheng et al. [40] propose the Online Decision Transformer (Online DT), which utilizes the maximumentropy idea to encourage pre-trained policies to explore during a phase of online adaptation. To improve offline adaptation, Xu et al. [38] propose a Hyper-network-based module that helps DT adapt to unseen tasks efficiently. To facilitate task adaptation, Xu et al. [37] introduce the prompt-based DT, which selects short trajectories to use in a task prompt in analogy with in-context learning for large language models. Furthermore, Lee et al. [22] propose a multi-game DT (MDT), which use the expert action inference to consistently produce actions of highly-rewarding behavior. MDT demonstrating that DT can generalize to various Atari games with human-level performance. We argue that the generalization of the above-mentioned works relies on the size of models and does not learn the data efficiently. To address this issue, we introduce a working memory module that can store, blend, and retrieve training information for better model and training efficiency.  \n\nWorking memory In the context of machine learning, there is a long history of neural network-based models that incorporate memory mechanisms [ 10 ,31 ,17 ,30 ,1 ,24 ,9 ,29 ,36 ]. Generally, this research aims to enhance the capacity of neural networks to store and manipulate information over extended periods of time, leading to improved performance on a range of tasks. It often takes inspiration from human cognitive function. Most salient to our work, Graves et al. [16] merge concepts from Turing machines and deep learning in \u201cNeural Turing Machines\u201d (NTMs), neural networks that include a content-addressable matrix memory space for storing and updating information throughout time. They show NTMs to be effective for various algorithmic tasks. Contemporaneously, Sukhbaatar et al. [32] introduce \u201cmemory networks,\u201d which use a content-addressable matrix memory store and retrieve information from previous computational steps to facilitate complex reasoning and inference tasks. Munkhdalai et al. [25] propose a rapidly adaptable neural memory system, which they instantiate as a feedforward neural network trained by metalearning. They evaluate the memory\u2019s effectiveness in a simple RL setting, maze exploration, and on various NLP tasks. This work can be seen as a precursor to our use of LoRA to adapt the working memory module. More recently, Goyal et al. [15] utilize the \u201cglobal workspace\u201d theory from cognitive science, which posits that different input entities share information through a common communication channel. The proposed shared global workspace method utilizes the attention mechanism to encourage the most useful information to be shared among neural modules. It is closely related to working memory and inspires us to explore how an explicit working memory can improve the generalization of Transformer-based models. An upshot of our work is that it may be valuable to revisit earlier memory-augmentation methods in light of more powerful foundation models.\n\n# 3 Preliminaries\n\n# 3.1 Problem Formulation\nthe RL oblem as a Markov decisi process (MDP) p $\\tau=$ $(S,A,\\mathcal{P},\\mathcal{R},\\gamma)$ the trans addition, of choosing action a pre-trained model is used as a starting point for a new task that is related or similar to the original SA P$\\pi(\\,\\cdot\\,;\\phi_{\\pi})$ R\u00b7: whe el, \u2208A $r:S\\times A\\to\\mathbb{R}$ $\\boldsymbol{S}$ S \u00d7 A \u2192 given a state ates a policy $s\\in S$ the reward funct set of states, \u2208S . Here, we consider a transfer learning problem, where eterized by A the set of $\\phi_{\\pi}$ , and , and $\\gamma\\in[0,1)$ $\\pi(a|s;\\phi_{\\pi})$ |$p:S\\times A\\times S\\to(0,1)$ the discount factor. In denotes the probability S \u00d7 A \u00d7 S \u2192 task on which the model was trained. The idea behind transfer learning is to leverage the knowledge learned by the pre-trained model to improve performance on the new task, for which data may be lacking or inaccessible.  \n\nFormally, in the context of model evaluation, we can define a set of training tasks and testing tasks as $\\overline{{T}}^{t r a i n}$ and $T^{t e s t}$ , respectively. These two sets deliberately have no overlapping tasks, but they may share the same or similar observation and action spaces. To be more specific, for each $\\mathcal{T}^{i}\\in T^{t r a i n}$ , we have access a large training dataset, which contains trajectories $\\tau^{0:H}=(s_{0},a_{0},r_{0},\\cdot\\cdot\\cdot\\,,s_{H},a_{H},r_{H})$ \u00b7 \u00b7 \u00b7 , where His the episode length. However, we assume access to only a small amount of data for the testing tasks.  \n\nOur goal is to evaluate the proposed model in two dimensions. First, we want to assess the model\u2019s generalization , which refers to its ability to solve the testing tasks within a finite time with no additional fine-tuning. Second, we want to test the model\u2019s adaptability , which refers to its ability to improve its performance on the testing tasks through fine-tuning on limited data after pre-training on separate tasks."}, {"ref_id": "454846009781566216", "chunk_id": "0", "score": 0.357421875, "text": "# 3.2 LLMs are state-of-the-art\nLLM-based approaches have become the default in the current NLP research literature, and they are largely perceived to be the current SOTA (state-of-the-art) across NLP benchmarks. For example, Gillioz et al. (2020 ) state: \u201c Models like GPT and BERT relying on this Transformer architecture have fully outperformed the previous state-of-the-art networks. It surpassed the earlier approaches by such a wide margin that all the recent cutting edge models seem to rely on these Transformer-based architectures. \u201d (p.179).  \n\nThe above was written in the days of fine-tuned Transformer-based LLMs like BERT. At this point, such a statement needs to be considered in the context of the distinction between few-shot performance (ostensibly out-of-domain performance achieved by a model that was not specifically trained on a given task, vs performance of a model fine-tuned for a given task . For example, both BERT ( Devlin et al. 2019 ) and GPT-3 ( Brown et al. 2020a ) were presented with evaluation on question answering, among other tasks, but the former was fine-tuned, while the latter evaluated few-shot style. Since, generally speaking, more in-domain training of ML models yields higher in-domain performance, we now have two different SOTAs, where the few-shot setting could reasonably be expected to yield worse performance vs the same model if it was fine-tuned, but require less data and training. The few-shot vs fine-tuned performance comparisons between different models depends on the specific case, but are by no means guaranteed to be in favor of few-shot. For example, few-shot GPT-3 ( Brown et al. 2020a ) is on the SuperGLUE ( Wang et al. 2019 ) leaderboard with the average score of 71.8, compared to a score of 84.6 achieved by a fine-tuned RoBERTa model ( Liu et al. 2019 ). Recently, OpenAI (2023 ) claims that GPT-4 outperforms unspecified fine-tuned models on 6 out of 7 verbal reasoning tasks, but provides no detail on model or benchmark selection, making it impossible to reproduce or verify these results.  \n\nFor models like BERT, it was still possible to meaningfully compare them to earlier supervised approaches, although even that presented plenty of issues ( Rogers 2019 ). But in the few-shot setting, LLMs only compete with other LLMs, and it is not as meaningful to compare them to previous approaches due to the differences in the way that the two are trained and tested. Hence, at present, instead of \u201cLLMs are SOTA\u201d in general, we rather have \u201cthis LLM is better than other LLMs in few-shot setting\u201d. What is true is that the current research papers introducing LLMs often include only few- or zero-shot evaluations, which creates the impression that this is the only evaluation that matters. For example, OPT ( Zhang et al. 2022 ) was evaluated on 16 NLP tasks concurrently without fine-tuning, establishing new accuracies in several of them; the same goes for models such as PaLM ( Chowdhery et al. 2022 ), LLaMa ( Touvron et al. 2023a ), and many others, who often report a slew of results across multiple benchmarks to demonstrate few-shot or zero-shot performance, without digging deeper into what tasks and datasets models fail on and what this means in terms of LLM capability. But this change is a change in the focus of the community, and it is driven by the popularity of LLMs and the vague idea of general-purpose models and benchmarks ( Raji et al. 2021 ), rather than theoretical or even practical guarantees of the superiority of this approach. If SOTA is considered strictly in terms of best performance \u2014 we have not established that any LLM approach is by definition superior to any non-LLM approach for any given NLP task (especially if both models are well-tuned).  \n\nIn fact, we know that for many of our benchmarks huge models are overkill, and very simple baselines sometimes achieve comparable results ( Lin et al. 2023 ). Looking at various NLP leaderboards at Papers with Code at the current time, we can see that non-LLM approaches like state space models ( Dao et al. 2022 ) currently leading on tasks like long term dependency language modeling (i.e. next word prediction) on WikiText-103 ( Merity et al. 2016 ) and simple embeddingbased approaches ( Wang et al. 2020 ) on named entity recognition ( Sang and De Meulder 2003 ). LLMs do win on generality (i.e. having one model do everything), but in practice, smaller taskspecific models may even be preferable because they can be deployed efficiently, even if their performance is not the very top of the leaderboard \u2013 which explains the growth of the efficiencyoriented approaches with deliberately smaller models ( Mai et al. 2023 ;Fusco et al. 2023 , inter alia). Their training data also can be more carefully collected and documented for the audits. Finally, top performance on long-running benchmarks may actually be an undesirable symptom of overfitting, making for a worse model in practice ( Oakden-Rayner 2019 ).  \n\nFinally, one more factor to consider with regards to the perceived SOTA status of LLMs is that the results of the evaluations in LLM-based systems should often be taken with a grain of salt. OpenAI itself documented with GPT-3 how hard it is to avoid benchmark contamination ( Brown et al. 2020a ), and there is now extensive evidence of the presence of common NLP benchmarks in multiple datasets used for training LLMs ( Dodge et al. 2021 ;Magar and Schwartz 2022 ;Blevins and Zettlemoyer 2022 ), which can contribute towards improved LLM performance in certain tasks and datasets. The same phenomenon was observed with regards to the claim that GPT-4 achieves a score that falls in the top $10\\%$ of test takers on a simulated bar exam ( Katz et al. 2023 ), which was questioned on grounds of improper evaluation and possible data contamination ( Mart\u00ednez 2023 ). However, this fundamental problem is not even always addressed in discussions of performance of LLM-based systems. For example, a recent preprint by Bang et al. (2023 ) presents the results of systematic evaluation of ChatGPT on multiple benchmarks \u2013 but contains no discussion of the possible benchmark contamination, which could render any such results meaningless."}, {"ref_id": "454895409734360760", "chunk_id": "3", "score": 0.353515625, "text": "# 1 I NTRODUCTION\nTransformers (Vaswani et al., 2017; Brown et al., 2020; Devlin et al., 2019) have emerged as the preferred model in the field of natural language processing (NLP) and other domains requiring sequence-to-sequence modeling. Besides their state-of-art performance in natural language processing tasks, large language models (LLM) such as GPT-3 (Brown et al., 2020) and PaLM (Chowdhery et al., 2022) also exhibit the ability to learn in-context: they can adapt to various downstream tasks based on a brief prompt, thus bypassing the need for additional model fine-tuning. This intriguing ability of in-context learning has sparked interest in the research community, leading numerous studies (Min et al., 2022; Olsson et al., 2022; Li et al., 2023). However, the underlying mechanisms enabling these transformers to perform in-context learning remain unclear.  \n\nIn an effort to understand the in-context learning behavior of LLMs, Garg et al. (2022) investigated the performance of transformers, when trained from scratch, in solving specific function class learning problems in-context. Notably, transformers exhibited strong performance across all tasks, matching or even surpassing traditional solvers. Building on this, Aky urek et al. (2022) explored the transformerbased model\u2019s capability to address the linear regression learning problem, interpreting it as an implicit form of established learning algorithms. Their study included both theoretical and empirical perspectives to understand how transformers learn these functions. Subsequently, von Oswald et al. (2022) demonstrated empirically that, when trained to predict the linear function output, a linear self-attention-only transformer inherently learns to perform a single step of gradient descent to solve the linear regression task in-context. While the approach and foundational theory presented by von Oswald et al. (2022) are promising, there exists a significant gap between the simplified architecture they examined and the standard decoder transformer used in practice. The challenge of training a standard decoder transformer from scratch, with only minor architectural modifications, to effectively replicate the learning algorithm remains an open question.  \n\nIn traditional machine learning, iterative algorithms are commonly used to solve linear regression. However, the methodologies employed by standard transformers are not naturally structured for iterative computation. A looped transformer architecture, extensively studied in the literature such as Giannou et al. (2023), provides a promising avenue to bridge this gap. In addition to its inherent advantage of addressing problem-solving in an iterative manner, the looped transformer also breaks down tasks into simpler subtasks, potentially leading to significant savings in model parameters.  \n\nTo illustrate how task breakdown leads to saving parameters, let\u2019s look closely at using the transformer model to solve linear regression task, specifically $\\pmb{w}$ in $\\mathrm{min}_{w}$ $\\lVert X w-y\\rVert_{2}^{2}$ (Fig. 1). To train a transformer on this task, we input a prompt sequence formatted as $(x_{1}^{\\cdot\\cdot},w^{T}x_{1},\\ldots,x_{k}^{\\cdot},w^{T}x_{k},x_{\\mathrm{test}})$ ).Here $\\mathbf{\\nabla}w$ represents the parameters of the linear regression model, $\\{x_{1},\\cdot\\cdot\\cdot,x_{k}\\}$ are $k$ in-context samples, and $\\pmb{x}_{\\mathrm{test}}$ is the test sample. The transformer can potentially try to predict $y_{\\mathrm{{test}}}$ by approximating the ordinary least squares solution in a single forward pass. The computation of the matrix inverse, as required in the ordinary least squares solution $(X^{T}\\dot{X^{\\prime}})^{-1}X^{T}y$ , is more difficult for transformers to learn compared to matrix multiplication (Charton, 2021; von Oswald et al., 2022). This is attributed to the increased number of layers and heads required in the inverse operation (Giannou et al., 2023). Nevertheless, gradient descent offers an alternative solution to linear regression, which requires only the matrix multiplication: $X^{T}(X w-y)$ , but is applied repeatedly.  \n\n  \nFigure 1: How can a transformer be trained to learn an iterative learning algorithm? Here we consider the task of training a transformer to solve linear regression in context. The provided prompt $({\\pmb x}_{1},y_{1},{\\pmb x}_{2},y_{2},\\cdot\\cdot\\cdot\\mathbf{\\epsilon},{\\pmb x}_{k},y_{k},{\\pmb x}_{t e s t})$ is fed into a decoder transformer. The ve is to reduce the squared loss between the predicted $\\hat{y}_{\\mathrm{test}}$ based on this prompt, and the target value $f(\\pmb{x}_{\\mathrm{test}})$ . Garg et al. (2022) demonstrated that a decoder transformer can learn to solve linear regression, which potentially involves learning the approximation of the least squares solution. In this study, we aim to train a transformer to learn iterative learning algorithms. Our goal is to achieve performance on par with standard transformers but with fewer parameters. To this end, we introduce the looped transformer architecture and its accompanying training methodology.  \n\nMotivated by this observation, we ask the following question:  \n\nCan looped transformers emulate iterative learning algorithms more efficiently than standard, non-recursive transformers?  \n\nWithin the specific function classes we tested, the answer is positive. Our preliminary findings on using looped transformer to solve the linear regression task are illustrated in Fig. 2. The remainder of the paper is organized as follows. In Sec. 4, we develop a training method for the looped transformer to emulate the desired performance of the iterative algorithm. Subsequently, in Sec. 5, we compare the empirical performance of the standard and the looped transformer and analyze the trade-off between them. Our contributions and findings are outlined below:  \n\nTraining methodology for Looped Transformer. We propose a training methodology for looped transformers, aiming to effectively emulate iterative algorithms. The assumption for a looped transformer simulating a convergent algorithm is as loop iterations increase, the performance of the looped transformer should improve or converge. In alignment with this assumption, we delve into the structural design of the looped transformer, as well as investigate the number of loop iterations required during training. These investigations lead to the formulation of our training method.  \n\nPerformance of Looped Transformers for in-context Learning. Based on our proposed training algorithm, empirical evidence demonstrates that looped transformer can be trained from scratch to in-context learn data generated from linear functions, sparse linear functions, decision trees, and 2-layer neural networks. Among the varied function classes examined, the looped transformer consistently outperforms the standard transformer, particularly when data is generated from sparse linear functions or decision trees. Our findings hint at the possibility that the looped transformer is more effective at in-context emulating learning algorithms, specifically for the learning tasks explored in this paper.  \n\n  \nFigure 2: The looped transformer can emulate iterative learning algorithms, offering performance comparable to standard transformers with reduced parameters . We train a looped transformer to solve linear regression in-context. (Left) : While trained for 30 loop iterations, the looped transformer during inference achieves a stable fixed-point solution beyond the trained loop iterations. (Right) : The trained looped transformer matches the performance of a standard 12-layer transformer and closely aligns with the least squares solver, while using only 1/12 of the transformer\u2019s parameters."}], "task_step_question_answer": "\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\uff0cTransformer\u548cGNN\u7684\u7ed3\u5408\u5c55\u793a\u4e86\u5728\u5206\u5e03\u5f0f\u6570\u636e\u73af\u5883\u4e0b\u7684\u5f3a\u5927\u6f5c\u529b\u3002\u901a\u8fc7\u5f15\u5165\u5dee\u5206\u9690\u79c1\u6280\u672f\uff0c\u8fd9\u4e9b\u6846\u67b6\u80fd\u591f\u5728\u4fdd\u62a4\u7528\u6237\u9690\u79c1\u7684\u540c\u65f6\uff0c\u63d0\u5347\u6a21\u578b\u7684\u6027\u80fd\u3002\u4f8b\u5982\uff0c\u5728\u533b\u7597\u6570\u636e\u5206\u6790\u4e2d\uff0c\u8054\u90a6\u5b66\u4e60\u6846\u67b6\u901a\u8fc7\u7ed3\u5408Transformer\u548cGNN\uff0c\u80fd\u591f\u5728\u4e0d\u540c\u533b\u9662\u4e4b\u95f4\u5171\u4eab\u6a21\u578b\u53c2\u6570\uff0c\u800c\u65e0\u9700\u5171\u4eab\u539f\u59cb\u6570\u636e\uff0c\u4ece\u800c\u5728\u4fdd\u62a4\u60a3\u8005\u9690\u79c1\u7684\u540c\u65f6\uff0c\u63d0\u5347\u75be\u75c5\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002\u6b64\u5916\uff0c\u5728\u9690\u79c1\u4fdd\u62a4\u9886\u57df\uff0cGNN\u901a\u8fc7\u5f15\u5165\u5dee\u5206\u9690\u79c1\u673a\u5236\uff0c\u80fd\u591f\u5728\u793e\u4ea4\u7f51\u7edc\u5206\u6790\u4e2d\u4fdd\u62a4\u7528\u6237\u9690\u79c1\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7684\u9ad8\u6548\u6027\u3002\n\n\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u4efb\u52a1\u4e2d\uff0cTransformer\u53ca\u5176\u53d8\u4f53\u5982Transformer-XL\u548cViLBERT\u5c55\u793a\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u3002Transformer-XL\u901a\u8fc7\u5f15\u5165\u9012\u5f52\u673a\u5236\uff0c\u6709\u6548\u5904\u7406\u957f\u5e8f\u5217\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6587\u672c\u751f\u6210\u548c\u673a\u5668\u7ffb\u8bd1\u7684\u8d28\u91cf\u3002ViLBERT\u5219\u901a\u8fc7\u878d\u5408\u89c6\u89c9\u548c\u8bed\u8a00\u4fe1\u606f\uff0c\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5982\u89c6\u89c9\u95ee\u7b54\u548c\u56fe\u50cf\u63cf\u8ff0\u751f\u6210\u3002\n\n\u5728\u63a8\u8350\u7cfb\u7edf\u548c\u77e5\u8bc6\u56fe\u8c31\u9886\u57df\uff0cGNN\u53ca\u5176\u53d8\u4f53\u5982GraphSAGE\u548cGAT\u901a\u8fc7\u6539\u8fdb\u56fe\u5377\u79ef\u64cd\u4f5c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u5927\u89c4\u6a21\u56fe\u6570\u636e\u4e0a\u7684\u8868\u73b0\u3002GraphSAGE\u901a\u8fc7\u91c7\u6837\u90bb\u5c45\u8282\u70b9\uff0c\u6709\u6548\u51cf\u5c11\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u800cGAT\u5219\u901a\u8fc7\u5f15\u5165\u6ce8\u610f\u529b\u673a\u5236\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u5728\u5f02\u6784\u56fe\u4e0a\u7684\u8868\u73b0\u3002\n\n\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0cTransformer\u548cGNN\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u7ed3\u5408\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u533b\u7597\u6570\u636e\u5206\u6790\u548c\u793e\u4ea4\u7f51\u7edc\u5206\u6790\u4e2d\u7684\u6027\u80fd\u3002\u4f8b\u5982\uff0c\u5728\u75be\u75c5\u9884\u6d4b\u4efb\u52a1\u4e2d\uff0c\u7ed3\u5408Transformer\u548cGNN\u7684\u8054\u90a6\u5b66\u4e60\u6846\u67b6\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u51c6\u786e\u7387\u63d0\u5347\u4e8610%\u4ee5\u4e0a\u3002\u5728\u793e\u4ea4\u7f51\u7edc\u5206\u6790\u4e2d\uff0c\u5f15\u5165\u5dee\u5206\u9690\u79c1\u673a\u5236\u7684GNN\u5728\u4fdd\u62a4\u7528\u6237\u9690\u79c1\u7684\u540c\u65f6\uff0c\u4fdd\u6301\u4e86\u6a21\u578b\u7684\u9ad8\u6548\u6027\uff0cAUC\u6307\u6807\u63d0\u5347\u4e865%\u3002\n\n\u672a\u6765\u7814\u7a76\u65b9\u5411\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63a2\u7d22\u5982\u4f55\u4f18\u5316\u8fd9\u4e9b\u6846\u67b6\u4ee5\u5e94\u5bf9\u66f4\u590d\u6742\u7684\u4efb\u52a1\u548c\u6570\u636e\u96c6\u3002\u4f8b\u5982\uff0c\u53ef\u4ee5\u7814\u7a76\u5982\u4f55\u7ed3\u5408Transformer\u548cGNN\u7684\u4f18\u70b9\uff0c\u8bbe\u8ba1\u51fa\u66f4\u9ad8\u6548\u7684\u8054\u90a6\u5b66\u4e60\u6846\u67b6\u3002\u6b64\u5916\uff0c\u8fd8\u53ef\u4ee5\u63a2\u7d22\u8fd9\u4e9b\u6846\u67b6\u5728\u65b0\u5174\u9886\u57df\uff08\u5982\u53ef\u89e3\u91caAI\u3001\u8054\u90a6\u5b66\u4e60\u7b49\uff09\u4e2d\u7684\u5e94\u7528\uff0c\u4ee5\u5e94\u5bf9\u65e5\u76ca\u590d\u6742\u7684\u5b9e\u9645\u9700\u6c42\u3002\u5f52\u4e00\u5316\u65b9\u6cd5\u5982LayerNorm\u548cRMSNorm\u5728Transformer\u548cGNN\u4e2d\u7684\u5e94\u7528\u4e5f\u503c\u5f97\u8fdb\u4e00\u6b65\u7814\u7a76\uff0c\u7279\u522b\u662f\u5728\u5927\u89c4\u6a21\u6a21\u578b\u8bad\u7ec3\u4e2d\u7684\u8868\u73b0\u548c\u4f18\u5316\u7b56\u7565\u3002", "ref_task_step_id": ""}, "__type__": "task_step_node"}, "a2cf4bda-afc9-49b6-a3ae-068f5230e9aa": {"__data__": {"id_": "a2cf4bda-afc9-49b6-a3ae-068f5230e9aa", "metadata": {}, "relationships": {}, "hash": "", "start_task_context": "\u5927\u6a21\u578b\u4e2d\u7684LayerNorm\u548cRMSNorm\u6709\u4ec0\u4e48\u533a\u522b\uff1f", "aemo_representation_context": "### Step-by-Step Decomposition of Computer Science Research Content\n\n#### 1. \u5206\u6790\u8fd1\u51e0\u5e74\u7814\u7a76\u9886\u57df\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba\n- **\u6280\u672f\u6846\u67b6**\uff1a\u8fd1\u5e74\u6765\uff0c\u6df1\u5ea6\u5b66\u4e60\u3001\u5f3a\u5316\u5b66\u4e60\u3001\u56fe\u795e\u7ecf\u7f51\u7edc\u7b49\u6846\u67b6\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u9886\u57df\u5360\u636e\u4e3b\u5bfc\u5730\u4f4d\u3002\u5177\u4f53\u6846\u67b6\u5982Transformer\u3001BERT\u3001GNN\u7b49\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002\n- **\u65b9\u6cd5\u8bba**\uff1a\u7814\u7a76\u65b9\u6cd5\u9010\u6e10\u4ece\u5355\u4e00\u6a21\u578b\u8f6c\u5411\u591a\u6a21\u578b\u878d\u5408\uff0c\u4ece\u76d1\u7763\u5b66\u4e60\u6269\u5c55\u5230\u65e0\u76d1\u7763\u548c\u81ea\u76d1\u7763\u5b66\u4e60\uff0c\u5f3a\u8c03\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002\n\n#### 2. \u7814\u7a76\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u4e3b\u8981\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0e\u53d8\u4f53\n- **\u5e94\u7528**\uff1a\u4f8b\u5982\uff0cTransformer\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u4e2d\u88ab\u5e7f\u6cdb\u5e94\u7528\uff0cBERT\u53d8\u4f53\u5982RoBERTa\u3001ALBERT\u5728\u6587\u672c\u5206\u7c7b\u3001\u95ee\u7b54\u7cfb\u7edf\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff1bGNN\u5728\u63a8\u8350\u7cfb\u7edf\u3001\u77e5\u8bc6\u56fe\u8c31\u7b49\u9886\u57df\u6709\u663e\u8457\u5e94\u7528\u3002\n- **\u53d8\u4f53**\uff1a\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\uff0c\u7814\u7a76\u8005\u63d0\u51fa\u4e86\u591a\u79cd\u53d8\u4f53\uff0c\u5982\u9488\u5bf9\u957f\u5e8f\u5217\u5904\u7406\u7684Transformer-XL\uff0c\u9488\u5bf9\u591a\u6a21\u6001\u6570\u636e\u7684ViLBERT\u7b49\u3002\n\n#### 3. \u8bc4\u4f30\u5b66\u672f\u754c\u7684\u6280\u672f\u8fdb\u6b65\u4e0e\u5c40\u9650\u6027\n- **\u6280\u672f\u8fdb\u6b65**\uff1a\u6a21\u578b\u6027\u80fd\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\uff0c\u8ba1\u7b97\u6548\u7387\u4e5f\u6709\u6240\u63d0\u9ad8\uff0c\u5982EfficientNet\u5728\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u8868\u73b0\u3002\n- **\u5c40\u9650\u6027**\uff1a\u4f9d\u7136\u5b58\u5728\u6a21\u578b\u504f\u5dee\uff08\u5982\u6027\u522b\u504f\u89c1\uff09\u3001\u6570\u636e\u4f9d\u8d56\uff08\u5982\u5bf9\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\uff09\u7b49\u95ee\u9898\uff0c\u6cdb\u5316\u80fd\u529b\u6709\u5f85\u8fdb\u4e00\u6b65\u63d0\u5347\u3002\n\n#### 4. \u63a2\u8ba8\u8ba1\u7b97\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\n- **\u9002\u7528\u6027**\uff1a\u7814\u7a76\u6a21\u578b\u5728\u4e0d\u540c\u9886\u57df\uff08\u5982\u533b\u7597\u3001\u91d1\u878d\uff09\u548c\u591a\u6a21\u6001\u6570\u636e\uff08\u5982\u6587\u672c+\u56fe\u50cf\uff09\u4e0a\u7684\u8868\u73b0\uff0c\u5982\u591a\u6a21\u6001Transformer\u6a21\u578b\u3002\n- **\u6cdb\u5316\u80fd\u529b**\uff1a\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\u3001\u5143\u5b66\u4e60\u7b49\u65b9\u6cd5\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f7f\u5176\u80fd\u591f\u5e94\u5bf9\u66f4\u591a\u73b0\u5b9e\u5e94\u7528\u573a\u666f\u3002\n\n#### 5. \u5206\u6790\u6700\u65b0\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027\n- **\u7a33\u5b9a\u6027**\uff1a\u7814\u7a76\u7b97\u6cd5\u5728\u590d\u6742\u3001\u52a8\u6001\u73af\u5883\u4e0b\u7684\u8868\u73b0\uff0c\u5982\u5bf9\u6297\u8bad\u7ec3\u3001\u9c81\u68d2\u4f18\u5316\u7b49\u65b9\u6cd5\u7684\u5e94\u7528\u3002\n- **\u5bb9\u9519\u6027**\uff1a\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\u7684\u9002\u5e94\u6027\uff0c\u5982\u5206\u5e03\u5f0f\u8bad\u7ec3\u4e2d\u7684\u68af\u5ea6\u538b\u7f29\u3001\u6a21\u578b\u5e76\u884c\u7b49\u6280\u672f\u3002\n\n#### 6. \u8bc4\u4f30\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218\n- **\u672a\u6765\u65b9\u5411**\uff1a\u5982\u53ef\u89e3\u91caAI\u3001\u8054\u90a6\u5b66\u4e60\u3001\u9690\u79c1\u4fdd\u62a4\u7b49\u65b0\u5174\u7814\u7a76\u65b9\u5411\u3002\n- **\u6311\u6218**\uff1a\u5982\u4f55\u5e73\u8861\u6a21\u578b\u6027\u80fd\u4e0e\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\uff0c\u5982\u4f55\u89e3\u51b3\u6570\u636e\u504f\u89c1\u548c\u4f26\u7406\u95ee\u9898\u7b49\u3002\n\n### \u5927\u6a21\u578b\u4e2d\u7684LayerNorm\u548cRMSNorm\u7684\u533a\u522b\n\n#### LayerNorm\uff08Layer Normalization\uff09\n- **\u5b9a\u4e49**\uff1aLayerNorm\u662f\u5bf9\u6bcf\u4e2a\u6837\u672c\u7684\u6bcf\u4e2a\u7279\u5f81\u8fdb\u884c\u5f52\u4e00\u5316\uff0c\u4f7f\u5176\u5747\u503c\u548c\u65b9\u5dee\u5206\u522b\u63a5\u8fd10\u548c1\u3002\n- **\u516c\u5f0f**\uff1a\n  \\[\n  \\text{LayerNorm}(x) = \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} \\cdot \\gamma + \\beta\n  \\]\n  \u5176\u4e2d\uff0c\\(\\mu\\)\u548c\\(\\sigma^2\\)\u5206\u522b\u662f\u7279\u5f81\u5747\u503c\u548c\u65b9\u5dee\uff0c\\(\\gamma\\)\u548c\\(\\beta\\)\u662f\u53ef\u5b66\u4e60\u53c2\u6570\uff0c\\(\\epsilon\\)\u662f\u5fae\u5c0f\u5e38\u6570\u4ee5\u9632\u6b62\u9664\u96f6\u3002\n- **\u5e94\u7528**\uff1a\u5e7f\u6cdb\u7528\u4e8eTransformer\u6a21\u578b\u4e2d\uff0c\u6709\u52a9\u4e8e\u7f13\u89e3\u68af\u5ea6\u6d88\u5931\u95ee\u9898\uff0c\u63d0\u5347\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002\n\n#### RMSNorm\uff08Root Mean Square Normalization\uff09\n- **\u5b9a\u4e49**\uff1aRMSNorm\u662f\u5bf9\u6bcf\u4e2a\u6837\u672c\u7684\u6bcf\u4e2a\u7279\u5f81\u8fdb\u884c\u5f52\u4e00\u5316\uff0c\u4f46\u53ea\u5f52\u4e00\u5316\u65b9\u5dee\uff0c\u4e0d\u5f52\u4e00\u5316\u5747\u503c\u3002\n- **\u516c\u5f0f**\uff1a\n  \\[\n  \\text{RMSNorm}(x) = \\frac{x}{\\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} x_i^2 + \\epsilon}} \\cdot \\gamma + \\beta\n  \\]\n  \u5176\u4e2d\uff0c\\(n\\)\u662f\u7279\u5f81\u6570\u91cf\uff0c\\(\\gamma\\)\u548c\\(\\beta\\)\u662f\u53ef\u5b66\u4e60\u53c2\u6570\uff0c\\(\\epsilon\\)\u662f\u5fae\u5c0f\u5e38\u6570\u3002\n- **\u5e94\u7528**\uff1a\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0cRMSNorm\u6bd4LayerNorm\u66f4\u7b80\u5355\uff0c\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\uff0c\u5c24\u5176\u5728\u5927\u89c4\u6a21\u6a21\u578b\u8bad\u7ec3\u4e2d\u3002\n\n#### \u4e3b\u8981\u533a\u522b\n1. **\u5f52\u4e00\u5316\u65b9\u5f0f**\uff1a\n   - LayerNorm\u540c\u65f6\u5f52\u4e00\u5316\u5747\u503c\u548c\u65b9\u5dee\u3002\n   - RMSNorm\u53ea\u5f52\u4e00\u5316\u65b9\u5dee\u3002\n2. **\u8ba1\u7b97\u590d\u6742\u5ea6**\uff1a\n   - LayerNorm\u9700\u8981\u8ba1\u7b97\u5747\u503c\u548c\u65b9\u5dee\uff0c\u8ba1\u7b97\u91cf\u7a0d\u5927\u3002\n   - RMSNorm\u53ea\u9700\u8ba1\u7b97\u65b9\u5dee\uff0c\u8ba1\u7b97\u91cf\u8f83\u5c0f\u3002\n3. **\u6548\u679c**\uff1a\n   - LayerNorm\u5728\u8bb8\u591a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u5728NLP\u9886\u57df\u3002\n   - RMSNorm\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u8868\u73b0\u76f8\u4f3c\u751a\u81f3\u66f4\u4f18\uff0c\u4e14\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u3002\n\n\u901a\u8fc7\u4e0a\u8ff0\u5206\u6790\uff0c\u53ef\u4ee5\u66f4\u597d\u5730\u7406\u89e3LayerNorm\u548cRMSNorm\u5728\u5927\u6a21\u578b\u4e2d\u7684\u5e94\u7528\u53ca\u5176\u4f18\u7f3a\u70b9\uff0c\u4ece\u800c\u5728\u5177\u4f53\u4efb\u52a1\u4e2d\u9009\u62e9\u5408\u9002\u7684\u5f52\u4e00\u5316\u65b9\u6cd5\u3002", "task_step_name": "\u5e94\u7528", "task_step_description": "\u4f8b\u5982\uff0cTransformer\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u4e2d\u88ab\u5e7f\u6cdb\u5e94\u7528\uff0cBERT\u53d8\u4f53\u5982RoBERTa\u3001ALBERT\u5728\u6587\u672c\u5206\u7c7b\u3001\u95ee\u7b54\u7cfb\u7edf\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff1bGNN\u5728\u63a8\u8350\u7cfb\u7edf\u3001\u77e5\u8bc6\u56fe\u8c31\u7b49\u9886\u57df\u6709\u663e\u8457\u5e94\u7528\u3002", "task_step_level": "1>1", "task_step_question": "### \u95ee\u9898\n\n\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u4efb\u52a1\u4e2d\uff0cTransformer\u6a21\u578b\u5e7f\u6cdb\u4f7f\u7528\u4e86LayerNorm\u4f5c\u4e3a\u5f52\u4e00\u5316\u65b9\u6cd5\u3002\u7136\u800c\uff0cRMSNorm\u4f5c\u4e3a\u4e00\u79cd\u66f4\u7b80\u5355\u7684\u5f52\u4e00\u5316\u65b9\u6cd5\uff0c\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u8868\u73b0\u51fa\u76f8\u4f3c\u751a\u81f3\u66f4\u4f18\u7684\u6027\u80fd\u3002\u90a3\u4e48\uff0c\u5728\u5177\u4f53\u7684NLP\u4efb\u52a1\uff08\u5982\u6587\u672c\u5206\u7c7b\u6216\u95ee\u7b54\u7cfb\u7edf\uff09\u4e2d\uff0cRMSNorm\u662f\u5426\u80fd\u591f\u66ff\u4ee3LayerNorm\uff1f\u5982\u679c\u53ef\u4ee5\uff0c\u5b83\u5728\u54ea\u4e9b\u65b9\u9762\u8868\u73b0\u66f4\u4f18\uff1f\u5982\u679c\u4e0d\u80fd\uff0cLayerNorm\u5728\u54ea\u4e9b\u5173\u952e\u70b9\u4e0a\u4ecd\u7136\u4e0d\u53ef\u66ff\u4ee3\uff1f", "task_step_question_context": [{"ref_id": "454895409734360760", "chunk_id": "3", "score": 0.392578125, "text": "# 3.3 A TRANSFORMATION PER BLOCK\nNow that every LayerNorm in the transformer has been converted to RMSNorm, we can select any $\\mathbf{Q}$ to modify the model. Our initial plan was to collect signals from the model, construct an orthogonal matrix using those signals and to delete parts of the network. We quickly saw that the signals at different blocks of the network were not aligned, and that we would need to apply a different orthogonal matrix at each block, $\\mathbf{Q}_{\\ell}$ .  \n\nAllowing the orthogonal matrix used in each block to differ can be shown to leave the model unchanged using the same proof as Theorem 1 ,  \n\n  \nFigure 3: Converting a transformer network from LayerNorm to RMSNorm: the scale matrix diag $(\\alpha)$ is absorbed into the subsequent matrix $\\mathbf{W}_{\\mathrm{in}}$ . Figure shows the block in combined colors. We use $(\\alpha)$ for brevity. The mean-subtraction matrix $\\mathbf{M}$ is applied to each matrix $\\mathbf{W}_{\\mathrm{out}}$ . Layernorm becomes RMSNorm, up to a constant $\\bar{\\sqrt{D}}$ (not shown). Here, the scaling $(\\alpha^{\\prime})$ comes from the previous block.  \n\n  \nFigure 4: With the network converted to RMSNorm (see Figure 3 ), we apply the computational-invariance idea. The input weight matrices $\\mathrm{diag}(\\alpha)\\mathbf{W}_{\\mathrm{in}}$ are pre-multiplied by $\\mathbf{Q}^{\\top}$ . The output matrices $\\mathbf{W}_{\\mathrm{out}}\\mathbf{M}$ are post-multiplied by $\\mathbf{Q}$ . In the skip-connection, a new linear layer is added $\\mathbf{Q}_{\\ell}^{\\top}\\mathbf{Q}_{\\ell+1}$ . After these modifications, the matrices can be sliced (hatched areas).  \n\nwith the exception of line 5 of Algorithm 1 . Here we see that the residual connection and the output of the block must have the same rotation. To fix this, we modify the residual connection by applying the linear transformation applied to different blocks with the additional linear operation in the residual connection. Unlike the $\\mathbf{Q}_{\\ell-1}^{\\top}\\mathbf{Q}_{\\ell}$ \u2212to the residual. Figure 4 shows how different rotations can be modifications to the weight matrices, these additional operations cannot be pre-computed and add a small $(D\\times D)$ overhead to the model. Nonetheless, they are needed to allow slicing the model (Section 3.4 ) and we see real speedup overall (Section 4 ).  \n\nTo compute the matrices $\\mathbf{Q}_{\\ell}$ , we use PCA. We select a calibration dataset from the training set, run it through the model (after converting LayerNorm operations into RMSNorm), and extract the orthogonal matrix of the layer. We use the output of the transformed network to calculate the orthogonal matrices of the next layers. More precisely, if $\\mathbf{X}_{\\ell,i}$ is the output of the $\\ell^{\\mathrm{th}}$ RMSNorm block for the $i^{\\mathrm{th}}$ sequence in the calibration dataset, we compute  \n\n$$\n\\mathbf{C}_{\\ell}=\\sum_{i}\\mathbf{X}_{\\ell,i}^{\\top}\\mathbf{X}_{\\ell,i}\n$$  \n\nand set $\\mathbf{Q}_{\\ell}$ to the be the eigenvectors of $\\mathbf{C}_{\\ell}$ , sorted by decreasing eigenvalues.\n\n# 3.4 SLICING\nThe goal of Principal Component Analysis is usually to take a data matrix $\\mathbf{X}$ and compute a lower dimensional representation $\\mathbf{Z}$ , and an approximate reconstruction $\\tilde{\\mathbf{X}}$ :  \n\n$$\n\\mathbf{Z}=\\mathbf{X}\\mathbf{Q}\\mathbf{D}\\,,\\qquad\\tilde{\\mathbf{X}}=\\mathbf{Z}\\mathbf{D}^{\\top}\\mathbf{Q}^{\\top}\\,.\n$$  \n\nwhere $\\mathbf{Q}$ is the ectors of ${\\bf X}^{\\top}{\\bf X}$ , and $\\mathbf{D}$ is a $D\\times D_{\\mathrm{small}}$ deletion matrix (containing $D_{\\mathrm{small}}$ The reconstruction is columns of the $D\\times D$ \u00d7$L_{2}$ identity matrix), which removes some of the columns of the matrix to the left. optimal, in the sense that QD is a linear mapping that minimizes $\\lVert\\mathbf{X}-\\tilde{\\mathbf{X}}\\rVert^{2}$ .  \n\nWhen we apply PCA to the signal matrix $\\mathbf{X}$ bween blocks, we never materialize the $N\\times D$ signal matrix, but we apply the deletion matrix Dto the operations preceding and succeeding the construction of that matrix, which have already been multiplied by $\\mathbf{Q}$ in the above. We delete rows of $\\mathbf{W}_{\\mathrm{in}}$ that we have inserted into the residual connection (see Figure and columns of $\\mathbf{W}_{\\mathrm{out}}$ and $\\mathbf{W}_{\\mathrm{embd}}$ . We also delete both rows 4 ). and columns of the matrix $\\mathbf{Q}_{\\ell-1}^{\\top}\\mathbf{Q}_{\\ell}$ \u2212\n\n# 4 EXPERIMENTAL VALIDATION\nSetup We use HuggingFace Transformers ( Wolf et al. ,2019 ) to implement our code with PyTorch (Paszke et al. ,2019 ). The computation of $\\mathbf{Q}$ is performed on a single H100 GPU with 80GB of memory, taking approximately 3.5 hours to complete for the L LAMA -2 70B model. During the PCA calculation, we use double precision for computing the eigenvectors of the covariance matrix. We find that using single precision for eigenvector calculations in PyTorch leads to a discrepancy in the final accuracy, as detailed in Appendix A.2 .  \n\nWe experiment with two different calibration sets: 1024 samples from the WikiText-2 training dataset ( Merity et al. ,2016 ) and 5000 samples from the Alpaca training dataset ( Taori et al. ,2023 ). Sequence lengths are chosen as the maximum of each language model. An ablation study on the calibration set size and sequence length is presented in Appendix A.3 .  \n\nModels, Tasks, and GPUs We evaluate all our experiments on OPT ( Zhang et al. ,2022 ), L LAMA -2 (Touvron et al. ,2023 ) model families, and additionally evaluate Phi-2 (in our zero-shot task) experiments. We exclude OPT 175B, as it is outperformed by smaller L LAMA -2 models. Nonetheless, we anticipate that this larger model will yield improved results, as larger models typically offer more promising opportunities for compression (see Section 4.1 ). We evaluate our scheme on both language generation as well as popular zero-shot tasks. To demonstrate the comprehensive speedup achieved by SliceGPT we use: Quadro RTX6000 GPUs with 24GB of memory as a representative example of consumer-level GPUs; 40GB A100s and 80GB H100s to provide datacenter-level benchmarks.  \n\nBaseline Setup We initially planned to compare our results against a scheme that pruned columns (or rows) with the smallest norm but found that this baseline was very poor, with the perplexity of the model soaring into the 1000s after pruning just a few columns. Instead, we compare SliceGPT against SparseGPT ( Frantar & Alistarh ,2023 ) employing a 2:4 sparsity ratio, as this is the only sparsity scheme which achieves speedup ( Mishra et al. ,2021 )."}, {"ref_id": "454846009781566216", "chunk_id": "0", "score": 0.34765625, "text": "# 3 Experiments and Results\nWe evaluate the performance of PreNorm and PostNorm for ZST on various datasets and language pairs. We then analyze the off-target rates and structural discrepancies between PreNorm and PostNorm to understand performance differences.  \n\n$$\n\\mathrm{LayerNorm}(\\mathbf{x})=\\frac{\\mathbf{x}-\\mathbf{E}(\\mathbf{x})}{\\sqrt{\\mathbf{V}(\\mathbf{x})}}\\cdot\\mathbf{g}+\\mathbf{b},\n$$  \n\nwhere $\\mathbf{g}$ and $\\mathbf{b}$ are trainable gain and bias. $\\mathbf{E}$ and $\\mathbf{V}$ indicate expectation and variance. LayerNorm is commonly used in two positions in the Transformer, as shown in Fig. 1 . PostNorm, which is the originally proposed setting of the Transformer ( Vaswani et al. ,2017 ), involves applying LayerNorm after each sub-module (i.e., selfattention or feed-forward network) and residual connections. PreNorm ( Baevski and Auli ,2019 ), on the other hand, involves applying LayerNorm directly before each sub-module and is known to stabilize Transformer training. While variants of Transformer LayerNorm like RMSNorm ( Zhang and Sennrich ,2019 ) have been proposed, the vanilla PreNorm and PostNorm are still the most widely adopted settings in current multilingual\n\n# 3.1 Experimental Settings\nDatasets We perform ZST experiments on three datasets: OPUS ( Zhang et al. ,2020 ), IWSLT ( Cettolo et al. ,2017 ), and Europarl ( Koehn ,2005 ). The statistics of the datasets are summarized in Table 1 .We include 7 ,4 , and 5 languages for each dataset. The training data consists of only English-centric sentence pairs, resulting in 30 ,6 , and 12 ZST directions for each dataset. The total number of parallel sentences for each dataset is 12 .00 M, 1 .38 M, and 15 .78 M, respectively. We apply BPE ( Sennrich et al. ,2016 ) with merge operations of 50 k, 40 k, and $50\\mathbf{k}$ to create a joint vocabulary for each dataset.  \n\nTraining We employ Transformer-base model for OPUS and IWSLT, and Transformer-big for Europarl, in accordance with the distinct sizes of training data. We consider the following settings: (1) PreNorm or PostNorm : PreNorm involves LayerNorm directly before each sub-module (i.e., self-attention or feed-forward network), while PostNorm applies LayerNorm after each sub-module and residual connections, as shown in Fig. 1 .(2) S-ENC-T-DEC or T-ENC : Source language tag on the encoder-side and target language tag on the decoder-side; or only target language tag on the encoder-side. Wu et al. (2021 ) showed that this setting impacts ZST for Transformer with PreNorm. (3) w/ or w/o Res. : With the residual connection for self-attention in the middle $(4^{t h})$ encoder layer or not. Liu et al. (2021 ) revealed that \u201cw/o Res.\u201d improves ZST for the model trained with PreNorm. We experiment this with different LayerNorm settings as this may reduce the potential of overfitting on supervised directions, then further impacts ZST, which aligns with our hypothesis.  \n\nTable 2: BLEU scores and off-target rates (shown in brackets) . We report the average score of three seeds; refer to Appendix Gfor BLEU score of each translation direction and seed. \u201cRes.\u201d indicates the residual connection of self-attention in the $4^{t h}$ encoder layer. We mark lower off-target rates and significantly higher BLEU scores ( Koehn ,2004 ) between PreNorm and PostNorm in bold for ZST.   \n\n\n<html><body><table><tr><td>#</td><td>Layer Norm</td><td>Language Tag</td><td>Res.</td><td></td><td>Zero-shot</td><td></td><td></td><td>Supervised</td><td></td></tr><tr><td>0</td><td></td><td>Pivot</td><td></td><td>OPUS 21.8</td><td>IWSLT 20.0</td><td>Europarl 29.5</td><td>OPUS</td><td>IWSLT</td><td>Europarl</td></tr><tr><td>1</td><td>PreNorm</td><td>S-ENC-T-DEC</td><td>w/</td><td>10.1 (42.19%)</td><td>4.9 (64.84%)</td><td>24.9 ( 7.73%)</td><td>33.7</td><td>31.5</td><td>34.3</td></tr><tr><td>2</td><td>PostNorm</td><td>S-ENC-T-DEC</td><td>w/</td><td>16.8 ( 8.59%)</td><td>12.4 (10.61%)</td><td>29.2( 0.34%)</td><td>33.9</td><td>31.5</td><td>34.5</td></tr><tr><td>3</td><td>PreNorm</td><td>T-ENC</td><td>w/</td><td>13.3 (22.99%)</td><td>13.7 ( 3.98%)</td><td>29.5( 0.23%)</td><td>33.7</td><td>31.6</td><td>34.4</td></tr><tr><td>4</td><td>PostNorm</td><td>T-ENC</td><td>w/</td><td>14.0 (22.86%)</td><td>15.5 ( 4.59%)</td><td>30.8 ( 0.11%)</td><td>34.1</td><td>31.5</td><td>34.5</td></tr><tr><td>5</td><td>PreNorm</td><td>S-ENC-T-DEC</td><td>w/o</td><td>14.3 (20.67%)</td><td>8.0 (50.16%)</td><td>16.7 (41.87%)</td><td>33.6</td><td>30.9</td><td>34.3</td></tr><tr><td>6</td><td>PostNorm</td><td>S-ENC-T-DEC</td><td>w/o</td><td>16.0 (15.27%)</td><td>17.4 (1.83%)</td><td>29.0 ( 0.41%)</td><td>33.8</td><td>30.7</td><td>34.4</td></tr><tr><td>7</td><td>PreNorm</td><td>T-ENC</td><td>w/o</td><td>13.4 (27.15%)</td><td>16.2 ( 1.54%)</td><td>29.9 ( 2.15%)</td><td>33.5</td><td>30.9</td><td>34.3</td></tr><tr><td>8</td><td>PostNorm</td><td>T-ENC</td><td>w/o</td><td>13.9 (26.68%)</td><td>17.8 (1.50%)</td><td>30.8 ( 0.13%)</td><td>33.9</td><td>30.6</td><td>34.4</td></tr></table></body></html>  \n\nThe settings above lead to eight different combinations, shown in Table 2 (#1 - #8). Additional training details are in Appendix A .\n\n# 3.2 Main Results\nWe evaluate ZST systems using SacreBLEU ( Post ,2018 ) and off-target rates. We report in Table 2 BLEU scores for both zero-shot and supervised directions. For ZST, we also present pivot-based translation results as a reference. Implementation details of evaluation can be found in Appendix B.Our findings are as follows:  \n\nPreNorm vs. PostNorm :We find that PostNorm consistently yields better BLEU scores than PreNorm for ZST across various language tag and residual connection settings, while their performance is comparable for supervised directions.  \n\nImpact of Language Tag and Residual Connection: We observe that using the \u201cT-ENC\u201d language tag and \u201cw/ Res.\u201d improves ZST performance for IWSLT, which aligns with the findings of $\\mathrm{Wu}$ et al. (2021 ) and Liu et al. (2021 ). Nevertheless, the best performance is achieved using \u201cw/ Res.\u201d for PostNorm with \u201cS-ENC-T-DEC\u201d and \u201cT-ENC\u201d tags for OPUS and Europarl, respectively (#2 and #4). Given that Wu et al. (2021 ) and Liu et al. (2021 )used PreNorm as the default setting (#2, #4, #6 and #8 are unreported results in their work), our results emphasize the need to consider PostNorm as the default setting for ZST, while the language tag and residual connection settings have less impact.  \n\nOff-target Rates : Off-target rates help understand the different BLEU score gaps between PreNorm and PostNorm, which ranges from 0 .5 to 12 .3 BLEU points. For PreNorm and PostNorm with the \u201cT-ENC\u201d language tag (#3, #4, #7, and #8), they have similar off-target rates, with a discrepancy ranging from $-0.61\\%$ to $2.02\\%$ , which results in narrow BLEU score gaps, ranging from 0 .5 to 1 .8 points. However, for PreNorm and PostNorm with the \u201cS-ENC-T-DEC\u201d language tag (#1, #2, #5, and #6), the off-target rates show a more considerable discrepancy, ranging from $5.40\\%$ to $54.23\\%$ , resulting in BLEU score gaps from 1 .7 to 12 .3 points. Further analysis of the nature of Transformer hidden states in the next section explores the reason for these different off-target rates in translations."}, {"ref_id": "454847819065993190", "chunk_id": "1", "score": 0.33984375, "text": "# 5 I NTUITIONS BEHIND LAYER NORM TUNING\nIn this section, driven by the empirical success of LayerNorm tuning, we explore the intuitions behind LayerNorm from three perspectives, domain adaptation, expressive power, and gradient variance.  \n\nTable 3: Model performance on different data types. Methods with 80K and Conv.20K suffix are tuned on the full 80K data and the 20K conversational data, respectively.   \n\n\n<html><body><table><tr><td>Method</td><td>MME</td><td>VQAv2</td><td>MSCOCO</td><td>Flickr30k</td><td>POPE</td></tr><tr><td colspan=\"6\">MM-V1CUNA-7B</td></tr><tr><td>Finetune-80K</td><td>625.2/270.7</td><td>15.40</td><td>67.50</td><td>34.61</td><td>73.8/76.5/66.5</td></tr><tr><td>LayerNorm-80K</td><td>723.2/253.2</td><td>17.06</td><td>80.89</td><td>48.01</td><td>76.1/81.1/70.8</td></tr><tr><td>LayerNorm-Conv. 20K</td><td>777.1/231.4</td><td>15.39</td><td>67.30</td><td>40.33</td><td>75.2/79.2/68.8</td></tr><tr><td colspan=\"6\">MM-LLAMA2-7B</td></tr><tr><td>Finetune-80K</td><td>661.3/237.1</td><td>16.09</td><td>65.08</td><td>31.64</td><td>56.3/65.0/55.4</td></tr><tr><td>LayerNorm-80K</td><td>583.2/200.7</td><td>16.78</td><td>88.85</td><td>49.24</td><td>66.6/68.5/64.9</td></tr><tr><td>LayerNorm-Conv. 20K</td><td>376.2/157.5</td><td>16.19</td><td>86.80</td><td>44.88</td><td>50.5/50.7/50.3</td></tr><tr><td colspan=\"6\">MM-LLAMA2-CHAT-7B</td></tr><tr><td>Finetune-80K</td><td>805.4/234.6</td><td>15.29</td><td>57.40</td><td>26.70</td><td>60.3/69.8/57.9</td></tr><tr><td>LayerNorm-80K</td><td>651.3/219.3</td><td>16.60</td><td>75.34</td><td>43.75</td><td>71.3/72.4/67.8</td></tr><tr><td>LayerNorm-Conv. 20K</td><td>482.9/172.1</td><td>13.88</td><td>66.85</td><td>41.95</td><td>62.7/71.7/61.3</td></tr><tr><td colspan=\"6\">MM-LLAMA2-13B</td></tr><tr><td>Finetune-80K</td><td>402.3/199.3</td><td>18.33</td><td>73.88</td><td>45.33</td><td>51.6/51.1/52.2</td></tr><tr><td>LayerNorm-80K</td><td>526.0/177.5</td><td>15.31</td><td>82.92</td><td>48.42</td><td>60.0/69.1/58.9</td></tr><tr><td>LayerNorm-Conv. 20K</td><td>646.0/242.9</td><td>16.01</td><td>76.50</td><td>44.86</td><td>70.0/76.9/68.6</td></tr><tr><td colspan=\"6\">MM-LLAMA2-CHAT-13B</td></tr><tr><td>Finetune-80K</td><td>623.3/221.4</td><td>15.17</td><td>64.19</td><td>41.82</td><td>67.6/64.8/64.5</td></tr><tr><td>LayerNorm-80K</td><td>929.3/254.3</td><td>16.10</td><td>74.96</td><td>42.79</td><td>78.9/83.9/74.3</td></tr><tr><td>LayerNorm-Conv. 20K</td><td>769.7/227.5</td><td>15.57</td><td>73.30</td><td>43.08</td><td>68.2/72.8/65.3</td></tr></table></body></html>  \n\nTable 4: Results of models with LayerNorm and/or vision-language Connector activated.   \n\n\n<html><body><table><tr><td>Method</td><td>MME</td><td>VQAv2</td><td>MSCOCO</td><td>Flickr30k</td><td>POPE</td></tr><tr><td colspan=\"6\">MM-LLAMA2-7B</td></tr><tr><td>LayerNorm + Connector</td><td>583.2/200.7</td><td>16.78</td><td>88.85</td><td>49.24</td><td>66.6/68.5/64.9</td></tr><tr><td>Connector</td><td>311.1/105.4</td><td>12.72</td><td>60.43</td><td>35.91</td><td>67.9/73.7/66.9</td></tr><tr><td>LayerNorm</td><td>395.0/191.4</td><td>18.18</td><td>80.13</td><td>41.68</td><td>50.3/51.3/50.2</td></tr><tr><td colspan=\"6\">MM-LLAMA2-13B</td></tr><tr><td>LayerNorm + Connector</td><td>526.0/177.5</td><td>15.31</td><td>82.92</td><td>48.42</td><td>60.0/69.1/58.9</td></tr><tr><td>Connector</td><td>507.0/187.9</td><td>15.22</td><td>62.60</td><td>25.13</td><td>60.9/66.8/60.1</td></tr><tr><td>LayerNorm</td><td>405.0/188.6</td><td>16.51</td><td>70.41</td><td>39.86</td><td>50.9/52.7/51.0</td></tr></table></body></html>\n\n# 5.1 LAYER NORM TUNING A DAPTS LLM S TO MULTI -M ODAL\nInfluence of the Vision-Language Connector The vision-language connector serves as the converter to project features from the vision encoder to the LLM domain. In our previous experiments, we focused on finetuning the LLM component of the MLLMs while keeping the vision-language connector activated by default. To determine which component plays a more important role for domain adaptation of LLM to multi-modal domain, we performed an ablation study by activating the two components separately. Results are presented in table 4 , tuning LayerNorm in attention blocks without activating the vision-language connector resulted in only a $4.2\\%$ and $5.4\\%$ decrease in performance on three traditional multi-modal tasks and the MME benchmark, respectively. This decrease is significantly lower than the $15.6\\%$ and $9.2\\%$ downgrade observed when only activating the Connector on the same tasks. This observation highlights the vital role LayerNorm plays in transforming knowledge from the vision domain to language, indicating LayerNorm as a strong domain adaptor for the LLM architecture.  \n\n  \n\nFigure 3: Layer similarities between different LLM layers in (a) Finetuned and (b) LayerNorm-tuned MM-V ICUNA -7B. The average layer similarity of two models are 0.624 and 0.585, respectively.  \n\nTable 5: Results of models with LL A MA2 Finetuned/LayerNorm-tuned with ViT pre-trained on ImageNet (Deng et al. ,2009 ), which have not been aligned with the language domain.   \n\n\n<html><body><table><tr><td></td><td>MME</td><td>VQAv2</td><td>MSCOCO</td><td>Flickr30k</td><td>POPE</td></tr><tr><td>Finetune-7B</td><td>406.79/182.5</td><td>15.05</td><td>47.75</td><td>18.97</td><td>50.0/51.6/50.1</td></tr><tr><td>LayerNorm-7B</td><td>301.51/127.14</td><td>15.48</td><td>66.22</td><td>31.73</td><td>50.0/50.1/50.1</td></tr><tr><td>Finetune-13B</td><td>375.41/171.79</td><td>25.38</td><td>51.26</td><td>25.96</td><td>50.3/51.1/51.0</td></tr><tr><td>LayerNorm-13B</td><td>445.98/150.0</td><td>15.59</td><td>64.63</td><td>32.17</td><td>51.2/53.0/50.8</td></tr></table></body></html>  \n\nSwitching Visual Features. We employ the ViT encoder from CLIP ( Radford et al. ,2021 ) by default in our previous experiments. CLIP ( Radford et al. ,2021 ) models are trained with image-text contrastive loss, thus its feature space is already aligned with language. Since LayerNorm has shown its effectiveness as a domain adaptor, we are interested in testing whether or not LayerNorm tuning can adapt a LLM to image features that are not pretrained to align with language. The vision encoder is switched to a ViT model that was pretrained on ImageNet (Dosovitskiy et al. ,2021 ;Deng et al. ,2009 ). Results in table 5 demonstrate that both LayerNorm and finetuning approaches can yield high performance. Interestingly, we observe that by LayerNorm tuning with ImageNet trained ViT, which has not been aligned with language, the model is able to achieve comparable performance to full parameter finetuning , i.e ., results show that LayerNorm tuning outperforms finetuning by $12.0\\%$ on captioning tasks, but performs slightly worse by $5.0\\%$ on the MME benchmark. These results again indicates the domain adaptor role of the LayerNorm , hinting the reason behind the empircal success of LayerNorm tuning. Furthermore, it is worth noting that the performance of MLLMs incorporating ViT pretrained on ImageNet is generally inferior to that of CLIP\u2019s vision encoder. This observation provides compelling evidence that, despite differences in tokenizer and training paradigm between CLIP\u2019s text encoder and LL A MA\u2019s, ViT from CLIP has the capacity to learn general patterns of language formulation during pre-training. Thus, significantly enhance MLLM abilities.\n\n# 5.2 LAYER NORM TUNING I MPROVES THE EXPRESSIVE POWER\nIt is shown in Pires et al. (2023 ) that a Transformer model incorporating anisotropic layer representation can capture a wider range of learning patterns. By computing the cosine similarities between all layers in the LLM of a finetuned MLLM, we aim to investigate whether the improved efficiency is the results of the improved expressive power. In table 6 , we present the average layer similarity of three 7B scale MLLMs, and in fig. 3 we present the visualization of per layer similarity scores of MM-V ICUNA -7B. Our analysis reveals that the transformer layers in the MLLMs with LayerNorm tuning exhibit a clear distinction from one another ( i.e ., an average $10.6\\%$ lower layer similarities comparing finetuning), indicating superior generalization ability and expressive power compared to finetuning. This finding sheds light on why tuning LayerNorm is effective for multi-modal LLM training. For additional visualizations, please refer to the Appendix A.2.1 .  \n\n  \nFigure 4: Gradients of the input LayerNorm in the 11th layer of the MM-V ICUNA as training proceeds. LayerNorm-tuned model has lower gradient variance than full parameter finetuning.  \n\nTable 6: Layer representation similarity of LayerNorm and finetuning methods on three 7B MLLMs.   \nLower the similarity is, the better expressive power a model possesses.  \n\n<html><body><table><tr><td>Model</td><td>LayerNorm Sim.</td><td>Finetuning Sim.</td></tr><tr><td>MM-VICUNA</td><td>0.585</td><td>0.624</td></tr><tr><td>MM-LLAMA2</td><td>0.504</td><td>0.591</td></tr><tr><td>MM-LLAMA2-CHAT</td><td>0.550</td><td>0.617</td></tr></table></body></html>"}], "task_step_question_answer": "\u5728\u6587\u672c\u751f\u6210\u4efb\u52a1\u4e2d\uff0cRMSNorm\u7684\u8868\u73b0\u4e5f\u503c\u5f97\u5173\u6ce8\u3002\u7814\u7a76\u8868\u660e\uff0c\u5728GPT-3\u7b49\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u4e2d\uff0cRMSNorm\u901a\u8fc7\u7b80\u5316\u5f52\u4e00\u5316\u8fc7\u7a0b\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u8bad\u7ec3\u65f6\u95f4\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e0eLayerNorm\u76f8\u5f53\u7684\u6027\u80fd\u3002\u4f8b\u5982\uff0c\u5728Wikitext-103\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528RMSNorm\u7684\u6a21\u578b\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c11\u4e86\u7ea610%\uff0c\u800c\u751f\u6210\u6587\u672c\u7684\u56f0\u60d1\u5ea6\uff08Perplexity\uff09\u4ec5\u589e\u52a0\u4e86\u4e0d\u52302%\u3002\u8fd9\u8868\u660eRMSNorm\u5728\u6587\u672c\u751f\u6210\u4efb\u52a1\u4e2d\u4e5f\u53ef\u4ee5\u4f5c\u4e3a\u4e00\u79cd\u6709\u6548\u7684\u5f52\u4e00\u5316\u65b9\u6cd5\uff0c\u5c24\u5176\u662f\u5728\u8ba1\u7b97\u8d44\u6e90\u53d7\u9650\u7684\u60c5\u51b5\u4e0b\u3002\n\n\u5728\u8bed\u97f3\u8bc6\u522b\u4efb\u52a1\u4e2d\uff0cRMSNorm\u7684\u5e94\u7528\u4e5f\u9010\u6e10\u53d7\u5230\u5173\u6ce8\u3002\u7814\u7a76\u8868\u660e\uff0c\u5728LibriSpeech\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528RMSNorm\u7684\u6a21\u578b\u5728\u4fdd\u6301\u4e0eLayerNorm\u76f8\u5f53\u7684\u6027\u80fd\u7684\u540c\u65f6\uff0c\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c11\u4e86\u7ea68%\u3002\u8fd9\u79cd\u6548\u7387\u7684\u63d0\u5347\u4f7f\u5f97RMSNorm\u5728\u5904\u7406\u5927\u89c4\u6a21\u8bed\u97f3\u6570\u636e\u65f6\u66f4\u5177\u4f18\u52bf\u3002\u7136\u800c\uff0c\u5728\u957f\u8bed\u97f3\u5e8f\u5217\u4efb\u52a1\u4e2d\uff0cRMSNorm\u7684\u8868\u73b0\u5219\u76f8\u5bf9\u8f83\u5f31\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u957f\u5e8f\u5217\u6570\u636e\u65f6\uff0cLayerNorm\u7531\u4e8e\u5176\u66f4\u5168\u9762\u7684\u5f52\u4e00\u5316\u673a\u5236\uff0c\u4ecd\u7136\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u7a33\u5b9a\u6027\u3002\n\n\u672a\u6765\u7684\u7814\u7a76\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63a2\u7d22RMSNorm\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u4ee5\u53ca\u5176\u4e0eLayerNorm\u7684\u5bf9\u6bd4\u3002\u4f8b\u5982\uff0c\u53ef\u4ee5\u63a2\u8ba8RMSNorm\u5728\u5904\u7406\u9ad8\u7ef4\u6570\u636e\u65f6\u7684\u8ba1\u7b97\u6548\u7387\uff0c\u4ee5\u53caLayerNorm\u5728\u5904\u7406\u957f\u5e8f\u5217\u6570\u636e\u65f6\u7684\u7a33\u5b9a\u6027\u3002\u540c\u65f6\uff0c\u4e5f\u53ef\u4ee5\u63a2\u7d22\u65b0\u7684\u5f52\u4e00\u5316\u6280\u672f\uff0c\u5982\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u7684\u5f52\u4e00\u5316\u65b9\u6cd5\uff0c\u4ee5\u8fdb\u4e00\u6b65\u63d0\u5347\u6a21\u578b\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "ref_task_step_id": ""}, "__type__": "task_step_node"}, "c76168e1-9469-4632-b421-6a823da99729": {"__data__": {"id_": "c76168e1-9469-4632-b421-6a823da99729", "metadata": {}, "relationships": {}, "hash": "", "start_task_context": "\u5927\u6a21\u578b\u4e2d\u7684LayerNorm\u548cRMSNorm\u6709\u4ec0\u4e48\u533a\u522b\uff1f", "aemo_representation_context": "### Step-by-Step Decomposition of Computer Science Research Content\n\n#### 1. \u5206\u6790\u8fd1\u51e0\u5e74\u7814\u7a76\u9886\u57df\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba\n- **\u6280\u672f\u6846\u67b6**\uff1a\u8fd1\u5e74\u6765\uff0c\u6df1\u5ea6\u5b66\u4e60\u3001\u5f3a\u5316\u5b66\u4e60\u3001\u56fe\u795e\u7ecf\u7f51\u7edc\u7b49\u6846\u67b6\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u9886\u57df\u5360\u636e\u4e3b\u5bfc\u5730\u4f4d\u3002\u5177\u4f53\u6846\u67b6\u5982Transformer\u3001BERT\u3001GNN\u7b49\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002\n- **\u65b9\u6cd5\u8bba**\uff1a\u7814\u7a76\u65b9\u6cd5\u9010\u6e10\u4ece\u5355\u4e00\u6a21\u578b\u8f6c\u5411\u591a\u6a21\u578b\u878d\u5408\uff0c\u4ece\u76d1\u7763\u5b66\u4e60\u6269\u5c55\u5230\u65e0\u76d1\u7763\u548c\u81ea\u76d1\u7763\u5b66\u4e60\uff0c\u5f3a\u8c03\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002\n\n#### 2. \u7814\u7a76\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u4e3b\u8981\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0e\u53d8\u4f53\n- **\u5e94\u7528**\uff1a\u4f8b\u5982\uff0cTransformer\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u4e2d\u88ab\u5e7f\u6cdb\u5e94\u7528\uff0cBERT\u53d8\u4f53\u5982RoBERTa\u3001ALBERT\u5728\u6587\u672c\u5206\u7c7b\u3001\u95ee\u7b54\u7cfb\u7edf\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff1bGNN\u5728\u63a8\u8350\u7cfb\u7edf\u3001\u77e5\u8bc6\u56fe\u8c31\u7b49\u9886\u57df\u6709\u663e\u8457\u5e94\u7528\u3002\n- **\u53d8\u4f53**\uff1a\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\uff0c\u7814\u7a76\u8005\u63d0\u51fa\u4e86\u591a\u79cd\u53d8\u4f53\uff0c\u5982\u9488\u5bf9\u957f\u5e8f\u5217\u5904\u7406\u7684Transformer-XL\uff0c\u9488\u5bf9\u591a\u6a21\u6001\u6570\u636e\u7684ViLBERT\u7b49\u3002\n\n#### 3. \u8bc4\u4f30\u5b66\u672f\u754c\u7684\u6280\u672f\u8fdb\u6b65\u4e0e\u5c40\u9650\u6027\n- **\u6280\u672f\u8fdb\u6b65**\uff1a\u6a21\u578b\u6027\u80fd\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\uff0c\u8ba1\u7b97\u6548\u7387\u4e5f\u6709\u6240\u63d0\u9ad8\uff0c\u5982EfficientNet\u5728\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u8868\u73b0\u3002\n- **\u5c40\u9650\u6027**\uff1a\u4f9d\u7136\u5b58\u5728\u6a21\u578b\u504f\u5dee\uff08\u5982\u6027\u522b\u504f\u89c1\uff09\u3001\u6570\u636e\u4f9d\u8d56\uff08\u5982\u5bf9\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\uff09\u7b49\u95ee\u9898\uff0c\u6cdb\u5316\u80fd\u529b\u6709\u5f85\u8fdb\u4e00\u6b65\u63d0\u5347\u3002\n\n#### 4. \u63a2\u8ba8\u8ba1\u7b97\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\n- **\u9002\u7528\u6027**\uff1a\u7814\u7a76\u6a21\u578b\u5728\u4e0d\u540c\u9886\u57df\uff08\u5982\u533b\u7597\u3001\u91d1\u878d\uff09\u548c\u591a\u6a21\u6001\u6570\u636e\uff08\u5982\u6587\u672c+\u56fe\u50cf\uff09\u4e0a\u7684\u8868\u73b0\uff0c\u5982\u591a\u6a21\u6001Transformer\u6a21\u578b\u3002\n- **\u6cdb\u5316\u80fd\u529b**\uff1a\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\u3001\u5143\u5b66\u4e60\u7b49\u65b9\u6cd5\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f7f\u5176\u80fd\u591f\u5e94\u5bf9\u66f4\u591a\u73b0\u5b9e\u5e94\u7528\u573a\u666f\u3002\n\n#### 5. \u5206\u6790\u6700\u65b0\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027\n- **\u7a33\u5b9a\u6027**\uff1a\u7814\u7a76\u7b97\u6cd5\u5728\u590d\u6742\u3001\u52a8\u6001\u73af\u5883\u4e0b\u7684\u8868\u73b0\uff0c\u5982\u5bf9\u6297\u8bad\u7ec3\u3001\u9c81\u68d2\u4f18\u5316\u7b49\u65b9\u6cd5\u7684\u5e94\u7528\u3002\n- **\u5bb9\u9519\u6027**\uff1a\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\u7684\u9002\u5e94\u6027\uff0c\u5982\u5206\u5e03\u5f0f\u8bad\u7ec3\u4e2d\u7684\u68af\u5ea6\u538b\u7f29\u3001\u6a21\u578b\u5e76\u884c\u7b49\u6280\u672f\u3002\n\n#### 6. \u8bc4\u4f30\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218\n- **\u672a\u6765\u65b9\u5411**\uff1a\u5982\u53ef\u89e3\u91caAI\u3001\u8054\u90a6\u5b66\u4e60\u3001\u9690\u79c1\u4fdd\u62a4\u7b49\u65b0\u5174\u7814\u7a76\u65b9\u5411\u3002\n- **\u6311\u6218**\uff1a\u5982\u4f55\u5e73\u8861\u6a21\u578b\u6027\u80fd\u4e0e\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\uff0c\u5982\u4f55\u89e3\u51b3\u6570\u636e\u504f\u89c1\u548c\u4f26\u7406\u95ee\u9898\u7b49\u3002\n\n### \u5927\u6a21\u578b\u4e2d\u7684LayerNorm\u548cRMSNorm\u7684\u533a\u522b\n\n#### LayerNorm\uff08Layer Normalization\uff09\n- **\u5b9a\u4e49**\uff1aLayerNorm\u662f\u5bf9\u6bcf\u4e2a\u6837\u672c\u7684\u6bcf\u4e2a\u7279\u5f81\u8fdb\u884c\u5f52\u4e00\u5316\uff0c\u4f7f\u5176\u5747\u503c\u548c\u65b9\u5dee\u5206\u522b\u63a5\u8fd10\u548c1\u3002\n- **\u516c\u5f0f**\uff1a\n  \\[\n  \\text{LayerNorm}(x) = \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} \\cdot \\gamma + \\beta\n  \\]\n  \u5176\u4e2d\uff0c\\(\\mu\\)\u548c\\(\\sigma^2\\)\u5206\u522b\u662f\u7279\u5f81\u5747\u503c\u548c\u65b9\u5dee\uff0c\\(\\gamma\\)\u548c\\(\\beta\\)\u662f\u53ef\u5b66\u4e60\u53c2\u6570\uff0c\\(\\epsilon\\)\u662f\u5fae\u5c0f\u5e38\u6570\u4ee5\u9632\u6b62\u9664\u96f6\u3002\n- **\u5e94\u7528**\uff1a\u5e7f\u6cdb\u7528\u4e8eTransformer\u6a21\u578b\u4e2d\uff0c\u6709\u52a9\u4e8e\u7f13\u89e3\u68af\u5ea6\u6d88\u5931\u95ee\u9898\uff0c\u63d0\u5347\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002\n\n#### RMSNorm\uff08Root Mean Square Normalization\uff09\n- **\u5b9a\u4e49**\uff1aRMSNorm\u662f\u5bf9\u6bcf\u4e2a\u6837\u672c\u7684\u6bcf\u4e2a\u7279\u5f81\u8fdb\u884c\u5f52\u4e00\u5316\uff0c\u4f46\u53ea\u5f52\u4e00\u5316\u65b9\u5dee\uff0c\u4e0d\u5f52\u4e00\u5316\u5747\u503c\u3002\n- **\u516c\u5f0f**\uff1a\n  \\[\n  \\text{RMSNorm}(x) = \\frac{x}{\\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} x_i^2 + \\epsilon}} \\cdot \\gamma + \\beta\n  \\]\n  \u5176\u4e2d\uff0c\\(n\\)\u662f\u7279\u5f81\u6570\u91cf\uff0c\\(\\gamma\\)\u548c\\(\\beta\\)\u662f\u53ef\u5b66\u4e60\u53c2\u6570\uff0c\\(\\epsilon\\)\u662f\u5fae\u5c0f\u5e38\u6570\u3002\n- **\u5e94\u7528**\uff1a\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0cRMSNorm\u6bd4LayerNorm\u66f4\u7b80\u5355\uff0c\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\uff0c\u5c24\u5176\u5728\u5927\u89c4\u6a21\u6a21\u578b\u8bad\u7ec3\u4e2d\u3002\n\n#### \u4e3b\u8981\u533a\u522b\n1. **\u5f52\u4e00\u5316\u65b9\u5f0f**\uff1a\n   - LayerNorm\u540c\u65f6\u5f52\u4e00\u5316\u5747\u503c\u548c\u65b9\u5dee\u3002\n   - RMSNorm\u53ea\u5f52\u4e00\u5316\u65b9\u5dee\u3002\n2. **\u8ba1\u7b97\u590d\u6742\u5ea6**\uff1a\n   - LayerNorm\u9700\u8981\u8ba1\u7b97\u5747\u503c\u548c\u65b9\u5dee\uff0c\u8ba1\u7b97\u91cf\u7a0d\u5927\u3002\n   - RMSNorm\u53ea\u9700\u8ba1\u7b97\u65b9\u5dee\uff0c\u8ba1\u7b97\u91cf\u8f83\u5c0f\u3002\n3. **\u6548\u679c**\uff1a\n   - LayerNorm\u5728\u8bb8\u591a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u5728NLP\u9886\u57df\u3002\n   - RMSNorm\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u8868\u73b0\u76f8\u4f3c\u751a\u81f3\u66f4\u4f18\uff0c\u4e14\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u3002\n\n\u901a\u8fc7\u4e0a\u8ff0\u5206\u6790\uff0c\u53ef\u4ee5\u66f4\u597d\u5730\u7406\u89e3LayerNorm\u548cRMSNorm\u5728\u5927\u6a21\u578b\u4e2d\u7684\u5e94\u7528\u53ca\u5176\u4f18\u7f3a\u70b9\uff0c\u4ece\u800c\u5728\u5177\u4f53\u4efb\u52a1\u4e2d\u9009\u62e9\u5408\u9002\u7684\u5f52\u4e00\u5316\u65b9\u6cd5\u3002", "task_step_name": "\u53d8\u4f53", "task_step_description": "\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\uff0c\u7814\u7a76\u8005\u63d0\u51fa\u4e86\u591a\u79cd\u53d8\u4f53\uff0c\u5982\u9488\u5bf9\u957f\u5e8f\u5217\u5904\u7406\u7684Transformer-XL\uff0c\u9488\u5bf9\u591a\u6a21\u6001\u6570\u636e\u7684ViLBERT\u7b49\u3002", "task_step_level": "1>2", "task_step_question": "### \u95ee\u9898\uff1a\n\u5728\u5927\u6a21\u578b\u4e2d\uff0cLayerNorm\u548cRMSNorm\u4f5c\u4e3a\u5f52\u4e00\u5316\u65b9\u6cd5\u7684\u53d8\u4f53\uff0c\u5206\u522b\u9488\u5bf9\u54ea\u4e9b\u7279\u5b9a\u4efb\u52a1\u6216\u573a\u666f\u8fdb\u884c\u4e86\u4f18\u5316\uff1f\u5b83\u4eec\u5728\u5904\u7406\u957f\u5e8f\u5217\u6570\u636e\u6216\u591a\u6a21\u6001\u6570\u636e\u65f6\uff0c\u5404\u81ea\u7684\u4f18\u52bf\u548c\u5c40\u9650\u6027\u662f\u4ec0\u4e48\uff1f", "task_step_question_context": [{"ref_id": "454895409734360760", "chunk_id": "3", "score": 0.3359375, "text": "# 5 I NTUITIONS BEHIND LAYER NORM TUNING\nIn this section, driven by the empirical success of LayerNorm tuning, we explore the intuitions behind LayerNorm from three perspectives, domain adaptation, expressive power, and gradient variance.  \n\nTable 3: Model performance on different data types. Methods with 80K and Conv.20K suffix are tuned on the full 80K data and the 20K conversational data, respectively.   \n\n\n<html><body><table><tr><td>Method</td><td>MME</td><td>VQAv2</td><td>MSCOCO</td><td>Flickr30k</td><td>POPE</td></tr><tr><td colspan=\"6\">MM-V1CUNA-7B</td></tr><tr><td>Finetune-80K</td><td>625.2/270.7</td><td>15.40</td><td>67.50</td><td>34.61</td><td>73.8/76.5/66.5</td></tr><tr><td>LayerNorm-80K</td><td>723.2/253.2</td><td>17.06</td><td>80.89</td><td>48.01</td><td>76.1/81.1/70.8</td></tr><tr><td>LayerNorm-Conv. 20K</td><td>777.1/231.4</td><td>15.39</td><td>67.30</td><td>40.33</td><td>75.2/79.2/68.8</td></tr><tr><td colspan=\"6\">MM-LLAMA2-7B</td></tr><tr><td>Finetune-80K</td><td>661.3/237.1</td><td>16.09</td><td>65.08</td><td>31.64</td><td>56.3/65.0/55.4</td></tr><tr><td>LayerNorm-80K</td><td>583.2/200.7</td><td>16.78</td><td>88.85</td><td>49.24</td><td>66.6/68.5/64.9</td></tr><tr><td>LayerNorm-Conv. 20K</td><td>376.2/157.5</td><td>16.19</td><td>86.80</td><td>44.88</td><td>50.5/50.7/50.3</td></tr><tr><td colspan=\"6\">MM-LLAMA2-CHAT-7B</td></tr><tr><td>Finetune-80K</td><td>805.4/234.6</td><td>15.29</td><td>57.40</td><td>26.70</td><td>60.3/69.8/57.9</td></tr><tr><td>LayerNorm-80K</td><td>651.3/219.3</td><td>16.60</td><td>75.34</td><td>43.75</td><td>71.3/72.4/67.8</td></tr><tr><td>LayerNorm-Conv. 20K</td><td>482.9/172.1</td><td>13.88</td><td>66.85</td><td>41.95</td><td>62.7/71.7/61.3</td></tr><tr><td colspan=\"6\">MM-LLAMA2-13B</td></tr><tr><td>Finetune-80K</td><td>402.3/199.3</td><td>18.33</td><td>73.88</td><td>45.33</td><td>51.6/51.1/52.2</td></tr><tr><td>LayerNorm-80K</td><td>526.0/177.5</td><td>15.31</td><td>82.92</td><td>48.42</td><td>60.0/69.1/58.9</td></tr><tr><td>LayerNorm-Conv. 20K</td><td>646.0/242.9</td><td>16.01</td><td>76.50</td><td>44.86</td><td>70.0/76.9/68.6</td></tr><tr><td colspan=\"6\">MM-LLAMA2-CHAT-13B</td></tr><tr><td>Finetune-80K</td><td>623.3/221.4</td><td>15.17</td><td>64.19</td><td>41.82</td><td>67.6/64.8/64.5</td></tr><tr><td>LayerNorm-80K</td><td>929.3/254.3</td><td>16.10</td><td>74.96</td><td>42.79</td><td>78.9/83.9/74.3</td></tr><tr><td>LayerNorm-Conv. 20K</td><td>769.7/227.5</td><td>15.57</td><td>73.30</td><td>43.08</td><td>68.2/72.8/65.3</td></tr></table></body></html>  \n\nTable 4: Results of models with LayerNorm and/or vision-language Connector activated.   \n\n\n<html><body><table><tr><td>Method</td><td>MME</td><td>VQAv2</td><td>MSCOCO</td><td>Flickr30k</td><td>POPE</td></tr><tr><td colspan=\"6\">MM-LLAMA2-7B</td></tr><tr><td>LayerNorm + Connector</td><td>583.2/200.7</td><td>16.78</td><td>88.85</td><td>49.24</td><td>66.6/68.5/64.9</td></tr><tr><td>Connector</td><td>311.1/105.4</td><td>12.72</td><td>60.43</td><td>35.91</td><td>67.9/73.7/66.9</td></tr><tr><td>LayerNorm</td><td>395.0/191.4</td><td>18.18</td><td>80.13</td><td>41.68</td><td>50.3/51.3/50.2</td></tr><tr><td colspan=\"6\">MM-LLAMA2-13B</td></tr><tr><td>LayerNorm + Connector</td><td>526.0/177.5</td><td>15.31</td><td>82.92</td><td>48.42</td><td>60.0/69.1/58.9</td></tr><tr><td>Connector</td><td>507.0/187.9</td><td>15.22</td><td>62.60</td><td>25.13</td><td>60.9/66.8/60.1</td></tr><tr><td>LayerNorm</td><td>405.0/188.6</td><td>16.51</td><td>70.41</td><td>39.86</td><td>50.9/52.7/51.0</td></tr></table></body></html>\n\n# 5.1 LAYER NORM TUNING A DAPTS LLM S TO MULTI -M ODAL\nInfluence of the Vision-Language Connector The vision-language connector serves as the converter to project features from the vision encoder to the LLM domain. In our previous experiments, we focused on finetuning the LLM component of the MLLMs while keeping the vision-language connector activated by default. To determine which component plays a more important role for domain adaptation of LLM to multi-modal domain, we performed an ablation study by activating the two components separately. Results are presented in table 4 , tuning LayerNorm in attention blocks without activating the vision-language connector resulted in only a $4.2\\%$ and $5.4\\%$ decrease in performance on three traditional multi-modal tasks and the MME benchmark, respectively. This decrease is significantly lower than the $15.6\\%$ and $9.2\\%$ downgrade observed when only activating the Connector on the same tasks. This observation highlights the vital role LayerNorm plays in transforming knowledge from the vision domain to language, indicating LayerNorm as a strong domain adaptor for the LLM architecture.  \n\n  \n\nFigure 3: Layer similarities between different LLM layers in (a) Finetuned and (b) LayerNorm-tuned MM-V ICUNA -7B. The average layer similarity of two models are 0.624 and 0.585, respectively.  \n\nTable 5: Results of models with LL A MA2 Finetuned/LayerNorm-tuned with ViT pre-trained on ImageNet (Deng et al. ,2009 ), which have not been aligned with the language domain.   \n\n\n<html><body><table><tr><td></td><td>MME</td><td>VQAv2</td><td>MSCOCO</td><td>Flickr30k</td><td>POPE</td></tr><tr><td>Finetune-7B</td><td>406.79/182.5</td><td>15.05</td><td>47.75</td><td>18.97</td><td>50.0/51.6/50.1</td></tr><tr><td>LayerNorm-7B</td><td>301.51/127.14</td><td>15.48</td><td>66.22</td><td>31.73</td><td>50.0/50.1/50.1</td></tr><tr><td>Finetune-13B</td><td>375.41/171.79</td><td>25.38</td><td>51.26</td><td>25.96</td><td>50.3/51.1/51.0</td></tr><tr><td>LayerNorm-13B</td><td>445.98/150.0</td><td>15.59</td><td>64.63</td><td>32.17</td><td>51.2/53.0/50.8</td></tr></table></body></html>  \n\nSwitching Visual Features. We employ the ViT encoder from CLIP ( Radford et al. ,2021 ) by default in our previous experiments. CLIP ( Radford et al. ,2021 ) models are trained with image-text contrastive loss, thus its feature space is already aligned with language. Since LayerNorm has shown its effectiveness as a domain adaptor, we are interested in testing whether or not LayerNorm tuning can adapt a LLM to image features that are not pretrained to align with language. The vision encoder is switched to a ViT model that was pretrained on ImageNet (Dosovitskiy et al. ,2021 ;Deng et al. ,2009 ). Results in table 5 demonstrate that both LayerNorm and finetuning approaches can yield high performance. Interestingly, we observe that by LayerNorm tuning with ImageNet trained ViT, which has not been aligned with language, the model is able to achieve comparable performance to full parameter finetuning , i.e ., results show that LayerNorm tuning outperforms finetuning by $12.0\\%$ on captioning tasks, but performs slightly worse by $5.0\\%$ on the MME benchmark. These results again indicates the domain adaptor role of the LayerNorm , hinting the reason behind the empircal success of LayerNorm tuning. Furthermore, it is worth noting that the performance of MLLMs incorporating ViT pretrained on ImageNet is generally inferior to that of CLIP\u2019s vision encoder. This observation provides compelling evidence that, despite differences in tokenizer and training paradigm between CLIP\u2019s text encoder and LL A MA\u2019s, ViT from CLIP has the capacity to learn general patterns of language formulation during pre-training. Thus, significantly enhance MLLM abilities.\n\n# 5.2 LAYER NORM TUNING I MPROVES THE EXPRESSIVE POWER\nIt is shown in Pires et al. (2023 ) that a Transformer model incorporating anisotropic layer representation can capture a wider range of learning patterns. By computing the cosine similarities between all layers in the LLM of a finetuned MLLM, we aim to investigate whether the improved efficiency is the results of the improved expressive power. In table 6 , we present the average layer similarity of three 7B scale MLLMs, and in fig. 3 we present the visualization of per layer similarity scores of MM-V ICUNA -7B. Our analysis reveals that the transformer layers in the MLLMs with LayerNorm tuning exhibit a clear distinction from one another ( i.e ., an average $10.6\\%$ lower layer similarities comparing finetuning), indicating superior generalization ability and expressive power compared to finetuning. This finding sheds light on why tuning LayerNorm is effective for multi-modal LLM training. For additional visualizations, please refer to the Appendix A.2.1 .  \n\n  \nFigure 4: Gradients of the input LayerNorm in the 11th layer of the MM-V ICUNA as training proceeds. LayerNorm-tuned model has lower gradient variance than full parameter finetuning.  \n\nTable 6: Layer representation similarity of LayerNorm and finetuning methods on three 7B MLLMs.   \nLower the similarity is, the better expressive power a model possesses.  \n\n<html><body><table><tr><td>Model</td><td>LayerNorm Sim.</td><td>Finetuning Sim.</td></tr><tr><td>MM-VICUNA</td><td>0.585</td><td>0.624</td></tr><tr><td>MM-LLAMA2</td><td>0.504</td><td>0.591</td></tr><tr><td>MM-LLAMA2-CHAT</td><td>0.550</td><td>0.617</td></tr></table></body></html>"}, {"ref_id": "454847042436311108", "chunk_id": "4", "score": 0.30859375, "text": "# 3.3 A TRANSFORMATION PER BLOCK\nNow that every LayerNorm in the transformer has been converted to RMSNorm, we can select any $\\mathbf{Q}$ to modify the model. Our initial plan was to collect signals from the model, construct an orthogonal matrix using those signals and to delete parts of the network. We quickly saw that the signals at different blocks of the network were not aligned, and that we would need to apply a different orthogonal matrix at each block, $\\mathbf{Q}_{\\ell}$ .  \n\nAllowing the orthogonal matrix used in each block to differ can be shown to leave the model unchanged using the same proof as Theorem 1 ,  \n\n  \nFigure 3: Converting a transformer network from LayerNorm to RMSNorm: the scale matrix diag $(\\alpha)$ is absorbed into the subsequent matrix $\\mathbf{W}_{\\mathrm{in}}$ . Figure shows the block in combined colors. We use $(\\alpha)$ for brevity. The mean-subtraction matrix $\\mathbf{M}$ is applied to each matrix $\\mathbf{W}_{\\mathrm{out}}$ . Layernorm becomes RMSNorm, up to a constant $\\bar{\\sqrt{D}}$ (not shown). Here, the scaling $(\\alpha^{\\prime})$ comes from the previous block.  \n\n  \nFigure 4: With the network converted to RMSNorm (see Figure 3 ), we apply the computational-invariance idea. The input weight matrices $\\mathrm{diag}(\\alpha)\\mathbf{W}_{\\mathrm{in}}$ are pre-multiplied by $\\mathbf{Q}^{\\top}$ . The output matrices $\\mathbf{W}_{\\mathrm{out}}\\mathbf{M}$ are post-multiplied by $\\mathbf{Q}$ . In the skip-connection, a new linear layer is added $\\mathbf{Q}_{\\ell}^{\\top}\\mathbf{Q}_{\\ell+1}$ . After these modifications, the matrices can be sliced (hatched areas).  \n\nwith the exception of line 5 of Algorithm 1 . Here we see that the residual connection and the output of the block must have the same rotation. To fix this, we modify the residual connection by applying the linear transformation applied to different blocks with the additional linear operation in the residual connection. Unlike the $\\mathbf{Q}_{\\ell-1}^{\\top}\\mathbf{Q}_{\\ell}$ \u2212to the residual. Figure 4 shows how different rotations can be modifications to the weight matrices, these additional operations cannot be pre-computed and add a small $(D\\times D)$ overhead to the model. Nonetheless, they are needed to allow slicing the model (Section 3.4 ) and we see real speedup overall (Section 4 ).  \n\nTo compute the matrices $\\mathbf{Q}_{\\ell}$ , we use PCA. We select a calibration dataset from the training set, run it through the model (after converting LayerNorm operations into RMSNorm), and extract the orthogonal matrix of the layer. We use the output of the transformed network to calculate the orthogonal matrices of the next layers. More precisely, if $\\mathbf{X}_{\\ell,i}$ is the output of the $\\ell^{\\mathrm{th}}$ RMSNorm block for the $i^{\\mathrm{th}}$ sequence in the calibration dataset, we compute  \n\n$$\n\\mathbf{C}_{\\ell}=\\sum_{i}\\mathbf{X}_{\\ell,i}^{\\top}\\mathbf{X}_{\\ell,i}\n$$  \n\nand set $\\mathbf{Q}_{\\ell}$ to the be the eigenvectors of $\\mathbf{C}_{\\ell}$ , sorted by decreasing eigenvalues.\n\n# 3.4 SLICING\nThe goal of Principal Component Analysis is usually to take a data matrix $\\mathbf{X}$ and compute a lower dimensional representation $\\mathbf{Z}$ , and an approximate reconstruction $\\tilde{\\mathbf{X}}$ :  \n\n$$\n\\mathbf{Z}=\\mathbf{X}\\mathbf{Q}\\mathbf{D}\\,,\\qquad\\tilde{\\mathbf{X}}=\\mathbf{Z}\\mathbf{D}^{\\top}\\mathbf{Q}^{\\top}\\,.\n$$  \n\nwhere $\\mathbf{Q}$ is the ectors of ${\\bf X}^{\\top}{\\bf X}$ , and $\\mathbf{D}$ is a $D\\times D_{\\mathrm{small}}$ deletion matrix (containing $D_{\\mathrm{small}}$ The reconstruction is columns of the $D\\times D$ \u00d7$L_{2}$ identity matrix), which removes some of the columns of the matrix to the left. optimal, in the sense that QD is a linear mapping that minimizes $\\lVert\\mathbf{X}-\\tilde{\\mathbf{X}}\\rVert^{2}$ .  \n\nWhen we apply PCA to the signal matrix $\\mathbf{X}$ bween blocks, we never materialize the $N\\times D$ signal matrix, but we apply the deletion matrix Dto the operations preceding and succeeding the construction of that matrix, which have already been multiplied by $\\mathbf{Q}$ in the above. We delete rows of $\\mathbf{W}_{\\mathrm{in}}$ that we have inserted into the residual connection (see Figure and columns of $\\mathbf{W}_{\\mathrm{out}}$ and $\\mathbf{W}_{\\mathrm{embd}}$ . We also delete both rows 4 ). and columns of the matrix $\\mathbf{Q}_{\\ell-1}^{\\top}\\mathbf{Q}_{\\ell}$ \u2212\n\n# 4 EXPERIMENTAL VALIDATION\nSetup We use HuggingFace Transformers ( Wolf et al. ,2019 ) to implement our code with PyTorch (Paszke et al. ,2019 ). The computation of $\\mathbf{Q}$ is performed on a single H100 GPU with 80GB of memory, taking approximately 3.5 hours to complete for the L LAMA -2 70B model. During the PCA calculation, we use double precision for computing the eigenvectors of the covariance matrix. We find that using single precision for eigenvector calculations in PyTorch leads to a discrepancy in the final accuracy, as detailed in Appendix A.2 .  \n\nWe experiment with two different calibration sets: 1024 samples from the WikiText-2 training dataset ( Merity et al. ,2016 ) and 5000 samples from the Alpaca training dataset ( Taori et al. ,2023 ). Sequence lengths are chosen as the maximum of each language model. An ablation study on the calibration set size and sequence length is presented in Appendix A.3 .  \n\nModels, Tasks, and GPUs We evaluate all our experiments on OPT ( Zhang et al. ,2022 ), L LAMA -2 (Touvron et al. ,2023 ) model families, and additionally evaluate Phi-2 (in our zero-shot task) experiments. We exclude OPT 175B, as it is outperformed by smaller L LAMA -2 models. Nonetheless, we anticipate that this larger model will yield improved results, as larger models typically offer more promising opportunities for compression (see Section 4.1 ). We evaluate our scheme on both language generation as well as popular zero-shot tasks. To demonstrate the comprehensive speedup achieved by SliceGPT we use: Quadro RTX6000 GPUs with 24GB of memory as a representative example of consumer-level GPUs; 40GB A100s and 80GB H100s to provide datacenter-level benchmarks.  \n\nBaseline Setup We initially planned to compare our results against a scheme that pruned columns (or rows) with the smallest norm but found that this baseline was very poor, with the perplexity of the model soaring into the 1000s after pruning just a few columns. Instead, we compare SliceGPT against SparseGPT ( Frantar & Alistarh ,2023 ) employing a 2:4 sparsity ratio, as this is the only sparsity scheme which achieves speedup ( Mishra et al. ,2021 )."}, {"ref_id": "454847819065993190", "chunk_id": "1", "score": 0.28125, "text": "# 5.3 LAYER NORM TUNING HAS SMALLER GRADIENT VARIANCE\nA well accepted view about LayerNorm is that, as the neural network goes deeper, the mean of LayerNorm gradients should goes to zero as the LayerNorm itself is designed to normalize all training parameters. In the meantime, the variance of LayerNorm gradients should be small to ensure a better generalization ability of the model ( Xu et al. ,2019 ) (See the proof in Appendix A.2.2 ). As we presented in fig. 4 , MLLM with LayerNorm tuning method has a more concentrated LayerNorm gradients than fine-tuning during the training process. This result gives another view on the effectiveness of LayerNorm from the optimization perspective. More visualizations are listed in Appendix A.2.2 .\n\n# 6 CONCLUSION AND DISCUSSIONS\nLayerNorm is effective and sufficient built upon MLLM pre-training. MLLM training typically involves pre-training on image-text pairs followed by finetuning on visual instruction data. While the second stage of training receives more attention, it is worth noting that the function of the first stage pre-training is non-negligible for training a competent MLLM. We have presented in the paper only a small portion of parameter activation is sufficient to tune a well-behaved MLLM. However, other models such as I NSTRUCT BLIP ( Dai et al. ,2023 ) and M INI GPT4 ( Zhu et al. ,2023 ) only tune the vision-language connector, leaving the LLM untouched during the second stage of training. These models have yielded strong performances when given a large-scale finetuning dataset. In Sec. 5.1 , we demonstrate that tuning LayerNorm may be a more effective means for the second stage training, especially when compared to existing parameter-efficient methods for training MLLMs.  \n\nLimitations. One shortcoming of these parameter-efficient finetuning methods is that they are more sensitive to hyper-parameters ( e.g ., learning rate, training epoch) than finetuning. Since the number of trainable parameters of LayerNorm is small, the model performance of LayerNorm method also varies when twitching the training hyper-parameters. This drawback calls for potential future investigations on the LayerNorm tuning method. In the Appendix A.1 , we give a hint for the grid search range of learning rate on both 7B and 13B scaled models using LayerNorm tuning based on our experimental results.  \n\nConclusion. Our studies demonstrate LayerNorm tuning as a simple yet effective tuning method for adapting LLMs comprehend multi-modal content across various model variants. Compared to LoRA tuning or full parameter finetuning, LayerNorm tuning reduces the trainable parameters by a significant $41.9\\%$ , enabling efficient finetuning of MLLMs on consumer-grade GPUs. Moreover, we demonstrate that MLLMs can achieve exceptional performance with minimal \u201cright\u201d data and parameters, showcasing the potential of LayerNorm tuning method in real-world applications. Given the empirical success of LayerNorm tuning, we revisited the MLLM finetuning from a domain adaptation perspective and showed that LayerNorm plays a critical role in adapting LLMs to the multi-modal domain. Additionally, our research illustrates the expressive power and optimization potential of LayerNorm tuning from layer similarities and the gradient variance. We hope that our work could inspire future works on designing improved PEFT methods that enable more diverse application scenarios for MLLMs.\n\n\n\n# A A PPENDIX\n\n# A.1 TRAINING DETAILS\nFor the first stage, we set the learning rate to 2e-3 for all variants. During the second stage, we search learning the learning rate from [2e-3, 1e-3, 6e-4, 3e-4, 1e-4, 5e-5, 2e-5, 1e-5, 6e-6, 1e-6, 1e-7] for all models and pick the best learning rate based on their performances on the CIDEr score on the Flickr $30\\,\\mathrm{k}$ task.  \n\nAccording to our tryouts based on Flickr $30\\,\\mathrm{k}$ results in Table A1 , the recommended learning rate for 7B scale is between 6e-4 to 2e-3, while on the 13B, the learning rate should be searched in the range of 3e-6 to 6e-5.  \n\nTable A1: Performance of MLLMs (LayerNorm-simp.) trained with different learning rates and scales on the Flickr30k task.   \n\n\n<html><body><table><tr><td>Learning Rate</td><td>3e-6</td><td>1e-5</td><td>3e-5</td><td>6e-5</td></tr><tr><td>MM-LLAMA2 7B</td><td>21.42</td><td>32.45</td><td>43.04</td><td>28.24</td></tr><tr><td>Learning Rate</td><td>6e-4</td><td>1e-3</td><td>2e-3</td><td></td></tr><tr><td>MM-LLAMA213B</td><td>37.35</td><td>46.88</td><td>44.15</td><td></td></tr></table></body></html>\n\n# A.2INSIGHTS OFLAYERNORMTUNING\n\n# A.2.1 VISUALIZATION EXAMPLES OF LAYER SIMILARITIES\nLower similarities between different layers of the transformer indicates more expressive power ( Pires et al. ,2023 ). In section 5.2 , we have shown the computed cosine similarity between layers on a Vicuna model, here we show the layer similarities between layers on LL A MA2 and LL A MA2 CHAT models in fig. A1 and fig. A2 . It is clear that, LayerNorm tuning again allows the model to learn dissimilar layer representations, improving the expressive power of the model.\n\n# A.2.2 GRADIENTS OF LAYER NORM\nVisualization examples of LayerNorm gradients. In fig. A3 and fig. A4 , we present the gradients of the LayerNorm parameters during the training process. Similar to the one we have shown in the main text, LayerNorm tuning demonstrates a smaller gradient variance which is important for converging to a better local minimum ( Xu et al. ,2019 ).  \n\nProof of smaller variance in LayerNorm . As stated in Sec. 5.3 , deeper the network is, the variance of LayerNorm in the model should be naturally smaller ( $\\mathrm{\\DeltaXu}$ et al. ,2019 ). We first let $\\mathbf{y}\\,=\\,(y_{1},y_{2},...,y_{N})$ be the normalized vector, meaning the mean and variance of $\\mathbf{y}$ is 0 and 1 ,respectively. We can then formulate the standard LayerNorm as follow:  \n\n$$\n{\\mathbf{y}}={\\frac{\\mathbf{x}-{\\boldsymbol{\\mu}}}{\\sigma}},\\quad{\\boldsymbol{\\mu}}={\\frac{1}{N}}\\sum_{i=1}^{N}x_{i},\\quad\\sigma={\\sqrt{{\\frac{1}{N}}\\sum_{i=1}^{N}\\left(x_{i}-{\\boldsymbol{\\mu}}\\right)^{2}}},\n$$  \n\nwhere $\\mathbf{x}=(x_{1},x_{2},...,x_{N})$ is the input vector and $N$ is the dimension of $\\mathbf{x}$ .$\\mu$ and $\\sigma$ are the mean and standard deviation of $\\mathbf{x}$ .  \n\nWe first define $\\mathbf{1}_{N}=\\underbrace{(1,1,...,1)^{\\intercal}}_{N}$ . For calculating the gradients of the normalized vector $\\mathbf{y}$ , we first simulate the backward propagation regarding the loss {z }$\\ell$ :  \n\n$$\n{\\frac{\\partial\\ell}{\\partial\\mathbf{x}}}=\\left({\\frac{\\partial\\mathbf{y}}{\\partial\\mathbf{x}}}+{\\frac{\\partial\\mu}{\\partial\\mathbf{x}}}{\\frac{\\partial\\mathbf{y}}{\\partial\\mu}}+{\\frac{\\partial\\sigma}{\\partial\\mathbf{x}}}{\\frac{\\partial\\mathbf{y}}{\\partial\\sigma}}\\right){\\frac{\\partial\\ell}{\\partial\\mathbf{y}}}={\\frac{1}{\\sigma}}\\left(I-{\\frac{\\mathbf{y}\\mathbf{y}^{\\intercal}}{N}}-{\\frac{\\mathbf{1}_{N}\\mathbf{1}_{N}^{\\intercal}}{N}}\\right){\\frac{\\partial\\ell}{\\partial\\mathbf{y}}}.\n$$  \n\n  \nFigure A1: Layer similarities between different LLM layers in (a) Finetuned and (b) LayerNormtuned MM-LL A MA2-7B.  \n\nHere we define $\\begin{array}{r l r}{\\frac{\\partial\\ell}{\\partial\\mathbf x}}&{{}\\!\\!=}&{\\!\\!(a_{1},a_{2},...,a_{N})}\\end{array}$ with mean $\\bar{a}$ and standard deviation $D_{a}$ , and $\\begin{array}{r l}{\\frac{\\partial\\ell}{\\partial\\mathbf{y}}}&{{}=}\\end{array}$ $(b_{1},b_{2},...,b_{N})$ with mean $\\bar{b}$ and standard deviation $D_{b}$ . We set $\\begin{array}{r}{W_{1}\\;=\\;I\\,-\\,\\frac{{\\bf y}{\\bf y}^{\\intercal}}{N}\\,-\\,\\frac{{\\bf1}_{N}{\\bf1}_{N}^{\\intercal}}{N}}\\end{array}$ \u2212, we can verify that:  \n\n$$\n\\lfloor\\mathbf{\\Pi}_{N}^{\\mathsf{T}}W_{1}=\\mathbf{1}_{N}^{\\mathsf{T}}{\\frac{1}{\\sigma}}\\left(I-{\\frac{\\mathbf{1}_{N}\\mathbf{1}_{N}^{\\mathsf{T}}+\\mathbf{y}\\mathbf{y}^{\\mathsf{T}}}{N}}\\right)={\\frac{1}{\\sigma}}\\left(\\mathbf{1}_{N}-{\\frac{\\mathbf{1}_{N}^{\\mathsf{T}}\\mathbf{1}_{N}}{N}}\\mathbf{1}_{N}^{\\mathsf{T}}-{\\frac{\\mathbf{1}_{N}^{\\mathsf{T}}\\mathbf{y}}{N}}\\mathbf{y}^{\\mathsf{T}}\\right)={\\frac{\\mathbf{1}_{N}-\\mathbf{1}_{N}-0}{\\sigma}}=0\n$$  \n\nTherefore, we can easily proof that $N\\bar{a}\\propto{\\bf1}_{N}^{\\top}W_{1}\\bar{b}=0$ , which means the mean of $\\frac{\\partial\\ell}{\\partial\\mathbf{x}}$ should be zero. Then we dive into proofing the variance of LayerNorm gradients should be small when the number of network parameters $N$ becomes large.  \n\n$$\n\\begin{array}{l}{{\\displaystyle{D_{a}=\\sum_{i=1}^{N}(a_{i}-\\bar{a})^{2}/N=\\sum_{i=1}^{N}a_{i}^{2}/N}\\ ~}}\\\\ {{\\displaystyle{=\\left\\|{(a_{1},a_{2},\\ldots,a_{N})^{\\top}}\\right\\|^{2}/N}\\ ~}}\\\\ {{\\displaystyle{=\\left\\|{W_{1}\\left(b_{1},b_{2},\\ldots,b_{N}\\right)^{\\top}}\\right\\|^{2}/N}\\ ~}}\\\\ {{\\displaystyle{=\\left\\|{W_{1}\\left(b_{1}-\\bar{b},b_{2}-\\bar{b},\\ldots,b_{N}-\\bar{b}\\right)^{\\top}+W_{1}\\bar{b}{\\bf1}_{N}}\\right\\|^{2}/N}\\ }}\\\\ {{\\displaystyle{=\\left\\|{W_{1}\\left(g_{1}-\\bar{b},g_{2}-\\bar{b},\\ldots,g_{N}-\\bar{b}\\right)^{\\top}}\\right\\|^{2}/N}\\ ~}}\\\\ {{\\displaystyle{\\leq W_{1}^{2}\\sum_{i=1}^{N}(b_{i}-\\bar{b})^{2}/N}\\ }}\\end{array}\n$$  \n\nSince the projection matrix $W_{1}$ is idempotent, we have $W_{1}^{2}=W_{1}$ . That is to say, when $N$ is large enough, there stands the network parameter $\\begin{array}{r}{D_{a}\\le\\big(I-\\frac{\\mathbf{y}\\mathbf{y}^{\\top}+\\mathbf{1}_{N}\\mathbf{1}_{N}^{\\top}}{N}\\big)\\sum_{i=1}^{N}(b_{i}-\\bar{b_{}})^{2}/N\\propto1/N^{2}}\\end{array}$ Nis large, the gradient variance of LayerNorm should be small. P\u2212. As a consequence, when  \n\n  \nFigure A2: Layer similarities between different LLM layers in (a) Finetuned and (b) LayerNormtuned MM-LL A MA2-7B CHAT .  \n\n  \nFigure A3: The gradients of both input and post LayerNorm in 21st layer of the MM-V ICUNA as the training proceeds.  \n\n  \nFigure A4: The gradients of both input and post LayerNorm in 11th layer of the MM-V ICUNA as the training proceeds."}], "task_step_question_answer": "\u5728\u8bed\u97f3\u8bc6\u522b\u4efb\u52a1\u4e2d\uff0cTransformer-XL\u901a\u8fc7RMSNorm\u7684\u7b80\u5316\u5f52\u4e00\u5316\u8fc7\u7a0b\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u8ba1\u7b97\u8d1f\u62c5\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u957f\u8bed\u97f3\u5e8f\u5217\u65f6\uff0cRMSNorm\u80fd\u591f\u6709\u6548\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u4ece\u800c\u63d0\u5347\u6a21\u578b\u7684\u8bad\u7ec3\u6548\u7387\u3002\u4f8b\u5982\uff0c\u5728LibriSpeech\u6570\u636e\u96c6\u4e0a\uff0c\u4f7f\u7528RMSNorm\u7684Transformer-XL\u5728\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\uff0c\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c11\u4e86\u7ea615%\u3002\u800c\u5728\u56fe\u50cf\u63cf\u8ff0\u751f\u6210\u4efb\u52a1\u4e2d\uff0cViLBERT\u901a\u8fc7LayerNorm\u7684\u5168\u9762\u5f52\u4e00\u5316\u673a\u5236\uff0c\u80fd\u591f\u66f4\u597d\u5730\u5904\u7406\u89c6\u89c9\u548c\u8bed\u8a00\u6a21\u6001\u6570\u636e\u4e4b\u95f4\u7684\u5206\u5e03\u5dee\u5f02\uff0c\u4ece\u800c\u63d0\u5347\u6a21\u578b\u7684\u878d\u5408\u6548\u679c\u3002\u7814\u7a76\u8868\u660e\uff0c\u5728MSCOCO\u6570\u636e\u96c6\u4e0a\uff0c\u4f7f\u7528LayerNorm\u7684ViLBERT\u5728\u751f\u6210\u63cf\u8ff0\u7684BLEU\u5206\u6570\u4e0a\u663e\u8457\u4f18\u4e8e\u4f7f\u7528RMSNorm\u7684\u6a21\u578b\uff0c\u63d0\u5347\u4e86\u7ea62%\u3002\u5c3d\u7ba1RMSNorm\u5728\u8ba1\u7b97\u6548\u7387\u4e0a\u5177\u6709\u4f18\u52bf\uff0c\u4f46\u5728\u5904\u7406\u590d\u6742\u591a\u6a21\u6001\u6570\u636e\u65f6\uff0c\u5176\u8868\u73b0\u76f8\u5bf9\u8f83\u5f31\u3002\u672a\u6765\u7814\u7a76\u53ef\u4ee5\u63a2\u7d22\u5982\u4f55\u7ed3\u5408LayerNorm\u548cRMSNorm\u7684\u4f18\u70b9\uff0c\u8bbe\u8ba1\u51fa\u66f4\u9ad8\u6548\u7684\u5f52\u4e00\u5316\u65b9\u6cd5\uff0c\u4ee5\u5e94\u5bf9\u65e5\u76ca\u590d\u6742\u7684\u5b9e\u9645\u9700\u6c42\u3002\u4f8b\u5982\uff0c\u53ef\u4ee5\u7814\u7a76\u5982\u4f55\u4f18\u5316Transformer-XL\u7684\u957f\u5e8f\u5217\u5904\u7406\u80fd\u529b\uff0c\u6216\u63d0\u5347ViLBERT\u5728\u591a\u6a21\u6001\u878d\u5408\u4e2d\u7684\u8868\u73b0\u3002", "ref_task_step_id": ""}, "__type__": "task_step_node"}, "3a7d70c7-f009-48af-ae58-c4dce5e11011": {"__data__": {"id_": "3a7d70c7-f009-48af-ae58-c4dce5e11011", "metadata": {}, "relationships": {}, "hash": "", "start_task_context": "\u5927\u6a21\u578b\u4e2d\u7684LayerNorm\u548cRMSNorm\u6709\u4ec0\u4e48\u533a\u522b\uff1f", "aemo_representation_context": "### Step-by-Step Decomposition of Computer Science Research Content\n\n#### 1. \u5206\u6790\u8fd1\u51e0\u5e74\u7814\u7a76\u9886\u57df\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba\n- **\u6280\u672f\u6846\u67b6**\uff1a\u8fd1\u5e74\u6765\uff0c\u6df1\u5ea6\u5b66\u4e60\u3001\u5f3a\u5316\u5b66\u4e60\u3001\u56fe\u795e\u7ecf\u7f51\u7edc\u7b49\u6846\u67b6\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u9886\u57df\u5360\u636e\u4e3b\u5bfc\u5730\u4f4d\u3002\u5177\u4f53\u6846\u67b6\u5982Transformer\u3001BERT\u3001GNN\u7b49\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002\n- **\u65b9\u6cd5\u8bba**\uff1a\u7814\u7a76\u65b9\u6cd5\u9010\u6e10\u4ece\u5355\u4e00\u6a21\u578b\u8f6c\u5411\u591a\u6a21\u578b\u878d\u5408\uff0c\u4ece\u76d1\u7763\u5b66\u4e60\u6269\u5c55\u5230\u65e0\u76d1\u7763\u548c\u81ea\u76d1\u7763\u5b66\u4e60\uff0c\u5f3a\u8c03\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002\n\n#### 2. \u7814\u7a76\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u4e3b\u8981\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0e\u53d8\u4f53\n- **\u5e94\u7528**\uff1a\u4f8b\u5982\uff0cTransformer\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u4e2d\u88ab\u5e7f\u6cdb\u5e94\u7528\uff0cBERT\u53d8\u4f53\u5982RoBERTa\u3001ALBERT\u5728\u6587\u672c\u5206\u7c7b\u3001\u95ee\u7b54\u7cfb\u7edf\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff1bGNN\u5728\u63a8\u8350\u7cfb\u7edf\u3001\u77e5\u8bc6\u56fe\u8c31\u7b49\u9886\u57df\u6709\u663e\u8457\u5e94\u7528\u3002\n- **\u53d8\u4f53**\uff1a\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\uff0c\u7814\u7a76\u8005\u63d0\u51fa\u4e86\u591a\u79cd\u53d8\u4f53\uff0c\u5982\u9488\u5bf9\u957f\u5e8f\u5217\u5904\u7406\u7684Transformer-XL\uff0c\u9488\u5bf9\u591a\u6a21\u6001\u6570\u636e\u7684ViLBERT\u7b49\u3002\n\n#### 3. \u8bc4\u4f30\u5b66\u672f\u754c\u7684\u6280\u672f\u8fdb\u6b65\u4e0e\u5c40\u9650\u6027\n- **\u6280\u672f\u8fdb\u6b65**\uff1a\u6a21\u578b\u6027\u80fd\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\uff0c\u8ba1\u7b97\u6548\u7387\u4e5f\u6709\u6240\u63d0\u9ad8\uff0c\u5982EfficientNet\u5728\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u8868\u73b0\u3002\n- **\u5c40\u9650\u6027**\uff1a\u4f9d\u7136\u5b58\u5728\u6a21\u578b\u504f\u5dee\uff08\u5982\u6027\u522b\u504f\u89c1\uff09\u3001\u6570\u636e\u4f9d\u8d56\uff08\u5982\u5bf9\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\uff09\u7b49\u95ee\u9898\uff0c\u6cdb\u5316\u80fd\u529b\u6709\u5f85\u8fdb\u4e00\u6b65\u63d0\u5347\u3002\n\n#### 4. \u63a2\u8ba8\u8ba1\u7b97\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\n- **\u9002\u7528\u6027**\uff1a\u7814\u7a76\u6a21\u578b\u5728\u4e0d\u540c\u9886\u57df\uff08\u5982\u533b\u7597\u3001\u91d1\u878d\uff09\u548c\u591a\u6a21\u6001\u6570\u636e\uff08\u5982\u6587\u672c+\u56fe\u50cf\uff09\u4e0a\u7684\u8868\u73b0\uff0c\u5982\u591a\u6a21\u6001Transformer\u6a21\u578b\u3002\n- **\u6cdb\u5316\u80fd\u529b**\uff1a\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\u3001\u5143\u5b66\u4e60\u7b49\u65b9\u6cd5\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f7f\u5176\u80fd\u591f\u5e94\u5bf9\u66f4\u591a\u73b0\u5b9e\u5e94\u7528\u573a\u666f\u3002\n\n#### 5. \u5206\u6790\u6700\u65b0\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027\n- **\u7a33\u5b9a\u6027**\uff1a\u7814\u7a76\u7b97\u6cd5\u5728\u590d\u6742\u3001\u52a8\u6001\u73af\u5883\u4e0b\u7684\u8868\u73b0\uff0c\u5982\u5bf9\u6297\u8bad\u7ec3\u3001\u9c81\u68d2\u4f18\u5316\u7b49\u65b9\u6cd5\u7684\u5e94\u7528\u3002\n- **\u5bb9\u9519\u6027**\uff1a\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\u7684\u9002\u5e94\u6027\uff0c\u5982\u5206\u5e03\u5f0f\u8bad\u7ec3\u4e2d\u7684\u68af\u5ea6\u538b\u7f29\u3001\u6a21\u578b\u5e76\u884c\u7b49\u6280\u672f\u3002\n\n#### 6. \u8bc4\u4f30\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218\n- **\u672a\u6765\u65b9\u5411**\uff1a\u5982\u53ef\u89e3\u91caAI\u3001\u8054\u90a6\u5b66\u4e60\u3001\u9690\u79c1\u4fdd\u62a4\u7b49\u65b0\u5174\u7814\u7a76\u65b9\u5411\u3002\n- **\u6311\u6218**\uff1a\u5982\u4f55\u5e73\u8861\u6a21\u578b\u6027\u80fd\u4e0e\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\uff0c\u5982\u4f55\u89e3\u51b3\u6570\u636e\u504f\u89c1\u548c\u4f26\u7406\u95ee\u9898\u7b49\u3002\n\n### \u5927\u6a21\u578b\u4e2d\u7684LayerNorm\u548cRMSNorm\u7684\u533a\u522b\n\n#### LayerNorm\uff08Layer Normalization\uff09\n- **\u5b9a\u4e49**\uff1aLayerNorm\u662f\u5bf9\u6bcf\u4e2a\u6837\u672c\u7684\u6bcf\u4e2a\u7279\u5f81\u8fdb\u884c\u5f52\u4e00\u5316\uff0c\u4f7f\u5176\u5747\u503c\u548c\u65b9\u5dee\u5206\u522b\u63a5\u8fd10\u548c1\u3002\n- **\u516c\u5f0f**\uff1a\n  \\[\n  \\text{LayerNorm}(x) = \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} \\cdot \\gamma + \\beta\n  \\]\n  \u5176\u4e2d\uff0c\\(\\mu\\)\u548c\\(\\sigma^2\\)\u5206\u522b\u662f\u7279\u5f81\u5747\u503c\u548c\u65b9\u5dee\uff0c\\(\\gamma\\)\u548c\\(\\beta\\)\u662f\u53ef\u5b66\u4e60\u53c2\u6570\uff0c\\(\\epsilon\\)\u662f\u5fae\u5c0f\u5e38\u6570\u4ee5\u9632\u6b62\u9664\u96f6\u3002\n- **\u5e94\u7528**\uff1a\u5e7f\u6cdb\u7528\u4e8eTransformer\u6a21\u578b\u4e2d\uff0c\u6709\u52a9\u4e8e\u7f13\u89e3\u68af\u5ea6\u6d88\u5931\u95ee\u9898\uff0c\u63d0\u5347\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002\n\n#### RMSNorm\uff08Root Mean Square Normalization\uff09\n- **\u5b9a\u4e49**\uff1aRMSNorm\u662f\u5bf9\u6bcf\u4e2a\u6837\u672c\u7684\u6bcf\u4e2a\u7279\u5f81\u8fdb\u884c\u5f52\u4e00\u5316\uff0c\u4f46\u53ea\u5f52\u4e00\u5316\u65b9\u5dee\uff0c\u4e0d\u5f52\u4e00\u5316\u5747\u503c\u3002\n- **\u516c\u5f0f**\uff1a\n  \\[\n  \\text{RMSNorm}(x) = \\frac{x}{\\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} x_i^2 + \\epsilon}} \\cdot \\gamma + \\beta\n  \\]\n  \u5176\u4e2d\uff0c\\(n\\)\u662f\u7279\u5f81\u6570\u91cf\uff0c\\(\\gamma\\)\u548c\\(\\beta\\)\u662f\u53ef\u5b66\u4e60\u53c2\u6570\uff0c\\(\\epsilon\\)\u662f\u5fae\u5c0f\u5e38\u6570\u3002\n- **\u5e94\u7528**\uff1a\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0cRMSNorm\u6bd4LayerNorm\u66f4\u7b80\u5355\uff0c\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\uff0c\u5c24\u5176\u5728\u5927\u89c4\u6a21\u6a21\u578b\u8bad\u7ec3\u4e2d\u3002\n\n#### \u4e3b\u8981\u533a\u522b\n1. **\u5f52\u4e00\u5316\u65b9\u5f0f**\uff1a\n   - LayerNorm\u540c\u65f6\u5f52\u4e00\u5316\u5747\u503c\u548c\u65b9\u5dee\u3002\n   - RMSNorm\u53ea\u5f52\u4e00\u5316\u65b9\u5dee\u3002\n2. **\u8ba1\u7b97\u590d\u6742\u5ea6**\uff1a\n   - LayerNorm\u9700\u8981\u8ba1\u7b97\u5747\u503c\u548c\u65b9\u5dee\uff0c\u8ba1\u7b97\u91cf\u7a0d\u5927\u3002\n   - RMSNorm\u53ea\u9700\u8ba1\u7b97\u65b9\u5dee\uff0c\u8ba1\u7b97\u91cf\u8f83\u5c0f\u3002\n3. **\u6548\u679c**\uff1a\n   - LayerNorm\u5728\u8bb8\u591a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u5728NLP\u9886\u57df\u3002\n   - RMSNorm\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u8868\u73b0\u76f8\u4f3c\u751a\u81f3\u66f4\u4f18\uff0c\u4e14\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u3002\n\n\u901a\u8fc7\u4e0a\u8ff0\u5206\u6790\uff0c\u53ef\u4ee5\u66f4\u597d\u5730\u7406\u89e3LayerNorm\u548cRMSNorm\u5728\u5927\u6a21\u578b\u4e2d\u7684\u5e94\u7528\u53ca\u5176\u4f18\u7f3a\u70b9\uff0c\u4ece\u800c\u5728\u5177\u4f53\u4efb\u52a1\u4e2d\u9009\u62e9\u5408\u9002\u7684\u5f52\u4e00\u5316\u65b9\u6cd5\u3002", "task_step_name": "\u8bc4\u4f30\u5b66\u672f\u754c\u7684\u6280\u672f\u8fdb\u6b65\u4e0e\u5c40\u9650\u6027", "task_step_description": "\u6a21\u578b\u6027\u80fd\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\uff0c\u8ba1\u7b97\u6548\u7387\u4e5f\u6709\u6240\u63d0\u9ad8\uff0c\u5982EfficientNet\u5728\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u8868\u73b0\u3002\u4f9d\u7136\u5b58\u5728\u6a21\u578b\u504f\u5dee\uff08\u5982\u6027\u522b\u504f\u89c1\uff09\u3001\u6570\u636e\u4f9d\u8d56\uff08\u5982\u5bf9\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\uff09\u7b49\u95ee\u9898\uff0c\u6cdb\u5316\u80fd\u529b\u6709\u5f85\u8fdb\u4e00\u6b65\u63d0\u5347\u3002", "task_step_level": "2", "task_step_question": "### \u95ee\u9898\n\n\u5728\u8bc4\u4f30\u5b66\u672f\u754c\u7684\u6280\u672f\u8fdb\u6b65\u4e0e\u5c40\u9650\u6027\u65f6\uff0cLayerNorm\u548cRMSNorm\u4f5c\u4e3a\u5927\u6a21\u578b\u4e2d\u5e38\u7528\u7684\u5f52\u4e00\u5316\u65b9\u6cd5\uff0c\u5b83\u4eec\u5728\u63d0\u5347\u6a21\u578b\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u6709\u54ea\u4e9b\u5177\u4f53\u7684\u8d21\u732e\uff1f\u540c\u65f6\uff0c\u8fd9\u4e24\u79cd\u65b9\u6cd5\u5728\u89e3\u51b3\u6a21\u578b\u504f\u5dee\uff08\u5982\u6027\u522b\u504f\u89c1\uff09\u548c\u6570\u636e\u4f9d\u8d56\uff08\u5982\u5bf9\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\uff09\u7b49\u95ee\u9898\u4e0a\u662f\u5426\u5b58\u5728\u5c40\u9650\u6027\uff1f\u5982\u4f55\u8fdb\u4e00\u6b65\u4f18\u5316\u8fd9\u4e9b\u5f52\u4e00\u5316\u65b9\u6cd5\u4ee5\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff1f", "task_step_question_context": [{"ref_id": "455038427524247598", "chunk_id": "4", "score": 0.3125, "text": "# 5 I NTUITIONS BEHIND LAYER NORM TUNING\nIn this section, driven by the empirical success of LayerNorm tuning, we explore the intuitions behind LayerNorm from three perspectives, domain adaptation, expressive power, and gradient variance.  \n\nTable 3: Model performance on different data types. Methods with 80K and Conv.20K suffix are tuned on the full 80K data and the 20K conversational data, respectively.   \n\n\n<html><body><table><tr><td>Method</td><td>MME</td><td>VQAv2</td><td>MSCOCO</td><td>Flickr30k</td><td>POPE</td></tr><tr><td colspan=\"6\">MM-V1CUNA-7B</td></tr><tr><td>Finetune-80K</td><td>625.2/270.7</td><td>15.40</td><td>67.50</td><td>34.61</td><td>73.8/76.5/66.5</td></tr><tr><td>LayerNorm-80K</td><td>723.2/253.2</td><td>17.06</td><td>80.89</td><td>48.01</td><td>76.1/81.1/70.8</td></tr><tr><td>LayerNorm-Conv. 20K</td><td>777.1/231.4</td><td>15.39</td><td>67.30</td><td>40.33</td><td>75.2/79.2/68.8</td></tr><tr><td colspan=\"6\">MM-LLAMA2-7B</td></tr><tr><td>Finetune-80K</td><td>661.3/237.1</td><td>16.09</td><td>65.08</td><td>31.64</td><td>56.3/65.0/55.4</td></tr><tr><td>LayerNorm-80K</td><td>583.2/200.7</td><td>16.78</td><td>88.85</td><td>49.24</td><td>66.6/68.5/64.9</td></tr><tr><td>LayerNorm-Conv. 20K</td><td>376.2/157.5</td><td>16.19</td><td>86.80</td><td>44.88</td><td>50.5/50.7/50.3</td></tr><tr><td colspan=\"6\">MM-LLAMA2-CHAT-7B</td></tr><tr><td>Finetune-80K</td><td>805.4/234.6</td><td>15.29</td><td>57.40</td><td>26.70</td><td>60.3/69.8/57.9</td></tr><tr><td>LayerNorm-80K</td><td>651.3/219.3</td><td>16.60</td><td>75.34</td><td>43.75</td><td>71.3/72.4/67.8</td></tr><tr><td>LayerNorm-Conv. 20K</td><td>482.9/172.1</td><td>13.88</td><td>66.85</td><td>41.95</td><td>62.7/71.7/61.3</td></tr><tr><td colspan=\"6\">MM-LLAMA2-13B</td></tr><tr><td>Finetune-80K</td><td>402.3/199.3</td><td>18.33</td><td>73.88</td><td>45.33</td><td>51.6/51.1/52.2</td></tr><tr><td>LayerNorm-80K</td><td>526.0/177.5</td><td>15.31</td><td>82.92</td><td>48.42</td><td>60.0/69.1/58.9</td></tr><tr><td>LayerNorm-Conv. 20K</td><td>646.0/242.9</td><td>16.01</td><td>76.50</td><td>44.86</td><td>70.0/76.9/68.6</td></tr><tr><td colspan=\"6\">MM-LLAMA2-CHAT-13B</td></tr><tr><td>Finetune-80K</td><td>623.3/221.4</td><td>15.17</td><td>64.19</td><td>41.82</td><td>67.6/64.8/64.5</td></tr><tr><td>LayerNorm-80K</td><td>929.3/254.3</td><td>16.10</td><td>74.96</td><td>42.79</td><td>78.9/83.9/74.3</td></tr><tr><td>LayerNorm-Conv. 20K</td><td>769.7/227.5</td><td>15.57</td><td>73.30</td><td>43.08</td><td>68.2/72.8/65.3</td></tr></table></body></html>  \n\nTable 4: Results of models with LayerNorm and/or vision-language Connector activated.   \n\n\n<html><body><table><tr><td>Method</td><td>MME</td><td>VQAv2</td><td>MSCOCO</td><td>Flickr30k</td><td>POPE</td></tr><tr><td colspan=\"6\">MM-LLAMA2-7B</td></tr><tr><td>LayerNorm + Connector</td><td>583.2/200.7</td><td>16.78</td><td>88.85</td><td>49.24</td><td>66.6/68.5/64.9</td></tr><tr><td>Connector</td><td>311.1/105.4</td><td>12.72</td><td>60.43</td><td>35.91</td><td>67.9/73.7/66.9</td></tr><tr><td>LayerNorm</td><td>395.0/191.4</td><td>18.18</td><td>80.13</td><td>41.68</td><td>50.3/51.3/50.2</td></tr><tr><td colspan=\"6\">MM-LLAMA2-13B</td></tr><tr><td>LayerNorm + Connector</td><td>526.0/177.5</td><td>15.31</td><td>82.92</td><td>48.42</td><td>60.0/69.1/58.9</td></tr><tr><td>Connector</td><td>507.0/187.9</td><td>15.22</td><td>62.60</td><td>25.13</td><td>60.9/66.8/60.1</td></tr><tr><td>LayerNorm</td><td>405.0/188.6</td><td>16.51</td><td>70.41</td><td>39.86</td><td>50.9/52.7/51.0</td></tr></table></body></html>\n\n# 5.1 LAYER NORM TUNING A DAPTS LLM S TO MULTI -M ODAL\nInfluence of the Vision-Language Connector The vision-language connector serves as the converter to project features from the vision encoder to the LLM domain. In our previous experiments, we focused on finetuning the LLM component of the MLLMs while keeping the vision-language connector activated by default. To determine which component plays a more important role for domain adaptation of LLM to multi-modal domain, we performed an ablation study by activating the two components separately. Results are presented in table 4 , tuning LayerNorm in attention blocks without activating the vision-language connector resulted in only a $4.2\\%$ and $5.4\\%$ decrease in performance on three traditional multi-modal tasks and the MME benchmark, respectively. This decrease is significantly lower than the $15.6\\%$ and $9.2\\%$ downgrade observed when only activating the Connector on the same tasks. This observation highlights the vital role LayerNorm plays in transforming knowledge from the vision domain to language, indicating LayerNorm as a strong domain adaptor for the LLM architecture.  \n\n  \n\nFigure 3: Layer similarities between different LLM layers in (a) Finetuned and (b) LayerNorm-tuned MM-V ICUNA -7B. The average layer similarity of two models are 0.624 and 0.585, respectively.  \n\nTable 5: Results of models with LL A MA2 Finetuned/LayerNorm-tuned with ViT pre-trained on ImageNet (Deng et al. ,2009 ), which have not been aligned with the language domain.   \n\n\n<html><body><table><tr><td></td><td>MME</td><td>VQAv2</td><td>MSCOCO</td><td>Flickr30k</td><td>POPE</td></tr><tr><td>Finetune-7B</td><td>406.79/182.5</td><td>15.05</td><td>47.75</td><td>18.97</td><td>50.0/51.6/50.1</td></tr><tr><td>LayerNorm-7B</td><td>301.51/127.14</td><td>15.48</td><td>66.22</td><td>31.73</td><td>50.0/50.1/50.1</td></tr><tr><td>Finetune-13B</td><td>375.41/171.79</td><td>25.38</td><td>51.26</td><td>25.96</td><td>50.3/51.1/51.0</td></tr><tr><td>LayerNorm-13B</td><td>445.98/150.0</td><td>15.59</td><td>64.63</td><td>32.17</td><td>51.2/53.0/50.8</td></tr></table></body></html>  \n\nSwitching Visual Features. We employ the ViT encoder from CLIP ( Radford et al. ,2021 ) by default in our previous experiments. CLIP ( Radford et al. ,2021 ) models are trained with image-text contrastive loss, thus its feature space is already aligned with language. Since LayerNorm has shown its effectiveness as a domain adaptor, we are interested in testing whether or not LayerNorm tuning can adapt a LLM to image features that are not pretrained to align with language. The vision encoder is switched to a ViT model that was pretrained on ImageNet (Dosovitskiy et al. ,2021 ;Deng et al. ,2009 ). Results in table 5 demonstrate that both LayerNorm and finetuning approaches can yield high performance. Interestingly, we observe that by LayerNorm tuning with ImageNet trained ViT, which has not been aligned with language, the model is able to achieve comparable performance to full parameter finetuning , i.e ., results show that LayerNorm tuning outperforms finetuning by $12.0\\%$ on captioning tasks, but performs slightly worse by $5.0\\%$ on the MME benchmark. These results again indicates the domain adaptor role of the LayerNorm , hinting the reason behind the empircal success of LayerNorm tuning. Furthermore, it is worth noting that the performance of MLLMs incorporating ViT pretrained on ImageNet is generally inferior to that of CLIP\u2019s vision encoder. This observation provides compelling evidence that, despite differences in tokenizer and training paradigm between CLIP\u2019s text encoder and LL A MA\u2019s, ViT from CLIP has the capacity to learn general patterns of language formulation during pre-training. Thus, significantly enhance MLLM abilities.\n\n# 5.2 LAYER NORM TUNING I MPROVES THE EXPRESSIVE POWER\nIt is shown in Pires et al. (2023 ) that a Transformer model incorporating anisotropic layer representation can capture a wider range of learning patterns. By computing the cosine similarities between all layers in the LLM of a finetuned MLLM, we aim to investigate whether the improved efficiency is the results of the improved expressive power. In table 6 , we present the average layer similarity of three 7B scale MLLMs, and in fig. 3 we present the visualization of per layer similarity scores of MM-V ICUNA -7B. Our analysis reveals that the transformer layers in the MLLMs with LayerNorm tuning exhibit a clear distinction from one another ( i.e ., an average $10.6\\%$ lower layer similarities comparing finetuning), indicating superior generalization ability and expressive power compared to finetuning. This finding sheds light on why tuning LayerNorm is effective for multi-modal LLM training. For additional visualizations, please refer to the Appendix A.2.1 .  \n\n  \nFigure 4: Gradients of the input LayerNorm in the 11th layer of the MM-V ICUNA as training proceeds. LayerNorm-tuned model has lower gradient variance than full parameter finetuning.  \n\nTable 6: Layer representation similarity of LayerNorm and finetuning methods on three 7B MLLMs.   \nLower the similarity is, the better expressive power a model possesses.  \n\n<html><body><table><tr><td>Model</td><td>LayerNorm Sim.</td><td>Finetuning Sim.</td></tr><tr><td>MM-VICUNA</td><td>0.585</td><td>0.624</td></tr><tr><td>MM-LLAMA2</td><td>0.504</td><td>0.591</td></tr><tr><td>MM-LLAMA2-CHAT</td><td>0.550</td><td>0.617</td></tr></table></body></html>"}, {"ref_id": "454984283955145766", "chunk_id": "8", "score": 0.2265625, "text": "# 3 Experiments and Results\nWe evaluate the performance of PreNorm and PostNorm for ZST on various datasets and language pairs. We then analyze the off-target rates and structural discrepancies between PreNorm and PostNorm to understand performance differences.  \n\n$$\n\\mathrm{LayerNorm}(\\mathbf{x})=\\frac{\\mathbf{x}-\\mathbf{E}(\\mathbf{x})}{\\sqrt{\\mathbf{V}(\\mathbf{x})}}\\cdot\\mathbf{g}+\\mathbf{b},\n$$  \n\nwhere $\\mathbf{g}$ and $\\mathbf{b}$ are trainable gain and bias. $\\mathbf{E}$ and $\\mathbf{V}$ indicate expectation and variance. LayerNorm is commonly used in two positions in the Transformer, as shown in Fig. 1 . PostNorm, which is the originally proposed setting of the Transformer ( Vaswani et al. ,2017 ), involves applying LayerNorm after each sub-module (i.e., selfattention or feed-forward network) and residual connections. PreNorm ( Baevski and Auli ,2019 ), on the other hand, involves applying LayerNorm directly before each sub-module and is known to stabilize Transformer training. While variants of Transformer LayerNorm like RMSNorm ( Zhang and Sennrich ,2019 ) have been proposed, the vanilla PreNorm and PostNorm are still the most widely adopted settings in current multilingual\n\n# 3.1 Experimental Settings\nDatasets We perform ZST experiments on three datasets: OPUS ( Zhang et al. ,2020 ), IWSLT ( Cettolo et al. ,2017 ), and Europarl ( Koehn ,2005 ). The statistics of the datasets are summarized in Table 1 .We include 7 ,4 , and 5 languages for each dataset. The training data consists of only English-centric sentence pairs, resulting in 30 ,6 , and 12 ZST directions for each dataset. The total number of parallel sentences for each dataset is 12 .00 M, 1 .38 M, and 15 .78 M, respectively. We apply BPE ( Sennrich et al. ,2016 ) with merge operations of 50 k, 40 k, and $50\\mathbf{k}$ to create a joint vocabulary for each dataset.  \n\nTraining We employ Transformer-base model for OPUS and IWSLT, and Transformer-big for Europarl, in accordance with the distinct sizes of training data. We consider the following settings: (1) PreNorm or PostNorm : PreNorm involves LayerNorm directly before each sub-module (i.e., self-attention or feed-forward network), while PostNorm applies LayerNorm after each sub-module and residual connections, as shown in Fig. 1 .(2) S-ENC-T-DEC or T-ENC : Source language tag on the encoder-side and target language tag on the decoder-side; or only target language tag on the encoder-side. Wu et al. (2021 ) showed that this setting impacts ZST for Transformer with PreNorm. (3) w/ or w/o Res. : With the residual connection for self-attention in the middle $(4^{t h})$ encoder layer or not. Liu et al. (2021 ) revealed that \u201cw/o Res.\u201d improves ZST for the model trained with PreNorm. We experiment this with different LayerNorm settings as this may reduce the potential of overfitting on supervised directions, then further impacts ZST, which aligns with our hypothesis.  \n\nTable 2: BLEU scores and off-target rates (shown in brackets) . We report the average score of three seeds; refer to Appendix Gfor BLEU score of each translation direction and seed. \u201cRes.\u201d indicates the residual connection of self-attention in the $4^{t h}$ encoder layer. We mark lower off-target rates and significantly higher BLEU scores ( Koehn ,2004 ) between PreNorm and PostNorm in bold for ZST.   \n\n\n<html><body><table><tr><td>#</td><td>Layer Norm</td><td>Language Tag</td><td>Res.</td><td></td><td>Zero-shot</td><td></td><td></td><td>Supervised</td><td></td></tr><tr><td>0</td><td></td><td>Pivot</td><td></td><td>OPUS 21.8</td><td>IWSLT 20.0</td><td>Europarl 29.5</td><td>OPUS</td><td>IWSLT</td><td>Europarl</td></tr><tr><td>1</td><td>PreNorm</td><td>S-ENC-T-DEC</td><td>w/</td><td>10.1 (42.19%)</td><td>4.9 (64.84%)</td><td>24.9 ( 7.73%)</td><td>33.7</td><td>31.5</td><td>34.3</td></tr><tr><td>2</td><td>PostNorm</td><td>S-ENC-T-DEC</td><td>w/</td><td>16.8 ( 8.59%)</td><td>12.4 (10.61%)</td><td>29.2( 0.34%)</td><td>33.9</td><td>31.5</td><td>34.5</td></tr><tr><td>3</td><td>PreNorm</td><td>T-ENC</td><td>w/</td><td>13.3 (22.99%)</td><td>13.7 ( 3.98%)</td><td>29.5( 0.23%)</td><td>33.7</td><td>31.6</td><td>34.4</td></tr><tr><td>4</td><td>PostNorm</td><td>T-ENC</td><td>w/</td><td>14.0 (22.86%)</td><td>15.5 ( 4.59%)</td><td>30.8 ( 0.11%)</td><td>34.1</td><td>31.5</td><td>34.5</td></tr><tr><td>5</td><td>PreNorm</td><td>S-ENC-T-DEC</td><td>w/o</td><td>14.3 (20.67%)</td><td>8.0 (50.16%)</td><td>16.7 (41.87%)</td><td>33.6</td><td>30.9</td><td>34.3</td></tr><tr><td>6</td><td>PostNorm</td><td>S-ENC-T-DEC</td><td>w/o</td><td>16.0 (15.27%)</td><td>17.4 (1.83%)</td><td>29.0 ( 0.41%)</td><td>33.8</td><td>30.7</td><td>34.4</td></tr><tr><td>7</td><td>PreNorm</td><td>T-ENC</td><td>w/o</td><td>13.4 (27.15%)</td><td>16.2 ( 1.54%)</td><td>29.9 ( 2.15%)</td><td>33.5</td><td>30.9</td><td>34.3</td></tr><tr><td>8</td><td>PostNorm</td><td>T-ENC</td><td>w/o</td><td>13.9 (26.68%)</td><td>17.8 (1.50%)</td><td>30.8 ( 0.13%)</td><td>33.9</td><td>30.6</td><td>34.4</td></tr></table></body></html>  \n\nThe settings above lead to eight different combinations, shown in Table 2 (#1 - #8). Additional training details are in Appendix A .\n\n# 3.2 Main Results\nWe evaluate ZST systems using SacreBLEU ( Post ,2018 ) and off-target rates. We report in Table 2 BLEU scores for both zero-shot and supervised directions. For ZST, we also present pivot-based translation results as a reference. Implementation details of evaluation can be found in Appendix B.Our findings are as follows:  \n\nPreNorm vs. PostNorm :We find that PostNorm consistently yields better BLEU scores than PreNorm for ZST across various language tag and residual connection settings, while their performance is comparable for supervised directions.  \n\nImpact of Language Tag and Residual Connection: We observe that using the \u201cT-ENC\u201d language tag and \u201cw/ Res.\u201d improves ZST performance for IWSLT, which aligns with the findings of $\\mathrm{Wu}$ et al. (2021 ) and Liu et al. (2021 ). Nevertheless, the best performance is achieved using \u201cw/ Res.\u201d for PostNorm with \u201cS-ENC-T-DEC\u201d and \u201cT-ENC\u201d tags for OPUS and Europarl, respectively (#2 and #4). Given that Wu et al. (2021 ) and Liu et al. (2021 )used PreNorm as the default setting (#2, #4, #6 and #8 are unreported results in their work), our results emphasize the need to consider PostNorm as the default setting for ZST, while the language tag and residual connection settings have less impact.  \n\nOff-target Rates : Off-target rates help understand the different BLEU score gaps between PreNorm and PostNorm, which ranges from 0 .5 to 12 .3 BLEU points. For PreNorm and PostNorm with the \u201cT-ENC\u201d language tag (#3, #4, #7, and #8), they have similar off-target rates, with a discrepancy ranging from $-0.61\\%$ to $2.02\\%$ , which results in narrow BLEU score gaps, ranging from 0 .5 to 1 .8 points. However, for PreNorm and PostNorm with the \u201cS-ENC-T-DEC\u201d language tag (#1, #2, #5, and #6), the off-target rates show a more considerable discrepancy, ranging from $5.40\\%$ to $54.23\\%$ , resulting in BLEU score gaps from 1 .7 to 12 .3 points. Further analysis of the nature of Transformer hidden states in the next section explores the reason for these different off-target rates in translations."}, {"ref_id": "454846009781566216", "chunk_id": "0", "score": 0.1904296875, "text": "# I EMPIRICAL RESULTS REGARDING THEOREM 1\nNote that Theorem 1 assumes that feature vectors $n$ are normally-distributed, which may not necessarily occur in practice (although given that observables and observable-derived feature vectors are only introduced in this work, it is hard to say whether this is false more generally; more research is necessary, including research on the sorts of observables that practitioners wish to analyze in practice). However, the intention of Theorem 1 is to provide motivation that underpins what we found empirically: namely, that feature vectors computer by taking LayerNorm into account have extremely high cosine similarities with feature vectors computed without taking LayerNorm into account.  \n\nTable 6: Cosine similarities between the feature vectors used in Section 4.3, computed with and without LayerNorms   \n\n\n<html><body><table><tr><td>Task</td><td>Cosines similarity</td><td>Angle (radians)</td></tr><tr><td>Subject pronoun prediction (attention 6::6)</td><td>0.99779</td><td>0.0664</td></tr><tr><td>C VS.H Python</td><td>0.99936</td><td>0.0358</td></tr><tr><td>Political party p prediction</td><td>0.99900</td><td>0.0447</td></tr></table></body></html>  \n\nIn particular, for the feature vectors considered in Section 4.3, these cosine similarities and angles are given in Table 6. For reference, note that the upper bound on the angle between these feature vectors according to Theorem 1 is approximately 0.0442 radians. The feature vectors for subject pronoun prediction have a higher angle between them of 0.0664 radians, but this can be attributed to the fact that the circuit for these feature vectors goes through multiple LayerNorms. Additionally, the angle for the political party prediction feature vector is also slightly higher than the bound predicted by the theorem; but it is worth noting that the theorem predicts a bound on the expected angle, rather than a bound on the maximum angle; this also might be due to the scaling matrix $W$ in the LayerNorm (see Appendix E)."}], "task_step_question_answer": "\u5728\u533b\u7597\u9886\u57df\uff0c\u6a21\u578b\u504f\u5dee\u7684\u8868\u73b0\u540c\u6837\u503c\u5f97\u5173\u6ce8\u3002\u4f8b\u5982\uff0c\u67d0\u4e9b\u533b\u7597\u8bca\u65ad\u6a21\u578b\u5728\u5904\u7406\u4e0d\u540c\u79cd\u65cf\u60a3\u8005\u7684\u6570\u636e\u65f6\uff0c\u8868\u73b0\u51fa\u663e\u8457\u7684\u8bca\u65ad\u504f\u5dee\u3002\u7814\u7a76\u8868\u660e\uff0c\u8fd9\u4e9b\u6a21\u578b\u5728\u8bca\u65ad\u975e\u6d32\u88d4\u60a3\u8005\u65f6\uff0c\u8bef\u8bca\u7387\u663e\u8457\u9ad8\u4e8e\u5176\u4ed6\u79cd\u65cf\u3002\u8fd9\u79cd\u504f\u5dee\u4e0d\u4ec5\u5f71\u54cd\u4e86\u8bca\u65ad\u7684\u51c6\u786e\u6027\uff0c\u8fd8\u53ef\u80fd\u5bfc\u81f4\u533b\u7597\u8d44\u6e90\u7684\u4e0d\u516c\u5e73\u5206\u914d\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u7814\u7a76\u8005\u63d0\u51fa\u4e86\u57fa\u4e8e\u516c\u5e73\u6027\u7ea6\u675f\u7684\u6a21\u578b\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u8c03\u6574\u635f\u5931\u51fd\u6570\uff0c\u51cf\u5c11\u6a21\u578b\u5bf9\u4e0d\u540c\u79cd\u65cf\u60a3\u8005\u7684\u8bca\u65ad\u504f\u5dee\u3002\n\n\u5728\u91d1\u878d\u9886\u57df\uff0c\u6a21\u578b\u504f\u5dee\u7684\u8868\u73b0\u4e5f\u5f15\u8d77\u4e86\u5e7f\u6cdb\u5173\u6ce8\u3002\u4f8b\u5982\uff0c\u67d0\u4e9b\u4fe1\u7528\u8bc4\u5206\u6a21\u578b\u5728\u5904\u7406\u4e0d\u540c\u5e74\u9f84\u6bb5\u7684\u7528\u6237\u6570\u636e\u65f6\uff0c\u8868\u73b0\u51fa\u663e\u8457\u7684\u8bc4\u5206\u504f\u5dee\u3002\u7814\u7a76\u8868\u660e\uff0c\u8fd9\u4e9b\u6a21\u578b\u5728\u5904\u7406\u5e74\u8f7b\u7528\u6237\u7684\u4fe1\u7528\u8bc4\u5206\u65f6\uff0c\u503e\u5411\u4e8e\u7ed9\u51fa\u8f83\u4f4e\u7684\u8bc4\u5206\uff0c\u5bfc\u81f4\u5e74\u8f7b\u7528\u6237\u5728\u7533\u8bf7\u8d37\u6b3e\u65f6\u9762\u4e34\u66f4\u9ad8\u7684\u62d2\u7edd\u7387\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u7814\u7a76\u8005\u63d0\u51fa\u4e86\u57fa\u4e8e\u5bf9\u6297\u8bad\u7ec3\u7684\u6a21\u578b\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5f15\u5165\u5bf9\u6297\u6837\u672c\uff0c\u8feb\u4f7f\u6a21\u578b\u5b66\u4e60\u5230\u66f4\u516c\u5e73\u7684\u4fe1\u7528\u8bc4\u5206\u7b56\u7565\u3002\n\n\u5728\u65e0\u76d1\u7763\u548c\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u9762\uff0c\u5177\u4f53\u7684\u7814\u7a76\u6848\u4f8b\u5c55\u793a\u4e86\u8fd9\u4e9b\u65b9\u6cd5\u5728\u51cf\u5c11\u6570\u636e\u4f9d\u8d56\u65b9\u9762\u7684\u6210\u529f\u5e94\u7528\u3002\u4f8b\u5982\uff0c\u5728\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0c\u7814\u7a76\u8005\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\uff0c\u5728\u672a\u6807\u6ce8\u7684ImageNet\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u73af\u5883\u4e0b\u7684\u5206\u7c7b\u6027\u80fd\u3002\u6b64\u5916\uff0c\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u4e2d\uff0c\u8de8\u6a21\u6001\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u901a\u8fc7\u5229\u7528\u6587\u672c\u548c\u56fe\u50cf\u4e4b\u95f4\u7684\u5173\u8054\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u73af\u5883\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u3002\n\n\u52a8\u6001\u5f52\u4e00\u5316\u548c\u81ea\u9002\u5e94\u5f52\u4e00\u5316\u7b56\u7565\u5728\u5b9e\u9645\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e5f\u53d6\u5f97\u4e86\u663e\u8457\u6548\u679c\u3002\u4f8b\u5982\uff0c\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u4e2d\uff0c\u52a8\u6001\u5f52\u4e00\u5316\u901a\u8fc7\u6839\u636e\u8f93\u5165\u6587\u672c\u7684\u5206\u5e03\u52a8\u6001\u8c03\u6574\u5f52\u4e00\u5316\u53c2\u6570\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u6587\u672c\u5206\u7c7b\u548c\u60c5\u611f\u5206\u6790\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002\u5728\u56fe\u50cf\u5904\u7406\u4efb\u52a1\u4e2d\uff0c\u81ea\u9002\u5e94\u5f52\u4e00\u5316\u901a\u8fc7\u5f15\u5165\u53ef\u5b66\u4e60\u7684\u53c2\u6570\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u6839\u636e\u4efb\u52a1\u9700\u6c42\u81ea\u52a8\u8c03\u6574\u5f52\u4e00\u5316\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u56fe\u50cf\u5206\u7c7b\u548c\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002\n\n\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u53ef\u4ee5\u8fdb\u4e00\u6b65\u6269\u5c55\u5230\u8054\u90a6\u5b66\u4e60\u548c\u9690\u79c1\u4fdd\u62a4\u7b49\u9886\u57df\u3002\u4f8b\u5982\uff0c\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\uff0c\u7814\u7a76\u8005\u53ef\u4ee5\u901a\u8fc7\u5f15\u5165\u52a8\u6001\u5f52\u4e00\u5316\u548c\u81ea\u9002\u5e94\u5f52\u4e00\u5316\u7b56\u7565\uff0c\u63d0\u5347\u6a21\u578b\u5728\u5206\u5e03\u5f0f\u8bad\u7ec3\u4e2d\u7684\u6027\u80fd\u3002\u5728\u9690\u79c1\u4fdd\u62a4\u65b9\u9762\uff0c\u7814\u7a76\u8005\u53ef\u4ee5\u901a\u8fc7\u7ed3\u5408\u65e0\u76d1\u7763\u5b66\u4e60\u548c\u81ea\u9002\u5e94\u5f52\u4e00\u5316\u7b56\u7565\uff0c\u63d0\u5347\u6a21\u578b\u5728\u9690\u79c1\u654f\u611f\u4efb\u52a1\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002\u8fd9\u4e9b\u65b0\u5174\u7814\u7a76\u65b9\u5411\u4e0d\u4ec5\u80fd\u591f\u63d0\u5347\u6a21\u578b\u7684\u6027\u80fd\uff0c\u8fd8\u80fd\u591f\u89e3\u51b3\u6a21\u578b\u504f\u5dee\u548c\u6570\u636e\u4f9d\u8d56\u95ee\u9898\uff0c\u4e3a\u5b66\u672f\u754c\u63d0\u4f9b\u65b0\u7684\u7814\u7a76\u601d\u8def\u3002", "ref_task_step_id": ""}, "__type__": "task_step_node"}, "51c8b9ac-a041-4af7-b164-9d6e7eb750af": {"__data__": {"id_": "51c8b9ac-a041-4af7-b164-9d6e7eb750af", "metadata": {}, "relationships": {}, "hash": "", "start_task_context": "\u5927\u6a21\u578b\u4e2d\u7684LayerNorm\u548cRMSNorm\u6709\u4ec0\u4e48\u533a\u522b\uff1f", "aemo_representation_context": "### Step-by-Step Decomposition of Computer Science Research Content\n\n#### 1. \u5206\u6790\u8fd1\u51e0\u5e74\u7814\u7a76\u9886\u57df\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba\n- **\u6280\u672f\u6846\u67b6**\uff1a\u8fd1\u5e74\u6765\uff0c\u6df1\u5ea6\u5b66\u4e60\u3001\u5f3a\u5316\u5b66\u4e60\u3001\u56fe\u795e\u7ecf\u7f51\u7edc\u7b49\u6846\u67b6\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u9886\u57df\u5360\u636e\u4e3b\u5bfc\u5730\u4f4d\u3002\u5177\u4f53\u6846\u67b6\u5982Transformer\u3001BERT\u3001GNN\u7b49\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002\n- **\u65b9\u6cd5\u8bba**\uff1a\u7814\u7a76\u65b9\u6cd5\u9010\u6e10\u4ece\u5355\u4e00\u6a21\u578b\u8f6c\u5411\u591a\u6a21\u578b\u878d\u5408\uff0c\u4ece\u76d1\u7763\u5b66\u4e60\u6269\u5c55\u5230\u65e0\u76d1\u7763\u548c\u81ea\u76d1\u7763\u5b66\u4e60\uff0c\u5f3a\u8c03\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002\n\n#### 2. \u7814\u7a76\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u4e3b\u8981\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0e\u53d8\u4f53\n- **\u5e94\u7528**\uff1a\u4f8b\u5982\uff0cTransformer\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u4e2d\u88ab\u5e7f\u6cdb\u5e94\u7528\uff0cBERT\u53d8\u4f53\u5982RoBERTa\u3001ALBERT\u5728\u6587\u672c\u5206\u7c7b\u3001\u95ee\u7b54\u7cfb\u7edf\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff1bGNN\u5728\u63a8\u8350\u7cfb\u7edf\u3001\u77e5\u8bc6\u56fe\u8c31\u7b49\u9886\u57df\u6709\u663e\u8457\u5e94\u7528\u3002\n- **\u53d8\u4f53**\uff1a\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\uff0c\u7814\u7a76\u8005\u63d0\u51fa\u4e86\u591a\u79cd\u53d8\u4f53\uff0c\u5982\u9488\u5bf9\u957f\u5e8f\u5217\u5904\u7406\u7684Transformer-XL\uff0c\u9488\u5bf9\u591a\u6a21\u6001\u6570\u636e\u7684ViLBERT\u7b49\u3002\n\n#### 3. \u8bc4\u4f30\u5b66\u672f\u754c\u7684\u6280\u672f\u8fdb\u6b65\u4e0e\u5c40\u9650\u6027\n- **\u6280\u672f\u8fdb\u6b65**\uff1a\u6a21\u578b\u6027\u80fd\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\uff0c\u8ba1\u7b97\u6548\u7387\u4e5f\u6709\u6240\u63d0\u9ad8\uff0c\u5982EfficientNet\u5728\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u8868\u73b0\u3002\n- **\u5c40\u9650\u6027**\uff1a\u4f9d\u7136\u5b58\u5728\u6a21\u578b\u504f\u5dee\uff08\u5982\u6027\u522b\u504f\u89c1\uff09\u3001\u6570\u636e\u4f9d\u8d56\uff08\u5982\u5bf9\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\uff09\u7b49\u95ee\u9898\uff0c\u6cdb\u5316\u80fd\u529b\u6709\u5f85\u8fdb\u4e00\u6b65\u63d0\u5347\u3002\n\n#### 4. \u63a2\u8ba8\u8ba1\u7b97\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\n- **\u9002\u7528\u6027**\uff1a\u7814\u7a76\u6a21\u578b\u5728\u4e0d\u540c\u9886\u57df\uff08\u5982\u533b\u7597\u3001\u91d1\u878d\uff09\u548c\u591a\u6a21\u6001\u6570\u636e\uff08\u5982\u6587\u672c+\u56fe\u50cf\uff09\u4e0a\u7684\u8868\u73b0\uff0c\u5982\u591a\u6a21\u6001Transformer\u6a21\u578b\u3002\n- **\u6cdb\u5316\u80fd\u529b**\uff1a\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\u3001\u5143\u5b66\u4e60\u7b49\u65b9\u6cd5\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f7f\u5176\u80fd\u591f\u5e94\u5bf9\u66f4\u591a\u73b0\u5b9e\u5e94\u7528\u573a\u666f\u3002\n\n#### 5. \u5206\u6790\u6700\u65b0\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027\n- **\u7a33\u5b9a\u6027**\uff1a\u7814\u7a76\u7b97\u6cd5\u5728\u590d\u6742\u3001\u52a8\u6001\u73af\u5883\u4e0b\u7684\u8868\u73b0\uff0c\u5982\u5bf9\u6297\u8bad\u7ec3\u3001\u9c81\u68d2\u4f18\u5316\u7b49\u65b9\u6cd5\u7684\u5e94\u7528\u3002\n- **\u5bb9\u9519\u6027**\uff1a\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\u7684\u9002\u5e94\u6027\uff0c\u5982\u5206\u5e03\u5f0f\u8bad\u7ec3\u4e2d\u7684\u68af\u5ea6\u538b\u7f29\u3001\u6a21\u578b\u5e76\u884c\u7b49\u6280\u672f\u3002\n\n#### 6. \u8bc4\u4f30\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218\n- **\u672a\u6765\u65b9\u5411**\uff1a\u5982\u53ef\u89e3\u91caAI\u3001\u8054\u90a6\u5b66\u4e60\u3001\u9690\u79c1\u4fdd\u62a4\u7b49\u65b0\u5174\u7814\u7a76\u65b9\u5411\u3002\n- **\u6311\u6218**\uff1a\u5982\u4f55\u5e73\u8861\u6a21\u578b\u6027\u80fd\u4e0e\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\uff0c\u5982\u4f55\u89e3\u51b3\u6570\u636e\u504f\u89c1\u548c\u4f26\u7406\u95ee\u9898\u7b49\u3002\n\n### \u5927\u6a21\u578b\u4e2d\u7684LayerNorm\u548cRMSNorm\u7684\u533a\u522b\n\n#### LayerNorm\uff08Layer Normalization\uff09\n- **\u5b9a\u4e49**\uff1aLayerNorm\u662f\u5bf9\u6bcf\u4e2a\u6837\u672c\u7684\u6bcf\u4e2a\u7279\u5f81\u8fdb\u884c\u5f52\u4e00\u5316\uff0c\u4f7f\u5176\u5747\u503c\u548c\u65b9\u5dee\u5206\u522b\u63a5\u8fd10\u548c1\u3002\n- **\u516c\u5f0f**\uff1a\n  \\[\n  \\text{LayerNorm}(x) = \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} \\cdot \\gamma + \\beta\n  \\]\n  \u5176\u4e2d\uff0c\\(\\mu\\)\u548c\\(\\sigma^2\\)\u5206\u522b\u662f\u7279\u5f81\u5747\u503c\u548c\u65b9\u5dee\uff0c\\(\\gamma\\)\u548c\\(\\beta\\)\u662f\u53ef\u5b66\u4e60\u53c2\u6570\uff0c\\(\\epsilon\\)\u662f\u5fae\u5c0f\u5e38\u6570\u4ee5\u9632\u6b62\u9664\u96f6\u3002\n- **\u5e94\u7528**\uff1a\u5e7f\u6cdb\u7528\u4e8eTransformer\u6a21\u578b\u4e2d\uff0c\u6709\u52a9\u4e8e\u7f13\u89e3\u68af\u5ea6\u6d88\u5931\u95ee\u9898\uff0c\u63d0\u5347\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002\n\n#### RMSNorm\uff08Root Mean Square Normalization\uff09\n- **\u5b9a\u4e49**\uff1aRMSNorm\u662f\u5bf9\u6bcf\u4e2a\u6837\u672c\u7684\u6bcf\u4e2a\u7279\u5f81\u8fdb\u884c\u5f52\u4e00\u5316\uff0c\u4f46\u53ea\u5f52\u4e00\u5316\u65b9\u5dee\uff0c\u4e0d\u5f52\u4e00\u5316\u5747\u503c\u3002\n- **\u516c\u5f0f**\uff1a\n  \\[\n  \\text{RMSNorm}(x) = \\frac{x}{\\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} x_i^2 + \\epsilon}} \\cdot \\gamma + \\beta\n  \\]\n  \u5176\u4e2d\uff0c\\(n\\)\u662f\u7279\u5f81\u6570\u91cf\uff0c\\(\\gamma\\)\u548c\\(\\beta\\)\u662f\u53ef\u5b66\u4e60\u53c2\u6570\uff0c\\(\\epsilon\\)\u662f\u5fae\u5c0f\u5e38\u6570\u3002\n- **\u5e94\u7528**\uff1a\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0cRMSNorm\u6bd4LayerNorm\u66f4\u7b80\u5355\uff0c\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\uff0c\u5c24\u5176\u5728\u5927\u89c4\u6a21\u6a21\u578b\u8bad\u7ec3\u4e2d\u3002\n\n#### \u4e3b\u8981\u533a\u522b\n1. **\u5f52\u4e00\u5316\u65b9\u5f0f**\uff1a\n   - LayerNorm\u540c\u65f6\u5f52\u4e00\u5316\u5747\u503c\u548c\u65b9\u5dee\u3002\n   - RMSNorm\u53ea\u5f52\u4e00\u5316\u65b9\u5dee\u3002\n2. **\u8ba1\u7b97\u590d\u6742\u5ea6**\uff1a\n   - LayerNorm\u9700\u8981\u8ba1\u7b97\u5747\u503c\u548c\u65b9\u5dee\uff0c\u8ba1\u7b97\u91cf\u7a0d\u5927\u3002\n   - RMSNorm\u53ea\u9700\u8ba1\u7b97\u65b9\u5dee\uff0c\u8ba1\u7b97\u91cf\u8f83\u5c0f\u3002\n3. **\u6548\u679c**\uff1a\n   - LayerNorm\u5728\u8bb8\u591a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u5728NLP\u9886\u57df\u3002\n   - RMSNorm\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u8868\u73b0\u76f8\u4f3c\u751a\u81f3\u66f4\u4f18\uff0c\u4e14\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u3002\n\n\u901a\u8fc7\u4e0a\u8ff0\u5206\u6790\uff0c\u53ef\u4ee5\u66f4\u597d\u5730\u7406\u89e3LayerNorm\u548cRMSNorm\u5728\u5927\u6a21\u578b\u4e2d\u7684\u5e94\u7528\u53ca\u5176\u4f18\u7f3a\u70b9\uff0c\u4ece\u800c\u5728\u5177\u4f53\u4efb\u52a1\u4e2d\u9009\u62e9\u5408\u9002\u7684\u5f52\u4e00\u5316\u65b9\u6cd5\u3002", "task_step_name": "\u6280\u672f\u8fdb\u6b65", "task_step_description": "\u6a21\u578b\u6027\u80fd\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\uff0c\u8ba1\u7b97\u6548\u7387\u4e5f\u6709\u6240\u63d0\u9ad8\uff0c\u5982EfficientNet\u5728\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u8868\u73b0\u3002", "task_step_level": "2>1", "task_step_question": "### \u95ee\u9898\n\n\u5728\u5927\u6a21\u578b\u4e2d\uff0cLayerNorm\u548cRMSNorm\u4f5c\u4e3a\u4e24\u79cd\u5e38\u7528\u7684\u5f52\u4e00\u5316\u65b9\u6cd5\uff0c\u5b83\u4eec\u5728\u63d0\u5347\u6a21\u578b\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u6709\u54ea\u4e9b\u5177\u4f53\u7684\u6280\u672f\u8fdb\u6b65\uff1f\u7279\u522b\u662f\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u548c\u590d\u6742\u4efb\u52a1\u4e2d\uff0c\u8fd9\u4e24\u79cd\u65b9\u6cd5\u5982\u4f55\u5f71\u54cd\u6a21\u578b\u7684\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff1f", "task_step_question_context": [{"ref_id": "454847042436311108", "chunk_id": "4", "score": 0.3359375, "text": "# 2 Problem Formulation\n\n# 2.1 Related Work\nLarge Language Models (LLMs). The advent of LLMs has led to a paradigm shift in the study of natural language processing (NLP), computer vision, information retrieval, and other domains[Menghani, 2023, Chen et al., 2023, Jiang et al., 2023]. The impressive effectiveness and generalizability of LLMs has come at the price of a drastic increase in LLM sizes [Treviso et al., 2023] and consequent challenges, including huge amounts of computational resources and data required to train, and prohibitive expenses at both training and deployment stages [Bender et al., 2021].  \n\nEfficient Machine Learning (ML) Inference. LLMs belong to a class of models called foundation models [Bommasani et al., 2021] \u2013 models that are trained once and can then be used to serve a wide variety of tasks. As such, we expect inference cost to dominate the overall cost of such models and hence focus on works that reduce the cost of ML inference [Menghani, 2023]. The most common approach for efficient ML inference is model compression i.e., replacing a large model with a smaller model of comparable accuracy. Common techniques for model compression include (i) model pruning [Hassibi et al., 1993, LeCun et al., 1989] which drops parts of the model with minimal accuracy loss, (ii) quantization [Jacob et al., 2018, Vanhoucke et al., 2011] which reduces model memory footprints and inference latency by reducing the precision of data representation (e.g., FP32 to INT8), (iii) knowledge distillation [Hinton et al., 2015, Urban et al., 2016] which trains small student models to mimic large teacher models, and (iv) Neural Architecture Search [Elsken et al., 2019, Zoph and Le, 2016] which tunes model architecture to improve model performance, under inference cost constraints. Such static efficiency optimizations typically produce a fixed model with lower inference cost and lower accuracy compared to the large model which may not suffice for foundation models like LLMs, whose core premise is that the same model will serve a range of tasks, each with its own accuracy/cost constraints. This is already manifesting in inference platforms described in Section 1 which need more dynamic optimizations to meet the demands of all users.  \n\nHybrid ML Inference. Recent works [Kag et al., 2022, Ding et al., 2022] have introduced a new inference paradigm called hybrid inference which uses two models of different sizes instead of a single model for inference. The smaller model (e.g. Llama2 [Touvron et al., 2023]) generally has lower inference cost but also lower accuracy than the larger model (e.g. GPT-4 [OpenAI, 2023]). The key idea is to identify and route easy queries to the small model so that inference cost can be reduced while maintaining response quality. By tuning a threshold on query difficulty we can dynamically trade off quality and cost for the same inference setup. [Kag et al., 2022] study this setup for image classification and propose to train the small model, large model, and router from scratch. However LLM training is expensive and retraining LLMs from scratch for every scenario goes against the very premise of inference with pre-trained foundation models. Moreover text generation [Iqbal and Qureshi, 2022] is often more ambiguous and challenging than image classification due to which novel techniques are required for effective hybrid LLM inference for text generation.  \n\nInference with Multiple LLMs. Some recent works [Jiang et al., 2023, Chen et al., 2023, Leviathan et al., 2023, Kim et al., 2023] use multiple LLMs for inference but these approaches typically call more than one LLM for a single query that can incur significant computational overheads. Specifically [Jiang et al., 2023] calls an ensemble of LLMs at inference time due to which the inference cost will be proportional to the number of models in the system. [Chen et al., 2023] performs inference using a cascade of LLMs where responses to the query are generated sequentially by the LLMs in the cascade until one of the models has a confidence score higher than a predefined threshold. Our work provides high quality responses while always making a single LLM call for all queries and will thus incur much lower computational cost than both of these works on average. Speculative decoding, introduced in [Leviathan et al., 2023, Kim et al., 2023] speeds up decoding of expensive models by invoking small-and-efficient decoders on the \u201ceasy\u201d decoding steps. Instead, in our work we are interested in query routing which assigns \u201ceasy\u201d queries to small models to reduce overall inference costs while maintaining high performance. While the two approaches have different goals, an interesting line of future work would be to combine these so that our router assigns queries to the small or large model based on query difficulty and then speculative decoding is applied on top to speed up inference for queries assigned to the large model thereby leading to further cost reduction."}, {"ref_id": "454845744505973136", "chunk_id": "6", "score": 0.333984375, "text": "# 6 Conclusion\nLLM-based RL algorithms have shown generalization across multiple tasks and games. We argue that this ability comes from implicit memory that fits a large number of parameters to the training data, which is inefficient in terms of model size. In contrast, we propose a new approach inspired by the concept of \u201cworking memory\u201d called Decision Transformers with Memory (DT-Mem), which stores training experience explicitly in a content-addressable matrix module for later retrieval and use. Evaluation demonstrates that DT-Mem achieves better generalization on Atari games with only $10\\%$ of the model parameters compared to the state-of-the-art method. Furthermore, we demonstrate that fine-tuning DT-Memwith a small amount of data can produce state-of-the-art results on both Atari games and the Meta-World environment, when compared to MDT [22], PDT [37], and HDT [38].  \n\nLimitations The first limitation of our work is the sample efficiency of memory fine-tuning. The $10\\%$ fine-tuning dataset is still sizeable, and we plan to explore more sample-efficient methods in the future. We could for instance consider a setting with more tasks, each one with less data so that the inter-task generalization would be even more crucial to its performance. Additionally, this work does not propose a control strategy for collecting data on a new task. For future work, we plan to investigate online data collection methods, which includes the design and learning of exploration strategies for an efficient fine-tuning on new tasks. Finally, the approach has been intuitively motivated, but it would be valuable to have a theoretical grounding that would show the structural limits of large models and how equipping them with a memory component overcomes them.  \n\nSocietal Impact We do not foresee any significant societal impact resulting from our proposed method. The current algorithm is not designed to interact with humans, nor any realistic environment yet. If one chooses to extend our methods to such situations, caution should be exercised to ensure that any safety and ethical concerns are appropriately addressed. As our work is categorized in the offline-RL domain, it is feasible to supplement its training with a dataset that aligns with human intents and values. However, one must be wary that the way our architecture generalizes across tasks is still not well understood and as a consequence we cannot guarantee the generalization of its desirable features: performance, robustness, fairness, etc. By working towards methods that improve the computational efficiency of large models, we contribute to increase their access and reduce their ecological impact.\n\n\n\n# A Implementation Details\n\n# A.1 DT-Mem network architecture\nTable 3 summarizes the different model configurations used for evaluation. In this section, we describe these model configurations in detail. While Table 3 provides a summary, we will also provide additional information here. DT-Mem, PDT and HDT are all share the same transformer architectures. However, for task-adaptation, HDT utilizes a pre-trained $2.3\\mathbf{M}$ hyper-network, while DT-Mem introduces 147K LoRA parameters. To compare with MDT, we use the same parameter size as reported in [22].  \n\nTable 3: Detailed Model Sizes   \n\n\n<html><body><table><tr><td>Model</td><td>Layers</td><td>Hidden size (d)</td><td>Heads</td><td>Params</td><td>Memory Size</td><td>Memory Module Params</td></tr><tr><td>HDT</td><td>4</td><td>512</td><td>8</td><td>13M</td><td>N.A.</td><td>N.A.</td></tr><tr><td>MDT-200M</td><td>10</td><td>1280</td><td>20</td><td>200M</td><td>N.A.</td><td>N.A.</td></tr><tr><td>DT-Mem</td><td>4</td><td>512</td><td>8</td><td>13M</td><td>559K</td><td>7M</td></tr></table></body></html>\n\n# A.2 Hyper-parameters\nIn this section, we will delve into the specifics of the model parameters. Understanding these parameters is key to understanding the workings of the model. It is worth noting that the source code for this model is publicly available at https://github.com/luciferkonn/DT_Mem/tree/main .This allows for a deeper understanding of the model\u2019s inner workings and may facilitate the replication of its results.  \n\nTable 4: Hyperparameters for DT-Mem training   \n\n\n<html><body><table><tr><td>Hyperparameters</td><td>Value</td></tr><tr><td>K (length of context)</td><td>28</td></tr><tr><td>dropoutrate</td><td>0.1</td></tr><tr><td>maximum epochs</td><td>1000</td></tr><tr><td>steps for each epoch</td><td>1000</td></tr><tr><td>optimizer learning rate</td><td>1e-4</td></tr><tr><td>weight decay</td><td>1e-4</td></tr><tr><td>gradient norm clip</td><td>1.</td></tr><tr><td>data points for each dataset</td><td>500,000</td></tr><tr><td>batch size</td><td>64</td></tr><tr><td>memory slots</td><td>1290</td></tr><tr><td>activation</td><td>GELU</td></tr><tr><td>optimizer</td><td>Adamw</td></tr><tr><td>scheduler</td><td>LambdaLR</td></tr></table></body></html>\n\n# A.3 Training and fine-tuning algorithm\nIn this section, we present the pre-training DT-Memin Appendix A.3 and fine-tuning DT-Mem with LoRA in Appendix 5.5.  \n\nWe pre-train DT-Mem on multiple offline datasets. Each gradient update of the DT-Memmodel considers information from each training task.  \n\nWe fine-tune the memory module to adapt to each downstream task. To achieve this, we fix the pre-trained DT-Mem model parameters and add additional LoRA parameters for the memory module feed-forward neural networks. The fine-tune dataset is used to update these LoRA parameters only.\n\n# Algorithm 1 Pre-train DT-Mem\n1: for T episodes do   \n2: 3: 4: for Sample trajectories Split trajectories into different segments with length K and calculate return-to-go in the Task $\\mathcal{T}_{i}\\in T^{t r a i n}\\;.$ do $\\tau=(s_{0},a_{0},r_{0},\\cdot\\cdot\\cdot\\,,s_{H},a_{H},r_{H})$ m the dataset $\\mathcal{D}_{i}$ .  \ninput sequence.   \n5: Given $\\hat{\\tau}_{t+1:t+K}$ , compute the sequence embedding $e_{s e q}$ .  \n6: Update the working memory and retrieve the relative information as $E_{o u t}$   \n7: Given $E_{o u t}$ , predict actions $\\tilde{a}_{t}$ , reward $\\tilde{r}_{t}$ , and return-to-go ${\\tilde{R}}_{t}$ .  \n8: Compute the loss according to Eqn. 1.   \n9: Update all modules parameters.   \n10: end for   \n11: end for  \n\nAlgorithm 2 Fine-tuning DT-Mem  \n\n$\\hat{B}^{q},\\hat{B}^{k},\\hat{B}^{v},\\hat{A}^{q},\\hat{A}^{k},\\hat{A}^{v},B^{q},A^{q},B^{k},A^{k}$ Require: Fine-tuning dataset $\\mathcal{T}^{i}~\\in~T^{t e s t}$ .dataset $\\mathcal{D}^{i}$ for $\\mathcal{T}^{i}$ .Initialize LoRA parameters 1: for T steps do   \n2: Split trajectories into different segments with length $\\mathbf{K}$ and calculate return-to-go in the input sequence.   \n3: Given $\\hat{\\tau}_{t+1:t+K}$ , compute the sequence embedding $e_{s e q}$ .  \n4: Update working memory using $\\hat{Q}\\,=\\,M(\\hat{W}^{q}+\\hat{B}^{q}\\bar{A}^{q})$ ,$\\hat{K}\\,=\\,M(\\hat{W}^{k}\\,+\\,\\hat{B}^{k}\\hat{A}^{k}),\\hat{V}\\,=$ $M(\\hat{W}^{v}+\\hat{B}^{v}\\hat{A}^{v})$ ,$Q=M(W^{q}+B^{q}A^{q}),K=M(W^{k}+B^{k}A^{k})$   \n5: Retrieve the relative information as $E_{o u t}$   \n6: Given $E_{o u t}$ , predict actions $\\tilde{a}_{t}$ , reward $\\tilde{r}_{t}$ , and return-to-go ${\\tilde{R}}_{t}$ .  \n7: Compute the loss according to Eqn. 1.   \n8: Update LoRA parameters only.   \n9: end for"}, {"ref_id": "455038427524247598", "chunk_id": "4", "score": 0.212890625, "text": "# 8 Discussion\nThe worst group performance of a model is affected by two factors: the quality of the representation of the core features produced by the feature extractor and the weight assigned to the core features in the last classification layer. In contrast to prior work, we consider the quality of the feature extractor in isolation, focusing on realistic datasets and large-scale models. We find that many of the popular group robustness methods improve the worst group performance primarily by learning a better last layer and not by learning a better feature representation. Similarly, regularization techniques such as early stopping and strong weight decay can improve the worst group accuracy by learning a better last layer, but do not lead to a consistent improvement in terms of the quality of the learned feature representations. On the other hand, the base model architecture and pre-training strategy have a major effect on the quality of the feature representations.  \n\nOur observations suggest an important open question: is it possible to significantly improve upon standard ERM in terms of the quality of the learned representations for a given base model? In future work, it would be interesting to evaluate methods such as Rich Feature Construction [ 99 ], gradient starvation [ 68 ], ensembling [ 47 ] and other techniques for increasing feature diversity [e.g. 48 ]. We hope that our work will also inspire new group robustness methods targeted specifically at improving the quality of the core feature representations.\n\n\n\n# Appendix Outline\nThis appendix is structured as follows. In Section A we describe the datasets, augmentation policies and models used in this paper. In Section Bwe provide details on the methods, implementations and hyper-parameters, as well as detailed results for the experiments in Section 5 . In Section Cwe provide additional details on the experiments in Section 6 . In Section Dwe provide additional details and results for the experiments in Section 7 . In Section Ewe provide additional results on the MultiNLI dataset. Finally, in Section Fwe describe the limitation, broader impact, compute and licenses.  \n\nTools and packages. During the work on this paper, we used the following tools and packages: NumPy[24],SciPy[87],PyTorch[67],TorchVision[55],Jupyter notebooks[42],Matplotlib[33],Pandas [57 ], Weights&Biases [7 ], timm [90 ], transformers [92 ], vissl [21 ].\n\n# A Data and Models\nIn this section, we describe the datasets, data augmentation policies and models used throughout the paper."}], "task_step_question_answer": "\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\uff0cLayerNorm\u548cRMSNorm\u7684\u5e94\u7528\u4e5f\u5c55\u73b0\u51fa\u4e0d\u540c\u7684\u4f18\u52bf\u3002\u4f8b\u5982\uff0c\u5728\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u4e2d\uff0cLayerNorm\u901a\u8fc7\u5176\u5168\u5c40\u5f52\u4e00\u5316\u7684\u7279\u6027\uff0c\u80fd\u591f\u66f4\u597d\u5730\u5904\u7406\u4e0d\u540c\u5c3a\u5ea6\u7684\u76ee\u6807\uff0c\u63d0\u5347\u68c0\u6d4b\u7cbe\u5ea6\u3002\u800cRMSNorm\u5219\u901a\u8fc7\u5176\u9ad8\u6548\u7684\u8ba1\u7b97\u8fc7\u7a0b\uff0c\u5728\u4fdd\u6301\u68c0\u6d4b\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u8bad\u7ec3\u65f6\u95f4\u3002\u5728\u533b\u7597\u8bca\u65ad\u9886\u57df\uff0cLayerNorm\u5728\u5904\u7406\u591a\u6a21\u6001\u6570\u636e\uff08\u5982CT\u56fe\u50cf\u548c\u75c5\u5386\u6587\u672c\uff09\u65f6\uff0c\u80fd\u591f\u66f4\u597d\u5730\u6355\u6349\u4e0d\u540c\u6a21\u6001\u4e4b\u95f4\u7684\u5173\u8054\uff0c\u63d0\u5347\u8bca\u65ad\u51c6\u786e\u6027\u3002\u800cRMSNorm\u5219\u5728\u5904\u7406\u5927\u89c4\u6a21\u533b\u7597\u6570\u636e\u65f6\uff0c\u901a\u8fc7\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\uff0c\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u3002\n\n\u5728\u8054\u90a6\u5b66\u4e60\u548c\u9690\u79c1\u4fdd\u62a4\u9886\u57df\uff0cLayerNorm\u548cRMSNorm\u7684\u5e94\u7528\u4e5f\u5c55\u73b0\u51fa\u6f5c\u529b\u3002\u4f8b\u5982\uff0c\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\uff0cLayerNorm\u901a\u8fc7\u5176\u7a33\u5b9a\u6027\uff0c\u80fd\u591f\u6709\u6548\u7f13\u89e3\u4e0d\u540c\u5ba2\u6237\u7aef\u6570\u636e\u5206\u5e03\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002\u800cRMSNorm\u5219\u901a\u8fc7\u5176\u9ad8\u6548\u6027\uff0c\u51cf\u5c11\u4e86\u901a\u4fe1\u5f00\u9500\uff0c\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u3002\u5728\u9690\u79c1\u4fdd\u62a4\u65b9\u9762\uff0cLayerNorm\u901a\u8fc7\u5176\u5168\u5c40\u5f52\u4e00\u5316\u7684\u7279\u6027\uff0c\u80fd\u591f\u66f4\u597d\u5730\u4fdd\u62a4\u654f\u611f\u6570\u636e\uff0c\u800cRMSNorm\u5219\u901a\u8fc7\u5176\u9ad8\u6548\u7684\u8ba1\u7b97\u8fc7\u7a0b\uff0c\u51cf\u5c11\u4e86\u9690\u79c1\u6cc4\u9732\u7684\u98ce\u9669\u3002\n\n\u672a\u6765\u7684\u7814\u7a76\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63a2\u7d22\u5982\u4f55\u7ed3\u5408LayerNorm\u548cRMSNorm\u7684\u4f18\u70b9\uff0c\u8bbe\u8ba1\u51fa\u66f4\u9ad8\u6548\u7684\u5f52\u4e00\u5316\u65b9\u6cd5\u3002\u4f8b\u5982\uff0c\u53ef\u4ee5\u7814\u7a76\u5982\u4f55\u6839\u636e\u8f93\u5165\u6570\u636e\u7684\u7279\u6027\u52a8\u6001\u9009\u62e9\u4f7f\u7528LayerNorm\u6216RMSNorm\uff0c\u4ee5\u8fdb\u4e00\u6b65\u63d0\u5347\u6a21\u578b\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u3002\u6b64\u5916\uff0c\u8fd8\u53ef\u4ee5\u63a2\u7d22\u8fd9\u4e9b\u65b9\u6cd5\u5728\u65b0\u5174\u7814\u7a76\u9886\u57df\uff08\u5982\u9690\u79c1\u4fdd\u62a4\u3001\u8054\u90a6\u5b66\u4e60\u7b49\uff09\u4e2d\u7684\u5e94\u7528\uff0c\u4ee5\u5e94\u5bf9\u65e5\u76ca\u590d\u6742\u7684\u5b9e\u9645\u9700\u6c42\u3002", "ref_task_step_id": ""}, "__type__": "task_step_node"}, "299715cd-eee2-4361-ac2c-b3ffbb24a419": {"__data__": {"id_": "299715cd-eee2-4361-ac2c-b3ffbb24a419", "metadata": {}, "relationships": {}, "hash": "", "start_task_context": "\u5927\u6a21\u578b\u4e2d\u7684LayerNorm\u548cRMSNorm\u6709\u4ec0\u4e48\u533a\u522b\uff1f", "aemo_representation_context": "### Step-by-Step Decomposition of Computer Science Research Content\n\n#### 1. \u5206\u6790\u8fd1\u51e0\u5e74\u7814\u7a76\u9886\u57df\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba\n- **\u6280\u672f\u6846\u67b6**\uff1a\u8fd1\u5e74\u6765\uff0c\u6df1\u5ea6\u5b66\u4e60\u3001\u5f3a\u5316\u5b66\u4e60\u3001\u56fe\u795e\u7ecf\u7f51\u7edc\u7b49\u6846\u67b6\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u9886\u57df\u5360\u636e\u4e3b\u5bfc\u5730\u4f4d\u3002\u5177\u4f53\u6846\u67b6\u5982Transformer\u3001BERT\u3001GNN\u7b49\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002\n- **\u65b9\u6cd5\u8bba**\uff1a\u7814\u7a76\u65b9\u6cd5\u9010\u6e10\u4ece\u5355\u4e00\u6a21\u578b\u8f6c\u5411\u591a\u6a21\u578b\u878d\u5408\uff0c\u4ece\u76d1\u7763\u5b66\u4e60\u6269\u5c55\u5230\u65e0\u76d1\u7763\u548c\u81ea\u76d1\u7763\u5b66\u4e60\uff0c\u5f3a\u8c03\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002\n\n#### 2. \u7814\u7a76\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u4e3b\u8981\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0e\u53d8\u4f53\n- **\u5e94\u7528**\uff1a\u4f8b\u5982\uff0cTransformer\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u4e2d\u88ab\u5e7f\u6cdb\u5e94\u7528\uff0cBERT\u53d8\u4f53\u5982RoBERTa\u3001ALBERT\u5728\u6587\u672c\u5206\u7c7b\u3001\u95ee\u7b54\u7cfb\u7edf\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff1bGNN\u5728\u63a8\u8350\u7cfb\u7edf\u3001\u77e5\u8bc6\u56fe\u8c31\u7b49\u9886\u57df\u6709\u663e\u8457\u5e94\u7528\u3002\n- **\u53d8\u4f53**\uff1a\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\uff0c\u7814\u7a76\u8005\u63d0\u51fa\u4e86\u591a\u79cd\u53d8\u4f53\uff0c\u5982\u9488\u5bf9\u957f\u5e8f\u5217\u5904\u7406\u7684Transformer-XL\uff0c\u9488\u5bf9\u591a\u6a21\u6001\u6570\u636e\u7684ViLBERT\u7b49\u3002\n\n#### 3. \u8bc4\u4f30\u5b66\u672f\u754c\u7684\u6280\u672f\u8fdb\u6b65\u4e0e\u5c40\u9650\u6027\n- **\u6280\u672f\u8fdb\u6b65**\uff1a\u6a21\u578b\u6027\u80fd\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\uff0c\u8ba1\u7b97\u6548\u7387\u4e5f\u6709\u6240\u63d0\u9ad8\uff0c\u5982EfficientNet\u5728\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u8868\u73b0\u3002\n- **\u5c40\u9650\u6027**\uff1a\u4f9d\u7136\u5b58\u5728\u6a21\u578b\u504f\u5dee\uff08\u5982\u6027\u522b\u504f\u89c1\uff09\u3001\u6570\u636e\u4f9d\u8d56\uff08\u5982\u5bf9\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\uff09\u7b49\u95ee\u9898\uff0c\u6cdb\u5316\u80fd\u529b\u6709\u5f85\u8fdb\u4e00\u6b65\u63d0\u5347\u3002\n\n#### 4. \u63a2\u8ba8\u8ba1\u7b97\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\n- **\u9002\u7528\u6027**\uff1a\u7814\u7a76\u6a21\u578b\u5728\u4e0d\u540c\u9886\u57df\uff08\u5982\u533b\u7597\u3001\u91d1\u878d\uff09\u548c\u591a\u6a21\u6001\u6570\u636e\uff08\u5982\u6587\u672c+\u56fe\u50cf\uff09\u4e0a\u7684\u8868\u73b0\uff0c\u5982\u591a\u6a21\u6001Transformer\u6a21\u578b\u3002\n- **\u6cdb\u5316\u80fd\u529b**\uff1a\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\u3001\u5143\u5b66\u4e60\u7b49\u65b9\u6cd5\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f7f\u5176\u80fd\u591f\u5e94\u5bf9\u66f4\u591a\u73b0\u5b9e\u5e94\u7528\u573a\u666f\u3002\n\n#### 5. \u5206\u6790\u6700\u65b0\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027\n- **\u7a33\u5b9a\u6027**\uff1a\u7814\u7a76\u7b97\u6cd5\u5728\u590d\u6742\u3001\u52a8\u6001\u73af\u5883\u4e0b\u7684\u8868\u73b0\uff0c\u5982\u5bf9\u6297\u8bad\u7ec3\u3001\u9c81\u68d2\u4f18\u5316\u7b49\u65b9\u6cd5\u7684\u5e94\u7528\u3002\n- **\u5bb9\u9519\u6027**\uff1a\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\u7684\u9002\u5e94\u6027\uff0c\u5982\u5206\u5e03\u5f0f\u8bad\u7ec3\u4e2d\u7684\u68af\u5ea6\u538b\u7f29\u3001\u6a21\u578b\u5e76\u884c\u7b49\u6280\u672f\u3002\n\n#### 6. \u8bc4\u4f30\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218\n- **\u672a\u6765\u65b9\u5411**\uff1a\u5982\u53ef\u89e3\u91caAI\u3001\u8054\u90a6\u5b66\u4e60\u3001\u9690\u79c1\u4fdd\u62a4\u7b49\u65b0\u5174\u7814\u7a76\u65b9\u5411\u3002\n- **\u6311\u6218**\uff1a\u5982\u4f55\u5e73\u8861\u6a21\u578b\u6027\u80fd\u4e0e\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\uff0c\u5982\u4f55\u89e3\u51b3\u6570\u636e\u504f\u89c1\u548c\u4f26\u7406\u95ee\u9898\u7b49\u3002\n\n### \u5927\u6a21\u578b\u4e2d\u7684LayerNorm\u548cRMSNorm\u7684\u533a\u522b\n\n#### LayerNorm\uff08Layer Normalization\uff09\n- **\u5b9a\u4e49**\uff1aLayerNorm\u662f\u5bf9\u6bcf\u4e2a\u6837\u672c\u7684\u6bcf\u4e2a\u7279\u5f81\u8fdb\u884c\u5f52\u4e00\u5316\uff0c\u4f7f\u5176\u5747\u503c\u548c\u65b9\u5dee\u5206\u522b\u63a5\u8fd10\u548c1\u3002\n- **\u516c\u5f0f**\uff1a\n  \\[\n  \\text{LayerNorm}(x) = \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} \\cdot \\gamma + \\beta\n  \\]\n  \u5176\u4e2d\uff0c\\(\\mu\\)\u548c\\(\\sigma^2\\)\u5206\u522b\u662f\u7279\u5f81\u5747\u503c\u548c\u65b9\u5dee\uff0c\\(\\gamma\\)\u548c\\(\\beta\\)\u662f\u53ef\u5b66\u4e60\u53c2\u6570\uff0c\\(\\epsilon\\)\u662f\u5fae\u5c0f\u5e38\u6570\u4ee5\u9632\u6b62\u9664\u96f6\u3002\n- **\u5e94\u7528**\uff1a\u5e7f\u6cdb\u7528\u4e8eTransformer\u6a21\u578b\u4e2d\uff0c\u6709\u52a9\u4e8e\u7f13\u89e3\u68af\u5ea6\u6d88\u5931\u95ee\u9898\uff0c\u63d0\u5347\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002\n\n#### RMSNorm\uff08Root Mean Square Normalization\uff09\n- **\u5b9a\u4e49**\uff1aRMSNorm\u662f\u5bf9\u6bcf\u4e2a\u6837\u672c\u7684\u6bcf\u4e2a\u7279\u5f81\u8fdb\u884c\u5f52\u4e00\u5316\uff0c\u4f46\u53ea\u5f52\u4e00\u5316\u65b9\u5dee\uff0c\u4e0d\u5f52\u4e00\u5316\u5747\u503c\u3002\n- **\u516c\u5f0f**\uff1a\n  \\[\n  \\text{RMSNorm}(x) = \\frac{x}{\\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} x_i^2 + \\epsilon}} \\cdot \\gamma + \\beta\n  \\]\n  \u5176\u4e2d\uff0c\\(n\\)\u662f\u7279\u5f81\u6570\u91cf\uff0c\\(\\gamma\\)\u548c\\(\\beta\\)\u662f\u53ef\u5b66\u4e60\u53c2\u6570\uff0c\\(\\epsilon\\)\u662f\u5fae\u5c0f\u5e38\u6570\u3002\n- **\u5e94\u7528**\uff1a\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0cRMSNorm\u6bd4LayerNorm\u66f4\u7b80\u5355\uff0c\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\uff0c\u5c24\u5176\u5728\u5927\u89c4\u6a21\u6a21\u578b\u8bad\u7ec3\u4e2d\u3002\n\n#### \u4e3b\u8981\u533a\u522b\n1. **\u5f52\u4e00\u5316\u65b9\u5f0f**\uff1a\n   - LayerNorm\u540c\u65f6\u5f52\u4e00\u5316\u5747\u503c\u548c\u65b9\u5dee\u3002\n   - RMSNorm\u53ea\u5f52\u4e00\u5316\u65b9\u5dee\u3002\n2. **\u8ba1\u7b97\u590d\u6742\u5ea6**\uff1a\n   - LayerNorm\u9700\u8981\u8ba1\u7b97\u5747\u503c\u548c\u65b9\u5dee\uff0c\u8ba1\u7b97\u91cf\u7a0d\u5927\u3002\n   - RMSNorm\u53ea\u9700\u8ba1\u7b97\u65b9\u5dee\uff0c\u8ba1\u7b97\u91cf\u8f83\u5c0f\u3002\n3. **\u6548\u679c**\uff1a\n   - LayerNorm\u5728\u8bb8\u591a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u5728NLP\u9886\u57df\u3002\n   - RMSNorm\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u8868\u73b0\u76f8\u4f3c\u751a\u81f3\u66f4\u4f18\uff0c\u4e14\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u3002\n\n\u901a\u8fc7\u4e0a\u8ff0\u5206\u6790\uff0c\u53ef\u4ee5\u66f4\u597d\u5730\u7406\u89e3LayerNorm\u548cRMSNorm\u5728\u5927\u6a21\u578b\u4e2d\u7684\u5e94\u7528\u53ca\u5176\u4f18\u7f3a\u70b9\uff0c\u4ece\u800c\u5728\u5177\u4f53\u4efb\u52a1\u4e2d\u9009\u62e9\u5408\u9002\u7684\u5f52\u4e00\u5316\u65b9\u6cd5\u3002", "task_step_name": "\u5c40\u9650\u6027", "task_step_description": "\u4f9d\u7136\u5b58\u5728\u6a21\u578b\u504f\u5dee\uff08\u5982\u6027\u522b\u504f\u89c1\uff09\u3001\u6570\u636e\u4f9d\u8d56\uff08\u5982\u5bf9\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\uff09\u7b49\u95ee\u9898\uff0c\u6cdb\u5316\u80fd\u529b\u6709\u5f85\u8fdb\u4e00\u6b65\u63d0\u5347\u3002", "task_step_level": "2>2", "task_step_question": "### \u95ee\u9898\uff1a\u5728\u5927\u6a21\u578b\u4e2d\u4f7f\u7528LayerNorm\u548cRMSNorm\u65f6\uff0c\u8fd9\u4e24\u79cd\u5f52\u4e00\u5316\u65b9\u6cd5\u5728\u7f13\u89e3\u6a21\u578b\u504f\u5dee\uff08\u5982\u6027\u522b\u504f\u89c1\uff09\u548c\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u6709\u4f55\u4e0d\u540c\u7684\u8868\u73b0\uff1f\n\n#### \u80cc\u666f\uff1a\nLayerNorm\u548cRMSNorm\u662f\u4e24\u79cd\u5e38\u7528\u7684\u5f52\u4e00\u5316\u65b9\u6cd5\uff0c\u5206\u522b\u5728\u5927\u6a21\u578b\u4e2d\u7528\u4e8e\u7a33\u5b9a\u8bad\u7ec3\u548c\u63d0\u5347\u6027\u80fd\u3002\u7136\u800c\uff0c\u6a21\u578b\u504f\u5dee\uff08\u5982\u6027\u522b\u504f\u89c1\uff09\u548c\u6570\u636e\u4f9d\u8d56\uff08\u5982\u5bf9\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\uff09\u7b49\u95ee\u9898\u4f9d\u7136\u5b58\u5728\uff0c\u6cdb\u5316\u80fd\u529b\u6709\u5f85\u8fdb\u4e00\u6b65\u63d0\u5347\u3002\n\n#### \u95ee\u9898\u5206\u6790\uff1a\n1. **\u6a21\u578b\u504f\u5dee**\uff1a\n   - LayerNorm\u901a\u8fc7\u5f52\u4e00\u5316\u5747\u503c\u548c\u65b9\u5dee\uff0c\u53ef\u80fd\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u66f4\u597d\u5730\u5e73\u8861\u7279\u5f81\u5206\u5e03\uff0c\u4ece\u800c\u51cf\u5c11\u504f\u5dee\u3002\n   - RMSNorm\u53ea\u5f52\u4e00\u5316\u65b9\u5dee\uff0c\u53ef\u80fd\u5728\u5904\u7406\u504f\u5dee\u65f6\u8868\u73b0\u4e0d\u540c\uff0c\u5c24\u5176\u662f\u5728\u7279\u5f81\u5747\u503c\u5bf9\u504f\u5dee\u6709\u663e\u8457\u5f71\u54cd\u7684\u60c5\u51b5\u4e0b\u3002\n\n2. **\u6cdb\u5316\u80fd\u529b**\uff1a\n   - LayerNorm\u5728NLP\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u53ef\u80fd\u6709\u52a9\u4e8e\u63d0\u5347\u6a21\u578b\u5728\u672a\u89c1\u6570\u636e\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u3002\n   - RMSNorm\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\uff0c\u53ef\u80fd\u5728\u5927\u89c4\u6a21\u6570\u636e\u8bad\u7ec3\u4e2d\u8868\u73b0\u66f4\u4f18\uff0c\u4f46\u5176\u5728\u6cdb\u5316\u80fd\u529b\u4e0a\u7684\u8868\u73b0\u662f\u5426\u4e0eLayerNorm\u76f8\u5f53\u6216\u66f4\u4f18\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u3002\n\n#### \u5177\u4f53\u95ee\u9898\uff1a\n- \u5728\u5b58\u5728\u6a21\u578b\u504f\u5dee\uff08\u5982\u6027\u522b\u504f\u89c1\uff09\u7684\u60c5\u51b5\u4e0b\uff0cLayerNorm\u548cRMSNorm\u5206\u522b\u5982\u4f55\u5f71\u54cd\u6a21\u578b\u7684\u504f\u5dee\u8868\u73b0\uff1f\n- \u5728\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u65b9\u9762\uff0cLayerNorm\u548cRMSNorm\u5404\u81ea\u7684\u8868\u73b0\u5982\u4f55\uff1f\u662f\u5426\u5b58\u5728\u7279\u5b9a\u4efb\u52a1\u6216\u6570\u636e\u96c6\u4e0a\u5176\u4e2d\u4e00\u79cd\u65b9\u6cd5\u660e\u663e\u4f18\u4e8e\u53e6\u4e00\u79cd\uff1f\n- \u7ed3\u5408\u8ba1\u7b97\u6548\u7387\u548c\u6a21\u578b\u6027\u80fd\uff0c\u5982\u4f55\u5728\u5177\u4f53\u4efb\u52a1\u4e2d\u9009\u62e9\u5408\u9002\u7684\u5f52\u4e00\u5316\u65b9\u6cd5\u4ee5\u5e73\u8861\u504f\u5dee\u7f13\u89e3\u548c\u6cdb\u5316\u80fd\u529b\u63d0\u5347\uff1f", "task_step_question_context": [{"ref_id": "454849448879258560", "chunk_id": "2", "score": 0.369140625, "text": "# 2 Problem Formulation\n\n# 2.1 Related Work\nLarge Language Models (LLMs). The advent of LLMs has led to a paradigm shift in the study of natural language processing (NLP), computer vision, information retrieval, and other domains[Menghani, 2023, Chen et al., 2023, Jiang et al., 2023]. The impressive effectiveness and generalizability of LLMs has come at the price of a drastic increase in LLM sizes [Treviso et al., 2023] and consequent challenges, including huge amounts of computational resources and data required to train, and prohibitive expenses at both training and deployment stages [Bender et al., 2021].  \n\nEfficient Machine Learning (ML) Inference. LLMs belong to a class of models called foundation models [Bommasani et al., 2021] \u2013 models that are trained once and can then be used to serve a wide variety of tasks. As such, we expect inference cost to dominate the overall cost of such models and hence focus on works that reduce the cost of ML inference [Menghani, 2023]. The most common approach for efficient ML inference is model compression i.e., replacing a large model with a smaller model of comparable accuracy. Common techniques for model compression include (i) model pruning [Hassibi et al., 1993, LeCun et al., 1989] which drops parts of the model with minimal accuracy loss, (ii) quantization [Jacob et al., 2018, Vanhoucke et al., 2011] which reduces model memory footprints and inference latency by reducing the precision of data representation (e.g., FP32 to INT8), (iii) knowledge distillation [Hinton et al., 2015, Urban et al., 2016] which trains small student models to mimic large teacher models, and (iv) Neural Architecture Search [Elsken et al., 2019, Zoph and Le, 2016] which tunes model architecture to improve model performance, under inference cost constraints. Such static efficiency optimizations typically produce a fixed model with lower inference cost and lower accuracy compared to the large model which may not suffice for foundation models like LLMs, whose core premise is that the same model will serve a range of tasks, each with its own accuracy/cost constraints. This is already manifesting in inference platforms described in Section 1 which need more dynamic optimizations to meet the demands of all users.  \n\nHybrid ML Inference. Recent works [Kag et al., 2022, Ding et al., 2022] have introduced a new inference paradigm called hybrid inference which uses two models of different sizes instead of a single model for inference. The smaller model (e.g. Llama2 [Touvron et al., 2023]) generally has lower inference cost but also lower accuracy than the larger model (e.g. GPT-4 [OpenAI, 2023]). The key idea is to identify and route easy queries to the small model so that inference cost can be reduced while maintaining response quality. By tuning a threshold on query difficulty we can dynamically trade off quality and cost for the same inference setup. [Kag et al., 2022] study this setup for image classification and propose to train the small model, large model, and router from scratch. However LLM training is expensive and retraining LLMs from scratch for every scenario goes against the very premise of inference with pre-trained foundation models. Moreover text generation [Iqbal and Qureshi, 2022] is often more ambiguous and challenging than image classification due to which novel techniques are required for effective hybrid LLM inference for text generation.  \n\nInference with Multiple LLMs. Some recent works [Jiang et al., 2023, Chen et al., 2023, Leviathan et al., 2023, Kim et al., 2023] use multiple LLMs for inference but these approaches typically call more than one LLM for a single query that can incur significant computational overheads. Specifically [Jiang et al., 2023] calls an ensemble of LLMs at inference time due to which the inference cost will be proportional to the number of models in the system. [Chen et al., 2023] performs inference using a cascade of LLMs where responses to the query are generated sequentially by the LLMs in the cascade until one of the models has a confidence score higher than a predefined threshold. Our work provides high quality responses while always making a single LLM call for all queries and will thus incur much lower computational cost than both of these works on average. Speculative decoding, introduced in [Leviathan et al., 2023, Kim et al., 2023] speeds up decoding of expensive models by invoking small-and-efficient decoders on the \u201ceasy\u201d decoding steps. Instead, in our work we are interested in query routing which assigns \u201ceasy\u201d queries to small models to reduce overall inference costs while maintaining high performance. While the two approaches have different goals, an interesting line of future work would be to combine these so that our router assigns queries to the small or large model based on query difficulty and then speculative decoding is applied on top to speed up inference for queries assigned to the large model thereby leading to further cost reduction."}, {"ref_id": "455038427552559154", "chunk_id": "6", "score": 0.212890625, "text": "# D.4. Further analysis of the norm of classifiers\nIn the Experiment section, we find that our method increases model confidence by changing the angle between the classifier weights and features, which also increases the norm of the feature. With a larger norm, the model could make a more confident prediction. In addition to the norm of the feature, the norm of a classifier also plays an important role in classification. If the weight of a class has a large norm, it would make a more confident prediction. Besides, if the model is trained with a large number of samples for a single class, it would be more confident in this class. Therefore, we hypothesize that more training samples may have a positive relation to the norm of classifier weights. We plot the correlation between the norm of classifier weights and its corresponding training samples in Fig. 7 . As seen, there is a positive correlation between these two factors. Note that this phenomenon is more evident on PACS compared to OfficeHome because the training number for each class is relatively small and similar to each other, resulting in a less obvious correlation."}, {"ref_id": "454895483053685384", "chunk_id": "2", "score": 0.2119140625, "text": "# 6 Conclusion\nLLM-based RL algorithms have shown generalization across multiple tasks and games. We argue that this ability comes from implicit memory that fits a large number of parameters to the training data, which is inefficient in terms of model size. In contrast, we propose a new approach inspired by the concept of \u201cworking memory\u201d called Decision Transformers with Memory (DT-Mem), which stores training experience explicitly in a content-addressable matrix module for later retrieval and use. Evaluation demonstrates that DT-Mem achieves better generalization on Atari games with only $10\\%$ of the model parameters compared to the state-of-the-art method. Furthermore, we demonstrate that fine-tuning DT-Memwith a small amount of data can produce state-of-the-art results on both Atari games and the Meta-World environment, when compared to MDT [22], PDT [37], and HDT [38].  \n\nLimitations The first limitation of our work is the sample efficiency of memory fine-tuning. The $10\\%$ fine-tuning dataset is still sizeable, and we plan to explore more sample-efficient methods in the future. We could for instance consider a setting with more tasks, each one with less data so that the inter-task generalization would be even more crucial to its performance. Additionally, this work does not propose a control strategy for collecting data on a new task. For future work, we plan to investigate online data collection methods, which includes the design and learning of exploration strategies for an efficient fine-tuning on new tasks. Finally, the approach has been intuitively motivated, but it would be valuable to have a theoretical grounding that would show the structural limits of large models and how equipping them with a memory component overcomes them.  \n\nSocietal Impact We do not foresee any significant societal impact resulting from our proposed method. The current algorithm is not designed to interact with humans, nor any realistic environment yet. If one chooses to extend our methods to such situations, caution should be exercised to ensure that any safety and ethical concerns are appropriately addressed. As our work is categorized in the offline-RL domain, it is feasible to supplement its training with a dataset that aligns with human intents and values. However, one must be wary that the way our architecture generalizes across tasks is still not well understood and as a consequence we cannot guarantee the generalization of its desirable features: performance, robustness, fairness, etc. By working towards methods that improve the computational efficiency of large models, we contribute to increase their access and reduce their ecological impact.\n\n\n\n# A Implementation Details\n\n# A.1 DT-Mem network architecture\nTable 3 summarizes the different model configurations used for evaluation. In this section, we describe these model configurations in detail. While Table 3 provides a summary, we will also provide additional information here. DT-Mem, PDT and HDT are all share the same transformer architectures. However, for task-adaptation, HDT utilizes a pre-trained $2.3\\mathbf{M}$ hyper-network, while DT-Mem introduces 147K LoRA parameters. To compare with MDT, we use the same parameter size as reported in [22].  \n\nTable 3: Detailed Model Sizes   \n\n\n<html><body><table><tr><td>Model</td><td>Layers</td><td>Hidden size (d)</td><td>Heads</td><td>Params</td><td>Memory Size</td><td>Memory Module Params</td></tr><tr><td>HDT</td><td>4</td><td>512</td><td>8</td><td>13M</td><td>N.A.</td><td>N.A.</td></tr><tr><td>MDT-200M</td><td>10</td><td>1280</td><td>20</td><td>200M</td><td>N.A.</td><td>N.A.</td></tr><tr><td>DT-Mem</td><td>4</td><td>512</td><td>8</td><td>13M</td><td>559K</td><td>7M</td></tr></table></body></html>\n\n# A.2 Hyper-parameters\nIn this section, we will delve into the specifics of the model parameters. Understanding these parameters is key to understanding the workings of the model. It is worth noting that the source code for this model is publicly available at https://github.com/luciferkonn/DT_Mem/tree/main .This allows for a deeper understanding of the model\u2019s inner workings and may facilitate the replication of its results.  \n\nTable 4: Hyperparameters for DT-Mem training   \n\n\n<html><body><table><tr><td>Hyperparameters</td><td>Value</td></tr><tr><td>K (length of context)</td><td>28</td></tr><tr><td>dropoutrate</td><td>0.1</td></tr><tr><td>maximum epochs</td><td>1000</td></tr><tr><td>steps for each epoch</td><td>1000</td></tr><tr><td>optimizer learning rate</td><td>1e-4</td></tr><tr><td>weight decay</td><td>1e-4</td></tr><tr><td>gradient norm clip</td><td>1.</td></tr><tr><td>data points for each dataset</td><td>500,000</td></tr><tr><td>batch size</td><td>64</td></tr><tr><td>memory slots</td><td>1290</td></tr><tr><td>activation</td><td>GELU</td></tr><tr><td>optimizer</td><td>Adamw</td></tr><tr><td>scheduler</td><td>LambdaLR</td></tr></table></body></html>\n\n# A.3 Training and fine-tuning algorithm\nIn this section, we present the pre-training DT-Memin Appendix A.3 and fine-tuning DT-Mem with LoRA in Appendix 5.5.  \n\nWe pre-train DT-Mem on multiple offline datasets. Each gradient update of the DT-Memmodel considers information from each training task.  \n\nWe fine-tune the memory module to adapt to each downstream task. To achieve this, we fix the pre-trained DT-Mem model parameters and add additional LoRA parameters for the memory module feed-forward neural networks. The fine-tune dataset is used to update these LoRA parameters only.\n\n# Algorithm 1 Pre-train DT-Mem\n1: for T episodes do   \n2: 3: 4: for Sample trajectories Split trajectories into different segments with length K and calculate return-to-go in the Task $\\mathcal{T}_{i}\\in T^{t r a i n}\\;.$ do $\\tau=(s_{0},a_{0},r_{0},\\cdot\\cdot\\cdot\\,,s_{H},a_{H},r_{H})$ m the dataset $\\mathcal{D}_{i}$ .  \ninput sequence.   \n5: Given $\\hat{\\tau}_{t+1:t+K}$ , compute the sequence embedding $e_{s e q}$ .  \n6: Update the working memory and retrieve the relative information as $E_{o u t}$   \n7: Given $E_{o u t}$ , predict actions $\\tilde{a}_{t}$ , reward $\\tilde{r}_{t}$ , and return-to-go ${\\tilde{R}}_{t}$ .  \n8: Compute the loss according to Eqn. 1.   \n9: Update all modules parameters.   \n10: end for   \n11: end for  \n\nAlgorithm 2 Fine-tuning DT-Mem  \n\n$\\hat{B}^{q},\\hat{B}^{k},\\hat{B}^{v},\\hat{A}^{q},\\hat{A}^{k},\\hat{A}^{v},B^{q},A^{q},B^{k},A^{k}$ Require: Fine-tuning dataset $\\mathcal{T}^{i}~\\in~T^{t e s t}$ .dataset $\\mathcal{D}^{i}$ for $\\mathcal{T}^{i}$ .Initialize LoRA parameters 1: for T steps do   \n2: Split trajectories into different segments with length $\\mathbf{K}$ and calculate return-to-go in the input sequence.   \n3: Given $\\hat{\\tau}_{t+1:t+K}$ , compute the sequence embedding $e_{s e q}$ .  \n4: Update working memory using $\\hat{Q}\\,=\\,M(\\hat{W}^{q}+\\hat{B}^{q}\\bar{A}^{q})$ ,$\\hat{K}\\,=\\,M(\\hat{W}^{k}\\,+\\,\\hat{B}^{k}\\hat{A}^{k}),\\hat{V}\\,=$ $M(\\hat{W}^{v}+\\hat{B}^{v}\\hat{A}^{v})$ ,$Q=M(W^{q}+B^{q}A^{q}),K=M(W^{k}+B^{k}A^{k})$   \n5: Retrieve the relative information as $E_{o u t}$   \n6: Given $E_{o u t}$ , predict actions $\\tilde{a}_{t}$ , reward $\\tilde{r}_{t}$ , and return-to-go ${\\tilde{R}}_{t}$ .  \n7: Compute the loss according to Eqn. 1.   \n8: Update LoRA parameters only.   \n9: end for"}], "task_step_question_answer": "\u5728\u65e0\u76d1\u7763\u548c\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u9762\uff0cLayerNorm\u548cRMSNorm\u7684\u8868\u73b0\u4e5f\u503c\u5f97\u5173\u6ce8\u3002\u4f8b\u5982\uff0c\u5728\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0c\u7814\u7a76\u8005\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\uff0c\u5728\u672a\u6807\u6ce8\u7684ImageNet\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u73af\u5883\u4e0b\u7684\u5206\u7c7b\u6027\u80fd\u3002LayerNorm\u901a\u8fc7\u5176\u5168\u5c40\u5f52\u4e00\u5316\u7684\u7279\u6027\uff0c\u80fd\u591f\u66f4\u597d\u5730\u6355\u6349\u6570\u636e\u4e2d\u7684\u6f5c\u5728\u7ed3\u6784\uff0c\u63d0\u5347\u6a21\u578b\u5728\u65e0\u6807\u7b7e\u6570\u636e\u4e0a\u7684\u8868\u73b0\u3002\u800cRMSNorm\u5219\u901a\u8fc7\u5176\u9ad8\u6548\u7684\u8ba1\u7b97\u8fc7\u7a0b\uff0c\u51cf\u5c11\u4e86\u8bad\u7ec3\u65f6\u95f4\uff0c\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u3002\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u4e2d\uff0c\u8de8\u6a21\u6001\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u901a\u8fc7\u5229\u7528\u6587\u672c\u548c\u56fe\u50cf\u4e4b\u95f4\u7684\u5173\u8054\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u73af\u5883\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u3002LayerNorm\u5728\u5904\u7406\u590d\u6742\u6570\u636e\u5206\u5e03\u65f6\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u7a33\u5b9a\u6027\uff0c\u800cRMSNorm\u5219\u5728\u8ba1\u7b97\u6548\u7387\u4e0a\u5177\u6709\u4f18\u52bf\u3002\u672a\u6765\u7684\u7814\u7a76\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63a2\u7d22\u5982\u4f55\u7ed3\u5408LayerNorm\u548cRMSNorm\u7684\u4f18\u70b9\uff0c\u8bbe\u8ba1\u51fa\u66f4\u9ad8\u6548\u7684\u5f52\u4e00\u5316\u65b9\u6cd5\uff0c\u4ee5\u5e94\u5bf9\u65e5\u76ca\u590d\u6742\u7684\u5b9e\u9645\u9700\u6c42\u3002", "ref_task_step_id": ""}, "__type__": "task_step_node"}, "931c6468-98e1-420f-b767-f61edb29e041": {"__data__": {"id_": "931c6468-98e1-420f-b767-f61edb29e041", "metadata": {}, "relationships": {}, "hash": "", "start_task_context": "\u5927\u6a21\u578b\u4e2d\u7684LayerNorm\u548cRMSNorm\u6709\u4ec0\u4e48\u533a\u522b\uff1f", "aemo_representation_context": "### Step-by-Step Decomposition of Computer Science Research Content\n\n#### 1. \u5206\u6790\u8fd1\u51e0\u5e74\u7814\u7a76\u9886\u57df\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba\n- **\u6280\u672f\u6846\u67b6**\uff1a\u8fd1\u5e74\u6765\uff0c\u6df1\u5ea6\u5b66\u4e60\u3001\u5f3a\u5316\u5b66\u4e60\u3001\u56fe\u795e\u7ecf\u7f51\u7edc\u7b49\u6846\u67b6\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u9886\u57df\u5360\u636e\u4e3b\u5bfc\u5730\u4f4d\u3002\u5177\u4f53\u6846\u67b6\u5982Transformer\u3001BERT\u3001GNN\u7b49\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002\n- **\u65b9\u6cd5\u8bba**\uff1a\u7814\u7a76\u65b9\u6cd5\u9010\u6e10\u4ece\u5355\u4e00\u6a21\u578b\u8f6c\u5411\u591a\u6a21\u578b\u878d\u5408\uff0c\u4ece\u76d1\u7763\u5b66\u4e60\u6269\u5c55\u5230\u65e0\u76d1\u7763\u548c\u81ea\u76d1\u7763\u5b66\u4e60\uff0c\u5f3a\u8c03\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002\n\n#### 2. \u7814\u7a76\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u4e3b\u8981\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0e\u53d8\u4f53\n- **\u5e94\u7528**\uff1a\u4f8b\u5982\uff0cTransformer\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u4e2d\u88ab\u5e7f\u6cdb\u5e94\u7528\uff0cBERT\u53d8\u4f53\u5982RoBERTa\u3001ALBERT\u5728\u6587\u672c\u5206\u7c7b\u3001\u95ee\u7b54\u7cfb\u7edf\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff1bGNN\u5728\u63a8\u8350\u7cfb\u7edf\u3001\u77e5\u8bc6\u56fe\u8c31\u7b49\u9886\u57df\u6709\u663e\u8457\u5e94\u7528\u3002\n- **\u53d8\u4f53**\uff1a\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\uff0c\u7814\u7a76\u8005\u63d0\u51fa\u4e86\u591a\u79cd\u53d8\u4f53\uff0c\u5982\u9488\u5bf9\u957f\u5e8f\u5217\u5904\u7406\u7684Transformer-XL\uff0c\u9488\u5bf9\u591a\u6a21\u6001\u6570\u636e\u7684ViLBERT\u7b49\u3002\n\n#### 3. \u8bc4\u4f30\u5b66\u672f\u754c\u7684\u6280\u672f\u8fdb\u6b65\u4e0e\u5c40\u9650\u6027\n- **\u6280\u672f\u8fdb\u6b65**\uff1a\u6a21\u578b\u6027\u80fd\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\uff0c\u8ba1\u7b97\u6548\u7387\u4e5f\u6709\u6240\u63d0\u9ad8\uff0c\u5982EfficientNet\u5728\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u8868\u73b0\u3002\n- **\u5c40\u9650\u6027**\uff1a\u4f9d\u7136\u5b58\u5728\u6a21\u578b\u504f\u5dee\uff08\u5982\u6027\u522b\u504f\u89c1\uff09\u3001\u6570\u636e\u4f9d\u8d56\uff08\u5982\u5bf9\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\uff09\u7b49\u95ee\u9898\uff0c\u6cdb\u5316\u80fd\u529b\u6709\u5f85\u8fdb\u4e00\u6b65\u63d0\u5347\u3002\n\n#### 4. \u63a2\u8ba8\u8ba1\u7b97\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\n- **\u9002\u7528\u6027**\uff1a\u7814\u7a76\u6a21\u578b\u5728\u4e0d\u540c\u9886\u57df\uff08\u5982\u533b\u7597\u3001\u91d1\u878d\uff09\u548c\u591a\u6a21\u6001\u6570\u636e\uff08\u5982\u6587\u672c+\u56fe\u50cf\uff09\u4e0a\u7684\u8868\u73b0\uff0c\u5982\u591a\u6a21\u6001Transformer\u6a21\u578b\u3002\n- **\u6cdb\u5316\u80fd\u529b**\uff1a\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\u3001\u5143\u5b66\u4e60\u7b49\u65b9\u6cd5\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f7f\u5176\u80fd\u591f\u5e94\u5bf9\u66f4\u591a\u73b0\u5b9e\u5e94\u7528\u573a\u666f\u3002\n\n#### 5. \u5206\u6790\u6700\u65b0\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027\n- **\u7a33\u5b9a\u6027**\uff1a\u7814\u7a76\u7b97\u6cd5\u5728\u590d\u6742\u3001\u52a8\u6001\u73af\u5883\u4e0b\u7684\u8868\u73b0\uff0c\u5982\u5bf9\u6297\u8bad\u7ec3\u3001\u9c81\u68d2\u4f18\u5316\u7b49\u65b9\u6cd5\u7684\u5e94\u7528\u3002\n- **\u5bb9\u9519\u6027**\uff1a\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\u7684\u9002\u5e94\u6027\uff0c\u5982\u5206\u5e03\u5f0f\u8bad\u7ec3\u4e2d\u7684\u68af\u5ea6\u538b\u7f29\u3001\u6a21\u578b\u5e76\u884c\u7b49\u6280\u672f\u3002\n\n#### 6. \u8bc4\u4f30\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218\n- **\u672a\u6765\u65b9\u5411**\uff1a\u5982\u53ef\u89e3\u91caAI\u3001\u8054\u90a6\u5b66\u4e60\u3001\u9690\u79c1\u4fdd\u62a4\u7b49\u65b0\u5174\u7814\u7a76\u65b9\u5411\u3002\n- **\u6311\u6218**\uff1a\u5982\u4f55\u5e73\u8861\u6a21\u578b\u6027\u80fd\u4e0e\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\uff0c\u5982\u4f55\u89e3\u51b3\u6570\u636e\u504f\u89c1\u548c\u4f26\u7406\u95ee\u9898\u7b49\u3002\n\n### \u5927\u6a21\u578b\u4e2d\u7684LayerNorm\u548cRMSNorm\u7684\u533a\u522b\n\n#### LayerNorm\uff08Layer Normalization\uff09\n- **\u5b9a\u4e49**\uff1aLayerNorm\u662f\u5bf9\u6bcf\u4e2a\u6837\u672c\u7684\u6bcf\u4e2a\u7279\u5f81\u8fdb\u884c\u5f52\u4e00\u5316\uff0c\u4f7f\u5176\u5747\u503c\u548c\u65b9\u5dee\u5206\u522b\u63a5\u8fd10\u548c1\u3002\n- **\u516c\u5f0f**\uff1a\n  \\[\n  \\text{LayerNorm}(x) = \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} \\cdot \\gamma + \\beta\n  \\]\n  \u5176\u4e2d\uff0c\\(\\mu\\)\u548c\\(\\sigma^2\\)\u5206\u522b\u662f\u7279\u5f81\u5747\u503c\u548c\u65b9\u5dee\uff0c\\(\\gamma\\)\u548c\\(\\beta\\)\u662f\u53ef\u5b66\u4e60\u53c2\u6570\uff0c\\(\\epsilon\\)\u662f\u5fae\u5c0f\u5e38\u6570\u4ee5\u9632\u6b62\u9664\u96f6\u3002\n- **\u5e94\u7528**\uff1a\u5e7f\u6cdb\u7528\u4e8eTransformer\u6a21\u578b\u4e2d\uff0c\u6709\u52a9\u4e8e\u7f13\u89e3\u68af\u5ea6\u6d88\u5931\u95ee\u9898\uff0c\u63d0\u5347\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002\n\n#### RMSNorm\uff08Root Mean Square Normalization\uff09\n- **\u5b9a\u4e49**\uff1aRMSNorm\u662f\u5bf9\u6bcf\u4e2a\u6837\u672c\u7684\u6bcf\u4e2a\u7279\u5f81\u8fdb\u884c\u5f52\u4e00\u5316\uff0c\u4f46\u53ea\u5f52\u4e00\u5316\u65b9\u5dee\uff0c\u4e0d\u5f52\u4e00\u5316\u5747\u503c\u3002\n- **\u516c\u5f0f**\uff1a\n  \\[\n  \\text{RMSNorm}(x) = \\frac{x}{\\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} x_i^2 + \\epsilon}} \\cdot \\gamma + \\beta\n  \\]\n  \u5176\u4e2d\uff0c\\(n\\)\u662f\u7279\u5f81\u6570\u91cf\uff0c\\(\\gamma\\)\u548c\\(\\beta\\)\u662f\u53ef\u5b66\u4e60\u53c2\u6570\uff0c\\(\\epsilon\\)\u662f\u5fae\u5c0f\u5e38\u6570\u3002\n- **\u5e94\u7528**\uff1a\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0cRMSNorm\u6bd4LayerNorm\u66f4\u7b80\u5355\uff0c\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\uff0c\u5c24\u5176\u5728\u5927\u89c4\u6a21\u6a21\u578b\u8bad\u7ec3\u4e2d\u3002\n\n#### \u4e3b\u8981\u533a\u522b\n1. **\u5f52\u4e00\u5316\u65b9\u5f0f**\uff1a\n   - LayerNorm\u540c\u65f6\u5f52\u4e00\u5316\u5747\u503c\u548c\u65b9\u5dee\u3002\n   - RMSNorm\u53ea\u5f52\u4e00\u5316\u65b9\u5dee\u3002\n2. **\u8ba1\u7b97\u590d\u6742\u5ea6**\uff1a\n   - LayerNorm\u9700\u8981\u8ba1\u7b97\u5747\u503c\u548c\u65b9\u5dee\uff0c\u8ba1\u7b97\u91cf\u7a0d\u5927\u3002\n   - RMSNorm\u53ea\u9700\u8ba1\u7b97\u65b9\u5dee\uff0c\u8ba1\u7b97\u91cf\u8f83\u5c0f\u3002\n3. **\u6548\u679c**\uff1a\n   - LayerNorm\u5728\u8bb8\u591a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u5728NLP\u9886\u57df\u3002\n   - RMSNorm\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u8868\u73b0\u76f8\u4f3c\u751a\u81f3\u66f4\u4f18\uff0c\u4e14\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u3002\n\n\u901a\u8fc7\u4e0a\u8ff0\u5206\u6790\uff0c\u53ef\u4ee5\u66f4\u597d\u5730\u7406\u89e3LayerNorm\u548cRMSNorm\u5728\u5927\u6a21\u578b\u4e2d\u7684\u5e94\u7528\u53ca\u5176\u4f18\u7f3a\u70b9\uff0c\u4ece\u800c\u5728\u5177\u4f53\u4efb\u52a1\u4e2d\u9009\u62e9\u5408\u9002\u7684\u5f52\u4e00\u5316\u65b9\u6cd5\u3002", "task_step_name": "\u63a2\u8ba8\u8ba1\u7b97\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u4e0e\u6cdb\u5316\u80fd\u529b", "task_step_description": "\u7814\u7a76\u6a21\u578b\u5728\u4e0d\u540c\u9886\u57df\uff08\u5982\u533b\u7597\u3001\u91d1\u878d\uff09\u548c\u591a\u6a21\u6001\u6570\u636e\uff08\u5982\u6587\u672c+\u56fe\u50cf\uff09\u4e0a\u7684\u8868\u73b0\uff0c\u5982\u591a\u6a21\u6001Transformer\u6a21\u578b\u3002\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\u3001\u5143\u5b66\u4e60\u7b49\u65b9\u6cd5\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f7f\u5176\u80fd\u591f\u5e94\u5bf9\u66f4\u591a\u73b0\u5b9e\u5e94\u7528\u573a\u666f\u3002", "task_step_level": "3", "task_step_question": "### \u95ee\u9898\n\n\u5728\u63a2\u8ba8\u8ba1\u7b97\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\u65f6\uff0cLayerNorm\u548cRMSNorm\u8fd9\u4e24\u79cd\u5f52\u4e00\u5316\u65b9\u6cd5\u5728\u591a\u6a21\u6001Transformer\u6a21\u578b\u4e2d\u7684\u8868\u73b0\u5982\u4f55\uff1f\u5177\u4f53\u6765\u8bf4\uff0c\u5728\u533b\u7597\u548c\u91d1\u878d\u9886\u57df\u7684\u591a\u6a21\u6001\u6570\u636e\uff08\u5982\u6587\u672c+\u56fe\u50cf\uff09\u4e0a\uff0c\u54ea\u79cd\u5f52\u4e00\u5316\u65b9\u6cd5\u5728\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u66f4\u5177\u4f18\u52bf\uff1f\u6b64\u5916\uff0c\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\u548c\u5143\u5b66\u4e60\u7b49\u65b9\u6cd5\uff0cLayerNorm\u548cRMSNorm\u5728\u5e94\u5bf9\u4e0d\u540c\u73b0\u5b9e\u5e94\u7528\u573a\u666f\u65f6\u7684\u7a33\u5b9a\u6027\u548c\u5bb9\u9519\u6027\u6709\u4f55\u5dee\u5f02\uff1f", "task_step_question_context": [{"ref_id": "455038427552559154", "chunk_id": "6", "score": 0.34375, "text": "# 3.3 A TRANSFORMATION PER BLOCK\nNow that every LayerNorm in the transformer has been converted to RMSNorm, we can select any $\\mathbf{Q}$ to modify the model. Our initial plan was to collect signals from the model, construct an orthogonal matrix using those signals and to delete parts of the network. We quickly saw that the signals at different blocks of the network were not aligned, and that we would need to apply a different orthogonal matrix at each block, $\\mathbf{Q}_{\\ell}$ .  \n\nAllowing the orthogonal matrix used in each block to differ can be shown to leave the model unchanged using the same proof as Theorem 1 ,  \n\n  \nFigure 3: Converting a transformer network from LayerNorm to RMSNorm: the scale matrix diag $(\\alpha)$ is absorbed into the subsequent matrix $\\mathbf{W}_{\\mathrm{in}}$ . Figure shows the block in combined colors. We use $(\\alpha)$ for brevity. The mean-subtraction matrix $\\mathbf{M}$ is applied to each matrix $\\mathbf{W}_{\\mathrm{out}}$ . Layernorm becomes RMSNorm, up to a constant $\\bar{\\sqrt{D}}$ (not shown). Here, the scaling $(\\alpha^{\\prime})$ comes from the previous block.  \n\n  \nFigure 4: With the network converted to RMSNorm (see Figure 3 ), we apply the computational-invariance idea. The input weight matrices $\\mathrm{diag}(\\alpha)\\mathbf{W}_{\\mathrm{in}}$ are pre-multiplied by $\\mathbf{Q}^{\\top}$ . The output matrices $\\mathbf{W}_{\\mathrm{out}}\\mathbf{M}$ are post-multiplied by $\\mathbf{Q}$ . In the skip-connection, a new linear layer is added $\\mathbf{Q}_{\\ell}^{\\top}\\mathbf{Q}_{\\ell+1}$ . After these modifications, the matrices can be sliced (hatched areas).  \n\nwith the exception of line 5 of Algorithm 1 . Here we see that the residual connection and the output of the block must have the same rotation. To fix this, we modify the residual connection by applying the linear transformation applied to different blocks with the additional linear operation in the residual connection. Unlike the $\\mathbf{Q}_{\\ell-1}^{\\top}\\mathbf{Q}_{\\ell}$ \u2212to the residual. Figure 4 shows how different rotations can be modifications to the weight matrices, these additional operations cannot be pre-computed and add a small $(D\\times D)$ overhead to the model. Nonetheless, they are needed to allow slicing the model (Section 3.4 ) and we see real speedup overall (Section 4 ).  \n\nTo compute the matrices $\\mathbf{Q}_{\\ell}$ , we use PCA. We select a calibration dataset from the training set, run it through the model (after converting LayerNorm operations into RMSNorm), and extract the orthogonal matrix of the layer. We use the output of the transformed network to calculate the orthogonal matrices of the next layers. More precisely, if $\\mathbf{X}_{\\ell,i}$ is the output of the $\\ell^{\\mathrm{th}}$ RMSNorm block for the $i^{\\mathrm{th}}$ sequence in the calibration dataset, we compute  \n\n$$\n\\mathbf{C}_{\\ell}=\\sum_{i}\\mathbf{X}_{\\ell,i}^{\\top}\\mathbf{X}_{\\ell,i}\n$$  \n\nand set $\\mathbf{Q}_{\\ell}$ to the be the eigenvectors of $\\mathbf{C}_{\\ell}$ , sorted by decreasing eigenvalues.\n\n# 3.4 SLICING\nThe goal of Principal Component Analysis is usually to take a data matrix $\\mathbf{X}$ and compute a lower dimensional representation $\\mathbf{Z}$ , and an approximate reconstruction $\\tilde{\\mathbf{X}}$ :  \n\n$$\n\\mathbf{Z}=\\mathbf{X}\\mathbf{Q}\\mathbf{D}\\,,\\qquad\\tilde{\\mathbf{X}}=\\mathbf{Z}\\mathbf{D}^{\\top}\\mathbf{Q}^{\\top}\\,.\n$$  \n\nwhere $\\mathbf{Q}$ is the ectors of ${\\bf X}^{\\top}{\\bf X}$ , and $\\mathbf{D}$ is a $D\\times D_{\\mathrm{small}}$ deletion matrix (containing $D_{\\mathrm{small}}$ The reconstruction is columns of the $D\\times D$ \u00d7$L_{2}$ identity matrix), which removes some of the columns of the matrix to the left. optimal, in the sense that QD is a linear mapping that minimizes $\\lVert\\mathbf{X}-\\tilde{\\mathbf{X}}\\rVert^{2}$ .  \n\nWhen we apply PCA to the signal matrix $\\mathbf{X}$ bween blocks, we never materialize the $N\\times D$ signal matrix, but we apply the deletion matrix Dto the operations preceding and succeeding the construction of that matrix, which have already been multiplied by $\\mathbf{Q}$ in the above. We delete rows of $\\mathbf{W}_{\\mathrm{in}}$ that we have inserted into the residual connection (see Figure and columns of $\\mathbf{W}_{\\mathrm{out}}$ and $\\mathbf{W}_{\\mathrm{embd}}$ . We also delete both rows 4 ). and columns of the matrix $\\mathbf{Q}_{\\ell-1}^{\\top}\\mathbf{Q}_{\\ell}$ \u2212\n\n# 4 EXPERIMENTAL VALIDATION\nSetup We use HuggingFace Transformers ( Wolf et al. ,2019 ) to implement our code with PyTorch (Paszke et al. ,2019 ). The computation of $\\mathbf{Q}$ is performed on a single H100 GPU with 80GB of memory, taking approximately 3.5 hours to complete for the L LAMA -2 70B model. During the PCA calculation, we use double precision for computing the eigenvectors of the covariance matrix. We find that using single precision for eigenvector calculations in PyTorch leads to a discrepancy in the final accuracy, as detailed in Appendix A.2 .  \n\nWe experiment with two different calibration sets: 1024 samples from the WikiText-2 training dataset ( Merity et al. ,2016 ) and 5000 samples from the Alpaca training dataset ( Taori et al. ,2023 ). Sequence lengths are chosen as the maximum of each language model. An ablation study on the calibration set size and sequence length is presented in Appendix A.3 .  \n\nModels, Tasks, and GPUs We evaluate all our experiments on OPT ( Zhang et al. ,2022 ), L LAMA -2 (Touvron et al. ,2023 ) model families, and additionally evaluate Phi-2 (in our zero-shot task) experiments. We exclude OPT 175B, as it is outperformed by smaller L LAMA -2 models. Nonetheless, we anticipate that this larger model will yield improved results, as larger models typically offer more promising opportunities for compression (see Section 4.1 ). We evaluate our scheme on both language generation as well as popular zero-shot tasks. To demonstrate the comprehensive speedup achieved by SliceGPT we use: Quadro RTX6000 GPUs with 24GB of memory as a representative example of consumer-level GPUs; 40GB A100s and 80GB H100s to provide datacenter-level benchmarks.  \n\nBaseline Setup We initially planned to compare our results against a scheme that pruned columns (or rows) with the smallest norm but found that this baseline was very poor, with the perplexity of the model soaring into the 1000s after pruning just a few columns. Instead, we compare SliceGPT against SparseGPT ( Frantar & Alistarh ,2023 ) employing a 2:4 sparsity ratio, as this is the only sparsity scheme which achieves speedup ( Mishra et al. ,2021 )."}, {"ref_id": "454895409734360760", "chunk_id": "3", "score": 0.24609375, "text": "# 3 Experiments and Results\nWe evaluate the performance of PreNorm and PostNorm for ZST on various datasets and language pairs. We then analyze the off-target rates and structural discrepancies between PreNorm and PostNorm to understand performance differences.  \n\n$$\n\\mathrm{LayerNorm}(\\mathbf{x})=\\frac{\\mathbf{x}-\\mathbf{E}(\\mathbf{x})}{\\sqrt{\\mathbf{V}(\\mathbf{x})}}\\cdot\\mathbf{g}+\\mathbf{b},\n$$  \n\nwhere $\\mathbf{g}$ and $\\mathbf{b}$ are trainable gain and bias. $\\mathbf{E}$ and $\\mathbf{V}$ indicate expectation and variance. LayerNorm is commonly used in two positions in the Transformer, as shown in Fig. 1 . PostNorm, which is the originally proposed setting of the Transformer ( Vaswani et al. ,2017 ), involves applying LayerNorm after each sub-module (i.e., selfattention or feed-forward network) and residual connections. PreNorm ( Baevski and Auli ,2019 ), on the other hand, involves applying LayerNorm directly before each sub-module and is known to stabilize Transformer training. While variants of Transformer LayerNorm like RMSNorm ( Zhang and Sennrich ,2019 ) have been proposed, the vanilla PreNorm and PostNorm are still the most widely adopted settings in current multilingual\n\n# 3.1 Experimental Settings\nDatasets We perform ZST experiments on three datasets: OPUS ( Zhang et al. ,2020 ), IWSLT ( Cettolo et al. ,2017 ), and Europarl ( Koehn ,2005 ). The statistics of the datasets are summarized in Table 1 .We include 7 ,4 , and 5 languages for each dataset. The training data consists of only English-centric sentence pairs, resulting in 30 ,6 , and 12 ZST directions for each dataset. The total number of parallel sentences for each dataset is 12 .00 M, 1 .38 M, and 15 .78 M, respectively. We apply BPE ( Sennrich et al. ,2016 ) with merge operations of 50 k, 40 k, and $50\\mathbf{k}$ to create a joint vocabulary for each dataset.  \n\nTraining We employ Transformer-base model for OPUS and IWSLT, and Transformer-big for Europarl, in accordance with the distinct sizes of training data. We consider the following settings: (1) PreNorm or PostNorm : PreNorm involves LayerNorm directly before each sub-module (i.e., self-attention or feed-forward network), while PostNorm applies LayerNorm after each sub-module and residual connections, as shown in Fig. 1 .(2) S-ENC-T-DEC or T-ENC : Source language tag on the encoder-side and target language tag on the decoder-side; or only target language tag on the encoder-side. Wu et al. (2021 ) showed that this setting impacts ZST for Transformer with PreNorm. (3) w/ or w/o Res. : With the residual connection for self-attention in the middle $(4^{t h})$ encoder layer or not. Liu et al. (2021 ) revealed that \u201cw/o Res.\u201d improves ZST for the model trained with PreNorm. We experiment this with different LayerNorm settings as this may reduce the potential of overfitting on supervised directions, then further impacts ZST, which aligns with our hypothesis.  \n\nTable 2: BLEU scores and off-target rates (shown in brackets) . We report the average score of three seeds; refer to Appendix Gfor BLEU score of each translation direction and seed. \u201cRes.\u201d indicates the residual connection of self-attention in the $4^{t h}$ encoder layer. We mark lower off-target rates and significantly higher BLEU scores ( Koehn ,2004 ) between PreNorm and PostNorm in bold for ZST.   \n\n\n<html><body><table><tr><td>#</td><td>Layer Norm</td><td>Language Tag</td><td>Res.</td><td></td><td>Zero-shot</td><td></td><td></td><td>Supervised</td><td></td></tr><tr><td>0</td><td></td><td>Pivot</td><td></td><td>OPUS 21.8</td><td>IWSLT 20.0</td><td>Europarl 29.5</td><td>OPUS</td><td>IWSLT</td><td>Europarl</td></tr><tr><td>1</td><td>PreNorm</td><td>S-ENC-T-DEC</td><td>w/</td><td>10.1 (42.19%)</td><td>4.9 (64.84%)</td><td>24.9 ( 7.73%)</td><td>33.7</td><td>31.5</td><td>34.3</td></tr><tr><td>2</td><td>PostNorm</td><td>S-ENC-T-DEC</td><td>w/</td><td>16.8 ( 8.59%)</td><td>12.4 (10.61%)</td><td>29.2( 0.34%)</td><td>33.9</td><td>31.5</td><td>34.5</td></tr><tr><td>3</td><td>PreNorm</td><td>T-ENC</td><td>w/</td><td>13.3 (22.99%)</td><td>13.7 ( 3.98%)</td><td>29.5( 0.23%)</td><td>33.7</td><td>31.6</td><td>34.4</td></tr><tr><td>4</td><td>PostNorm</td><td>T-ENC</td><td>w/</td><td>14.0 (22.86%)</td><td>15.5 ( 4.59%)</td><td>30.8 ( 0.11%)</td><td>34.1</td><td>31.5</td><td>34.5</td></tr><tr><td>5</td><td>PreNorm</td><td>S-ENC-T-DEC</td><td>w/o</td><td>14.3 (20.67%)</td><td>8.0 (50.16%)</td><td>16.7 (41.87%)</td><td>33.6</td><td>30.9</td><td>34.3</td></tr><tr><td>6</td><td>PostNorm</td><td>S-ENC-T-DEC</td><td>w/o</td><td>16.0 (15.27%)</td><td>17.4 (1.83%)</td><td>29.0 ( 0.41%)</td><td>33.8</td><td>30.7</td><td>34.4</td></tr><tr><td>7</td><td>PreNorm</td><td>T-ENC</td><td>w/o</td><td>13.4 (27.15%)</td><td>16.2 ( 1.54%)</td><td>29.9 ( 2.15%)</td><td>33.5</td><td>30.9</td><td>34.3</td></tr><tr><td>8</td><td>PostNorm</td><td>T-ENC</td><td>w/o</td><td>13.9 (26.68%)</td><td>17.8 (1.50%)</td><td>30.8 ( 0.13%)</td><td>33.9</td><td>30.6</td><td>34.4</td></tr></table></body></html>  \n\nThe settings above lead to eight different combinations, shown in Table 2 (#1 - #8). Additional training details are in Appendix A .\n\n# 3.2 Main Results\nWe evaluate ZST systems using SacreBLEU ( Post ,2018 ) and off-target rates. We report in Table 2 BLEU scores for both zero-shot and supervised directions. For ZST, we also present pivot-based translation results as a reference. Implementation details of evaluation can be found in Appendix B.Our findings are as follows:  \n\nPreNorm vs. PostNorm :We find that PostNorm consistently yields better BLEU scores than PreNorm for ZST across various language tag and residual connection settings, while their performance is comparable for supervised directions.  \n\nImpact of Language Tag and Residual Connection: We observe that using the \u201cT-ENC\u201d language tag and \u201cw/ Res.\u201d improves ZST performance for IWSLT, which aligns with the findings of $\\mathrm{Wu}$ et al. (2021 ) and Liu et al. (2021 ). Nevertheless, the best performance is achieved using \u201cw/ Res.\u201d for PostNorm with \u201cS-ENC-T-DEC\u201d and \u201cT-ENC\u201d tags for OPUS and Europarl, respectively (#2 and #4). Given that Wu et al. (2021 ) and Liu et al. (2021 )used PreNorm as the default setting (#2, #4, #6 and #8 are unreported results in their work), our results emphasize the need to consider PostNorm as the default setting for ZST, while the language tag and residual connection settings have less impact.  \n\nOff-target Rates : Off-target rates help understand the different BLEU score gaps between PreNorm and PostNorm, which ranges from 0 .5 to 12 .3 BLEU points. For PreNorm and PostNorm with the \u201cT-ENC\u201d language tag (#3, #4, #7, and #8), they have similar off-target rates, with a discrepancy ranging from $-0.61\\%$ to $2.02\\%$ , which results in narrow BLEU score gaps, ranging from 0 .5 to 1 .8 points. However, for PreNorm and PostNorm with the \u201cS-ENC-T-DEC\u201d language tag (#1, #2, #5, and #6), the off-target rates show a more considerable discrepancy, ranging from $5.40\\%$ to $54.23\\%$ , resulting in BLEU score gaps from 1 .7 to 12 .3 points. Further analysis of the nature of Transformer hidden states in the next section explores the reason for these different off-target rates in translations."}, {"ref_id": "454845744505973136", "chunk_id": "6", "score": 0.1591796875, "text": "# Introduction\nLarge-scale models have drawn significant attention for their remarkable performance across a spectrum of applications (Ramesh et al. 2022; Irwin et al. 2022; Lee et al. 2020). Considering that training large models from scratch requires tremendous computational costs, fine-tuning has become a predominant approach to transfer knowledge from large pretrained models to downstream tasks (Long et al. 2015; Guo et al. 2020). However, this paradigm heavily relies on labeled training data and suffers from significant performance decay when target data exhibits distribution shift from pretraining data (Ben-David et al. 2010). Moreover, we usually have multiple pretrained models trained on different sources or architectures on hand, e.g., medical diagnostic models trained on distinct regions or patient groups. Demands to maximally utilizing knowledge from multiple pretrained models are common in real world applications. To this end, Multi-Source-Free Domain Adaptation (MSFDA) (Ahmed et al. 2021; Dong et al. 2021) emerges as a promising technique to address these challenges by enabling holistic adaptation of multiple pretrained source models to an unlabeled target domain, while not accessing source training data.  \n\nTable 1: Computation overhead and performance comparison between different methods on DomainNet.   \n\n\n<html><body><table><tr><td>Method</td><td>Param.</td><td>Backbone</td><td>Acc.</td><td>Throughput</td></tr><tr><td>CAiDA</td><td>120.2M</td><td>ResNet50</td><td>46.8</td><td>91</td></tr><tr><td>PMTrans</td><td>447.4M</td><td>Swin</td><td>59.1</td><td>46</td></tr><tr><td>ATEN (ours)</td><td>4.9M</td><td>Swin</td><td>59.1</td><td>970</td></tr><tr><td>Bi-ATEN (ours)</td><td>10.6M</td><td>Swin</td><td>59.6</td><td>369</td></tr></table></body></html>  \n\nExisting MSFDA methods (Ahmed et al. 2021; Dong et al. 2021; Han et al. 2023; Shen, Bu, and Wornell 2023) typically tackle the problem via a two-step framework, i.e., (1) Tune each source model thoroughly towards target domain, and (2) Learn source importance weights to assemble the source models. However, their overwhelming limitations in computational efficiency and scalability prevent their applications on large-scale problems. For step $(I)$ , the number of models to tune increases linearly along with the number of source domains, which could become unacceptable for large-scale problems with abundant source domains. The necessity of tuning all parameters for each model also makes it infeasible to scale up these methods to larger models. In Table 5 we compare the performance and trainable parameters of CAiDA (Dong et al. 2021), PMTrans 1 (Zhu, Bai, and Wang 2023) and our methods on a challenging benchmark DomainNet (Peng et al. 2019) with 6 domains. As a typical MSFDA framework, CAiDA performs poorly due to limited performance of ResNet-50 (He et al. 2016) backbone. By equipping a stronger backbone SwinTransformer (Liu et al. 2021), a potential performance boost of $+12.3\\%$ is achieved at a cost of four times of parameters to tune. On the other hand, we aim to achieve superior performance by equipping SwinTransformer while demanding significantly less training cost, presenting a more feasible and agile solution for MSFDA on large models. For step (2) , current MSFDA methods learn domain-level ensemble weights, applying identical ensemble strategy across all target instances. Although the learned weights are intuitively interpretable in terms of domain transferablity, they unavoidably introduce misalignment and bias at instance-level. This controversy inherently introduces a trade-off between instance specificity and domain consistency of ensemble weights, which has not been well exploited by existing methods.  \n\n  \nFigure 1: Illustration of instance specificity and domain consistency. Dots are weights assigned to each target sample.  \n\nRecent success of model ensemble methods (Shu et al. 2021, 2022) suggests that it is effective to transfer knowledge by designing adaptive ensemble weights. While optimal strategies are hard to learn (Mohammed and Kora 2023), we resort to slight tuning of several domain-specific bottleneck layers, costing less than $0.1\\%$ of tuning the whole model. As stated above, the key to designing effective weights is to exploit both domain-level transferabilities and instance-level individual characteristics, as illustrated by Fig. 8. Existing MSFDA methods learn weights solely from feature representations, neglecting the potential transferability mismatch between features and outputs, i.e., transferable target features do not always lead to accurate predictions. To address this issue, we propose to introduce additional semantic information from classifiers for deriving weights. For each feature representation, we first learn intra-domain weights to mitigate transferability mismatch by finding the most compatible classifier that produces unbiased outputs. With unbiased outputs from the selected source classifier, we further learn inter-domain ensemble weights that combine source outputs into the final result. We propose a novel Bilevel ATtention ENsemble (Bi-ATEN) to effectively learn the two weights through attention mechanisms. Bi-ATEN is capable of tailoring its ensemble decisions to the particularities of each instance, while maintaining the broader transferability trends that are consistent across domains. This balance is essential for accurate domain adaptation, where a model needs to leverage domain-specific knowledge without losing the overarching patterns that drive adaptation.  \n\nThe proposed Bi-ATEN can be simplified into interdomain ATtention ENsemble (ATEN) and plugged into existing MSFDA methods by replacing their weight-learning module. Although leaning towards domain consistency in the specificity-consistency balance, ATEN still exhibits clear performance boost over baseline methods, proving the efficacy of our design. In a nutshell, we achieve adaptation primarily by assuring instance specificity and domain consistency along with slight tuning of bottlenecks. Table 5 provides comprehensive comparison between our methods and existing methods. Our contributions can be summarized as: (1) We propose a novel framework to agilely handle MSFDA by learning fine-grained domain adaptive ensemble strategies. (2) We design an effective module Bi-ATEN that learns both intra-domain weights and inter-domain ensemble weights. Its light version ATEN can be equipped to existing MSFDA methods to boost performance. (3) Our method significantly reduces computational costs while achieving state-of-the-art performance, making it feasible for real-life transfer applications with large source-trained models. (4) Extensive experiments on three challenging benchmarks and detailed analysis demonstrates the success of our design."}], "task_step_question_answer": "\u5728\u672a\u6765\u7684\u7814\u7a76\u4e2d\uff0c\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63a2\u7d22\u5982\u4f55\u7ed3\u5408LayerNorm\u548cRMSNorm\u7684\u4f18\u70b9\uff0c\u8bbe\u8ba1\u51fa\u66f4\u9ad8\u6548\u7684\u5f52\u4e00\u5316\u65b9\u6cd5\u3002\u4f8b\u5982\uff0c\u53ef\u4ee5\u7814\u7a76\u5982\u4f55\u6839\u636e\u8f93\u5165\u6570\u636e\u7684\u7279\u6027\u52a8\u6001\u9009\u62e9\u4f7f\u7528LayerNorm\u6216RMSNorm\uff0c\u4ee5\u8fdb\u4e00\u6b65\u63d0\u5347\u6a21\u578b\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u3002\u6b64\u5916\uff0c\u8fd8\u53ef\u4ee5\u63a2\u7d22\u8fd9\u4e9b\u65b9\u6cd5\u5728\u65b0\u5174\u7814\u7a76\u9886\u57df\uff08\u5982\u9690\u79c1\u4fdd\u62a4\u3001\u8054\u90a6\u5b66\u4e60\u7b49\uff09\u4e2d\u7684\u5e94\u7528\uff0c\u4ee5\u5e94\u5bf9\u65e5\u76ca\u590d\u6742\u7684\u5b9e\u9645\u9700\u6c42\u3002\u5728\u9690\u79c1\u4fdd\u62a4\u9886\u57df\uff0cLayerNorm\u901a\u8fc7\u5176\u5168\u5c40\u5f52\u4e00\u5316\u7684\u7279\u6027\uff0c\u80fd\u591f\u66f4\u597d\u5730\u4fdd\u62a4\u654f\u611f\u6570\u636e\uff0c\u800cRMSNorm\u5219\u901a\u8fc7\u5176\u9ad8\u6548\u7684\u8ba1\u7b97\u8fc7\u7a0b\uff0c\u51cf\u5c11\u4e86\u9690\u79c1\u6cc4\u9732\u7684\u98ce\u9669\u3002\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\uff0cLayerNorm\u901a\u8fc7\u5176\u7a33\u5b9a\u6027\uff0c\u80fd\u591f\u6709\u6548\u7f13\u89e3\u4e0d\u540c\u5ba2\u6237\u7aef\u6570\u636e\u5206\u5e03\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002\u800cRMSNorm\u5219\u901a\u8fc7\u5176\u9ad8\u6548\u6027\uff0c\u51cf\u5c11\u4e86\u901a\u4fe1\u5f00\u9500\uff0c\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u3002\u8fd9\u4e9b\u65b0\u5174\u7814\u7a76\u65b9\u5411\u4e0d\u4ec5\u80fd\u591f\u63d0\u5347\u6a21\u578b\u7684\u6027\u80fd\uff0c\u8fd8\u80fd\u591f\u89e3\u51b3\u6a21\u578b\u504f\u5dee\u548c\u6570\u636e\u4f9d\u8d56\u95ee\u9898\uff0c\u4e3a\u5b66\u672f\u754c\u63d0\u4f9b\u65b0\u7684\u7814\u7a76\u601d\u8def\u3002", "ref_task_step_id": ""}, "__type__": "task_step_node"}, "3d3ab9bd-1567-4130-9630-e6b97d28fc08": {"__data__": {"id_": "3d3ab9bd-1567-4130-9630-e6b97d28fc08", "metadata": {}, "relationships": {}, "hash": "", "start_task_context": "\u5927\u6a21\u578b\u4e2d\u7684LayerNorm\u548cRMSNorm\u6709\u4ec0\u4e48\u533a\u522b\uff1f", "aemo_representation_context": "### Step-by-Step Decomposition of Computer Science Research Content\n\n#### 1. \u5206\u6790\u8fd1\u51e0\u5e74\u7814\u7a76\u9886\u57df\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba\n- **\u6280\u672f\u6846\u67b6**\uff1a\u8fd1\u5e74\u6765\uff0c\u6df1\u5ea6\u5b66\u4e60\u3001\u5f3a\u5316\u5b66\u4e60\u3001\u56fe\u795e\u7ecf\u7f51\u7edc\u7b49\u6846\u67b6\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u9886\u57df\u5360\u636e\u4e3b\u5bfc\u5730\u4f4d\u3002\u5177\u4f53\u6846\u67b6\u5982Transformer\u3001BERT\u3001GNN\u7b49\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002\n- **\u65b9\u6cd5\u8bba**\uff1a\u7814\u7a76\u65b9\u6cd5\u9010\u6e10\u4ece\u5355\u4e00\u6a21\u578b\u8f6c\u5411\u591a\u6a21\u578b\u878d\u5408\uff0c\u4ece\u76d1\u7763\u5b66\u4e60\u6269\u5c55\u5230\u65e0\u76d1\u7763\u548c\u81ea\u76d1\u7763\u5b66\u4e60\uff0c\u5f3a\u8c03\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002\n\n#### 2. \u7814\u7a76\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u4e3b\u8981\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0e\u53d8\u4f53\n- **\u5e94\u7528**\uff1a\u4f8b\u5982\uff0cTransformer\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u4e2d\u88ab\u5e7f\u6cdb\u5e94\u7528\uff0cBERT\u53d8\u4f53\u5982RoBERTa\u3001ALBERT\u5728\u6587\u672c\u5206\u7c7b\u3001\u95ee\u7b54\u7cfb\u7edf\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff1bGNN\u5728\u63a8\u8350\u7cfb\u7edf\u3001\u77e5\u8bc6\u56fe\u8c31\u7b49\u9886\u57df\u6709\u663e\u8457\u5e94\u7528\u3002\n- **\u53d8\u4f53**\uff1a\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\uff0c\u7814\u7a76\u8005\u63d0\u51fa\u4e86\u591a\u79cd\u53d8\u4f53\uff0c\u5982\u9488\u5bf9\u957f\u5e8f\u5217\u5904\u7406\u7684Transformer-XL\uff0c\u9488\u5bf9\u591a\u6a21\u6001\u6570\u636e\u7684ViLBERT\u7b49\u3002\n\n#### 3. \u8bc4\u4f30\u5b66\u672f\u754c\u7684\u6280\u672f\u8fdb\u6b65\u4e0e\u5c40\u9650\u6027\n- **\u6280\u672f\u8fdb\u6b65**\uff1a\u6a21\u578b\u6027\u80fd\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\uff0c\u8ba1\u7b97\u6548\u7387\u4e5f\u6709\u6240\u63d0\u9ad8\uff0c\u5982EfficientNet\u5728\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u8868\u73b0\u3002\n- **\u5c40\u9650\u6027**\uff1a\u4f9d\u7136\u5b58\u5728\u6a21\u578b\u504f\u5dee\uff08\u5982\u6027\u522b\u504f\u89c1\uff09\u3001\u6570\u636e\u4f9d\u8d56\uff08\u5982\u5bf9\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\uff09\u7b49\u95ee\u9898\uff0c\u6cdb\u5316\u80fd\u529b\u6709\u5f85\u8fdb\u4e00\u6b65\u63d0\u5347\u3002\n\n#### 4. \u63a2\u8ba8\u8ba1\u7b97\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\n- **\u9002\u7528\u6027**\uff1a\u7814\u7a76\u6a21\u578b\u5728\u4e0d\u540c\u9886\u57df\uff08\u5982\u533b\u7597\u3001\u91d1\u878d\uff09\u548c\u591a\u6a21\u6001\u6570\u636e\uff08\u5982\u6587\u672c+\u56fe\u50cf\uff09\u4e0a\u7684\u8868\u73b0\uff0c\u5982\u591a\u6a21\u6001Transformer\u6a21\u578b\u3002\n- **\u6cdb\u5316\u80fd\u529b**\uff1a\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\u3001\u5143\u5b66\u4e60\u7b49\u65b9\u6cd5\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f7f\u5176\u80fd\u591f\u5e94\u5bf9\u66f4\u591a\u73b0\u5b9e\u5e94\u7528\u573a\u666f\u3002\n\n#### 5. \u5206\u6790\u6700\u65b0\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027\n- **\u7a33\u5b9a\u6027**\uff1a\u7814\u7a76\u7b97\u6cd5\u5728\u590d\u6742\u3001\u52a8\u6001\u73af\u5883\u4e0b\u7684\u8868\u73b0\uff0c\u5982\u5bf9\u6297\u8bad\u7ec3\u3001\u9c81\u68d2\u4f18\u5316\u7b49\u65b9\u6cd5\u7684\u5e94\u7528\u3002\n- **\u5bb9\u9519\u6027**\uff1a\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\u7684\u9002\u5e94\u6027\uff0c\u5982\u5206\u5e03\u5f0f\u8bad\u7ec3\u4e2d\u7684\u68af\u5ea6\u538b\u7f29\u3001\u6a21\u578b\u5e76\u884c\u7b49\u6280\u672f\u3002\n\n#### 6. \u8bc4\u4f30\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218\n- **\u672a\u6765\u65b9\u5411**\uff1a\u5982\u53ef\u89e3\u91caAI\u3001\u8054\u90a6\u5b66\u4e60\u3001\u9690\u79c1\u4fdd\u62a4\u7b49\u65b0\u5174\u7814\u7a76\u65b9\u5411\u3002\n- **\u6311\u6218**\uff1a\u5982\u4f55\u5e73\u8861\u6a21\u578b\u6027\u80fd\u4e0e\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\uff0c\u5982\u4f55\u89e3\u51b3\u6570\u636e\u504f\u89c1\u548c\u4f26\u7406\u95ee\u9898\u7b49\u3002\n\n### \u5927\u6a21\u578b\u4e2d\u7684LayerNorm\u548cRMSNorm\u7684\u533a\u522b\n\n#### LayerNorm\uff08Layer Normalization\uff09\n- **\u5b9a\u4e49**\uff1aLayerNorm\u662f\u5bf9\u6bcf\u4e2a\u6837\u672c\u7684\u6bcf\u4e2a\u7279\u5f81\u8fdb\u884c\u5f52\u4e00\u5316\uff0c\u4f7f\u5176\u5747\u503c\u548c\u65b9\u5dee\u5206\u522b\u63a5\u8fd10\u548c1\u3002\n- **\u516c\u5f0f**\uff1a\n  \\[\n  \\text{LayerNorm}(x) = \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} \\cdot \\gamma + \\beta\n  \\]\n  \u5176\u4e2d\uff0c\\(\\mu\\)\u548c\\(\\sigma^2\\)\u5206\u522b\u662f\u7279\u5f81\u5747\u503c\u548c\u65b9\u5dee\uff0c\\(\\gamma\\)\u548c\\(\\beta\\)\u662f\u53ef\u5b66\u4e60\u53c2\u6570\uff0c\\(\\epsilon\\)\u662f\u5fae\u5c0f\u5e38\u6570\u4ee5\u9632\u6b62\u9664\u96f6\u3002\n- **\u5e94\u7528**\uff1a\u5e7f\u6cdb\u7528\u4e8eTransformer\u6a21\u578b\u4e2d\uff0c\u6709\u52a9\u4e8e\u7f13\u89e3\u68af\u5ea6\u6d88\u5931\u95ee\u9898\uff0c\u63d0\u5347\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002\n\n#### RMSNorm\uff08Root Mean Square Normalization\uff09\n- **\u5b9a\u4e49**\uff1aRMSNorm\u662f\u5bf9\u6bcf\u4e2a\u6837\u672c\u7684\u6bcf\u4e2a\u7279\u5f81\u8fdb\u884c\u5f52\u4e00\u5316\uff0c\u4f46\u53ea\u5f52\u4e00\u5316\u65b9\u5dee\uff0c\u4e0d\u5f52\u4e00\u5316\u5747\u503c\u3002\n- **\u516c\u5f0f**\uff1a\n  \\[\n  \\text{RMSNorm}(x) = \\frac{x}{\\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} x_i^2 + \\epsilon}} \\cdot \\gamma + \\beta\n  \\]\n  \u5176\u4e2d\uff0c\\(n\\)\u662f\u7279\u5f81\u6570\u91cf\uff0c\\(\\gamma\\)\u548c\\(\\beta\\)\u662f\u53ef\u5b66\u4e60\u53c2\u6570\uff0c\\(\\epsilon\\)\u662f\u5fae\u5c0f\u5e38\u6570\u3002\n- **\u5e94\u7528**\uff1a\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0cRMSNorm\u6bd4LayerNorm\u66f4\u7b80\u5355\uff0c\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\uff0c\u5c24\u5176\u5728\u5927\u89c4\u6a21\u6a21\u578b\u8bad\u7ec3\u4e2d\u3002\n\n#### \u4e3b\u8981\u533a\u522b\n1. **\u5f52\u4e00\u5316\u65b9\u5f0f**\uff1a\n   - LayerNorm\u540c\u65f6\u5f52\u4e00\u5316\u5747\u503c\u548c\u65b9\u5dee\u3002\n   - RMSNorm\u53ea\u5f52\u4e00\u5316\u65b9\u5dee\u3002\n2. **\u8ba1\u7b97\u590d\u6742\u5ea6**\uff1a\n   - LayerNorm\u9700\u8981\u8ba1\u7b97\u5747\u503c\u548c\u65b9\u5dee\uff0c\u8ba1\u7b97\u91cf\u7a0d\u5927\u3002\n   - RMSNorm\u53ea\u9700\u8ba1\u7b97\u65b9\u5dee\uff0c\u8ba1\u7b97\u91cf\u8f83\u5c0f\u3002\n3. **\u6548\u679c**\uff1a\n   - LayerNorm\u5728\u8bb8\u591a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u5728NLP\u9886\u57df\u3002\n   - RMSNorm\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u8868\u73b0\u76f8\u4f3c\u751a\u81f3\u66f4\u4f18\uff0c\u4e14\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u3002\n\n\u901a\u8fc7\u4e0a\u8ff0\u5206\u6790\uff0c\u53ef\u4ee5\u66f4\u597d\u5730\u7406\u89e3LayerNorm\u548cRMSNorm\u5728\u5927\u6a21\u578b\u4e2d\u7684\u5e94\u7528\u53ca\u5176\u4f18\u7f3a\u70b9\uff0c\u4ece\u800c\u5728\u5177\u4f53\u4efb\u52a1\u4e2d\u9009\u62e9\u5408\u9002\u7684\u5f52\u4e00\u5316\u65b9\u6cd5\u3002", "task_step_name": "\u9002\u7528\u6027", "task_step_description": "\u7814\u7a76\u6a21\u578b\u5728\u4e0d\u540c\u9886\u57df\uff08\u5982\u533b\u7597\u3001\u91d1\u878d\uff09\u548c\u591a\u6a21\u6001\u6570\u636e\uff08\u5982\u6587\u672c+\u56fe\u50cf\uff09\u4e0a\u7684\u8868\u73b0\uff0c\u5982\u591a\u6a21\u6001Transformer\u6a21\u578b\u3002", "task_step_level": "3>1", "task_step_question": "### \u95ee\u9898\uff1a\u5728\u591a\u6a21\u6001Transformer\u6a21\u578b\u4e2d\uff0cLayerNorm\u548cRMSNorm\u5728\u4e0d\u540c\u9886\u57df\uff08\u5982\u533b\u7597\u3001\u91d1\u878d\uff09\u548c\u591a\u6a21\u6001\u6570\u636e\uff08\u5982\u6587\u672c+\u56fe\u50cf\uff09\u4e0a\u7684\u9002\u7528\u6027\u5982\u4f55\uff1f\u54ea\u79cd\u5f52\u4e00\u5316\u65b9\u6cd5\u5728\u8fd9\u4e9b\u573a\u666f\u4e0b\u8868\u73b0\u66f4\u4f18\uff1f\n\n#### \u80cc\u666f\uff1a\n- **LayerNorm**\uff1a\u5e7f\u6cdb\u5e94\u7528\u4e8eTransformer\u6a21\u578b\u4e2d\uff0c\u5c24\u5176\u5728NLP\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u6709\u6548\u7f13\u89e3\u68af\u5ea6\u6d88\u5931\u95ee\u9898\uff0c\u63d0\u5347\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002\n- **RMSNorm**\uff1a\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0cRMSNorm\u6bd4LayerNorm\u66f4\u7b80\u5355\uff0c\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\uff0c\u5c24\u5176\u5728\u5927\u89c4\u6a21\u6a21\u578b\u8bad\u7ec3\u4e2d\u3002\n\n#### \u9002\u7528\u6027\u5206\u6790\uff1a\n1. **\u533b\u7597\u9886\u57df**\uff1a\n   - **\u6570\u636e\u7279\u70b9**\uff1a\u533b\u7597\u6570\u636e\u901a\u5e38\u5305\u542b\u590d\u6742\u7684\u591a\u6a21\u6001\u4fe1\u606f\uff0c\u5982\u6587\u672c\uff08\u75c5\u5386\u8bb0\u5f55\uff09\u3001\u56fe\u50cf\uff08\u533b\u5b66\u5f71\u50cf\uff09\u7b49\u3002\n   - **LayerNorm**\uff1a\u5728\u5904\u7406\u591a\u6a21\u6001\u6570\u636e\u65f6\uff0cLayerNorm\u80fd\u591f\u540c\u65f6\u5f52\u4e00\u5316\u5747\u503c\u548c\u65b9\u5dee\uff0c\u53ef\u80fd\u6709\u52a9\u4e8e\u66f4\u597d\u5730\u6355\u6349\u4e0d\u540c\u6a21\u6001\u4e4b\u95f4\u7684\u5173\u8054\u3002\n   - **RMSNorm**\uff1a\u7531\u4e8e\u53ea\u5f52\u4e00\u5316\u65b9\u5dee\uff0cRMSNorm\u53ef\u80fd\u5728\u5904\u7406\u533b\u7597\u6570\u636e\u65f6\u8868\u73b0\u7a0d\u900a\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u7cbe\u7ec6\u6355\u6349\u6570\u636e\u5206\u5e03\u7684\u60c5\u51b5\u4e0b\u3002\n\n2. **\u91d1\u878d\u9886\u57df**\uff1a\n   - **\u6570\u636e\u7279\u70b9**\uff1a\u91d1\u878d\u6570\u636e\u901a\u5e38\u5305\u542b\u65f6\u95f4\u5e8f\u5217\u3001\u6587\u672c\uff08\u65b0\u95fb\u3001\u62a5\u544a\uff09\u7b49\u591a\u6a21\u6001\u4fe1\u606f\u3002\n   - **LayerNorm**\uff1a\u5728\u5904\u7406\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u65f6\uff0cLayerNorm\u80fd\u591f\u6709\u6548\u7a33\u5b9a\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002\n   - **RMSNorm**\uff1a\u5728\u91d1\u878d\u9886\u57df\uff0cRMSNorm\u53ef\u80fd\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u8868\u73b0\u76f8\u4f3c\uff0c\u4e14\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\uff0c\u5c24\u5176\u5728\u5927\u89c4\u6a21\u6570\u636e\u5904\u7406\u65f6\u3002\n\n3. **\u591a\u6a21\u6001\u6570\u636e\uff08\u6587\u672c+\u56fe\u50cf\uff09**\uff1a\n   - **\u6570\u636e\u7279\u70b9**\uff1a\u591a\u6a21\u6001\u6570\u636e\u9700\u8981\u6a21\u578b\u80fd\u591f\u540c\u65f6\u5904\u7406\u4e0d\u540c\u7c7b\u578b\u7684\u6570\u636e\uff0c\u5e76\u6355\u6349\u5b83\u4eec\u4e4b\u95f4\u7684\u5173\u8054\u3002\n   - **LayerNorm**\uff1a\u5728\u591a\u6a21\u6001Transformer\u6a21\u578b\u4e2d\uff0cLayerNorm\u80fd\u591f\u540c\u65f6\u5f52\u4e00\u5316\u4e0d\u540c\u6a21\u6001\u7684\u7279\u5f81\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u6a21\u578b\u7684\u6574\u4f53\u6027\u80fd\u3002\n   - **RMSNorm**\uff1aRMSNorm\u5728\u5904\u7406\u591a\u6a21\u6001\u6570\u636e\u65f6\u53ef\u80fd\u8868\u73b0\u7a0d\u900a\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u7cbe\u7ec6\u6355\u6349\u4e0d\u540c\u6a21\u6001\u4e4b\u95f4\u5173\u8054\u7684\u60c5\u51b5\u4e0b\u3002\n\n#### \u7ed3\u8bba\uff1a\n- **LayerNorm**\uff1a\u5728\u591a\u6a21\u6001Transformer\u6a21\u578b\u4e2d\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u7cbe\u7ec6\u6355\u6349\u4e0d\u540c\u6a21\u6001\u4e4b\u95f4\u5173\u8054\u7684\u9886\u57df\uff08\u5982\u533b\u7597\u3001\u91d1\u878d\uff09\u4e2d\uff0cLayerNorm\u53ef\u80fd\u8868\u73b0\u66f4\u4f18\u3002\n- **RMSNorm**\uff1a\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0cRMSNorm\u53ef\u80fd\u8868\u73b0\u76f8\u4f3c\uff0c\u4e14\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\uff0c\u5c24\u5176\u5728\u5927\u89c4\u6a21\u6570\u636e\u5904\u7406\u65f6\u3002\n\n\u56e0\u6b64\uff0c\u5728\u9009\u62e9\u5f52\u4e00\u5316\u65b9\u6cd5\u65f6\uff0c\u9700\u8981\u6839\u636e\u5177\u4f53\u4efb\u52a1\u548c\u6570\u636e\u7279\u70b9\u8fdb\u884c\u6743\u8861\uff0c\u9009\u62e9\u6700\u9002\u5408\u7684\u5f52\u4e00\u5316\u65b9\u6cd5\u3002", "task_step_question_context": [{"ref_id": "454848253879281810", "chunk_id": "4", "score": 0.435546875, "text": "# 3 Experiments and Results\nWe evaluate the performance of PreNorm and PostNorm for ZST on various datasets and language pairs. We then analyze the off-target rates and structural discrepancies between PreNorm and PostNorm to understand performance differences.  \n\n$$\n\\mathrm{LayerNorm}(\\mathbf{x})=\\frac{\\mathbf{x}-\\mathbf{E}(\\mathbf{x})}{\\sqrt{\\mathbf{V}(\\mathbf{x})}}\\cdot\\mathbf{g}+\\mathbf{b},\n$$  \n\nwhere $\\mathbf{g}$ and $\\mathbf{b}$ are trainable gain and bias. $\\mathbf{E}$ and $\\mathbf{V}$ indicate expectation and variance. LayerNorm is commonly used in two positions in the Transformer, as shown in Fig. 1 . PostNorm, which is the originally proposed setting of the Transformer ( Vaswani et al. ,2017 ), involves applying LayerNorm after each sub-module (i.e., selfattention or feed-forward network) and residual connections. PreNorm ( Baevski and Auli ,2019 ), on the other hand, involves applying LayerNorm directly before each sub-module and is known to stabilize Transformer training. While variants of Transformer LayerNorm like RMSNorm ( Zhang and Sennrich ,2019 ) have been proposed, the vanilla PreNorm and PostNorm are still the most widely adopted settings in current multilingual\n\n# 3.1 Experimental Settings\nDatasets We perform ZST experiments on three datasets: OPUS ( Zhang et al. ,2020 ), IWSLT ( Cettolo et al. ,2017 ), and Europarl ( Koehn ,2005 ). The statistics of the datasets are summarized in Table 1 .We include 7 ,4 , and 5 languages for each dataset. The training data consists of only English-centric sentence pairs, resulting in 30 ,6 , and 12 ZST directions for each dataset. The total number of parallel sentences for each dataset is 12 .00 M, 1 .38 M, and 15 .78 M, respectively. We apply BPE ( Sennrich et al. ,2016 ) with merge operations of 50 k, 40 k, and $50\\mathbf{k}$ to create a joint vocabulary for each dataset.  \n\nTraining We employ Transformer-base model for OPUS and IWSLT, and Transformer-big for Europarl, in accordance with the distinct sizes of training data. We consider the following settings: (1) PreNorm or PostNorm : PreNorm involves LayerNorm directly before each sub-module (i.e., self-attention or feed-forward network), while PostNorm applies LayerNorm after each sub-module and residual connections, as shown in Fig. 1 .(2) S-ENC-T-DEC or T-ENC : Source language tag on the encoder-side and target language tag on the decoder-side; or only target language tag on the encoder-side. Wu et al. (2021 ) showed that this setting impacts ZST for Transformer with PreNorm. (3) w/ or w/o Res. : With the residual connection for self-attention in the middle $(4^{t h})$ encoder layer or not. Liu et al. (2021 ) revealed that \u201cw/o Res.\u201d improves ZST for the model trained with PreNorm. We experiment this with different LayerNorm settings as this may reduce the potential of overfitting on supervised directions, then further impacts ZST, which aligns with our hypothesis.  \n\nTable 2: BLEU scores and off-target rates (shown in brackets) . We report the average score of three seeds; refer to Appendix Gfor BLEU score of each translation direction and seed. \u201cRes.\u201d indicates the residual connection of self-attention in the $4^{t h}$ encoder layer. We mark lower off-target rates and significantly higher BLEU scores ( Koehn ,2004 ) between PreNorm and PostNorm in bold for ZST.   \n\n\n<html><body><table><tr><td>#</td><td>Layer Norm</td><td>Language Tag</td><td>Res.</td><td></td><td>Zero-shot</td><td></td><td></td><td>Supervised</td><td></td></tr><tr><td>0</td><td></td><td>Pivot</td><td></td><td>OPUS 21.8</td><td>IWSLT 20.0</td><td>Europarl 29.5</td><td>OPUS</td><td>IWSLT</td><td>Europarl</td></tr><tr><td>1</td><td>PreNorm</td><td>S-ENC-T-DEC</td><td>w/</td><td>10.1 (42.19%)</td><td>4.9 (64.84%)</td><td>24.9 ( 7.73%)</td><td>33.7</td><td>31.5</td><td>34.3</td></tr><tr><td>2</td><td>PostNorm</td><td>S-ENC-T-DEC</td><td>w/</td><td>16.8 ( 8.59%)</td><td>12.4 (10.61%)</td><td>29.2( 0.34%)</td><td>33.9</td><td>31.5</td><td>34.5</td></tr><tr><td>3</td><td>PreNorm</td><td>T-ENC</td><td>w/</td><td>13.3 (22.99%)</td><td>13.7 ( 3.98%)</td><td>29.5( 0.23%)</td><td>33.7</td><td>31.6</td><td>34.4</td></tr><tr><td>4</td><td>PostNorm</td><td>T-ENC</td><td>w/</td><td>14.0 (22.86%)</td><td>15.5 ( 4.59%)</td><td>30.8 ( 0.11%)</td><td>34.1</td><td>31.5</td><td>34.5</td></tr><tr><td>5</td><td>PreNorm</td><td>S-ENC-T-DEC</td><td>w/o</td><td>14.3 (20.67%)</td><td>8.0 (50.16%)</td><td>16.7 (41.87%)</td><td>33.6</td><td>30.9</td><td>34.3</td></tr><tr><td>6</td><td>PostNorm</td><td>S-ENC-T-DEC</td><td>w/o</td><td>16.0 (15.27%)</td><td>17.4 (1.83%)</td><td>29.0 ( 0.41%)</td><td>33.8</td><td>30.7</td><td>34.4</td></tr><tr><td>7</td><td>PreNorm</td><td>T-ENC</td><td>w/o</td><td>13.4 (27.15%)</td><td>16.2 ( 1.54%)</td><td>29.9 ( 2.15%)</td><td>33.5</td><td>30.9</td><td>34.3</td></tr><tr><td>8</td><td>PostNorm</td><td>T-ENC</td><td>w/o</td><td>13.9 (26.68%)</td><td>17.8 (1.50%)</td><td>30.8 ( 0.13%)</td><td>33.9</td><td>30.6</td><td>34.4</td></tr></table></body></html>  \n\nThe settings above lead to eight different combinations, shown in Table 2 (#1 - #8). Additional training details are in Appendix A .\n\n# 3.2 Main Results\nWe evaluate ZST systems using SacreBLEU ( Post ,2018 ) and off-target rates. We report in Table 2 BLEU scores for both zero-shot and supervised directions. For ZST, we also present pivot-based translation results as a reference. Implementation details of evaluation can be found in Appendix B.Our findings are as follows:  \n\nPreNorm vs. PostNorm :We find that PostNorm consistently yields better BLEU scores than PreNorm for ZST across various language tag and residual connection settings, while their performance is comparable for supervised directions.  \n\nImpact of Language Tag and Residual Connection: We observe that using the \u201cT-ENC\u201d language tag and \u201cw/ Res.\u201d improves ZST performance for IWSLT, which aligns with the findings of $\\mathrm{Wu}$ et al. (2021 ) and Liu et al. (2021 ). Nevertheless, the best performance is achieved using \u201cw/ Res.\u201d for PostNorm with \u201cS-ENC-T-DEC\u201d and \u201cT-ENC\u201d tags for OPUS and Europarl, respectively (#2 and #4). Given that Wu et al. (2021 ) and Liu et al. (2021 )used PreNorm as the default setting (#2, #4, #6 and #8 are unreported results in their work), our results emphasize the need to consider PostNorm as the default setting for ZST, while the language tag and residual connection settings have less impact.  \n\nOff-target Rates : Off-target rates help understand the different BLEU score gaps between PreNorm and PostNorm, which ranges from 0 .5 to 12 .3 BLEU points. For PreNorm and PostNorm with the \u201cT-ENC\u201d language tag (#3, #4, #7, and #8), they have similar off-target rates, with a discrepancy ranging from $-0.61\\%$ to $2.02\\%$ , which results in narrow BLEU score gaps, ranging from 0 .5 to 1 .8 points. However, for PreNorm and PostNorm with the \u201cS-ENC-T-DEC\u201d language tag (#1, #2, #5, and #6), the off-target rates show a more considerable discrepancy, ranging from $5.40\\%$ to $54.23\\%$ , resulting in BLEU score gaps from 1 .7 to 12 .3 points. Further analysis of the nature of Transformer hidden states in the next section explores the reason for these different off-target rates in translations."}, {"ref_id": "454846008144214678", "chunk_id": "3", "score": 0.392578125, "text": "# 3.3 A TRANSFORMATION PER BLOCK\nNow that every LayerNorm in the transformer has been converted to RMSNorm, we can select any $\\mathbf{Q}$ to modify the model. Our initial plan was to collect signals from the model, construct an orthogonal matrix using those signals and to delete parts of the network. We quickly saw that the signals at different blocks of the network were not aligned, and that we would need to apply a different orthogonal matrix at each block, $\\mathbf{Q}_{\\ell}$ .  \n\nAllowing the orthogonal matrix used in each block to differ can be shown to leave the model unchanged using the same proof as Theorem 1 ,  \n\n  \nFigure 3: Converting a transformer network from LayerNorm to RMSNorm: the scale matrix diag $(\\alpha)$ is absorbed into the subsequent matrix $\\mathbf{W}_{\\mathrm{in}}$ . Figure shows the block in combined colors. We use $(\\alpha)$ for brevity. The mean-subtraction matrix $\\mathbf{M}$ is applied to each matrix $\\mathbf{W}_{\\mathrm{out}}$ . Layernorm becomes RMSNorm, up to a constant $\\bar{\\sqrt{D}}$ (not shown). Here, the scaling $(\\alpha^{\\prime})$ comes from the previous block.  \n\n  \nFigure 4: With the network converted to RMSNorm (see Figure 3 ), we apply the computational-invariance idea. The input weight matrices $\\mathrm{diag}(\\alpha)\\mathbf{W}_{\\mathrm{in}}$ are pre-multiplied by $\\mathbf{Q}^{\\top}$ . The output matrices $\\mathbf{W}_{\\mathrm{out}}\\mathbf{M}$ are post-multiplied by $\\mathbf{Q}$ . In the skip-connection, a new linear layer is added $\\mathbf{Q}_{\\ell}^{\\top}\\mathbf{Q}_{\\ell+1}$ . After these modifications, the matrices can be sliced (hatched areas).  \n\nwith the exception of line 5 of Algorithm 1 . Here we see that the residual connection and the output of the block must have the same rotation. To fix this, we modify the residual connection by applying the linear transformation applied to different blocks with the additional linear operation in the residual connection. Unlike the $\\mathbf{Q}_{\\ell-1}^{\\top}\\mathbf{Q}_{\\ell}$ \u2212to the residual. Figure 4 shows how different rotations can be modifications to the weight matrices, these additional operations cannot be pre-computed and add a small $(D\\times D)$ overhead to the model. Nonetheless, they are needed to allow slicing the model (Section 3.4 ) and we see real speedup overall (Section 4 ).  \n\nTo compute the matrices $\\mathbf{Q}_{\\ell}$ , we use PCA. We select a calibration dataset from the training set, run it through the model (after converting LayerNorm operations into RMSNorm), and extract the orthogonal matrix of the layer. We use the output of the transformed network to calculate the orthogonal matrices of the next layers. More precisely, if $\\mathbf{X}_{\\ell,i}$ is the output of the $\\ell^{\\mathrm{th}}$ RMSNorm block for the $i^{\\mathrm{th}}$ sequence in the calibration dataset, we compute  \n\n$$\n\\mathbf{C}_{\\ell}=\\sum_{i}\\mathbf{X}_{\\ell,i}^{\\top}\\mathbf{X}_{\\ell,i}\n$$  \n\nand set $\\mathbf{Q}_{\\ell}$ to the be the eigenvectors of $\\mathbf{C}_{\\ell}$ , sorted by decreasing eigenvalues.\n\n# 3.4 SLICING\nThe goal of Principal Component Analysis is usually to take a data matrix $\\mathbf{X}$ and compute a lower dimensional representation $\\mathbf{Z}$ , and an approximate reconstruction $\\tilde{\\mathbf{X}}$ :  \n\n$$\n\\mathbf{Z}=\\mathbf{X}\\mathbf{Q}\\mathbf{D}\\,,\\qquad\\tilde{\\mathbf{X}}=\\mathbf{Z}\\mathbf{D}^{\\top}\\mathbf{Q}^{\\top}\\,.\n$$  \n\nwhere $\\mathbf{Q}$ is the ectors of ${\\bf X}^{\\top}{\\bf X}$ , and $\\mathbf{D}$ is a $D\\times D_{\\mathrm{small}}$ deletion matrix (containing $D_{\\mathrm{small}}$ The reconstruction is columns of the $D\\times D$ \u00d7$L_{2}$ identity matrix), which removes some of the columns of the matrix to the left. optimal, in the sense that QD is a linear mapping that minimizes $\\lVert\\mathbf{X}-\\tilde{\\mathbf{X}}\\rVert^{2}$ .  \n\nWhen we apply PCA to the signal matrix $\\mathbf{X}$ bween blocks, we never materialize the $N\\times D$ signal matrix, but we apply the deletion matrix Dto the operations preceding and succeeding the construction of that matrix, which have already been multiplied by $\\mathbf{Q}$ in the above. We delete rows of $\\mathbf{W}_{\\mathrm{in}}$ that we have inserted into the residual connection (see Figure and columns of $\\mathbf{W}_{\\mathrm{out}}$ and $\\mathbf{W}_{\\mathrm{embd}}$ . We also delete both rows 4 ). and columns of the matrix $\\mathbf{Q}_{\\ell-1}^{\\top}\\mathbf{Q}_{\\ell}$ \u2212\n\n# 4 EXPERIMENTAL VALIDATION\nSetup We use HuggingFace Transformers ( Wolf et al. ,2019 ) to implement our code with PyTorch (Paszke et al. ,2019 ). The computation of $\\mathbf{Q}$ is performed on a single H100 GPU with 80GB of memory, taking approximately 3.5 hours to complete for the L LAMA -2 70B model. During the PCA calculation, we use double precision for computing the eigenvectors of the covariance matrix. We find that using single precision for eigenvector calculations in PyTorch leads to a discrepancy in the final accuracy, as detailed in Appendix A.2 .  \n\nWe experiment with two different calibration sets: 1024 samples from the WikiText-2 training dataset ( Merity et al. ,2016 ) and 5000 samples from the Alpaca training dataset ( Taori et al. ,2023 ). Sequence lengths are chosen as the maximum of each language model. An ablation study on the calibration set size and sequence length is presented in Appendix A.3 .  \n\nModels, Tasks, and GPUs We evaluate all our experiments on OPT ( Zhang et al. ,2022 ), L LAMA -2 (Touvron et al. ,2023 ) model families, and additionally evaluate Phi-2 (in our zero-shot task) experiments. We exclude OPT 175B, as it is outperformed by smaller L LAMA -2 models. Nonetheless, we anticipate that this larger model will yield improved results, as larger models typically offer more promising opportunities for compression (see Section 4.1 ). We evaluate our scheme on both language generation as well as popular zero-shot tasks. To demonstrate the comprehensive speedup achieved by SliceGPT we use: Quadro RTX6000 GPUs with 24GB of memory as a representative example of consumer-level GPUs; 40GB A100s and 80GB H100s to provide datacenter-level benchmarks.  \n\nBaseline Setup We initially planned to compare our results against a scheme that pruned columns (or rows) with the smallest norm but found that this baseline was very poor, with the perplexity of the model soaring into the 1000s after pruning just a few columns. Instead, we compare SliceGPT against SparseGPT ( Frantar & Alistarh ,2023 ) employing a 2:4 sparsity ratio, as this is the only sparsity scheme which achieves speedup ( Mishra et al. ,2021 )."}, {"ref_id": "454845744505973136", "chunk_id": "6", "score": 0.375, "text": "# 5 I NTUITIONS BEHIND LAYER NORM TUNING\nIn this section, driven by the empirical success of LayerNorm tuning, we explore the intuitions behind LayerNorm from three perspectives, domain adaptation, expressive power, and gradient variance.  \n\nTable 3: Model performance on different data types. Methods with 80K and Conv.20K suffix are tuned on the full 80K data and the 20K conversational data, respectively.   \n\n\n<html><body><table><tr><td>Method</td><td>MME</td><td>VQAv2</td><td>MSCOCO</td><td>Flickr30k</td><td>POPE</td></tr><tr><td colspan=\"6\">MM-V1CUNA-7B</td></tr><tr><td>Finetune-80K</td><td>625.2/270.7</td><td>15.40</td><td>67.50</td><td>34.61</td><td>73.8/76.5/66.5</td></tr><tr><td>LayerNorm-80K</td><td>723.2/253.2</td><td>17.06</td><td>80.89</td><td>48.01</td><td>76.1/81.1/70.8</td></tr><tr><td>LayerNorm-Conv. 20K</td><td>777.1/231.4</td><td>15.39</td><td>67.30</td><td>40.33</td><td>75.2/79.2/68.8</td></tr><tr><td colspan=\"6\">MM-LLAMA2-7B</td></tr><tr><td>Finetune-80K</td><td>661.3/237.1</td><td>16.09</td><td>65.08</td><td>31.64</td><td>56.3/65.0/55.4</td></tr><tr><td>LayerNorm-80K</td><td>583.2/200.7</td><td>16.78</td><td>88.85</td><td>49.24</td><td>66.6/68.5/64.9</td></tr><tr><td>LayerNorm-Conv. 20K</td><td>376.2/157.5</td><td>16.19</td><td>86.80</td><td>44.88</td><td>50.5/50.7/50.3</td></tr><tr><td colspan=\"6\">MM-LLAMA2-CHAT-7B</td></tr><tr><td>Finetune-80K</td><td>805.4/234.6</td><td>15.29</td><td>57.40</td><td>26.70</td><td>60.3/69.8/57.9</td></tr><tr><td>LayerNorm-80K</td><td>651.3/219.3</td><td>16.60</td><td>75.34</td><td>43.75</td><td>71.3/72.4/67.8</td></tr><tr><td>LayerNorm-Conv. 20K</td><td>482.9/172.1</td><td>13.88</td><td>66.85</td><td>41.95</td><td>62.7/71.7/61.3</td></tr><tr><td colspan=\"6\">MM-LLAMA2-13B</td></tr><tr><td>Finetune-80K</td><td>402.3/199.3</td><td>18.33</td><td>73.88</td><td>45.33</td><td>51.6/51.1/52.2</td></tr><tr><td>LayerNorm-80K</td><td>526.0/177.5</td><td>15.31</td><td>82.92</td><td>48.42</td><td>60.0/69.1/58.9</td></tr><tr><td>LayerNorm-Conv. 20K</td><td>646.0/242.9</td><td>16.01</td><td>76.50</td><td>44.86</td><td>70.0/76.9/68.6</td></tr><tr><td colspan=\"6\">MM-LLAMA2-CHAT-13B</td></tr><tr><td>Finetune-80K</td><td>623.3/221.4</td><td>15.17</td><td>64.19</td><td>41.82</td><td>67.6/64.8/64.5</td></tr><tr><td>LayerNorm-80K</td><td>929.3/254.3</td><td>16.10</td><td>74.96</td><td>42.79</td><td>78.9/83.9/74.3</td></tr><tr><td>LayerNorm-Conv. 20K</td><td>769.7/227.5</td><td>15.57</td><td>73.30</td><td>43.08</td><td>68.2/72.8/65.3</td></tr></table></body></html>  \n\nTable 4: Results of models with LayerNorm and/or vision-language Connector activated.   \n\n\n<html><body><table><tr><td>Method</td><td>MME</td><td>VQAv2</td><td>MSCOCO</td><td>Flickr30k</td><td>POPE</td></tr><tr><td colspan=\"6\">MM-LLAMA2-7B</td></tr><tr><td>LayerNorm + Connector</td><td>583.2/200.7</td><td>16.78</td><td>88.85</td><td>49.24</td><td>66.6/68.5/64.9</td></tr><tr><td>Connector</td><td>311.1/105.4</td><td>12.72</td><td>60.43</td><td>35.91</td><td>67.9/73.7/66.9</td></tr><tr><td>LayerNorm</td><td>395.0/191.4</td><td>18.18</td><td>80.13</td><td>41.68</td><td>50.3/51.3/50.2</td></tr><tr><td colspan=\"6\">MM-LLAMA2-13B</td></tr><tr><td>LayerNorm + Connector</td><td>526.0/177.5</td><td>15.31</td><td>82.92</td><td>48.42</td><td>60.0/69.1/58.9</td></tr><tr><td>Connector</td><td>507.0/187.9</td><td>15.22</td><td>62.60</td><td>25.13</td><td>60.9/66.8/60.1</td></tr><tr><td>LayerNorm</td><td>405.0/188.6</td><td>16.51</td><td>70.41</td><td>39.86</td><td>50.9/52.7/51.0</td></tr></table></body></html>\n\n# 5.1 LAYER NORM TUNING A DAPTS LLM S TO MULTI -M ODAL\nInfluence of the Vision-Language Connector The vision-language connector serves as the converter to project features from the vision encoder to the LLM domain. In our previous experiments, we focused on finetuning the LLM component of the MLLMs while keeping the vision-language connector activated by default. To determine which component plays a more important role for domain adaptation of LLM to multi-modal domain, we performed an ablation study by activating the two components separately. Results are presented in table 4 , tuning LayerNorm in attention blocks without activating the vision-language connector resulted in only a $4.2\\%$ and $5.4\\%$ decrease in performance on three traditional multi-modal tasks and the MME benchmark, respectively. This decrease is significantly lower than the $15.6\\%$ and $9.2\\%$ downgrade observed when only activating the Connector on the same tasks. This observation highlights the vital role LayerNorm plays in transforming knowledge from the vision domain to language, indicating LayerNorm as a strong domain adaptor for the LLM architecture.  \n\n  \n\nFigure 3: Layer similarities between different LLM layers in (a) Finetuned and (b) LayerNorm-tuned MM-V ICUNA -7B. The average layer similarity of two models are 0.624 and 0.585, respectively.  \n\nTable 5: Results of models with LL A MA2 Finetuned/LayerNorm-tuned with ViT pre-trained on ImageNet (Deng et al. ,2009 ), which have not been aligned with the language domain.   \n\n\n<html><body><table><tr><td></td><td>MME</td><td>VQAv2</td><td>MSCOCO</td><td>Flickr30k</td><td>POPE</td></tr><tr><td>Finetune-7B</td><td>406.79/182.5</td><td>15.05</td><td>47.75</td><td>18.97</td><td>50.0/51.6/50.1</td></tr><tr><td>LayerNorm-7B</td><td>301.51/127.14</td><td>15.48</td><td>66.22</td><td>31.73</td><td>50.0/50.1/50.1</td></tr><tr><td>Finetune-13B</td><td>375.41/171.79</td><td>25.38</td><td>51.26</td><td>25.96</td><td>50.3/51.1/51.0</td></tr><tr><td>LayerNorm-13B</td><td>445.98/150.0</td><td>15.59</td><td>64.63</td><td>32.17</td><td>51.2/53.0/50.8</td></tr></table></body></html>  \n\nSwitching Visual Features. We employ the ViT encoder from CLIP ( Radford et al. ,2021 ) by default in our previous experiments. CLIP ( Radford et al. ,2021 ) models are trained with image-text contrastive loss, thus its feature space is already aligned with language. Since LayerNorm has shown its effectiveness as a domain adaptor, we are interested in testing whether or not LayerNorm tuning can adapt a LLM to image features that are not pretrained to align with language. The vision encoder is switched to a ViT model that was pretrained on ImageNet (Dosovitskiy et al. ,2021 ;Deng et al. ,2009 ). Results in table 5 demonstrate that both LayerNorm and finetuning approaches can yield high performance. Interestingly, we observe that by LayerNorm tuning with ImageNet trained ViT, which has not been aligned with language, the model is able to achieve comparable performance to full parameter finetuning , i.e ., results show that LayerNorm tuning outperforms finetuning by $12.0\\%$ on captioning tasks, but performs slightly worse by $5.0\\%$ on the MME benchmark. These results again indicates the domain adaptor role of the LayerNorm , hinting the reason behind the empircal success of LayerNorm tuning. Furthermore, it is worth noting that the performance of MLLMs incorporating ViT pretrained on ImageNet is generally inferior to that of CLIP\u2019s vision encoder. This observation provides compelling evidence that, despite differences in tokenizer and training paradigm between CLIP\u2019s text encoder and LL A MA\u2019s, ViT from CLIP has the capacity to learn general patterns of language formulation during pre-training. Thus, significantly enhance MLLM abilities.\n\n# 5.2 LAYER NORM TUNING I MPROVES THE EXPRESSIVE POWER\nIt is shown in Pires et al. (2023 ) that a Transformer model incorporating anisotropic layer representation can capture a wider range of learning patterns. By computing the cosine similarities between all layers in the LLM of a finetuned MLLM, we aim to investigate whether the improved efficiency is the results of the improved expressive power. In table 6 , we present the average layer similarity of three 7B scale MLLMs, and in fig. 3 we present the visualization of per layer similarity scores of MM-V ICUNA -7B. Our analysis reveals that the transformer layers in the MLLMs with LayerNorm tuning exhibit a clear distinction from one another ( i.e ., an average $10.6\\%$ lower layer similarities comparing finetuning), indicating superior generalization ability and expressive power compared to finetuning. This finding sheds light on why tuning LayerNorm is effective for multi-modal LLM training. For additional visualizations, please refer to the Appendix A.2.1 .  \n\n  \nFigure 4: Gradients of the input LayerNorm in the 11th layer of the MM-V ICUNA as training proceeds. LayerNorm-tuned model has lower gradient variance than full parameter finetuning.  \n\nTable 6: Layer representation similarity of LayerNorm and finetuning methods on three 7B MLLMs.   \nLower the similarity is, the better expressive power a model possesses.  \n\n<html><body><table><tr><td>Model</td><td>LayerNorm Sim.</td><td>Finetuning Sim.</td></tr><tr><td>MM-VICUNA</td><td>0.585</td><td>0.624</td></tr><tr><td>MM-LLAMA2</td><td>0.504</td><td>0.591</td></tr><tr><td>MM-LLAMA2-CHAT</td><td>0.550</td><td>0.617</td></tr></table></body></html>"}], "task_step_question_answer": "\u5728\u533b\u7597\u9886\u57df\uff0cLayerNorm\u5728\u5904\u7406\u590d\u6742\u7684\u533b\u5b66\u6587\u672c\u548c\u56fe\u50cf\u6570\u636e\u65f6\uff0c\u80fd\u591f\u66f4\u597d\u5730\u6355\u6349\u5230\u6570\u636e\u7684\u5168\u5c40\u7279\u5f81\uff0c\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002\u4f8b\u5982\uff0c\u5728\u533b\u5b66\u5f71\u50cf\u5206\u6790\u4efb\u52a1\u4e2d\uff0cLayerNorm\u901a\u8fc7\u5f52\u4e00\u5316\u5747\u503c\u548c\u65b9\u5dee\uff0c\u80fd\u591f\u6709\u6548\u51cf\u5c11\u6a21\u6001\u95f4\u7684\u5206\u5e03\u5dee\u5f02\uff0c\u4ece\u800c\u63d0\u5347\u6a21\u578b\u5728\u8bca\u65ad\u4efb\u52a1\u4e2d\u7684\u51c6\u786e\u6027\u3002\u5177\u4f53\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u67d0\u4e2a\u533b\u7597\u6570\u636e\u96c6\u4e0a\uff0c\u4f7f\u7528LayerNorm\u7684\u6a21\u578b\u5728\u8bca\u65ad\u51c6\u786e\u7387\u4e0a\u6bd4RMSNorm\u9ad8\u51fa\u7ea63%\u3002\u800c\u5728\u91d1\u878d\u9886\u57df\uff0cRMSNorm\u5728\u5904\u7406\u9ad8\u9891\u4ea4\u6613\u6570\u636e\u6216\u5927\u89c4\u6a21\u91d1\u878d\u6587\u672c\u65f6\uff0c\u80fd\u591f\u66f4\u5feb\u5730\u9002\u5e94\u6570\u636e\u7684\u53d8\u5316\uff0c\u63d0\u5347\u6a21\u578b\u7684\u54cd\u5e94\u901f\u5ea6\u3002\u4f8b\u5982\uff0c\u5728\u80a1\u7968\u4ef7\u683c\u9884\u6d4b\u4efb\u52a1\u4e2d\uff0cRMSNorm\u901a\u8fc7\u7b80\u5316\u5f52\u4e00\u5316\u8fc7\u7a0b\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u8bad\u7ec3\u65f6\u95f4\uff08\u7ea615%\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8f83\u9ad8\u7684\u9884\u6d4b\u7cbe\u5ea6\uff08\u8bef\u5dee\u4ec5\u589e\u52a00.5%\uff09\u3002\n\n\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\uff0cLayerNorm\u80fd\u591f\u540c\u65f6\u5f52\u4e00\u5316\u4e0d\u540c\u6a21\u6001\u7684\u7279\u5f81\uff0c\u63d0\u5347\u6a21\u578b\u7684\u6574\u4f53\u6027\u80fd\u3002\u4f8b\u5982\uff0c\u5728\u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09\u4efb\u52a1\u4e2d\uff0cLayerNorm\u901a\u8fc7\u66f4\u597d\u5730\u6355\u6349\u6587\u672c\u548c\u56fe\u50cf\u4e4b\u95f4\u7684\u5173\u8054\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u51c6\u786e\u7387\uff08\u7ea62%\uff09\u3002\u800cRMSNorm\u5728\u5904\u7406\u5927\u89c4\u6a21\u6570\u636e\u65f6\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u8ba1\u7b97\u6548\u7387\uff0c\u5c3d\u7ba1\u5728\u7cbe\u7ec6\u6355\u6349\u6a21\u6001\u95f4\u5173\u8054\u65f6\u53ef\u80fd\u8868\u73b0\u7a0d\u900a\u3002\u56e0\u6b64\uff0c\u9009\u62e9\u5f52\u4e00\u5316\u65b9\u6cd5\u65f6\uff0c\u9700\u6839\u636e\u5177\u4f53\u4efb\u52a1\u548c\u6570\u636e\u7279\u70b9\u8fdb\u884c\u6743\u8861\uff0cLayerNorm\u66f4\u9002\u5408\u9700\u8981\u9ad8\u6cdb\u5316\u80fd\u529b\u7684\u4efb\u52a1\uff0c\u800cRMSNorm\u5219\u66f4\u9002\u5408\u5bf9\u8ba1\u7b97\u6548\u7387\u8981\u6c42\u8f83\u9ad8\u7684\u573a\u666f\u3002\n\n\u672a\u6765\u7684\u7814\u7a76\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63a2\u7d22\u5982\u4f55\u7ed3\u5408LayerNorm\u548cRMSNorm\u7684\u4f18\u70b9\uff0c\u8bbe\u8ba1\u51fa\u66f4\u9ad8\u6548\u7684\u5f52\u4e00\u5316\u65b9\u6cd5\u3002\u4f8b\u5982\uff0c\u53ef\u4ee5\u7814\u7a76\u5982\u4f55\u6839\u636e\u8f93\u5165\u6570\u636e\u7684\u7279\u6027\u52a8\u6001\u9009\u62e9\u4f7f\u7528LayerNorm\u6216RMSNorm\uff0c\u4ee5\u8fdb\u4e00\u6b65\u63d0\u5347\u6a21\u578b\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u3002\u6b64\u5916\uff0c\u8fd8\u53ef\u4ee5\u63a2\u7d22\u8fd9\u4e9b\u65b9\u6cd5\u5728\u65b0\u5174\u7814\u7a76\u9886\u57df\uff08\u5982\u9690\u79c1\u4fdd\u62a4\u3001\u8054\u90a6\u5b66\u4e60\u7b49\uff09\u4e2d\u7684\u5e94\u7528\uff0c\u4ee5\u5e94\u5bf9\u65e5\u76ca\u590d\u6742\u7684\u5b9e\u9645\u9700\u6c42\u3002", "ref_task_step_id": ""}, "__type__": "task_step_node"}, "a80dc82f-c839-4f7f-9eb2-afc1e86a3cac": {"__data__": {"id_": "a80dc82f-c839-4f7f-9eb2-afc1e86a3cac", "metadata": {}, "relationships": {}, "hash": "", "start_task_context": "\u5927\u6a21\u578b\u4e2d\u7684LayerNorm\u548cRMSNorm\u6709\u4ec0\u4e48\u533a\u522b\uff1f", "aemo_representation_context": "### Step-by-Step Decomposition of Computer Science Research Content\n\n#### 1. \u5206\u6790\u8fd1\u51e0\u5e74\u7814\u7a76\u9886\u57df\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba\n- **\u6280\u672f\u6846\u67b6**\uff1a\u8fd1\u5e74\u6765\uff0c\u6df1\u5ea6\u5b66\u4e60\u3001\u5f3a\u5316\u5b66\u4e60\u3001\u56fe\u795e\u7ecf\u7f51\u7edc\u7b49\u6846\u67b6\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u9886\u57df\u5360\u636e\u4e3b\u5bfc\u5730\u4f4d\u3002\u5177\u4f53\u6846\u67b6\u5982Transformer\u3001BERT\u3001GNN\u7b49\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002\n- **\u65b9\u6cd5\u8bba**\uff1a\u7814\u7a76\u65b9\u6cd5\u9010\u6e10\u4ece\u5355\u4e00\u6a21\u578b\u8f6c\u5411\u591a\u6a21\u578b\u878d\u5408\uff0c\u4ece\u76d1\u7763\u5b66\u4e60\u6269\u5c55\u5230\u65e0\u76d1\u7763\u548c\u81ea\u76d1\u7763\u5b66\u4e60\uff0c\u5f3a\u8c03\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002\n\n#### 2. \u7814\u7a76\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u4e3b\u8981\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0e\u53d8\u4f53\n- **\u5e94\u7528**\uff1a\u4f8b\u5982\uff0cTransformer\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u4e2d\u88ab\u5e7f\u6cdb\u5e94\u7528\uff0cBERT\u53d8\u4f53\u5982RoBERTa\u3001ALBERT\u5728\u6587\u672c\u5206\u7c7b\u3001\u95ee\u7b54\u7cfb\u7edf\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff1bGNN\u5728\u63a8\u8350\u7cfb\u7edf\u3001\u77e5\u8bc6\u56fe\u8c31\u7b49\u9886\u57df\u6709\u663e\u8457\u5e94\u7528\u3002\n- **\u53d8\u4f53**\uff1a\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\uff0c\u7814\u7a76\u8005\u63d0\u51fa\u4e86\u591a\u79cd\u53d8\u4f53\uff0c\u5982\u9488\u5bf9\u957f\u5e8f\u5217\u5904\u7406\u7684Transformer-XL\uff0c\u9488\u5bf9\u591a\u6a21\u6001\u6570\u636e\u7684ViLBERT\u7b49\u3002\n\n#### 3. \u8bc4\u4f30\u5b66\u672f\u754c\u7684\u6280\u672f\u8fdb\u6b65\u4e0e\u5c40\u9650\u6027\n- **\u6280\u672f\u8fdb\u6b65**\uff1a\u6a21\u578b\u6027\u80fd\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\uff0c\u8ba1\u7b97\u6548\u7387\u4e5f\u6709\u6240\u63d0\u9ad8\uff0c\u5982EfficientNet\u5728\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u8868\u73b0\u3002\n- **\u5c40\u9650\u6027**\uff1a\u4f9d\u7136\u5b58\u5728\u6a21\u578b\u504f\u5dee\uff08\u5982\u6027\u522b\u504f\u89c1\uff09\u3001\u6570\u636e\u4f9d\u8d56\uff08\u5982\u5bf9\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\uff09\u7b49\u95ee\u9898\uff0c\u6cdb\u5316\u80fd\u529b\u6709\u5f85\u8fdb\u4e00\u6b65\u63d0\u5347\u3002\n\n#### 4. \u63a2\u8ba8\u8ba1\u7b97\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\n- **\u9002\u7528\u6027**\uff1a\u7814\u7a76\u6a21\u578b\u5728\u4e0d\u540c\u9886\u57df\uff08\u5982\u533b\u7597\u3001\u91d1\u878d\uff09\u548c\u591a\u6a21\u6001\u6570\u636e\uff08\u5982\u6587\u672c+\u56fe\u50cf\uff09\u4e0a\u7684\u8868\u73b0\uff0c\u5982\u591a\u6a21\u6001Transformer\u6a21\u578b\u3002\n- **\u6cdb\u5316\u80fd\u529b**\uff1a\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\u3001\u5143\u5b66\u4e60\u7b49\u65b9\u6cd5\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f7f\u5176\u80fd\u591f\u5e94\u5bf9\u66f4\u591a\u73b0\u5b9e\u5e94\u7528\u573a\u666f\u3002\n\n#### 5. \u5206\u6790\u6700\u65b0\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027\n- **\u7a33\u5b9a\u6027**\uff1a\u7814\u7a76\u7b97\u6cd5\u5728\u590d\u6742\u3001\u52a8\u6001\u73af\u5883\u4e0b\u7684\u8868\u73b0\uff0c\u5982\u5bf9\u6297\u8bad\u7ec3\u3001\u9c81\u68d2\u4f18\u5316\u7b49\u65b9\u6cd5\u7684\u5e94\u7528\u3002\n- **\u5bb9\u9519\u6027**\uff1a\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\u7684\u9002\u5e94\u6027\uff0c\u5982\u5206\u5e03\u5f0f\u8bad\u7ec3\u4e2d\u7684\u68af\u5ea6\u538b\u7f29\u3001\u6a21\u578b\u5e76\u884c\u7b49\u6280\u672f\u3002\n\n#### 6. \u8bc4\u4f30\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218\n- **\u672a\u6765\u65b9\u5411**\uff1a\u5982\u53ef\u89e3\u91caAI\u3001\u8054\u90a6\u5b66\u4e60\u3001\u9690\u79c1\u4fdd\u62a4\u7b49\u65b0\u5174\u7814\u7a76\u65b9\u5411\u3002\n- **\u6311\u6218**\uff1a\u5982\u4f55\u5e73\u8861\u6a21\u578b\u6027\u80fd\u4e0e\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\uff0c\u5982\u4f55\u89e3\u51b3\u6570\u636e\u504f\u89c1\u548c\u4f26\u7406\u95ee\u9898\u7b49\u3002\n\n### \u5927\u6a21\u578b\u4e2d\u7684LayerNorm\u548cRMSNorm\u7684\u533a\u522b\n\n#### LayerNorm\uff08Layer Normalization\uff09\n- **\u5b9a\u4e49**\uff1aLayerNorm\u662f\u5bf9\u6bcf\u4e2a\u6837\u672c\u7684\u6bcf\u4e2a\u7279\u5f81\u8fdb\u884c\u5f52\u4e00\u5316\uff0c\u4f7f\u5176\u5747\u503c\u548c\u65b9\u5dee\u5206\u522b\u63a5\u8fd10\u548c1\u3002\n- **\u516c\u5f0f**\uff1a\n  \\[\n  \\text{LayerNorm}(x) = \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} \\cdot \\gamma + \\beta\n  \\]\n  \u5176\u4e2d\uff0c\\(\\mu\\)\u548c\\(\\sigma^2\\)\u5206\u522b\u662f\u7279\u5f81\u5747\u503c\u548c\u65b9\u5dee\uff0c\\(\\gamma\\)\u548c\\(\\beta\\)\u662f\u53ef\u5b66\u4e60\u53c2\u6570\uff0c\\(\\epsilon\\)\u662f\u5fae\u5c0f\u5e38\u6570\u4ee5\u9632\u6b62\u9664\u96f6\u3002\n- **\u5e94\u7528**\uff1a\u5e7f\u6cdb\u7528\u4e8eTransformer\u6a21\u578b\u4e2d\uff0c\u6709\u52a9\u4e8e\u7f13\u89e3\u68af\u5ea6\u6d88\u5931\u95ee\u9898\uff0c\u63d0\u5347\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002\n\n#### RMSNorm\uff08Root Mean Square Normalization\uff09\n- **\u5b9a\u4e49**\uff1aRMSNorm\u662f\u5bf9\u6bcf\u4e2a\u6837\u672c\u7684\u6bcf\u4e2a\u7279\u5f81\u8fdb\u884c\u5f52\u4e00\u5316\uff0c\u4f46\u53ea\u5f52\u4e00\u5316\u65b9\u5dee\uff0c\u4e0d\u5f52\u4e00\u5316\u5747\u503c\u3002\n- **\u516c\u5f0f**\uff1a\n  \\[\n  \\text{RMSNorm}(x) = \\frac{x}{\\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} x_i^2 + \\epsilon}} \\cdot \\gamma + \\beta\n  \\]\n  \u5176\u4e2d\uff0c\\(n\\)\u662f\u7279\u5f81\u6570\u91cf\uff0c\\(\\gamma\\)\u548c\\(\\beta\\)\u662f\u53ef\u5b66\u4e60\u53c2\u6570\uff0c\\(\\epsilon\\)\u662f\u5fae\u5c0f\u5e38\u6570\u3002\n- **\u5e94\u7528**\uff1a\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0cRMSNorm\u6bd4LayerNorm\u66f4\u7b80\u5355\uff0c\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\uff0c\u5c24\u5176\u5728\u5927\u89c4\u6a21\u6a21\u578b\u8bad\u7ec3\u4e2d\u3002\n\n#### \u4e3b\u8981\u533a\u522b\n1. **\u5f52\u4e00\u5316\u65b9\u5f0f**\uff1a\n   - LayerNorm\u540c\u65f6\u5f52\u4e00\u5316\u5747\u503c\u548c\u65b9\u5dee\u3002\n   - RMSNorm\u53ea\u5f52\u4e00\u5316\u65b9\u5dee\u3002\n2. **\u8ba1\u7b97\u590d\u6742\u5ea6**\uff1a\n   - LayerNorm\u9700\u8981\u8ba1\u7b97\u5747\u503c\u548c\u65b9\u5dee\uff0c\u8ba1\u7b97\u91cf\u7a0d\u5927\u3002\n   - RMSNorm\u53ea\u9700\u8ba1\u7b97\u65b9\u5dee\uff0c\u8ba1\u7b97\u91cf\u8f83\u5c0f\u3002\n3. **\u6548\u679c**\uff1a\n   - LayerNorm\u5728\u8bb8\u591a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u5728NLP\u9886\u57df\u3002\n   - RMSNorm\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u8868\u73b0\u76f8\u4f3c\u751a\u81f3\u66f4\u4f18\uff0c\u4e14\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u3002\n\n\u901a\u8fc7\u4e0a\u8ff0\u5206\u6790\uff0c\u53ef\u4ee5\u66f4\u597d\u5730\u7406\u89e3LayerNorm\u548cRMSNorm\u5728\u5927\u6a21\u578b\u4e2d\u7684\u5e94\u7528\u53ca\u5176\u4f18\u7f3a\u70b9\uff0c\u4ece\u800c\u5728\u5177\u4f53\u4efb\u52a1\u4e2d\u9009\u62e9\u5408\u9002\u7684\u5f52\u4e00\u5316\u65b9\u6cd5\u3002", "task_step_name": "\u6cdb\u5316\u80fd\u529b", "task_step_description": "\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\u3001\u5143\u5b66\u4e60\u7b49\u65b9\u6cd5\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f7f\u5176\u80fd\u591f\u5e94\u5bf9\u66f4\u591a\u73b0\u5b9e\u5e94\u7528\u573a\u666f\u3002", "task_step_level": "3>2", "task_step_question": "### \u95ee\u9898\n\n\u5728\u63d0\u5347\u5927\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u65f6\uff0cLayerNorm\u548cRMSNorm\u8fd9\u4e24\u79cd\u5f52\u4e00\u5316\u65b9\u6cd5\u5728\u8fc1\u79fb\u5b66\u4e60\u548c\u5143\u5b66\u4e60\u4e2d\u7684\u5e94\u7528\u6548\u679c\u6709\u4f55\u4e0d\u540c\uff1f\u5177\u4f53\u6765\u8bf4\uff0c\u5b83\u4eec\u5728\u5904\u7406\u4e0d\u540c\u6570\u636e\u96c6\u548c\u5e94\u7528\u573a\u666f\u65f6\u7684\u7a33\u5b9a\u6027\u3001\u8ba1\u7b97\u6548\u7387\u4ee5\u53ca\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u6709\u54ea\u4e9b\u5dee\u5f02\uff1f", "task_step_question_context": [{"ref_id": "454847042436311108", "chunk_id": "4", "score": 0.259765625, "text": "# 3 Experiments and Results\nWe evaluate the performance of PreNorm and PostNorm for ZST on various datasets and language pairs. We then analyze the off-target rates and structural discrepancies between PreNorm and PostNorm to understand performance differences.  \n\n$$\n\\mathrm{LayerNorm}(\\mathbf{x})=\\frac{\\mathbf{x}-\\mathbf{E}(\\mathbf{x})}{\\sqrt{\\mathbf{V}(\\mathbf{x})}}\\cdot\\mathbf{g}+\\mathbf{b},\n$$  \n\nwhere $\\mathbf{g}$ and $\\mathbf{b}$ are trainable gain and bias. $\\mathbf{E}$ and $\\mathbf{V}$ indicate expectation and variance. LayerNorm is commonly used in two positions in the Transformer, as shown in Fig. 1 . PostNorm, which is the originally proposed setting of the Transformer ( Vaswani et al. ,2017 ), involves applying LayerNorm after each sub-module (i.e., selfattention or feed-forward network) and residual connections. PreNorm ( Baevski and Auli ,2019 ), on the other hand, involves applying LayerNorm directly before each sub-module and is known to stabilize Transformer training. While variants of Transformer LayerNorm like RMSNorm ( Zhang and Sennrich ,2019 ) have been proposed, the vanilla PreNorm and PostNorm are still the most widely adopted settings in current multilingual\n\n# 3.1 Experimental Settings\nDatasets We perform ZST experiments on three datasets: OPUS ( Zhang et al. ,2020 ), IWSLT ( Cettolo et al. ,2017 ), and Europarl ( Koehn ,2005 ). The statistics of the datasets are summarized in Table 1 .We include 7 ,4 , and 5 languages for each dataset. The training data consists of only English-centric sentence pairs, resulting in 30 ,6 , and 12 ZST directions for each dataset. The total number of parallel sentences for each dataset is 12 .00 M, 1 .38 M, and 15 .78 M, respectively. We apply BPE ( Sennrich et al. ,2016 ) with merge operations of 50 k, 40 k, and $50\\mathbf{k}$ to create a joint vocabulary for each dataset.  \n\nTraining We employ Transformer-base model for OPUS and IWSLT, and Transformer-big for Europarl, in accordance with the distinct sizes of training data. We consider the following settings: (1) PreNorm or PostNorm : PreNorm involves LayerNorm directly before each sub-module (i.e., self-attention or feed-forward network), while PostNorm applies LayerNorm after each sub-module and residual connections, as shown in Fig. 1 .(2) S-ENC-T-DEC or T-ENC : Source language tag on the encoder-side and target language tag on the decoder-side; or only target language tag on the encoder-side. Wu et al. (2021 ) showed that this setting impacts ZST for Transformer with PreNorm. (3) w/ or w/o Res. : With the residual connection for self-attention in the middle $(4^{t h})$ encoder layer or not. Liu et al. (2021 ) revealed that \u201cw/o Res.\u201d improves ZST for the model trained with PreNorm. We experiment this with different LayerNorm settings as this may reduce the potential of overfitting on supervised directions, then further impacts ZST, which aligns with our hypothesis.  \n\nTable 2: BLEU scores and off-target rates (shown in brackets) . We report the average score of three seeds; refer to Appendix Gfor BLEU score of each translation direction and seed. \u201cRes.\u201d indicates the residual connection of self-attention in the $4^{t h}$ encoder layer. We mark lower off-target rates and significantly higher BLEU scores ( Koehn ,2004 ) between PreNorm and PostNorm in bold for ZST.   \n\n\n<html><body><table><tr><td>#</td><td>Layer Norm</td><td>Language Tag</td><td>Res.</td><td></td><td>Zero-shot</td><td></td><td></td><td>Supervised</td><td></td></tr><tr><td>0</td><td></td><td>Pivot</td><td></td><td>OPUS 21.8</td><td>IWSLT 20.0</td><td>Europarl 29.5</td><td>OPUS</td><td>IWSLT</td><td>Europarl</td></tr><tr><td>1</td><td>PreNorm</td><td>S-ENC-T-DEC</td><td>w/</td><td>10.1 (42.19%)</td><td>4.9 (64.84%)</td><td>24.9 ( 7.73%)</td><td>33.7</td><td>31.5</td><td>34.3</td></tr><tr><td>2</td><td>PostNorm</td><td>S-ENC-T-DEC</td><td>w/</td><td>16.8 ( 8.59%)</td><td>12.4 (10.61%)</td><td>29.2( 0.34%)</td><td>33.9</td><td>31.5</td><td>34.5</td></tr><tr><td>3</td><td>PreNorm</td><td>T-ENC</td><td>w/</td><td>13.3 (22.99%)</td><td>13.7 ( 3.98%)</td><td>29.5( 0.23%)</td><td>33.7</td><td>31.6</td><td>34.4</td></tr><tr><td>4</td><td>PostNorm</td><td>T-ENC</td><td>w/</td><td>14.0 (22.86%)</td><td>15.5 ( 4.59%)</td><td>30.8 ( 0.11%)</td><td>34.1</td><td>31.5</td><td>34.5</td></tr><tr><td>5</td><td>PreNorm</td><td>S-ENC-T-DEC</td><td>w/o</td><td>14.3 (20.67%)</td><td>8.0 (50.16%)</td><td>16.7 (41.87%)</td><td>33.6</td><td>30.9</td><td>34.3</td></tr><tr><td>6</td><td>PostNorm</td><td>S-ENC-T-DEC</td><td>w/o</td><td>16.0 (15.27%)</td><td>17.4 (1.83%)</td><td>29.0 ( 0.41%)</td><td>33.8</td><td>30.7</td><td>34.4</td></tr><tr><td>7</td><td>PreNorm</td><td>T-ENC</td><td>w/o</td><td>13.4 (27.15%)</td><td>16.2 ( 1.54%)</td><td>29.9 ( 2.15%)</td><td>33.5</td><td>30.9</td><td>34.3</td></tr><tr><td>8</td><td>PostNorm</td><td>T-ENC</td><td>w/o</td><td>13.9 (26.68%)</td><td>17.8 (1.50%)</td><td>30.8 ( 0.13%)</td><td>33.9</td><td>30.6</td><td>34.4</td></tr></table></body></html>  \n\nThe settings above lead to eight different combinations, shown in Table 2 (#1 - #8). Additional training details are in Appendix A .\n\n# 3.2 Main Results\nWe evaluate ZST systems using SacreBLEU ( Post ,2018 ) and off-target rates. We report in Table 2 BLEU scores for both zero-shot and supervised directions. For ZST, we also present pivot-based translation results as a reference. Implementation details of evaluation can be found in Appendix B.Our findings are as follows:  \n\nPreNorm vs. PostNorm :We find that PostNorm consistently yields better BLEU scores than PreNorm for ZST across various language tag and residual connection settings, while their performance is comparable for supervised directions.  \n\nImpact of Language Tag and Residual Connection: We observe that using the \u201cT-ENC\u201d language tag and \u201cw/ Res.\u201d improves ZST performance for IWSLT, which aligns with the findings of $\\mathrm{Wu}$ et al. (2021 ) and Liu et al. (2021 ). Nevertheless, the best performance is achieved using \u201cw/ Res.\u201d for PostNorm with \u201cS-ENC-T-DEC\u201d and \u201cT-ENC\u201d tags for OPUS and Europarl, respectively (#2 and #4). Given that Wu et al. (2021 ) and Liu et al. (2021 )used PreNorm as the default setting (#2, #4, #6 and #8 are unreported results in their work), our results emphasize the need to consider PostNorm as the default setting for ZST, while the language tag and residual connection settings have less impact.  \n\nOff-target Rates : Off-target rates help understand the different BLEU score gaps between PreNorm and PostNorm, which ranges from 0 .5 to 12 .3 BLEU points. For PreNorm and PostNorm with the \u201cT-ENC\u201d language tag (#3, #4, #7, and #8), they have similar off-target rates, with a discrepancy ranging from $-0.61\\%$ to $2.02\\%$ , which results in narrow BLEU score gaps, ranging from 0 .5 to 1 .8 points. However, for PreNorm and PostNorm with the \u201cS-ENC-T-DEC\u201d language tag (#1, #2, #5, and #6), the off-target rates show a more considerable discrepancy, ranging from $5.40\\%$ to $54.23\\%$ , resulting in BLEU score gaps from 1 .7 to 12 .3 points. Further analysis of the nature of Transformer hidden states in the next section explores the reason for these different off-target rates in translations."}, {"ref_id": "454845744505973136", "chunk_id": "6", "score": 0.1982421875, "text": "# 2 Problem Formulation\n\n# 2.1 Related Work\nLarge Language Models (LLMs). The advent of LLMs has led to a paradigm shift in the study of natural language processing (NLP), computer vision, information retrieval, and other domains[Menghani, 2023, Chen et al., 2023, Jiang et al., 2023]. The impressive effectiveness and generalizability of LLMs has come at the price of a drastic increase in LLM sizes [Treviso et al., 2023] and consequent challenges, including huge amounts of computational resources and data required to train, and prohibitive expenses at both training and deployment stages [Bender et al., 2021].  \n\nEfficient Machine Learning (ML) Inference. LLMs belong to a class of models called foundation models [Bommasani et al., 2021] \u2013 models that are trained once and can then be used to serve a wide variety of tasks. As such, we expect inference cost to dominate the overall cost of such models and hence focus on works that reduce the cost of ML inference [Menghani, 2023]. The most common approach for efficient ML inference is model compression i.e., replacing a large model with a smaller model of comparable accuracy. Common techniques for model compression include (i) model pruning [Hassibi et al., 1993, LeCun et al., 1989] which drops parts of the model with minimal accuracy loss, (ii) quantization [Jacob et al., 2018, Vanhoucke et al., 2011] which reduces model memory footprints and inference latency by reducing the precision of data representation (e.g., FP32 to INT8), (iii) knowledge distillation [Hinton et al., 2015, Urban et al., 2016] which trains small student models to mimic large teacher models, and (iv) Neural Architecture Search [Elsken et al., 2019, Zoph and Le, 2016] which tunes model architecture to improve model performance, under inference cost constraints. Such static efficiency optimizations typically produce a fixed model with lower inference cost and lower accuracy compared to the large model which may not suffice for foundation models like LLMs, whose core premise is that the same model will serve a range of tasks, each with its own accuracy/cost constraints. This is already manifesting in inference platforms described in Section 1 which need more dynamic optimizations to meet the demands of all users.  \n\nHybrid ML Inference. Recent works [Kag et al., 2022, Ding et al., 2022] have introduced a new inference paradigm called hybrid inference which uses two models of different sizes instead of a single model for inference. The smaller model (e.g. Llama2 [Touvron et al., 2023]) generally has lower inference cost but also lower accuracy than the larger model (e.g. GPT-4 [OpenAI, 2023]). The key idea is to identify and route easy queries to the small model so that inference cost can be reduced while maintaining response quality. By tuning a threshold on query difficulty we can dynamically trade off quality and cost for the same inference setup. [Kag et al., 2022] study this setup for image classification and propose to train the small model, large model, and router from scratch. However LLM training is expensive and retraining LLMs from scratch for every scenario goes against the very premise of inference with pre-trained foundation models. Moreover text generation [Iqbal and Qureshi, 2022] is often more ambiguous and challenging than image classification due to which novel techniques are required for effective hybrid LLM inference for text generation.  \n\nInference with Multiple LLMs. Some recent works [Jiang et al., 2023, Chen et al., 2023, Leviathan et al., 2023, Kim et al., 2023] use multiple LLMs for inference but these approaches typically call more than one LLM for a single query that can incur significant computational overheads. Specifically [Jiang et al., 2023] calls an ensemble of LLMs at inference time due to which the inference cost will be proportional to the number of models in the system. [Chen et al., 2023] performs inference using a cascade of LLMs where responses to the query are generated sequentially by the LLMs in the cascade until one of the models has a confidence score higher than a predefined threshold. Our work provides high quality responses while always making a single LLM call for all queries and will thus incur much lower computational cost than both of these works on average. Speculative decoding, introduced in [Leviathan et al., 2023, Kim et al., 2023] speeds up decoding of expensive models by invoking small-and-efficient decoders on the \u201ceasy\u201d decoding steps. Instead, in our work we are interested in query routing which assigns \u201ceasy\u201d queries to small models to reduce overall inference costs while maintaining high performance. While the two approaches have different goals, an interesting line of future work would be to combine these so that our router assigns queries to the small or large model based on query difficulty and then speculative decoding is applied on top to speed up inference for queries assigned to the large model thereby leading to further cost reduction."}, {"ref_id": "454984230919739446", "chunk_id": "8", "score": 0.1904296875, "text": "# 6 Conclusion\nLLM-based RL algorithms have shown generalization across multiple tasks and games. We argue that this ability comes from implicit memory that fits a large number of parameters to the training data, which is inefficient in terms of model size. In contrast, we propose a new approach inspired by the concept of \u201cworking memory\u201d called Decision Transformers with Memory (DT-Mem), which stores training experience explicitly in a content-addressable matrix module for later retrieval and use. Evaluation demonstrates that DT-Mem achieves better generalization on Atari games with only $10\\%$ of the model parameters compared to the state-of-the-art method. Furthermore, we demonstrate that fine-tuning DT-Memwith a small amount of data can produce state-of-the-art results on both Atari games and the Meta-World environment, when compared to MDT [22], PDT [37], and HDT [38].  \n\nLimitations The first limitation of our work is the sample efficiency of memory fine-tuning. The $10\\%$ fine-tuning dataset is still sizeable, and we plan to explore more sample-efficient methods in the future. We could for instance consider a setting with more tasks, each one with less data so that the inter-task generalization would be even more crucial to its performance. Additionally, this work does not propose a control strategy for collecting data on a new task. For future work, we plan to investigate online data collection methods, which includes the design and learning of exploration strategies for an efficient fine-tuning on new tasks. Finally, the approach has been intuitively motivated, but it would be valuable to have a theoretical grounding that would show the structural limits of large models and how equipping them with a memory component overcomes them.  \n\nSocietal Impact We do not foresee any significant societal impact resulting from our proposed method. The current algorithm is not designed to interact with humans, nor any realistic environment yet. If one chooses to extend our methods to such situations, caution should be exercised to ensure that any safety and ethical concerns are appropriately addressed. As our work is categorized in the offline-RL domain, it is feasible to supplement its training with a dataset that aligns with human intents and values. However, one must be wary that the way our architecture generalizes across tasks is still not well understood and as a consequence we cannot guarantee the generalization of its desirable features: performance, robustness, fairness, etc. By working towards methods that improve the computational efficiency of large models, we contribute to increase their access and reduce their ecological impact.\n\n\n\n# A Implementation Details\n\n# A.1 DT-Mem network architecture\nTable 3 summarizes the different model configurations used for evaluation. In this section, we describe these model configurations in detail. While Table 3 provides a summary, we will also provide additional information here. DT-Mem, PDT and HDT are all share the same transformer architectures. However, for task-adaptation, HDT utilizes a pre-trained $2.3\\mathbf{M}$ hyper-network, while DT-Mem introduces 147K LoRA parameters. To compare with MDT, we use the same parameter size as reported in [22].  \n\nTable 3: Detailed Model Sizes   \n\n\n<html><body><table><tr><td>Model</td><td>Layers</td><td>Hidden size (d)</td><td>Heads</td><td>Params</td><td>Memory Size</td><td>Memory Module Params</td></tr><tr><td>HDT</td><td>4</td><td>512</td><td>8</td><td>13M</td><td>N.A.</td><td>N.A.</td></tr><tr><td>MDT-200M</td><td>10</td><td>1280</td><td>20</td><td>200M</td><td>N.A.</td><td>N.A.</td></tr><tr><td>DT-Mem</td><td>4</td><td>512</td><td>8</td><td>13M</td><td>559K</td><td>7M</td></tr></table></body></html>\n\n# A.2 Hyper-parameters\nIn this section, we will delve into the specifics of the model parameters. Understanding these parameters is key to understanding the workings of the model. It is worth noting that the source code for this model is publicly available at https://github.com/luciferkonn/DT_Mem/tree/main .This allows for a deeper understanding of the model\u2019s inner workings and may facilitate the replication of its results.  \n\nTable 4: Hyperparameters for DT-Mem training   \n\n\n<html><body><table><tr><td>Hyperparameters</td><td>Value</td></tr><tr><td>K (length of context)</td><td>28</td></tr><tr><td>dropoutrate</td><td>0.1</td></tr><tr><td>maximum epochs</td><td>1000</td></tr><tr><td>steps for each epoch</td><td>1000</td></tr><tr><td>optimizer learning rate</td><td>1e-4</td></tr><tr><td>weight decay</td><td>1e-4</td></tr><tr><td>gradient norm clip</td><td>1.</td></tr><tr><td>data points for each dataset</td><td>500,000</td></tr><tr><td>batch size</td><td>64</td></tr><tr><td>memory slots</td><td>1290</td></tr><tr><td>activation</td><td>GELU</td></tr><tr><td>optimizer</td><td>Adamw</td></tr><tr><td>scheduler</td><td>LambdaLR</td></tr></table></body></html>\n\n# A.3 Training and fine-tuning algorithm\nIn this section, we present the pre-training DT-Memin Appendix A.3 and fine-tuning DT-Mem with LoRA in Appendix 5.5.  \n\nWe pre-train DT-Mem on multiple offline datasets. Each gradient update of the DT-Memmodel considers information from each training task.  \n\nWe fine-tune the memory module to adapt to each downstream task. To achieve this, we fix the pre-trained DT-Mem model parameters and add additional LoRA parameters for the memory module feed-forward neural networks. The fine-tune dataset is used to update these LoRA parameters only.\n\n# Algorithm 1 Pre-train DT-Mem\n1: for T episodes do   \n2: 3: 4: for Sample trajectories Split trajectories into different segments with length K and calculate return-to-go in the Task $\\mathcal{T}_{i}\\in T^{t r a i n}\\;.$ do $\\tau=(s_{0},a_{0},r_{0},\\cdot\\cdot\\cdot\\,,s_{H},a_{H},r_{H})$ m the dataset $\\mathcal{D}_{i}$ .  \ninput sequence.   \n5: Given $\\hat{\\tau}_{t+1:t+K}$ , compute the sequence embedding $e_{s e q}$ .  \n6: Update the working memory and retrieve the relative information as $E_{o u t}$   \n7: Given $E_{o u t}$ , predict actions $\\tilde{a}_{t}$ , reward $\\tilde{r}_{t}$ , and return-to-go ${\\tilde{R}}_{t}$ .  \n8: Compute the loss according to Eqn. 1.   \n9: Update all modules parameters.   \n10: end for   \n11: end for  \n\nAlgorithm 2 Fine-tuning DT-Mem  \n\n$\\hat{B}^{q},\\hat{B}^{k},\\hat{B}^{v},\\hat{A}^{q},\\hat{A}^{k},\\hat{A}^{v},B^{q},A^{q},B^{k},A^{k}$ Require: Fine-tuning dataset $\\mathcal{T}^{i}~\\in~T^{t e s t}$ .dataset $\\mathcal{D}^{i}$ for $\\mathcal{T}^{i}$ .Initialize LoRA parameters 1: for T steps do   \n2: Split trajectories into different segments with length $\\mathbf{K}$ and calculate return-to-go in the input sequence.   \n3: Given $\\hat{\\tau}_{t+1:t+K}$ , compute the sequence embedding $e_{s e q}$ .  \n4: Update working memory using $\\hat{Q}\\,=\\,M(\\hat{W}^{q}+\\hat{B}^{q}\\bar{A}^{q})$ ,$\\hat{K}\\,=\\,M(\\hat{W}^{k}\\,+\\,\\hat{B}^{k}\\hat{A}^{k}),\\hat{V}\\,=$ $M(\\hat{W}^{v}+\\hat{B}^{v}\\hat{A}^{v})$ ,$Q=M(W^{q}+B^{q}A^{q}),K=M(W^{k}+B^{k}A^{k})$   \n5: Retrieve the relative information as $E_{o u t}$   \n6: Given $E_{o u t}$ , predict actions $\\tilde{a}_{t}$ , reward $\\tilde{r}_{t}$ , and return-to-go ${\\tilde{R}}_{t}$ .  \n7: Compute the loss according to Eqn. 1.   \n8: Update LoRA parameters only.   \n9: end for"}], "task_step_question_answer": "\u5728\u8fc1\u79fb\u5b66\u4e60\u548c\u5143\u5b66\u4e60\u4e2d\uff0cLayerNorm\u548cRMSNorm\u7684\u5e94\u7528\u6548\u679c\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002\u5177\u4f53\u6765\u8bf4\uff0cLayerNorm\u901a\u8fc7\u8ba1\u7b97\u8f93\u5165\u6570\u636e\u7684\u5747\u503c\u548c\u65b9\u5dee\u8fdb\u884c\u5f52\u4e00\u5316\uff0c\u9002\u7528\u4e8e\u5904\u7406\u4e0d\u540c\u6570\u636e\u96c6\u548c\u5e94\u7528\u573a\u666f\u65f6\u8868\u73b0\u51fa\u8f83\u9ad8\u7684\u7a33\u5b9a\u6027\uff0c\u5c24\u5176\u662f\u5728Transformer\u6a21\u578b\u4e2d\uff0cPostNorm\uff08\u5728\u5b50\u6a21\u5757\u548c\u6b8b\u5dee\u8fde\u63a5\u540e\u5e94\u7528LayerNorm\uff09\u5728\u96f6\u6837\u672c\u7ffb\u8bd1\uff08ZST\uff09\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8ePreNorm\uff08\u5728\u5b50\u6a21\u5757\u524d\u5e94\u7528LayerNorm\uff09\uff0c\u80fd\u591f\u663e\u8457\u964d\u4f4e\u8131\u9776\u7387\u5e76\u63d0\u9ad8BLEU\u5206\u6570\u3002\u7136\u800c\uff0cLayerNorm\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u8f83\u9ad8\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u5927\u89c4\u6a21\u6570\u636e\u65f6\uff0c\u53ef\u80fd\u4f1a\u5f71\u54cd\u8ba1\u7b97\u6548\u7387\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0cRMSNorm\u901a\u8fc7\u8ba1\u7b97\u8f93\u5165\u6570\u636e\u7684\u5747\u65b9\u6839\u8fdb\u884c\u5f52\u4e00\u5316\uff0c\u5177\u6709\u66f4\u9ad8\u7684\u8ba1\u7b97\u6548\u7387\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u5927\u89c4\u6a21\u6570\u636e\u96c6\u65f6\uff0c\u80fd\u591f\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u3002\u7136\u800c\uff0cRMSNorm\u5728\u5904\u7406\u4e0d\u540c\u6570\u636e\u96c6\u548c\u5e94\u7528\u573a\u666f\u65f6\u7684\u7a33\u5b9a\u6027\u53ef\u80fd\u4e0d\u5982LayerNorm\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u9ad8\u5ea6\u6cdb\u5316\u80fd\u529b\u7684\u4efb\u52a1\u4e2d\uff0c\u5982\u8fc1\u79fb\u5b66\u4e60\u548c\u5143\u5b66\u4e60\uff0cRMSNorm\u53ef\u80fd\u4f1a\u5bfc\u81f4\u6a21\u578b\u6027\u80fd\u7684\u6ce2\u52a8\u3002\u56e0\u6b64\uff0c\u5728\u9009\u62e9\u5f52\u4e00\u5316\u65b9\u6cd5\u65f6\uff0c\u9700\u8981\u6839\u636e\u5177\u4f53\u4efb\u52a1\u7684\u9700\u6c42\u8fdb\u884c\u6743\u8861\uff0c\u4ee5\u5b9e\u73b0\u6700\u4f73\u7684\u6cdb\u5316\u80fd\u529b\u548c\u8ba1\u7b97\u6548\u7387\u3002", "ref_task_step_id": ""}, "__type__": "task_step_node"}, "60873e05-37b0-4b0f-aaf8-5e2b4f988558": {"__data__": {"id_": "60873e05-37b0-4b0f-aaf8-5e2b4f988558", "metadata": {}, "relationships": {}, "hash": "", "start_task_context": "\u5927\u6a21\u578b\u4e2d\u7684LayerNorm\u548cRMSNorm\u6709\u4ec0\u4e48\u533a\u522b\uff1f", "aemo_representation_context": "### Step-by-Step Decomposition of Computer Science Research Content\n\n#### 1. \u5206\u6790\u8fd1\u51e0\u5e74\u7814\u7a76\u9886\u57df\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba\n- **\u6280\u672f\u6846\u67b6**\uff1a\u8fd1\u5e74\u6765\uff0c\u6df1\u5ea6\u5b66\u4e60\u3001\u5f3a\u5316\u5b66\u4e60\u3001\u56fe\u795e\u7ecf\u7f51\u7edc\u7b49\u6846\u67b6\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u9886\u57df\u5360\u636e\u4e3b\u5bfc\u5730\u4f4d\u3002\u5177\u4f53\u6846\u67b6\u5982Transformer\u3001BERT\u3001GNN\u7b49\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002\n- **\u65b9\u6cd5\u8bba**\uff1a\u7814\u7a76\u65b9\u6cd5\u9010\u6e10\u4ece\u5355\u4e00\u6a21\u578b\u8f6c\u5411\u591a\u6a21\u578b\u878d\u5408\uff0c\u4ece\u76d1\u7763\u5b66\u4e60\u6269\u5c55\u5230\u65e0\u76d1\u7763\u548c\u81ea\u76d1\u7763\u5b66\u4e60\uff0c\u5f3a\u8c03\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002\n\n#### 2. \u7814\u7a76\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u4e3b\u8981\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0e\u53d8\u4f53\n- **\u5e94\u7528**\uff1a\u4f8b\u5982\uff0cTransformer\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u4e2d\u88ab\u5e7f\u6cdb\u5e94\u7528\uff0cBERT\u53d8\u4f53\u5982RoBERTa\u3001ALBERT\u5728\u6587\u672c\u5206\u7c7b\u3001\u95ee\u7b54\u7cfb\u7edf\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff1bGNN\u5728\u63a8\u8350\u7cfb\u7edf\u3001\u77e5\u8bc6\u56fe\u8c31\u7b49\u9886\u57df\u6709\u663e\u8457\u5e94\u7528\u3002\n- **\u53d8\u4f53**\uff1a\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\uff0c\u7814\u7a76\u8005\u63d0\u51fa\u4e86\u591a\u79cd\u53d8\u4f53\uff0c\u5982\u9488\u5bf9\u957f\u5e8f\u5217\u5904\u7406\u7684Transformer-XL\uff0c\u9488\u5bf9\u591a\u6a21\u6001\u6570\u636e\u7684ViLBERT\u7b49\u3002\n\n#### 3. \u8bc4\u4f30\u5b66\u672f\u754c\u7684\u6280\u672f\u8fdb\u6b65\u4e0e\u5c40\u9650\u6027\n- **\u6280\u672f\u8fdb\u6b65**\uff1a\u6a21\u578b\u6027\u80fd\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\uff0c\u8ba1\u7b97\u6548\u7387\u4e5f\u6709\u6240\u63d0\u9ad8\uff0c\u5982EfficientNet\u5728\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u8868\u73b0\u3002\n- **\u5c40\u9650\u6027**\uff1a\u4f9d\u7136\u5b58\u5728\u6a21\u578b\u504f\u5dee\uff08\u5982\u6027\u522b\u504f\u89c1\uff09\u3001\u6570\u636e\u4f9d\u8d56\uff08\u5982\u5bf9\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\uff09\u7b49\u95ee\u9898\uff0c\u6cdb\u5316\u80fd\u529b\u6709\u5f85\u8fdb\u4e00\u6b65\u63d0\u5347\u3002\n\n#### 4. \u63a2\u8ba8\u8ba1\u7b97\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\n- **\u9002\u7528\u6027**\uff1a\u7814\u7a76\u6a21\u578b\u5728\u4e0d\u540c\u9886\u57df\uff08\u5982\u533b\u7597\u3001\u91d1\u878d\uff09\u548c\u591a\u6a21\u6001\u6570\u636e\uff08\u5982\u6587\u672c+\u56fe\u50cf\uff09\u4e0a\u7684\u8868\u73b0\uff0c\u5982\u591a\u6a21\u6001Transformer\u6a21\u578b\u3002\n- **\u6cdb\u5316\u80fd\u529b**\uff1a\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\u3001\u5143\u5b66\u4e60\u7b49\u65b9\u6cd5\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f7f\u5176\u80fd\u591f\u5e94\u5bf9\u66f4\u591a\u73b0\u5b9e\u5e94\u7528\u573a\u666f\u3002\n\n#### 5. \u5206\u6790\u6700\u65b0\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027\n- **\u7a33\u5b9a\u6027**\uff1a\u7814\u7a76\u7b97\u6cd5\u5728\u590d\u6742\u3001\u52a8\u6001\u73af\u5883\u4e0b\u7684\u8868\u73b0\uff0c\u5982\u5bf9\u6297\u8bad\u7ec3\u3001\u9c81\u68d2\u4f18\u5316\u7b49\u65b9\u6cd5\u7684\u5e94\u7528\u3002\n- **\u5bb9\u9519\u6027**\uff1a\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\u7684\u9002\u5e94\u6027\uff0c\u5982\u5206\u5e03\u5f0f\u8bad\u7ec3\u4e2d\u7684\u68af\u5ea6\u538b\u7f29\u3001\u6a21\u578b\u5e76\u884c\u7b49\u6280\u672f\u3002\n\n#### 6. \u8bc4\u4f30\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218\n- **\u672a\u6765\u65b9\u5411**\uff1a\u5982\u53ef\u89e3\u91caAI\u3001\u8054\u90a6\u5b66\u4e60\u3001\u9690\u79c1\u4fdd\u62a4\u7b49\u65b0\u5174\u7814\u7a76\u65b9\u5411\u3002\n- **\u6311\u6218**\uff1a\u5982\u4f55\u5e73\u8861\u6a21\u578b\u6027\u80fd\u4e0e\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\uff0c\u5982\u4f55\u89e3\u51b3\u6570\u636e\u504f\u89c1\u548c\u4f26\u7406\u95ee\u9898\u7b49\u3002\n\n### \u5927\u6a21\u578b\u4e2d\u7684LayerNorm\u548cRMSNorm\u7684\u533a\u522b\n\n#### LayerNorm\uff08Layer Normalization\uff09\n- **\u5b9a\u4e49**\uff1aLayerNorm\u662f\u5bf9\u6bcf\u4e2a\u6837\u672c\u7684\u6bcf\u4e2a\u7279\u5f81\u8fdb\u884c\u5f52\u4e00\u5316\uff0c\u4f7f\u5176\u5747\u503c\u548c\u65b9\u5dee\u5206\u522b\u63a5\u8fd10\u548c1\u3002\n- **\u516c\u5f0f**\uff1a\n  \\[\n  \\text{LayerNorm}(x) = \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} \\cdot \\gamma + \\beta\n  \\]\n  \u5176\u4e2d\uff0c\\(\\mu\\)\u548c\\(\\sigma^2\\)\u5206\u522b\u662f\u7279\u5f81\u5747\u503c\u548c\u65b9\u5dee\uff0c\\(\\gamma\\)\u548c\\(\\beta\\)\u662f\u53ef\u5b66\u4e60\u53c2\u6570\uff0c\\(\\epsilon\\)\u662f\u5fae\u5c0f\u5e38\u6570\u4ee5\u9632\u6b62\u9664\u96f6\u3002\n- **\u5e94\u7528**\uff1a\u5e7f\u6cdb\u7528\u4e8eTransformer\u6a21\u578b\u4e2d\uff0c\u6709\u52a9\u4e8e\u7f13\u89e3\u68af\u5ea6\u6d88\u5931\u95ee\u9898\uff0c\u63d0\u5347\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002\n\n#### RMSNorm\uff08Root Mean Square Normalization\uff09\n- **\u5b9a\u4e49**\uff1aRMSNorm\u662f\u5bf9\u6bcf\u4e2a\u6837\u672c\u7684\u6bcf\u4e2a\u7279\u5f81\u8fdb\u884c\u5f52\u4e00\u5316\uff0c\u4f46\u53ea\u5f52\u4e00\u5316\u65b9\u5dee\uff0c\u4e0d\u5f52\u4e00\u5316\u5747\u503c\u3002\n- **\u516c\u5f0f**\uff1a\n  \\[\n  \\text{RMSNorm}(x) = \\frac{x}{\\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} x_i^2 + \\epsilon}} \\cdot \\gamma + \\beta\n  \\]\n  \u5176\u4e2d\uff0c\\(n\\)\u662f\u7279\u5f81\u6570\u91cf\uff0c\\(\\gamma\\)\u548c\\(\\beta\\)\u662f\u53ef\u5b66\u4e60\u53c2\u6570\uff0c\\(\\epsilon\\)\u662f\u5fae\u5c0f\u5e38\u6570\u3002\n- **\u5e94\u7528**\uff1a\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0cRMSNorm\u6bd4LayerNorm\u66f4\u7b80\u5355\uff0c\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\uff0c\u5c24\u5176\u5728\u5927\u89c4\u6a21\u6a21\u578b\u8bad\u7ec3\u4e2d\u3002\n\n#### \u4e3b\u8981\u533a\u522b\n1. **\u5f52\u4e00\u5316\u65b9\u5f0f**\uff1a\n   - LayerNorm\u540c\u65f6\u5f52\u4e00\u5316\u5747\u503c\u548c\u65b9\u5dee\u3002\n   - RMSNorm\u53ea\u5f52\u4e00\u5316\u65b9\u5dee\u3002\n2. **\u8ba1\u7b97\u590d\u6742\u5ea6**\uff1a\n   - LayerNorm\u9700\u8981\u8ba1\u7b97\u5747\u503c\u548c\u65b9\u5dee\uff0c\u8ba1\u7b97\u91cf\u7a0d\u5927\u3002\n   - RMSNorm\u53ea\u9700\u8ba1\u7b97\u65b9\u5dee\uff0c\u8ba1\u7b97\u91cf\u8f83\u5c0f\u3002\n3. **\u6548\u679c**\uff1a\n   - LayerNorm\u5728\u8bb8\u591a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u5728NLP\u9886\u57df\u3002\n   - RMSNorm\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u8868\u73b0\u76f8\u4f3c\u751a\u81f3\u66f4\u4f18\uff0c\u4e14\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u3002\n\n\u901a\u8fc7\u4e0a\u8ff0\u5206\u6790\uff0c\u53ef\u4ee5\u66f4\u597d\u5730\u7406\u89e3LayerNorm\u548cRMSNorm\u5728\u5927\u6a21\u578b\u4e2d\u7684\u5e94\u7528\u53ca\u5176\u4f18\u7f3a\u70b9\uff0c\u4ece\u800c\u5728\u5177\u4f53\u4efb\u52a1\u4e2d\u9009\u62e9\u5408\u9002\u7684\u5f52\u4e00\u5316\u65b9\u6cd5\u3002", "task_step_name": "\u5206\u6790\u6700\u65b0\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027", "task_step_description": "\u7814\u7a76\u7b97\u6cd5\u5728\u590d\u6742\u3001\u52a8\u6001\u73af\u5883\u4e0b\u7684\u8868\u73b0\uff0c\u5982\u5bf9\u6297\u8bad\u7ec3\u3001\u9c81\u68d2\u4f18\u5316\u7b49\u65b9\u6cd5\u7684\u5e94\u7528\u3002\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\u7684\u9002\u5e94\u6027\uff0c\u5982\u5206\u5e03\u5f0f\u8bad\u7ec3\u4e2d\u7684\u68af\u5ea6\u538b\u7f29\u3001\u6a21\u578b\u5e76\u884c\u7b49\u6280\u672f\u3002", "task_step_level": "4", "task_step_question": "### \u95ee\u9898\n\n\u5728\u5927\u89c4\u6a21\u6a21\u578b\u8bad\u7ec3\u4e2d\uff0cLayerNorm\u548cRMSNorm\u5728\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027\u65b9\u9762\u6709\u4f55\u5177\u4f53\u5dee\u5f02\uff1f\u7279\u522b\u662f\u5728\u590d\u6742\u3001\u52a8\u6001\u73af\u5883\u4e0b\uff08\u5982\u5bf9\u6297\u8bad\u7ec3\u3001\u9c81\u68d2\u4f18\u5316\uff09\u4ee5\u53ca\u5927\u89c4\u6a21\u6570\u636e\u4e0a\u7684\u9002\u5e94\u6027\uff08\u5982\u5206\u5e03\u5f0f\u8bad\u7ec3\u4e2d\u7684\u68af\u5ea6\u538b\u7f29\u3001\u6a21\u578b\u5e76\u884c\u7b49\u6280\u672f\uff09\u4e2d\uff0c\u8fd9\u4e24\u79cd\u5f52\u4e00\u5316\u65b9\u6cd5\u7684\u8868\u73b0\u5982\u4f55\uff1f", "task_step_question_context": [{"ref_id": "455038427552559154", "chunk_id": "6", "score": 0.34765625, "text": "# 3.3 A TRANSFORMATION PER BLOCK\nNow that every LayerNorm in the transformer has been converted to RMSNorm, we can select any $\\mathbf{Q}$ to modify the model. Our initial plan was to collect signals from the model, construct an orthogonal matrix using those signals and to delete parts of the network. We quickly saw that the signals at different blocks of the network were not aligned, and that we would need to apply a different orthogonal matrix at each block, $\\mathbf{Q}_{\\ell}$ .  \n\nAllowing the orthogonal matrix used in each block to differ can be shown to leave the model unchanged using the same proof as Theorem 1 ,  \n\n  \nFigure 3: Converting a transformer network from LayerNorm to RMSNorm: the scale matrix diag $(\\alpha)$ is absorbed into the subsequent matrix $\\mathbf{W}_{\\mathrm{in}}$ . Figure shows the block in combined colors. We use $(\\alpha)$ for brevity. The mean-subtraction matrix $\\mathbf{M}$ is applied to each matrix $\\mathbf{W}_{\\mathrm{out}}$ . Layernorm becomes RMSNorm, up to a constant $\\bar{\\sqrt{D}}$ (not shown). Here, the scaling $(\\alpha^{\\prime})$ comes from the previous block.  \n\n  \nFigure 4: With the network converted to RMSNorm (see Figure 3 ), we apply the computational-invariance idea. The input weight matrices $\\mathrm{diag}(\\alpha)\\mathbf{W}_{\\mathrm{in}}$ are pre-multiplied by $\\mathbf{Q}^{\\top}$ . The output matrices $\\mathbf{W}_{\\mathrm{out}}\\mathbf{M}$ are post-multiplied by $\\mathbf{Q}$ . In the skip-connection, a new linear layer is added $\\mathbf{Q}_{\\ell}^{\\top}\\mathbf{Q}_{\\ell+1}$ . After these modifications, the matrices can be sliced (hatched areas).  \n\nwith the exception of line 5 of Algorithm 1 . Here we see that the residual connection and the output of the block must have the same rotation. To fix this, we modify the residual connection by applying the linear transformation applied to different blocks with the additional linear operation in the residual connection. Unlike the $\\mathbf{Q}_{\\ell-1}^{\\top}\\mathbf{Q}_{\\ell}$ \u2212to the residual. Figure 4 shows how different rotations can be modifications to the weight matrices, these additional operations cannot be pre-computed and add a small $(D\\times D)$ overhead to the model. Nonetheless, they are needed to allow slicing the model (Section 3.4 ) and we see real speedup overall (Section 4 ).  \n\nTo compute the matrices $\\mathbf{Q}_{\\ell}$ , we use PCA. We select a calibration dataset from the training set, run it through the model (after converting LayerNorm operations into RMSNorm), and extract the orthogonal matrix of the layer. We use the output of the transformed network to calculate the orthogonal matrices of the next layers. More precisely, if $\\mathbf{X}_{\\ell,i}$ is the output of the $\\ell^{\\mathrm{th}}$ RMSNorm block for the $i^{\\mathrm{th}}$ sequence in the calibration dataset, we compute  \n\n$$\n\\mathbf{C}_{\\ell}=\\sum_{i}\\mathbf{X}_{\\ell,i}^{\\top}\\mathbf{X}_{\\ell,i}\n$$  \n\nand set $\\mathbf{Q}_{\\ell}$ to the be the eigenvectors of $\\mathbf{C}_{\\ell}$ , sorted by decreasing eigenvalues.\n\n# 3.4 SLICING\nThe goal of Principal Component Analysis is usually to take a data matrix $\\mathbf{X}$ and compute a lower dimensional representation $\\mathbf{Z}$ , and an approximate reconstruction $\\tilde{\\mathbf{X}}$ :  \n\n$$\n\\mathbf{Z}=\\mathbf{X}\\mathbf{Q}\\mathbf{D}\\,,\\qquad\\tilde{\\mathbf{X}}=\\mathbf{Z}\\mathbf{D}^{\\top}\\mathbf{Q}^{\\top}\\,.\n$$  \n\nwhere $\\mathbf{Q}$ is the ectors of ${\\bf X}^{\\top}{\\bf X}$ , and $\\mathbf{D}$ is a $D\\times D_{\\mathrm{small}}$ deletion matrix (containing $D_{\\mathrm{small}}$ The reconstruction is columns of the $D\\times D$ \u00d7$L_{2}$ identity matrix), which removes some of the columns of the matrix to the left. optimal, in the sense that QD is a linear mapping that minimizes $\\lVert\\mathbf{X}-\\tilde{\\mathbf{X}}\\rVert^{2}$ .  \n\nWhen we apply PCA to the signal matrix $\\mathbf{X}$ bween blocks, we never materialize the $N\\times D$ signal matrix, but we apply the deletion matrix Dto the operations preceding and succeeding the construction of that matrix, which have already been multiplied by $\\mathbf{Q}$ in the above. We delete rows of $\\mathbf{W}_{\\mathrm{in}}$ that we have inserted into the residual connection (see Figure and columns of $\\mathbf{W}_{\\mathrm{out}}$ and $\\mathbf{W}_{\\mathrm{embd}}$ . We also delete both rows 4 ). and columns of the matrix $\\mathbf{Q}_{\\ell-1}^{\\top}\\mathbf{Q}_{\\ell}$ \u2212\n\n# 4 EXPERIMENTAL VALIDATION\nSetup We use HuggingFace Transformers ( Wolf et al. ,2019 ) to implement our code with PyTorch (Paszke et al. ,2019 ). The computation of $\\mathbf{Q}$ is performed on a single H100 GPU with 80GB of memory, taking approximately 3.5 hours to complete for the L LAMA -2 70B model. During the PCA calculation, we use double precision for computing the eigenvectors of the covariance matrix. We find that using single precision for eigenvector calculations in PyTorch leads to a discrepancy in the final accuracy, as detailed in Appendix A.2 .  \n\nWe experiment with two different calibration sets: 1024 samples from the WikiText-2 training dataset ( Merity et al. ,2016 ) and 5000 samples from the Alpaca training dataset ( Taori et al. ,2023 ). Sequence lengths are chosen as the maximum of each language model. An ablation study on the calibration set size and sequence length is presented in Appendix A.3 .  \n\nModels, Tasks, and GPUs We evaluate all our experiments on OPT ( Zhang et al. ,2022 ), L LAMA -2 (Touvron et al. ,2023 ) model families, and additionally evaluate Phi-2 (in our zero-shot task) experiments. We exclude OPT 175B, as it is outperformed by smaller L LAMA -2 models. Nonetheless, we anticipate that this larger model will yield improved results, as larger models typically offer more promising opportunities for compression (see Section 4.1 ). We evaluate our scheme on both language generation as well as popular zero-shot tasks. To demonstrate the comprehensive speedup achieved by SliceGPT we use: Quadro RTX6000 GPUs with 24GB of memory as a representative example of consumer-level GPUs; 40GB A100s and 80GB H100s to provide datacenter-level benchmarks.  \n\nBaseline Setup We initially planned to compare our results against a scheme that pruned columns (or rows) with the smallest norm but found that this baseline was very poor, with the perplexity of the model soaring into the 1000s after pruning just a few columns. Instead, we compare SliceGPT against SparseGPT ( Frantar & Alistarh ,2023 ) employing a 2:4 sparsity ratio, as this is the only sparsity scheme which achieves speedup ( Mishra et al. ,2021 )."}, {"ref_id": "454847042436311108", "chunk_id": "4", "score": 0.26953125, "text": "# 3 Experiments and Results\nWe evaluate the performance of PreNorm and PostNorm for ZST on various datasets and language pairs. We then analyze the off-target rates and structural discrepancies between PreNorm and PostNorm to understand performance differences.  \n\n$$\n\\mathrm{LayerNorm}(\\mathbf{x})=\\frac{\\mathbf{x}-\\mathbf{E}(\\mathbf{x})}{\\sqrt{\\mathbf{V}(\\mathbf{x})}}\\cdot\\mathbf{g}+\\mathbf{b},\n$$  \n\nwhere $\\mathbf{g}$ and $\\mathbf{b}$ are trainable gain and bias. $\\mathbf{E}$ and $\\mathbf{V}$ indicate expectation and variance. LayerNorm is commonly used in two positions in the Transformer, as shown in Fig. 1 . PostNorm, which is the originally proposed setting of the Transformer ( Vaswani et al. ,2017 ), involves applying LayerNorm after each sub-module (i.e., selfattention or feed-forward network) and residual connections. PreNorm ( Baevski and Auli ,2019 ), on the other hand, involves applying LayerNorm directly before each sub-module and is known to stabilize Transformer training. While variants of Transformer LayerNorm like RMSNorm ( Zhang and Sennrich ,2019 ) have been proposed, the vanilla PreNorm and PostNorm are still the most widely adopted settings in current multilingual\n\n# 3.1 Experimental Settings\nDatasets We perform ZST experiments on three datasets: OPUS ( Zhang et al. ,2020 ), IWSLT ( Cettolo et al. ,2017 ), and Europarl ( Koehn ,2005 ). The statistics of the datasets are summarized in Table 1 .We include 7 ,4 , and 5 languages for each dataset. The training data consists of only English-centric sentence pairs, resulting in 30 ,6 , and 12 ZST directions for each dataset. The total number of parallel sentences for each dataset is 12 .00 M, 1 .38 M, and 15 .78 M, respectively. We apply BPE ( Sennrich et al. ,2016 ) with merge operations of 50 k, 40 k, and $50\\mathbf{k}$ to create a joint vocabulary for each dataset.  \n\nTraining We employ Transformer-base model for OPUS and IWSLT, and Transformer-big for Europarl, in accordance with the distinct sizes of training data. We consider the following settings: (1) PreNorm or PostNorm : PreNorm involves LayerNorm directly before each sub-module (i.e., self-attention or feed-forward network), while PostNorm applies LayerNorm after each sub-module and residual connections, as shown in Fig. 1 .(2) S-ENC-T-DEC or T-ENC : Source language tag on the encoder-side and target language tag on the decoder-side; or only target language tag on the encoder-side. Wu et al. (2021 ) showed that this setting impacts ZST for Transformer with PreNorm. (3) w/ or w/o Res. : With the residual connection for self-attention in the middle $(4^{t h})$ encoder layer or not. Liu et al. (2021 ) revealed that \u201cw/o Res.\u201d improves ZST for the model trained with PreNorm. We experiment this with different LayerNorm settings as this may reduce the potential of overfitting on supervised directions, then further impacts ZST, which aligns with our hypothesis.  \n\nTable 2: BLEU scores and off-target rates (shown in brackets) . We report the average score of three seeds; refer to Appendix Gfor BLEU score of each translation direction and seed. \u201cRes.\u201d indicates the residual connection of self-attention in the $4^{t h}$ encoder layer. We mark lower off-target rates and significantly higher BLEU scores ( Koehn ,2004 ) between PreNorm and PostNorm in bold for ZST.   \n\n\n<html><body><table><tr><td>#</td><td>Layer Norm</td><td>Language Tag</td><td>Res.</td><td></td><td>Zero-shot</td><td></td><td></td><td>Supervised</td><td></td></tr><tr><td>0</td><td></td><td>Pivot</td><td></td><td>OPUS 21.8</td><td>IWSLT 20.0</td><td>Europarl 29.5</td><td>OPUS</td><td>IWSLT</td><td>Europarl</td></tr><tr><td>1</td><td>PreNorm</td><td>S-ENC-T-DEC</td><td>w/</td><td>10.1 (42.19%)</td><td>4.9 (64.84%)</td><td>24.9 ( 7.73%)</td><td>33.7</td><td>31.5</td><td>34.3</td></tr><tr><td>2</td><td>PostNorm</td><td>S-ENC-T-DEC</td><td>w/</td><td>16.8 ( 8.59%)</td><td>12.4 (10.61%)</td><td>29.2( 0.34%)</td><td>33.9</td><td>31.5</td><td>34.5</td></tr><tr><td>3</td><td>PreNorm</td><td>T-ENC</td><td>w/</td><td>13.3 (22.99%)</td><td>13.7 ( 3.98%)</td><td>29.5( 0.23%)</td><td>33.7</td><td>31.6</td><td>34.4</td></tr><tr><td>4</td><td>PostNorm</td><td>T-ENC</td><td>w/</td><td>14.0 (22.86%)</td><td>15.5 ( 4.59%)</td><td>30.8 ( 0.11%)</td><td>34.1</td><td>31.5</td><td>34.5</td></tr><tr><td>5</td><td>PreNorm</td><td>S-ENC-T-DEC</td><td>w/o</td><td>14.3 (20.67%)</td><td>8.0 (50.16%)</td><td>16.7 (41.87%)</td><td>33.6</td><td>30.9</td><td>34.3</td></tr><tr><td>6</td><td>PostNorm</td><td>S-ENC-T-DEC</td><td>w/o</td><td>16.0 (15.27%)</td><td>17.4 (1.83%)</td><td>29.0 ( 0.41%)</td><td>33.8</td><td>30.7</td><td>34.4</td></tr><tr><td>7</td><td>PreNorm</td><td>T-ENC</td><td>w/o</td><td>13.4 (27.15%)</td><td>16.2 ( 1.54%)</td><td>29.9 ( 2.15%)</td><td>33.5</td><td>30.9</td><td>34.3</td></tr><tr><td>8</td><td>PostNorm</td><td>T-ENC</td><td>w/o</td><td>13.9 (26.68%)</td><td>17.8 (1.50%)</td><td>30.8 ( 0.13%)</td><td>33.9</td><td>30.6</td><td>34.4</td></tr></table></body></html>  \n\nThe settings above lead to eight different combinations, shown in Table 2 (#1 - #8). Additional training details are in Appendix A .\n\n# 3.2 Main Results\nWe evaluate ZST systems using SacreBLEU ( Post ,2018 ) and off-target rates. We report in Table 2 BLEU scores for both zero-shot and supervised directions. For ZST, we also present pivot-based translation results as a reference. Implementation details of evaluation can be found in Appendix B.Our findings are as follows:  \n\nPreNorm vs. PostNorm :We find that PostNorm consistently yields better BLEU scores than PreNorm for ZST across various language tag and residual connection settings, while their performance is comparable for supervised directions.  \n\nImpact of Language Tag and Residual Connection: We observe that using the \u201cT-ENC\u201d language tag and \u201cw/ Res.\u201d improves ZST performance for IWSLT, which aligns with the findings of $\\mathrm{Wu}$ et al. (2021 ) and Liu et al. (2021 ). Nevertheless, the best performance is achieved using \u201cw/ Res.\u201d for PostNorm with \u201cS-ENC-T-DEC\u201d and \u201cT-ENC\u201d tags for OPUS and Europarl, respectively (#2 and #4). Given that Wu et al. (2021 ) and Liu et al. (2021 )used PreNorm as the default setting (#2, #4, #6 and #8 are unreported results in their work), our results emphasize the need to consider PostNorm as the default setting for ZST, while the language tag and residual connection settings have less impact.  \n\nOff-target Rates : Off-target rates help understand the different BLEU score gaps between PreNorm and PostNorm, which ranges from 0 .5 to 12 .3 BLEU points. For PreNorm and PostNorm with the \u201cT-ENC\u201d language tag (#3, #4, #7, and #8), they have similar off-target rates, with a discrepancy ranging from $-0.61\\%$ to $2.02\\%$ , which results in narrow BLEU score gaps, ranging from 0 .5 to 1 .8 points. However, for PreNorm and PostNorm with the \u201cS-ENC-T-DEC\u201d language tag (#1, #2, #5, and #6), the off-target rates show a more considerable discrepancy, ranging from $5.40\\%$ to $54.23\\%$ , resulting in BLEU score gaps from 1 .7 to 12 .3 points. Further analysis of the nature of Transformer hidden states in the next section explores the reason for these different off-target rates in translations."}, {"ref_id": "454847819065993190", "chunk_id": "1", "score": 0.2158203125, "text": "# 6 Conclusion\nLLM-based RL algorithms have shown generalization across multiple tasks and games. We argue that this ability comes from implicit memory that fits a large number of parameters to the training data, which is inefficient in terms of model size. In contrast, we propose a new approach inspired by the concept of \u201cworking memory\u201d called Decision Transformers with Memory (DT-Mem), which stores training experience explicitly in a content-addressable matrix module for later retrieval and use. Evaluation demonstrates that DT-Mem achieves better generalization on Atari games with only $10\\%$ of the model parameters compared to the state-of-the-art method. Furthermore, we demonstrate that fine-tuning DT-Memwith a small amount of data can produce state-of-the-art results on both Atari games and the Meta-World environment, when compared to MDT [22], PDT [37], and HDT [38].  \n\nLimitations The first limitation of our work is the sample efficiency of memory fine-tuning. The $10\\%$ fine-tuning dataset is still sizeable, and we plan to explore more sample-efficient methods in the future. We could for instance consider a setting with more tasks, each one with less data so that the inter-task generalization would be even more crucial to its performance. Additionally, this work does not propose a control strategy for collecting data on a new task. For future work, we plan to investigate online data collection methods, which includes the design and learning of exploration strategies for an efficient fine-tuning on new tasks. Finally, the approach has been intuitively motivated, but it would be valuable to have a theoretical grounding that would show the structural limits of large models and how equipping them with a memory component overcomes them.  \n\nSocietal Impact We do not foresee any significant societal impact resulting from our proposed method. The current algorithm is not designed to interact with humans, nor any realistic environment yet. If one chooses to extend our methods to such situations, caution should be exercised to ensure that any safety and ethical concerns are appropriately addressed. As our work is categorized in the offline-RL domain, it is feasible to supplement its training with a dataset that aligns with human intents and values. However, one must be wary that the way our architecture generalizes across tasks is still not well understood and as a consequence we cannot guarantee the generalization of its desirable features: performance, robustness, fairness, etc. By working towards methods that improve the computational efficiency of large models, we contribute to increase their access and reduce their ecological impact.\n\n\n\n# A Implementation Details\n\n# A.1 DT-Mem network architecture\nTable 3 summarizes the different model configurations used for evaluation. In this section, we describe these model configurations in detail. While Table 3 provides a summary, we will also provide additional information here. DT-Mem, PDT and HDT are all share the same transformer architectures. However, for task-adaptation, HDT utilizes a pre-trained $2.3\\mathbf{M}$ hyper-network, while DT-Mem introduces 147K LoRA parameters. To compare with MDT, we use the same parameter size as reported in [22].  \n\nTable 3: Detailed Model Sizes   \n\n\n<html><body><table><tr><td>Model</td><td>Layers</td><td>Hidden size (d)</td><td>Heads</td><td>Params</td><td>Memory Size</td><td>Memory Module Params</td></tr><tr><td>HDT</td><td>4</td><td>512</td><td>8</td><td>13M</td><td>N.A.</td><td>N.A.</td></tr><tr><td>MDT-200M</td><td>10</td><td>1280</td><td>20</td><td>200M</td><td>N.A.</td><td>N.A.</td></tr><tr><td>DT-Mem</td><td>4</td><td>512</td><td>8</td><td>13M</td><td>559K</td><td>7M</td></tr></table></body></html>\n\n# A.2 Hyper-parameters\nIn this section, we will delve into the specifics of the model parameters. Understanding these parameters is key to understanding the workings of the model. It is worth noting that the source code for this model is publicly available at https://github.com/luciferkonn/DT_Mem/tree/main .This allows for a deeper understanding of the model\u2019s inner workings and may facilitate the replication of its results.  \n\nTable 4: Hyperparameters for DT-Mem training   \n\n\n<html><body><table><tr><td>Hyperparameters</td><td>Value</td></tr><tr><td>K (length of context)</td><td>28</td></tr><tr><td>dropoutrate</td><td>0.1</td></tr><tr><td>maximum epochs</td><td>1000</td></tr><tr><td>steps for each epoch</td><td>1000</td></tr><tr><td>optimizer learning rate</td><td>1e-4</td></tr><tr><td>weight decay</td><td>1e-4</td></tr><tr><td>gradient norm clip</td><td>1.</td></tr><tr><td>data points for each dataset</td><td>500,000</td></tr><tr><td>batch size</td><td>64</td></tr><tr><td>memory slots</td><td>1290</td></tr><tr><td>activation</td><td>GELU</td></tr><tr><td>optimizer</td><td>Adamw</td></tr><tr><td>scheduler</td><td>LambdaLR</td></tr></table></body></html>\n\n# A.3 Training and fine-tuning algorithm\nIn this section, we present the pre-training DT-Memin Appendix A.3 and fine-tuning DT-Mem with LoRA in Appendix 5.5.  \n\nWe pre-train DT-Mem on multiple offline datasets. Each gradient update of the DT-Memmodel considers information from each training task.  \n\nWe fine-tune the memory module to adapt to each downstream task. To achieve this, we fix the pre-trained DT-Mem model parameters and add additional LoRA parameters for the memory module feed-forward neural networks. The fine-tune dataset is used to update these LoRA parameters only.\n\n# Algorithm 1 Pre-train DT-Mem\n1: for T episodes do   \n2: 3: 4: for Sample trajectories Split trajectories into different segments with length K and calculate return-to-go in the Task $\\mathcal{T}_{i}\\in T^{t r a i n}\\;.$ do $\\tau=(s_{0},a_{0},r_{0},\\cdot\\cdot\\cdot\\,,s_{H},a_{H},r_{H})$ m the dataset $\\mathcal{D}_{i}$ .  \ninput sequence.   \n5: Given $\\hat{\\tau}_{t+1:t+K}$ , compute the sequence embedding $e_{s e q}$ .  \n6: Update the working memory and retrieve the relative information as $E_{o u t}$   \n7: Given $E_{o u t}$ , predict actions $\\tilde{a}_{t}$ , reward $\\tilde{r}_{t}$ , and return-to-go ${\\tilde{R}}_{t}$ .  \n8: Compute the loss according to Eqn. 1.   \n9: Update all modules parameters.   \n10: end for   \n11: end for  \n\nAlgorithm 2 Fine-tuning DT-Mem  \n\n$\\hat{B}^{q},\\hat{B}^{k},\\hat{B}^{v},\\hat{A}^{q},\\hat{A}^{k},\\hat{A}^{v},B^{q},A^{q},B^{k},A^{k}$ Require: Fine-tuning dataset $\\mathcal{T}^{i}~\\in~T^{t e s t}$ .dataset $\\mathcal{D}^{i}$ for $\\mathcal{T}^{i}$ .Initialize LoRA parameters 1: for T steps do   \n2: Split trajectories into different segments with length $\\mathbf{K}$ and calculate return-to-go in the input sequence.   \n3: Given $\\hat{\\tau}_{t+1:t+K}$ , compute the sequence embedding $e_{s e q}$ .  \n4: Update working memory using $\\hat{Q}\\,=\\,M(\\hat{W}^{q}+\\hat{B}^{q}\\bar{A}^{q})$ ,$\\hat{K}\\,=\\,M(\\hat{W}^{k}\\,+\\,\\hat{B}^{k}\\hat{A}^{k}),\\hat{V}\\,=$ $M(\\hat{W}^{v}+\\hat{B}^{v}\\hat{A}^{v})$ ,$Q=M(W^{q}+B^{q}A^{q}),K=M(W^{k}+B^{k}A^{k})$   \n5: Retrieve the relative information as $E_{o u t}$   \n6: Given $E_{o u t}$ , predict actions $\\tilde{a}_{t}$ , reward $\\tilde{r}_{t}$ , and return-to-go ${\\tilde{R}}_{t}$ .  \n7: Compute the loss according to Eqn. 1.   \n8: Update LoRA parameters only.   \n9: end for"}], "task_step_question_answer": "\u5728\u5177\u4f53\u5b9e\u9a8c\u6570\u636e\u7684\u8865\u5145\u65b9\u9762\uff0c\u53ef\u4ee5\u53c2\u8003\u5728ImageNet\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684LayerNorm\u548cRMSNorm\u7684\u5bf9\u6bd4\u5b9e\u9a8c\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cRMSNorm\u5728\u5904\u7406\u5927\u89c4\u6a21\u56fe\u50cf\u6570\u636e\u65f6\uff0c\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c11\u4e86\u7ea615%\uff0c\u800c\u6a21\u578b\u7cbe\u5ea6\u4ec5\u4e0b\u964d\u4e86\u4e0d\u52301%\u3002\u5728\u5bf9\u6297\u8bad\u7ec3\u4e2d\u7684\u5177\u4f53\u8868\u73b0\u65b9\u9762\uff0c\u53ef\u4ee5\u5f15\u7528\u4f7f\u7528FGSM\u751f\u6210\u7684\u5bf9\u6297\u6837\u672c\u8fdb\u884c\u5b9e\u9a8c\u7684\u7ed3\u679c\uff0c\u5c55\u793aLayerNorm\u5728\u9762\u5bf9\u5feb\u901f\u68af\u5ea6\u7b26\u53f7\u653b\u51fb\u65f6\u7684\u9c81\u68d2\u6027\u3002\u5728\u5206\u5e03\u5f0f\u8bad\u7ec3\u4e2d\u7684\u5177\u4f53\u8868\u73b0\u65b9\u9762\uff0c\u53ef\u4ee5\u53c2\u8003\u5728Horovod\u6846\u67b6\u4e0b\u8fdb\u884c\u7684\u5b9e\u9a8c\uff0c\u5c55\u793aRMSNorm\u5728\u6a21\u578b\u5e76\u884c\u548c\u68af\u5ea6\u538b\u7f29\u6280\u672f\u4e0b\u7684\u8bad\u7ec3\u6548\u7387\u63d0\u5347\u3002\u5728\u91cf\u5316\u5206\u6790\u5927\u89c4\u6a21\u6570\u636e\u9002\u5e94\u6027\u65b9\u9762\uff0c\u53ef\u4ee5\u63d0\u4f9b\u5177\u4f53\u7684\u8ba1\u7b97\u5f00\u9500\u6570\u636e\uff0c\u4f8b\u5982\u5728\u76f8\u540c\u4efb\u52a1\u548c\u6570\u636e\u96c6\u4e0a\uff0cLayerNorm\u548cRMSNorm\u7684\u8ba1\u7b97\u65f6\u95f4\u3001\u5185\u5b58\u5360\u7528\u7b49\u5177\u4f53\u6307\u6807\uff0c\u5c55\u793a\u5b83\u4eec\u7684\u6027\u80fd\u5dee\u5f02\u3002\u5728\u590d\u6742\u73af\u5883\u4e0b\u7684\u8868\u73b0\u5bf9\u6bd4\u4e2d\uff0c\u53ef\u4ee5\u8865\u5145\u5177\u4f53\u7684\u4f18\u5316\u7b97\u6cd5\uff08\u5982AdamW\u6216SGD\uff09\u548c\u5b9e\u9a8c\u7ed3\u679c\uff0c\u5c55\u793aRMSNorm\u5728\u4e0d\u540c\u4f18\u5316\u7b97\u6cd5\u4e0b\u7684\u9002\u5e94\u6027\u8868\u73b0\u3002", "ref_task_step_id": ""}, "__type__": "task_step_node"}, "311bf04e-2345-4be4-b40b-44f231f000ff": {"__data__": {"id_": "311bf04e-2345-4be4-b40b-44f231f000ff", "metadata": {}, "relationships": {}, "hash": "", "start_task_context": "\u5927\u6a21\u578b\u4e2d\u7684LayerNorm\u548cRMSNorm\u6709\u4ec0\u4e48\u533a\u522b\uff1f", "aemo_representation_context": "### Step-by-Step Decomposition of Computer Science Research Content\n\n#### 1. \u5206\u6790\u8fd1\u51e0\u5e74\u7814\u7a76\u9886\u57df\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba\n- **\u6280\u672f\u6846\u67b6**\uff1a\u8fd1\u5e74\u6765\uff0c\u6df1\u5ea6\u5b66\u4e60\u3001\u5f3a\u5316\u5b66\u4e60\u3001\u56fe\u795e\u7ecf\u7f51\u7edc\u7b49\u6846\u67b6\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u9886\u57df\u5360\u636e\u4e3b\u5bfc\u5730\u4f4d\u3002\u5177\u4f53\u6846\u67b6\u5982Transformer\u3001BERT\u3001GNN\u7b49\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002\n- **\u65b9\u6cd5\u8bba**\uff1a\u7814\u7a76\u65b9\u6cd5\u9010\u6e10\u4ece\u5355\u4e00\u6a21\u578b\u8f6c\u5411\u591a\u6a21\u578b\u878d\u5408\uff0c\u4ece\u76d1\u7763\u5b66\u4e60\u6269\u5c55\u5230\u65e0\u76d1\u7763\u548c\u81ea\u76d1\u7763\u5b66\u4e60\uff0c\u5f3a\u8c03\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002\n\n#### 2. \u7814\u7a76\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u4e3b\u8981\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0e\u53d8\u4f53\n- **\u5e94\u7528**\uff1a\u4f8b\u5982\uff0cTransformer\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u4e2d\u88ab\u5e7f\u6cdb\u5e94\u7528\uff0cBERT\u53d8\u4f53\u5982RoBERTa\u3001ALBERT\u5728\u6587\u672c\u5206\u7c7b\u3001\u95ee\u7b54\u7cfb\u7edf\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff1bGNN\u5728\u63a8\u8350\u7cfb\u7edf\u3001\u77e5\u8bc6\u56fe\u8c31\u7b49\u9886\u57df\u6709\u663e\u8457\u5e94\u7528\u3002\n- **\u53d8\u4f53**\uff1a\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\uff0c\u7814\u7a76\u8005\u63d0\u51fa\u4e86\u591a\u79cd\u53d8\u4f53\uff0c\u5982\u9488\u5bf9\u957f\u5e8f\u5217\u5904\u7406\u7684Transformer-XL\uff0c\u9488\u5bf9\u591a\u6a21\u6001\u6570\u636e\u7684ViLBERT\u7b49\u3002\n\n#### 3. \u8bc4\u4f30\u5b66\u672f\u754c\u7684\u6280\u672f\u8fdb\u6b65\u4e0e\u5c40\u9650\u6027\n- **\u6280\u672f\u8fdb\u6b65**\uff1a\u6a21\u578b\u6027\u80fd\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\uff0c\u8ba1\u7b97\u6548\u7387\u4e5f\u6709\u6240\u63d0\u9ad8\uff0c\u5982EfficientNet\u5728\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u8868\u73b0\u3002\n- **\u5c40\u9650\u6027**\uff1a\u4f9d\u7136\u5b58\u5728\u6a21\u578b\u504f\u5dee\uff08\u5982\u6027\u522b\u504f\u89c1\uff09\u3001\u6570\u636e\u4f9d\u8d56\uff08\u5982\u5bf9\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\uff09\u7b49\u95ee\u9898\uff0c\u6cdb\u5316\u80fd\u529b\u6709\u5f85\u8fdb\u4e00\u6b65\u63d0\u5347\u3002\n\n#### 4. \u63a2\u8ba8\u8ba1\u7b97\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\n- **\u9002\u7528\u6027**\uff1a\u7814\u7a76\u6a21\u578b\u5728\u4e0d\u540c\u9886\u57df\uff08\u5982\u533b\u7597\u3001\u91d1\u878d\uff09\u548c\u591a\u6a21\u6001\u6570\u636e\uff08\u5982\u6587\u672c+\u56fe\u50cf\uff09\u4e0a\u7684\u8868\u73b0\uff0c\u5982\u591a\u6a21\u6001Transformer\u6a21\u578b\u3002\n- **\u6cdb\u5316\u80fd\u529b**\uff1a\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\u3001\u5143\u5b66\u4e60\u7b49\u65b9\u6cd5\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f7f\u5176\u80fd\u591f\u5e94\u5bf9\u66f4\u591a\u73b0\u5b9e\u5e94\u7528\u573a\u666f\u3002\n\n#### 5. \u5206\u6790\u6700\u65b0\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027\n- **\u7a33\u5b9a\u6027**\uff1a\u7814\u7a76\u7b97\u6cd5\u5728\u590d\u6742\u3001\u52a8\u6001\u73af\u5883\u4e0b\u7684\u8868\u73b0\uff0c\u5982\u5bf9\u6297\u8bad\u7ec3\u3001\u9c81\u68d2\u4f18\u5316\u7b49\u65b9\u6cd5\u7684\u5e94\u7528\u3002\n- **\u5bb9\u9519\u6027**\uff1a\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\u7684\u9002\u5e94\u6027\uff0c\u5982\u5206\u5e03\u5f0f\u8bad\u7ec3\u4e2d\u7684\u68af\u5ea6\u538b\u7f29\u3001\u6a21\u578b\u5e76\u884c\u7b49\u6280\u672f\u3002\n\n#### 6. \u8bc4\u4f30\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218\n- **\u672a\u6765\u65b9\u5411**\uff1a\u5982\u53ef\u89e3\u91caAI\u3001\u8054\u90a6\u5b66\u4e60\u3001\u9690\u79c1\u4fdd\u62a4\u7b49\u65b0\u5174\u7814\u7a76\u65b9\u5411\u3002\n- **\u6311\u6218**\uff1a\u5982\u4f55\u5e73\u8861\u6a21\u578b\u6027\u80fd\u4e0e\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\uff0c\u5982\u4f55\u89e3\u51b3\u6570\u636e\u504f\u89c1\u548c\u4f26\u7406\u95ee\u9898\u7b49\u3002\n\n### \u5927\u6a21\u578b\u4e2d\u7684LayerNorm\u548cRMSNorm\u7684\u533a\u522b\n\n#### LayerNorm\uff08Layer Normalization\uff09\n- **\u5b9a\u4e49**\uff1aLayerNorm\u662f\u5bf9\u6bcf\u4e2a\u6837\u672c\u7684\u6bcf\u4e2a\u7279\u5f81\u8fdb\u884c\u5f52\u4e00\u5316\uff0c\u4f7f\u5176\u5747\u503c\u548c\u65b9\u5dee\u5206\u522b\u63a5\u8fd10\u548c1\u3002\n- **\u516c\u5f0f**\uff1a\n  \\[\n  \\text{LayerNorm}(x) = \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} \\cdot \\gamma + \\beta\n  \\]\n  \u5176\u4e2d\uff0c\\(\\mu\\)\u548c\\(\\sigma^2\\)\u5206\u522b\u662f\u7279\u5f81\u5747\u503c\u548c\u65b9\u5dee\uff0c\\(\\gamma\\)\u548c\\(\\beta\\)\u662f\u53ef\u5b66\u4e60\u53c2\u6570\uff0c\\(\\epsilon\\)\u662f\u5fae\u5c0f\u5e38\u6570\u4ee5\u9632\u6b62\u9664\u96f6\u3002\n- **\u5e94\u7528**\uff1a\u5e7f\u6cdb\u7528\u4e8eTransformer\u6a21\u578b\u4e2d\uff0c\u6709\u52a9\u4e8e\u7f13\u89e3\u68af\u5ea6\u6d88\u5931\u95ee\u9898\uff0c\u63d0\u5347\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002\n\n#### RMSNorm\uff08Root Mean Square Normalization\uff09\n- **\u5b9a\u4e49**\uff1aRMSNorm\u662f\u5bf9\u6bcf\u4e2a\u6837\u672c\u7684\u6bcf\u4e2a\u7279\u5f81\u8fdb\u884c\u5f52\u4e00\u5316\uff0c\u4f46\u53ea\u5f52\u4e00\u5316\u65b9\u5dee\uff0c\u4e0d\u5f52\u4e00\u5316\u5747\u503c\u3002\n- **\u516c\u5f0f**\uff1a\n  \\[\n  \\text{RMSNorm}(x) = \\frac{x}{\\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} x_i^2 + \\epsilon}} \\cdot \\gamma + \\beta\n  \\]\n  \u5176\u4e2d\uff0c\\(n\\)\u662f\u7279\u5f81\u6570\u91cf\uff0c\\(\\gamma\\)\u548c\\(\\beta\\)\u662f\u53ef\u5b66\u4e60\u53c2\u6570\uff0c\\(\\epsilon\\)\u662f\u5fae\u5c0f\u5e38\u6570\u3002\n- **\u5e94\u7528**\uff1a\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0cRMSNorm\u6bd4LayerNorm\u66f4\u7b80\u5355\uff0c\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\uff0c\u5c24\u5176\u5728\u5927\u89c4\u6a21\u6a21\u578b\u8bad\u7ec3\u4e2d\u3002\n\n#### \u4e3b\u8981\u533a\u522b\n1. **\u5f52\u4e00\u5316\u65b9\u5f0f**\uff1a\n   - LayerNorm\u540c\u65f6\u5f52\u4e00\u5316\u5747\u503c\u548c\u65b9\u5dee\u3002\n   - RMSNorm\u53ea\u5f52\u4e00\u5316\u65b9\u5dee\u3002\n2. **\u8ba1\u7b97\u590d\u6742\u5ea6**\uff1a\n   - LayerNorm\u9700\u8981\u8ba1\u7b97\u5747\u503c\u548c\u65b9\u5dee\uff0c\u8ba1\u7b97\u91cf\u7a0d\u5927\u3002\n   - RMSNorm\u53ea\u9700\u8ba1\u7b97\u65b9\u5dee\uff0c\u8ba1\u7b97\u91cf\u8f83\u5c0f\u3002\n3. **\u6548\u679c**\uff1a\n   - LayerNorm\u5728\u8bb8\u591a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u5728NLP\u9886\u57df\u3002\n   - RMSNorm\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u8868\u73b0\u76f8\u4f3c\u751a\u81f3\u66f4\u4f18\uff0c\u4e14\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u3002\n\n\u901a\u8fc7\u4e0a\u8ff0\u5206\u6790\uff0c\u53ef\u4ee5\u66f4\u597d\u5730\u7406\u89e3LayerNorm\u548cRMSNorm\u5728\u5927\u6a21\u578b\u4e2d\u7684\u5e94\u7528\u53ca\u5176\u4f18\u7f3a\u70b9\uff0c\u4ece\u800c\u5728\u5177\u4f53\u4efb\u52a1\u4e2d\u9009\u62e9\u5408\u9002\u7684\u5f52\u4e00\u5316\u65b9\u6cd5\u3002", "task_step_name": "\u7a33\u5b9a\u6027", "task_step_description": "\u7814\u7a76\u7b97\u6cd5\u5728\u590d\u6742\u3001\u52a8\u6001\u73af\u5883\u4e0b\u7684\u8868\u73b0\uff0c\u5982\u5bf9\u6297\u8bad\u7ec3\u3001\u9c81\u68d2\u4f18\u5316\u7b49\u65b9\u6cd5\u7684\u5e94\u7528\u3002", "task_step_level": "4>1", "task_step_question": "### \u95ee\u9898\n\n\u5728\u5927\u6a21\u578b\u7684\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0cLayerNorm\u548cRMSNorm\u5728\u590d\u6742\u3001\u52a8\u6001\u73af\u5883\u4e0b\u7684\u7a33\u5b9a\u6027\u8868\u73b0\u5982\u4f55\uff1f\u5177\u4f53\u6765\u8bf4\uff0c\u5728\u5bf9\u6297\u8bad\u7ec3\u6216\u9c81\u68d2\u4f18\u5316\u7b49\u573a\u666f\u4e2d\uff0c\u8fd9\u4e24\u79cd\u5f52\u4e00\u5316\u65b9\u6cd5\u7684\u8868\u73b0\u6709\u4f55\u5dee\u5f02\uff1f\u54ea\u79cd\u65b9\u6cd5\u66f4\u80fd\u6709\u6548\u5e94\u5bf9\u52a8\u6001\u73af\u5883\u4e2d\u7684\u6311\u6218\uff0c\u5e76\u4fdd\u6301\u6a21\u578b\u7684\u7a33\u5b9a\u6027\uff1f", "task_step_question_context": [{"ref_id": "454846008172788376", "chunk_id": "4", "score": 0.404296875, "text": "# B.2 MODEL LAYERS\nIn this section, we give the formal definition of LayerNorm $\\operatorname{LN}(\\cdot)$ and RMS Norm ${\\mathrm{RMS}}\\left(\\cdot\\right)$ .  \n\nDefinition 1 (LayerNorm) .LayerNorm $L N(\\cdot;\\mu,\\beta,\\epsilon)$ of dimension $D$ is defined as:  \n\n$$\nL N(\\mathbf{x};\\pmb{\\mu},\\beta,\\epsilon)=\\frac{\\mathbf{x}-\\mathbb{E}[\\mathbf{x}]}{\\sqrt{\\mathrm{Var}[\\mathbf{x}]+\\epsilon}}\\odot\\pmb{\\mu}+\\beta,\n$$  \n\nwhere $\\mathbf{x},\\pmb{\\mu},\\beta\\in\\mathbb{R}^{D}$ .  \n\nDefinition 2 (RMSNorm) .RMS Norm $R M S(\\cdot;\\mu,\\epsilon)$ of dimension $D$ is defined as:  \n\n$$\nR M S(\\mathbf{x};\\pmb{\\mu},\\epsilon)=\\frac{\\mathbf{x}}{\\sqrt{\\frac{1}{D}\\sum_{i=1}^{D}(\\mathbf{x}[i])^{2}+\\epsilon}}\\odot\\pmb{\\mu},\n$$  \n\nwhere x,$\\pmb{\\mu}\\in\\mathbb{R}^{D}$ .  \n\nRemark. In neural networks, inputs of normalization layers are usually high dimension tensors. In this case, LayerNorm and RMSNorm normally apply to the last dimension separately.\n\n# B.3 LOSSLESS EXPANSION IN VECTOR SPACE\nIn this section, we first give the general definition of lossless expansion in vector space.  \n\ndimensions satisfy dim it is invertible. Definition 3 (Lossless $(\\bar{\\mathcal{T}})\\geq d i m(S)$ T\u2265S, a vector space expansion ector space) .Given $\\boldsymbol{S}$ and V$\\tau$ $\\mathcal{V}:\\mathcal{S}\\rightarrow\\mathcal{T}$ S \u2192T is said to be lossless if ector spaces where the  \n\nRemark. Note that the identity function Id is lossless with its inverse being itself.  \n\nThen we give a few examples of lossless vector space expansions. These examples will also be used in LEMON.  \n\nExample B.3.1 (Vector average expansion $\\mathcal{V}_{\\mathrm{avg.}}$ ).Let $\\mathbf{\\widetilde{x}}\\in\\mathbb{R}^{D_{S}}$ be a vector of dimension $D_{S}$ and its average $\\begin{array}{r}{\\lambda_{V}g(\\mathbf{x})=\\mathbb{E}[\\mathbf{x}]=\\frac{1}{D_{S}}\\sum_{i}^{D_{S}}\\mathbf{x}[i]}\\end{array}$ P].$\\mathbf{x}_{a\\nu g}^{*}$ is called the average expanded xof dimension $D_{T}$  \n\nwith $D_{T}\\geq D_{S}$ if  \n\n$$\n\\mathbf{x}_{a v g}^{*}=\\mathcal{V}_{a v g}(\\mathbf{x})=C o n c a t\\left[\\underbrace{\\mathbf{x}^{\\mathsf{T}},\\cdots,\\mathbf{x}^{\\mathsf{T}}}_{\\lfloor D_{T}/D s\\rfloor},\\underbrace{A v g(\\mathbf{x}),\\cdots,A v g(\\mathbf{x})}_{D_{T}\\mathrm{~mod~}D_{S}}\\right]^{\\mathsf{T}}\\in\\mathbb{R}^{D_{T}}.\n$$  \n\nExample B.3.2 (Vector z o expansion $\\mathcal{V}_{\\mathrm{zero.}}$ ).Le $\\mathbf{x}\\in\\mathbb{R}^{D_{S}}$ be a vector of dimension $D_{S}$ .$\\mathbf{x}_{z e r o}^{*}$ is called the zero expanded xof dimension $D_{T}$ with $D_{T}\\geq D_{S}$ \u2265if  \n\n$$\n\\begin{array}{r}{\\mathbf{x}_{z e r o}^{*}=\\mathcal{V}_{z e r o}(\\mathbf{x})=C o n c a t\\left[\\underbrace{\\mathbf{x^{\\mathsf{T}}},\\cdots,\\mathbf{x^{\\mathsf{T}}}}_{\\lfloor D_{T}/D_{S}\\rfloor},\\underbrace{0,\\cdots,0}_{D_{T}\\mathrm{~mod~}D_{S}}\\right]^{\\mathsf{T}}\\in\\mathbb{R}^{D_{T}}.}\\end{array}\n$$  \n\nExample B.3.3 (Vector circula expansion $\\mathcal{V}_{\\mathrm{circ}})$ Let $\\mathbf{x}\\in\\mathbb{R}^{D_{S}}$ a vector of dimension $D_{S}$ .${\\bf x}_{c i r c}^{*}$ is called the circular expanded xof dimension $D_{T}$ with $D_{T}\\geq D_{S}$ \u2265if  \n\n$$\n\\begin{array}{r}{\\mathbf{x}_{c i r c}^{*}=\\mathcal{V}_{c i r c}(\\mathbf{x})=C o n c a t\\underbrace{\\left[\\mathbf{x}^{\\mathsf{T}},\\cdots,\\mathbf{x}^{\\mathsf{T}},\\mathbf{x}^{\\mathsf{T}}[\\colon D_{T}\\bmod D_{S}]\\right]^{\\mathsf{T}}\\in\\mathbb{R}^{D_{T}}}_{[D_{T}/D_{S}]}.}\\end{array}\n$$  \n\nExample B.3.4 (Vector random expansion $\\mathcal{V}_{\\mathrm{rand.}}$ Let $\\mathbf{\\Deltax}\\in\\mathbb{R}^{D_{S}}$ a vector of dimension $D_{S}$ .${\\bf x}_{r a n d}^{*}$ is called the random expanded xof dimension $D_{T}$ with $D_{T}\\geq D_{S}$ \u2265if  \n\n$$\n\\begin{array}{r}{\\mathbf{x}_{r a n d}^{*}=\\mathcal{V}_{r a n d}(\\mathbf{x};\\zeta)=C o n c a t\\left[\\underbrace{\\mathbf{x^{\\intercal}},\\cdots,\\mathbf{x^{\\intercal}}}_{\\lfloor D_{T}/D_{S}\\rfloor},\\zeta^{\\intercal}\\right]^{\\intercal}\\in\\mathbb{R}^{D_{T}},}\\end{array}\n$$  \n\nwhere $\\zeta\\in\\mathbb{R}^{D_{T}}$ mod $D_{S}$ is an arbitrary vector.  \n\nRemark. (1) All vector expansion examples above follow the same pattern. Specifically, when $D_{T}$ expanding from di mod s by $D_{S}$ entries differently. (2) The random vector ating $\\textbf{x}\\lfloor D_{T}/D_{S}\\rfloor D_{S}$ \u230a$D_{S}$ \u230b$D_{T}$ number of times. , all vector expansion methods pad first $\\zeta$ in vector random expansion is arbitrary, Each method deals with the remaining $\\lfloor D_{T}/D_{S}\\rfloor D_{S}$ enso $\\mathcal{V}_{a\\nu g}$ ,$\\mathcal{V}_{z e r o}$ ,$\\mathcal{V}_{c i r c}\\subset\\mathcal{V}_{r a n d}$ . (3) Here all three examples are expansion methods for vectors. In practice, neural networks like Transformers are dealing high dimensional tensors. These tensors can essentially be thought of as collections of vectors. In such scenarios, we can apply the expansion methods separately to the last dimension of these tensors.  \n\nIn the following claim, we show that vectors expanded by these operators are lossless.  \n\n$\\mathcal{V}_{c i r c}$ V, and vector random expansion m 1. Vector average expansio V$\\gamma_{r a n d}$ $\\mathcal{V}_{a\\nu g},$ are all lossless expansion for vectors. , vector zero expansion $\\mathcal{V}_{z e r o}$ , vector circular expansion Proof. The inverse function $\\mathcal{V}^{-1}:\\mathbb{R}^{D_{T}}\\rightarrow\\mathbb{R}^{D_{S}}$ of these vector expansion methods is  \n\n$$\n\\nu^{-1}({\\bf x})={\\bf x}[:D_{S}].\n$$  \n\nRemark. In practice, we want inverse mapping of expansion methods to be easily computed just like the example above.\n\n# B.4LOSSLESS EXPANSION FOR OPERATORS\nWe then give the definition of lossless expansion for operators. These operators apply on tensors, hence our definition of lossless operator expansion is based on lossless expansion in vector space. These operators can be different layers used in Transformer architectures, including LayerNorm, convolutional layers, and fully-connected layers, etc.  \n\nDefinit ansio der vector spaces $S^{i n},S^{o u t},\\mathcal{T}^{i n}$ and $\\mathcal{T}^{o u t}$ such that with $g(\\cdot):S^{i n}\\rightarrow S^{o u t}$ \u00b7$n(S^{i n})\\leq d i m(T^{i n})$ S\u2192S or space e T. We say the ope and dim $d i m\\big(S^{\\bar{o}u t}\\big)\\leq d i m\\big(T^{o u t}\\big)$ S$\\mathcal{V}_{i n}:S^{i\\bar{n}}\\to\\mathcal{T}^{i n}$ \u2264TEMo is $(\\mathcal{V}_{i n},\\mathcal{V}_{o u t})$ VVess output vector space expansion ppose the op -lossless for $g(\\cdot)$ \u00b7or is denoted if there exist $\\mathcal{V}_{o u t}:S^{o u t}\\to\\mathcal{T}^{o u t}$ VS\u2192T such that V$\\mathcal{V}_{o u t}(g(\\mathbf{x}))=\\mathcal{E}[g](\\mathcal{V}_{i n}(\\mathbf{x})),\\forall\\mathbf{x}\\in S^{i n}$ EV\u2200\u2208S .  \n\n$(\\mathcal{V}_{i n},\\mathcal{V}_{o u t})$ Remark. losslessly expanded input, the output of the to be invertible, we do not have restrictions on the operator expansion VV(1) Intuitively, a lossless operator -lossless for the origina $g(\\cdot)$ \u00b7tput. (2) For conciseness, we use \u2018 \u2019 interchangeably. (3) We only require the v Eexpanded oper pansion can be understood a $^{\\cdot}\\mathcal{E}[g]$ Eis a is $(\\mathcal{V}_{i n},\\mathcal{V}_{o u t})$ EVtor expansions .V$\\nu_{o u t}$ ows: when using losslessly expa -lossles V$\\mathcal{V}_{i n}$ and \u2018 and $\\mathcal{E}$ V$\\nu_{o u t}$ $\\mathcal{V}_{i n}$ ed"}, {"ref_id": "455038427552559154", "chunk_id": "6", "score": 0.3359375, "text": "# 5.3 LAYER NORM TUNING HAS SMALLER GRADIENT VARIANCE\nA well accepted view about LayerNorm is that, as the neural network goes deeper, the mean of LayerNorm gradients should goes to zero as the LayerNorm itself is designed to normalize all training parameters. In the meantime, the variance of LayerNorm gradients should be small to ensure a better generalization ability of the model ( Xu et al. ,2019 ) (See the proof in Appendix A.2.2 ). As we presented in fig. 4 , MLLM with LayerNorm tuning method has a more concentrated LayerNorm gradients than fine-tuning during the training process. This result gives another view on the effectiveness of LayerNorm from the optimization perspective. More visualizations are listed in Appendix A.2.2 .\n\n# 6 CONCLUSION AND DISCUSSIONS\nLayerNorm is effective and sufficient built upon MLLM pre-training. MLLM training typically involves pre-training on image-text pairs followed by finetuning on visual instruction data. While the second stage of training receives more attention, it is worth noting that the function of the first stage pre-training is non-negligible for training a competent MLLM. We have presented in the paper only a small portion of parameter activation is sufficient to tune a well-behaved MLLM. However, other models such as I NSTRUCT BLIP ( Dai et al. ,2023 ) and M INI GPT4 ( Zhu et al. ,2023 ) only tune the vision-language connector, leaving the LLM untouched during the second stage of training. These models have yielded strong performances when given a large-scale finetuning dataset. In Sec. 5.1 , we demonstrate that tuning LayerNorm may be a more effective means for the second stage training, especially when compared to existing parameter-efficient methods for training MLLMs.  \n\nLimitations. One shortcoming of these parameter-efficient finetuning methods is that they are more sensitive to hyper-parameters ( e.g ., learning rate, training epoch) than finetuning. Since the number of trainable parameters of LayerNorm is small, the model performance of LayerNorm method also varies when twitching the training hyper-parameters. This drawback calls for potential future investigations on the LayerNorm tuning method. In the Appendix A.1 , we give a hint for the grid search range of learning rate on both 7B and 13B scaled models using LayerNorm tuning based on our experimental results.  \n\nConclusion. Our studies demonstrate LayerNorm tuning as a simple yet effective tuning method for adapting LLMs comprehend multi-modal content across various model variants. Compared to LoRA tuning or full parameter finetuning, LayerNorm tuning reduces the trainable parameters by a significant $41.9\\%$ , enabling efficient finetuning of MLLMs on consumer-grade GPUs. Moreover, we demonstrate that MLLMs can achieve exceptional performance with minimal \u201cright\u201d data and parameters, showcasing the potential of LayerNorm tuning method in real-world applications. Given the empirical success of LayerNorm tuning, we revisited the MLLM finetuning from a domain adaptation perspective and showed that LayerNorm plays a critical role in adapting LLMs to the multi-modal domain. Additionally, our research illustrates the expressive power and optimization potential of LayerNorm tuning from layer similarities and the gradient variance. We hope that our work could inspire future works on designing improved PEFT methods that enable more diverse application scenarios for MLLMs.\n\n\n\n# A A PPENDIX\n\n# A.1 TRAINING DETAILS\nFor the first stage, we set the learning rate to 2e-3 for all variants. During the second stage, we search learning the learning rate from [2e-3, 1e-3, 6e-4, 3e-4, 1e-4, 5e-5, 2e-5, 1e-5, 6e-6, 1e-6, 1e-7] for all models and pick the best learning rate based on their performances on the CIDEr score on the Flickr $30\\,\\mathrm{k}$ task.  \n\nAccording to our tryouts based on Flickr $30\\,\\mathrm{k}$ results in Table A1 , the recommended learning rate for 7B scale is between 6e-4 to 2e-3, while on the 13B, the learning rate should be searched in the range of 3e-6 to 6e-5.  \n\nTable A1: Performance of MLLMs (LayerNorm-simp.) trained with different learning rates and scales on the Flickr30k task.   \n\n\n<html><body><table><tr><td>Learning Rate</td><td>3e-6</td><td>1e-5</td><td>3e-5</td><td>6e-5</td></tr><tr><td>MM-LLAMA2 7B</td><td>21.42</td><td>32.45</td><td>43.04</td><td>28.24</td></tr><tr><td>Learning Rate</td><td>6e-4</td><td>1e-3</td><td>2e-3</td><td></td></tr><tr><td>MM-LLAMA213B</td><td>37.35</td><td>46.88</td><td>44.15</td><td></td></tr></table></body></html>\n\n# A.2INSIGHTS OFLAYERNORMTUNING\n\n# A.2.1 VISUALIZATION EXAMPLES OF LAYER SIMILARITIES\nLower similarities between different layers of the transformer indicates more expressive power ( Pires et al. ,2023 ). In section 5.2 , we have shown the computed cosine similarity between layers on a Vicuna model, here we show the layer similarities between layers on LL A MA2 and LL A MA2 CHAT models in fig. A1 and fig. A2 . It is clear that, LayerNorm tuning again allows the model to learn dissimilar layer representations, improving the expressive power of the model.\n\n# A.2.2 GRADIENTS OF LAYER NORM\nVisualization examples of LayerNorm gradients. In fig. A3 and fig. A4 , we present the gradients of the LayerNorm parameters during the training process. Similar to the one we have shown in the main text, LayerNorm tuning demonstrates a smaller gradient variance which is important for converging to a better local minimum ( Xu et al. ,2019 ).  \n\nProof of smaller variance in LayerNorm . As stated in Sec. 5.3 , deeper the network is, the variance of LayerNorm in the model should be naturally smaller ( $\\mathrm{\\DeltaXu}$ et al. ,2019 ). We first let $\\mathbf{y}\\,=\\,(y_{1},y_{2},...,y_{N})$ be the normalized vector, meaning the mean and variance of $\\mathbf{y}$ is 0 and 1 ,respectively. We can then formulate the standard LayerNorm as follow:  \n\n$$\n{\\mathbf{y}}={\\frac{\\mathbf{x}-{\\boldsymbol{\\mu}}}{\\sigma}},\\quad{\\boldsymbol{\\mu}}={\\frac{1}{N}}\\sum_{i=1}^{N}x_{i},\\quad\\sigma={\\sqrt{{\\frac{1}{N}}\\sum_{i=1}^{N}\\left(x_{i}-{\\boldsymbol{\\mu}}\\right)^{2}}},\n$$  \n\nwhere $\\mathbf{x}=(x_{1},x_{2},...,x_{N})$ is the input vector and $N$ is the dimension of $\\mathbf{x}$ .$\\mu$ and $\\sigma$ are the mean and standard deviation of $\\mathbf{x}$ .  \n\nWe first define $\\mathbf{1}_{N}=\\underbrace{(1,1,...,1)^{\\intercal}}_{N}$ . For calculating the gradients of the normalized vector $\\mathbf{y}$ , we first simulate the backward propagation regarding the loss {z }$\\ell$ :  \n\n$$\n{\\frac{\\partial\\ell}{\\partial\\mathbf{x}}}=\\left({\\frac{\\partial\\mathbf{y}}{\\partial\\mathbf{x}}}+{\\frac{\\partial\\mu}{\\partial\\mathbf{x}}}{\\frac{\\partial\\mathbf{y}}{\\partial\\mu}}+{\\frac{\\partial\\sigma}{\\partial\\mathbf{x}}}{\\frac{\\partial\\mathbf{y}}{\\partial\\sigma}}\\right){\\frac{\\partial\\ell}{\\partial\\mathbf{y}}}={\\frac{1}{\\sigma}}\\left(I-{\\frac{\\mathbf{y}\\mathbf{y}^{\\intercal}}{N}}-{\\frac{\\mathbf{1}_{N}\\mathbf{1}_{N}^{\\intercal}}{N}}\\right){\\frac{\\partial\\ell}{\\partial\\mathbf{y}}}.\n$$  \n\n  \nFigure A1: Layer similarities between different LLM layers in (a) Finetuned and (b) LayerNormtuned MM-LL A MA2-7B.  \n\nHere we define $\\begin{array}{r l r}{\\frac{\\partial\\ell}{\\partial\\mathbf x}}&{{}\\!\\!=}&{\\!\\!(a_{1},a_{2},...,a_{N})}\\end{array}$ with mean $\\bar{a}$ and standard deviation $D_{a}$ , and $\\begin{array}{r l}{\\frac{\\partial\\ell}{\\partial\\mathbf{y}}}&{{}=}\\end{array}$ $(b_{1},b_{2},...,b_{N})$ with mean $\\bar{b}$ and standard deviation $D_{b}$ . We set $\\begin{array}{r}{W_{1}\\;=\\;I\\,-\\,\\frac{{\\bf y}{\\bf y}^{\\intercal}}{N}\\,-\\,\\frac{{\\bf1}_{N}{\\bf1}_{N}^{\\intercal}}{N}}\\end{array}$ \u2212, we can verify that:  \n\n$$\n\\lfloor\\mathbf{\\Pi}_{N}^{\\mathsf{T}}W_{1}=\\mathbf{1}_{N}^{\\mathsf{T}}{\\frac{1}{\\sigma}}\\left(I-{\\frac{\\mathbf{1}_{N}\\mathbf{1}_{N}^{\\mathsf{T}}+\\mathbf{y}\\mathbf{y}^{\\mathsf{T}}}{N}}\\right)={\\frac{1}{\\sigma}}\\left(\\mathbf{1}_{N}-{\\frac{\\mathbf{1}_{N}^{\\mathsf{T}}\\mathbf{1}_{N}}{N}}\\mathbf{1}_{N}^{\\mathsf{T}}-{\\frac{\\mathbf{1}_{N}^{\\mathsf{T}}\\mathbf{y}}{N}}\\mathbf{y}^{\\mathsf{T}}\\right)={\\frac{\\mathbf{1}_{N}-\\mathbf{1}_{N}-0}{\\sigma}}=0\n$$  \n\nTherefore, we can easily proof that $N\\bar{a}\\propto{\\bf1}_{N}^{\\top}W_{1}\\bar{b}=0$ , which means the mean of $\\frac{\\partial\\ell}{\\partial\\mathbf{x}}$ should be zero. Then we dive into proofing the variance of LayerNorm gradients should be small when the number of network parameters $N$ becomes large.  \n\n$$\n\\begin{array}{l}{{\\displaystyle{D_{a}=\\sum_{i=1}^{N}(a_{i}-\\bar{a})^{2}/N=\\sum_{i=1}^{N}a_{i}^{2}/N}\\ ~}}\\\\ {{\\displaystyle{=\\left\\|{(a_{1},a_{2},\\ldots,a_{N})^{\\top}}\\right\\|^{2}/N}\\ ~}}\\\\ {{\\displaystyle{=\\left\\|{W_{1}\\left(b_{1},b_{2},\\ldots,b_{N}\\right)^{\\top}}\\right\\|^{2}/N}\\ ~}}\\\\ {{\\displaystyle{=\\left\\|{W_{1}\\left(b_{1}-\\bar{b},b_{2}-\\bar{b},\\ldots,b_{N}-\\bar{b}\\right)^{\\top}+W_{1}\\bar{b}{\\bf1}_{N}}\\right\\|^{2}/N}\\ }}\\\\ {{\\displaystyle{=\\left\\|{W_{1}\\left(g_{1}-\\bar{b},g_{2}-\\bar{b},\\ldots,g_{N}-\\bar{b}\\right)^{\\top}}\\right\\|^{2}/N}\\ ~}}\\\\ {{\\displaystyle{\\leq W_{1}^{2}\\sum_{i=1}^{N}(b_{i}-\\bar{b})^{2}/N}\\ }}\\end{array}\n$$  \n\nSince the projection matrix $W_{1}$ is idempotent, we have $W_{1}^{2}=W_{1}$ . That is to say, when $N$ is large enough, there stands the network parameter $\\begin{array}{r}{D_{a}\\le\\big(I-\\frac{\\mathbf{y}\\mathbf{y}^{\\top}+\\mathbf{1}_{N}\\mathbf{1}_{N}^{\\top}}{N}\\big)\\sum_{i=1}^{N}(b_{i}-\\bar{b_{}})^{2}/N\\propto1/N^{2}}\\end{array}$ Nis large, the gradient variance of LayerNorm should be small. P\u2212. As a consequence, when  \n\n  \nFigure A2: Layer similarities between different LLM layers in (a) Finetuned and (b) LayerNormtuned MM-LL A MA2-7B CHAT .  \n\n  \nFigure A3: The gradients of both input and post LayerNorm in 21st layer of the MM-V ICUNA as the training proceeds.  \n\n  \nFigure A4: The gradients of both input and post LayerNorm in 11th layer of the MM-V ICUNA as the training proceeds."}, {"ref_id": "454847819065993190", "chunk_id": "1", "score": 0.318359375, "text": "# 3.3 A TRANSFORMATION PER BLOCK\nNow that every LayerNorm in the transformer has been converted to RMSNorm, we can select any $\\mathbf{Q}$ to modify the model. Our initial plan was to collect signals from the model, construct an orthogonal matrix using those signals and to delete parts of the network. We quickly saw that the signals at different blocks of the network were not aligned, and that we would need to apply a different orthogonal matrix at each block, $\\mathbf{Q}_{\\ell}$ .  \n\nAllowing the orthogonal matrix used in each block to differ can be shown to leave the model unchanged using the same proof as Theorem 1 ,  \n\n  \nFigure 3: Converting a transformer network from LayerNorm to RMSNorm: the scale matrix diag $(\\alpha)$ is absorbed into the subsequent matrix $\\mathbf{W}_{\\mathrm{in}}$ . Figure shows the block in combined colors. We use $(\\alpha)$ for brevity. The mean-subtraction matrix $\\mathbf{M}$ is applied to each matrix $\\mathbf{W}_{\\mathrm{out}}$ . Layernorm becomes RMSNorm, up to a constant $\\bar{\\sqrt{D}}$ (not shown). Here, the scaling $(\\alpha^{\\prime})$ comes from the previous block.  \n\n  \nFigure 4: With the network converted to RMSNorm (see Figure 3 ), we apply the computational-invariance idea. The input weight matrices $\\mathrm{diag}(\\alpha)\\mathbf{W}_{\\mathrm{in}}$ are pre-multiplied by $\\mathbf{Q}^{\\top}$ . The output matrices $\\mathbf{W}_{\\mathrm{out}}\\mathbf{M}$ are post-multiplied by $\\mathbf{Q}$ . In the skip-connection, a new linear layer is added $\\mathbf{Q}_{\\ell}^{\\top}\\mathbf{Q}_{\\ell+1}$ . After these modifications, the matrices can be sliced (hatched areas).  \n\nwith the exception of line 5 of Algorithm 1 . Here we see that the residual connection and the output of the block must have the same rotation. To fix this, we modify the residual connection by applying the linear transformation applied to different blocks with the additional linear operation in the residual connection. Unlike the $\\mathbf{Q}_{\\ell-1}^{\\top}\\mathbf{Q}_{\\ell}$ \u2212to the residual. Figure 4 shows how different rotations can be modifications to the weight matrices, these additional operations cannot be pre-computed and add a small $(D\\times D)$ overhead to the model. Nonetheless, they are needed to allow slicing the model (Section 3.4 ) and we see real speedup overall (Section 4 ).  \n\nTo compute the matrices $\\mathbf{Q}_{\\ell}$ , we use PCA. We select a calibration dataset from the training set, run it through the model (after converting LayerNorm operations into RMSNorm), and extract the orthogonal matrix of the layer. We use the output of the transformed network to calculate the orthogonal matrices of the next layers. More precisely, if $\\mathbf{X}_{\\ell,i}$ is the output of the $\\ell^{\\mathrm{th}}$ RMSNorm block for the $i^{\\mathrm{th}}$ sequence in the calibration dataset, we compute  \n\n$$\n\\mathbf{C}_{\\ell}=\\sum_{i}\\mathbf{X}_{\\ell,i}^{\\top}\\mathbf{X}_{\\ell,i}\n$$  \n\nand set $\\mathbf{Q}_{\\ell}$ to the be the eigenvectors of $\\mathbf{C}_{\\ell}$ , sorted by decreasing eigenvalues.\n\n# 3.4 SLICING\nThe goal of Principal Component Analysis is usually to take a data matrix $\\mathbf{X}$ and compute a lower dimensional representation $\\mathbf{Z}$ , and an approximate reconstruction $\\tilde{\\mathbf{X}}$ :  \n\n$$\n\\mathbf{Z}=\\mathbf{X}\\mathbf{Q}\\mathbf{D}\\,,\\qquad\\tilde{\\mathbf{X}}=\\mathbf{Z}\\mathbf{D}^{\\top}\\mathbf{Q}^{\\top}\\,.\n$$  \n\nwhere $\\mathbf{Q}$ is the ectors of ${\\bf X}^{\\top}{\\bf X}$ , and $\\mathbf{D}$ is a $D\\times D_{\\mathrm{small}}$ deletion matrix (containing $D_{\\mathrm{small}}$ The reconstruction is columns of the $D\\times D$ \u00d7$L_{2}$ identity matrix), which removes some of the columns of the matrix to the left. optimal, in the sense that QD is a linear mapping that minimizes $\\lVert\\mathbf{X}-\\tilde{\\mathbf{X}}\\rVert^{2}$ .  \n\nWhen we apply PCA to the signal matrix $\\mathbf{X}$ bween blocks, we never materialize the $N\\times D$ signal matrix, but we apply the deletion matrix Dto the operations preceding and succeeding the construction of that matrix, which have already been multiplied by $\\mathbf{Q}$ in the above. We delete rows of $\\mathbf{W}_{\\mathrm{in}}$ that we have inserted into the residual connection (see Figure and columns of $\\mathbf{W}_{\\mathrm{out}}$ and $\\mathbf{W}_{\\mathrm{embd}}$ . We also delete both rows 4 ). and columns of the matrix $\\mathbf{Q}_{\\ell-1}^{\\top}\\mathbf{Q}_{\\ell}$ \u2212\n\n# 4 EXPERIMENTAL VALIDATION\nSetup We use HuggingFace Transformers ( Wolf et al. ,2019 ) to implement our code with PyTorch (Paszke et al. ,2019 ). The computation of $\\mathbf{Q}$ is performed on a single H100 GPU with 80GB of memory, taking approximately 3.5 hours to complete for the L LAMA -2 70B model. During the PCA calculation, we use double precision for computing the eigenvectors of the covariance matrix. We find that using single precision for eigenvector calculations in PyTorch leads to a discrepancy in the final accuracy, as detailed in Appendix A.2 .  \n\nWe experiment with two different calibration sets: 1024 samples from the WikiText-2 training dataset ( Merity et al. ,2016 ) and 5000 samples from the Alpaca training dataset ( Taori et al. ,2023 ). Sequence lengths are chosen as the maximum of each language model. An ablation study on the calibration set size and sequence length is presented in Appendix A.3 .  \n\nModels, Tasks, and GPUs We evaluate all our experiments on OPT ( Zhang et al. ,2022 ), L LAMA -2 (Touvron et al. ,2023 ) model families, and additionally evaluate Phi-2 (in our zero-shot task) experiments. We exclude OPT 175B, as it is outperformed by smaller L LAMA -2 models. Nonetheless, we anticipate that this larger model will yield improved results, as larger models typically offer more promising opportunities for compression (see Section 4.1 ). We evaluate our scheme on both language generation as well as popular zero-shot tasks. To demonstrate the comprehensive speedup achieved by SliceGPT we use: Quadro RTX6000 GPUs with 24GB of memory as a representative example of consumer-level GPUs; 40GB A100s and 80GB H100s to provide datacenter-level benchmarks.  \n\nBaseline Setup We initially planned to compare our results against a scheme that pruned columns (or rows) with the smallest norm but found that this baseline was very poor, with the perplexity of the model soaring into the 1000s after pruning just a few columns. Instead, we compare SliceGPT against SparseGPT ( Frantar & Alistarh ,2023 ) employing a 2:4 sparsity ratio, as this is the only sparsity scheme which achieves speedup ( Mishra et al. ,2021 )."}], "task_step_question_answer": "\u5206\u6570/\u603b\u5206: 8/10", "ref_task_step_id": ""}, "__type__": "task_step_node"}, "ac024820-e0dd-4e64-b24a-658c8b08cd94": {"__data__": {"id_": "ac024820-e0dd-4e64-b24a-658c8b08cd94", "metadata": {}, "relationships": {}, "hash": "", "start_task_context": "\u5927\u6a21\u578b\u4e2d\u7684LayerNorm\u548cRMSNorm\u6709\u4ec0\u4e48\u533a\u522b\uff1f", "aemo_representation_context": "### Step-by-Step Decomposition of Computer Science Research Content\n\n#### 1. \u5206\u6790\u8fd1\u51e0\u5e74\u7814\u7a76\u9886\u57df\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba\n- **\u6280\u672f\u6846\u67b6**\uff1a\u8fd1\u5e74\u6765\uff0c\u6df1\u5ea6\u5b66\u4e60\u3001\u5f3a\u5316\u5b66\u4e60\u3001\u56fe\u795e\u7ecf\u7f51\u7edc\u7b49\u6846\u67b6\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u9886\u57df\u5360\u636e\u4e3b\u5bfc\u5730\u4f4d\u3002\u5177\u4f53\u6846\u67b6\u5982Transformer\u3001BERT\u3001GNN\u7b49\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002\n- **\u65b9\u6cd5\u8bba**\uff1a\u7814\u7a76\u65b9\u6cd5\u9010\u6e10\u4ece\u5355\u4e00\u6a21\u578b\u8f6c\u5411\u591a\u6a21\u578b\u878d\u5408\uff0c\u4ece\u76d1\u7763\u5b66\u4e60\u6269\u5c55\u5230\u65e0\u76d1\u7763\u548c\u81ea\u76d1\u7763\u5b66\u4e60\uff0c\u5f3a\u8c03\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002\n\n#### 2. \u7814\u7a76\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u4e3b\u8981\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0e\u53d8\u4f53\n- **\u5e94\u7528**\uff1a\u4f8b\u5982\uff0cTransformer\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u4e2d\u88ab\u5e7f\u6cdb\u5e94\u7528\uff0cBERT\u53d8\u4f53\u5982RoBERTa\u3001ALBERT\u5728\u6587\u672c\u5206\u7c7b\u3001\u95ee\u7b54\u7cfb\u7edf\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff1bGNN\u5728\u63a8\u8350\u7cfb\u7edf\u3001\u77e5\u8bc6\u56fe\u8c31\u7b49\u9886\u57df\u6709\u663e\u8457\u5e94\u7528\u3002\n- **\u53d8\u4f53**\uff1a\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\uff0c\u7814\u7a76\u8005\u63d0\u51fa\u4e86\u591a\u79cd\u53d8\u4f53\uff0c\u5982\u9488\u5bf9\u957f\u5e8f\u5217\u5904\u7406\u7684Transformer-XL\uff0c\u9488\u5bf9\u591a\u6a21\u6001\u6570\u636e\u7684ViLBERT\u7b49\u3002\n\n#### 3. \u8bc4\u4f30\u5b66\u672f\u754c\u7684\u6280\u672f\u8fdb\u6b65\u4e0e\u5c40\u9650\u6027\n- **\u6280\u672f\u8fdb\u6b65**\uff1a\u6a21\u578b\u6027\u80fd\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\uff0c\u8ba1\u7b97\u6548\u7387\u4e5f\u6709\u6240\u63d0\u9ad8\uff0c\u5982EfficientNet\u5728\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u8868\u73b0\u3002\n- **\u5c40\u9650\u6027**\uff1a\u4f9d\u7136\u5b58\u5728\u6a21\u578b\u504f\u5dee\uff08\u5982\u6027\u522b\u504f\u89c1\uff09\u3001\u6570\u636e\u4f9d\u8d56\uff08\u5982\u5bf9\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\uff09\u7b49\u95ee\u9898\uff0c\u6cdb\u5316\u80fd\u529b\u6709\u5f85\u8fdb\u4e00\u6b65\u63d0\u5347\u3002\n\n#### 4. \u63a2\u8ba8\u8ba1\u7b97\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\n- **\u9002\u7528\u6027**\uff1a\u7814\u7a76\u6a21\u578b\u5728\u4e0d\u540c\u9886\u57df\uff08\u5982\u533b\u7597\u3001\u91d1\u878d\uff09\u548c\u591a\u6a21\u6001\u6570\u636e\uff08\u5982\u6587\u672c+\u56fe\u50cf\uff09\u4e0a\u7684\u8868\u73b0\uff0c\u5982\u591a\u6a21\u6001Transformer\u6a21\u578b\u3002\n- **\u6cdb\u5316\u80fd\u529b**\uff1a\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\u3001\u5143\u5b66\u4e60\u7b49\u65b9\u6cd5\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f7f\u5176\u80fd\u591f\u5e94\u5bf9\u66f4\u591a\u73b0\u5b9e\u5e94\u7528\u573a\u666f\u3002\n\n#### 5. \u5206\u6790\u6700\u65b0\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027\n- **\u7a33\u5b9a\u6027**\uff1a\u7814\u7a76\u7b97\u6cd5\u5728\u590d\u6742\u3001\u52a8\u6001\u73af\u5883\u4e0b\u7684\u8868\u73b0\uff0c\u5982\u5bf9\u6297\u8bad\u7ec3\u3001\u9c81\u68d2\u4f18\u5316\u7b49\u65b9\u6cd5\u7684\u5e94\u7528\u3002\n- **\u5bb9\u9519\u6027**\uff1a\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\u7684\u9002\u5e94\u6027\uff0c\u5982\u5206\u5e03\u5f0f\u8bad\u7ec3\u4e2d\u7684\u68af\u5ea6\u538b\u7f29\u3001\u6a21\u578b\u5e76\u884c\u7b49\u6280\u672f\u3002\n\n#### 6. \u8bc4\u4f30\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218\n- **\u672a\u6765\u65b9\u5411**\uff1a\u5982\u53ef\u89e3\u91caAI\u3001\u8054\u90a6\u5b66\u4e60\u3001\u9690\u79c1\u4fdd\u62a4\u7b49\u65b0\u5174\u7814\u7a76\u65b9\u5411\u3002\n- **\u6311\u6218**\uff1a\u5982\u4f55\u5e73\u8861\u6a21\u578b\u6027\u80fd\u4e0e\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\uff0c\u5982\u4f55\u89e3\u51b3\u6570\u636e\u504f\u89c1\u548c\u4f26\u7406\u95ee\u9898\u7b49\u3002\n\n### \u5927\u6a21\u578b\u4e2d\u7684LayerNorm\u548cRMSNorm\u7684\u533a\u522b\n\n#### LayerNorm\uff08Layer Normalization\uff09\n- **\u5b9a\u4e49**\uff1aLayerNorm\u662f\u5bf9\u6bcf\u4e2a\u6837\u672c\u7684\u6bcf\u4e2a\u7279\u5f81\u8fdb\u884c\u5f52\u4e00\u5316\uff0c\u4f7f\u5176\u5747\u503c\u548c\u65b9\u5dee\u5206\u522b\u63a5\u8fd10\u548c1\u3002\n- **\u516c\u5f0f**\uff1a\n  \\[\n  \\text{LayerNorm}(x) = \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} \\cdot \\gamma + \\beta\n  \\]\n  \u5176\u4e2d\uff0c\\(\\mu\\)\u548c\\(\\sigma^2\\)\u5206\u522b\u662f\u7279\u5f81\u5747\u503c\u548c\u65b9\u5dee\uff0c\\(\\gamma\\)\u548c\\(\\beta\\)\u662f\u53ef\u5b66\u4e60\u53c2\u6570\uff0c\\(\\epsilon\\)\u662f\u5fae\u5c0f\u5e38\u6570\u4ee5\u9632\u6b62\u9664\u96f6\u3002\n- **\u5e94\u7528**\uff1a\u5e7f\u6cdb\u7528\u4e8eTransformer\u6a21\u578b\u4e2d\uff0c\u6709\u52a9\u4e8e\u7f13\u89e3\u68af\u5ea6\u6d88\u5931\u95ee\u9898\uff0c\u63d0\u5347\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002\n\n#### RMSNorm\uff08Root Mean Square Normalization\uff09\n- **\u5b9a\u4e49**\uff1aRMSNorm\u662f\u5bf9\u6bcf\u4e2a\u6837\u672c\u7684\u6bcf\u4e2a\u7279\u5f81\u8fdb\u884c\u5f52\u4e00\u5316\uff0c\u4f46\u53ea\u5f52\u4e00\u5316\u65b9\u5dee\uff0c\u4e0d\u5f52\u4e00\u5316\u5747\u503c\u3002\n- **\u516c\u5f0f**\uff1a\n  \\[\n  \\text{RMSNorm}(x) = \\frac{x}{\\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} x_i^2 + \\epsilon}} \\cdot \\gamma + \\beta\n  \\]\n  \u5176\u4e2d\uff0c\\(n\\)\u662f\u7279\u5f81\u6570\u91cf\uff0c\\(\\gamma\\)\u548c\\(\\beta\\)\u662f\u53ef\u5b66\u4e60\u53c2\u6570\uff0c\\(\\epsilon\\)\u662f\u5fae\u5c0f\u5e38\u6570\u3002\n- **\u5e94\u7528**\uff1a\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0cRMSNorm\u6bd4LayerNorm\u66f4\u7b80\u5355\uff0c\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\uff0c\u5c24\u5176\u5728\u5927\u89c4\u6a21\u6a21\u578b\u8bad\u7ec3\u4e2d\u3002\n\n#### \u4e3b\u8981\u533a\u522b\n1. **\u5f52\u4e00\u5316\u65b9\u5f0f**\uff1a\n   - LayerNorm\u540c\u65f6\u5f52\u4e00\u5316\u5747\u503c\u548c\u65b9\u5dee\u3002\n   - RMSNorm\u53ea\u5f52\u4e00\u5316\u65b9\u5dee\u3002\n2. **\u8ba1\u7b97\u590d\u6742\u5ea6**\uff1a\n   - LayerNorm\u9700\u8981\u8ba1\u7b97\u5747\u503c\u548c\u65b9\u5dee\uff0c\u8ba1\u7b97\u91cf\u7a0d\u5927\u3002\n   - RMSNorm\u53ea\u9700\u8ba1\u7b97\u65b9\u5dee\uff0c\u8ba1\u7b97\u91cf\u8f83\u5c0f\u3002\n3. **\u6548\u679c**\uff1a\n   - LayerNorm\u5728\u8bb8\u591a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u5728NLP\u9886\u57df\u3002\n   - RMSNorm\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u8868\u73b0\u76f8\u4f3c\u751a\u81f3\u66f4\u4f18\uff0c\u4e14\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u3002\n\n\u901a\u8fc7\u4e0a\u8ff0\u5206\u6790\uff0c\u53ef\u4ee5\u66f4\u597d\u5730\u7406\u89e3LayerNorm\u548cRMSNorm\u5728\u5927\u6a21\u578b\u4e2d\u7684\u5e94\u7528\u53ca\u5176\u4f18\u7f3a\u70b9\uff0c\u4ece\u800c\u5728\u5177\u4f53\u4efb\u52a1\u4e2d\u9009\u62e9\u5408\u9002\u7684\u5f52\u4e00\u5316\u65b9\u6cd5\u3002", "task_step_name": "\u5bb9\u9519\u6027", "task_step_description": "\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\u7684\u9002\u5e94\u6027\uff0c\u5982\u5206\u5e03\u5f0f\u8bad\u7ec3\u4e2d\u7684\u68af\u5ea6\u538b\u7f29\u3001\u6a21\u578b\u5e76\u884c\u7b49\u6280\u672f\u3002", "task_step_level": "4>2", "task_step_question": "### \u95ee\u9898\n\n\u5728\u5927\u89c4\u6a21\u6570\u636e\u8bad\u7ec3\u4e2d\uff0cLayerNorm\u548cRMSNorm\u5728\u5bb9\u9519\u6027\u65b9\u9762\u6709\u4f55\u5dee\u5f02\uff1f\u5177\u4f53\u6765\u8bf4\uff0c\u5b83\u4eec\u5728\u5206\u5e03\u5f0f\u8bad\u7ec3\u4e2d\u7684\u68af\u5ea6\u538b\u7f29\u548c\u6a21\u578b\u5e76\u884c\u7b49\u6280\u672f\u4e2d\u7684\u8868\u73b0\u5982\u4f55\uff1f\u54ea\u79cd\u5f52\u4e00\u5316\u65b9\u6cd5\u66f4\u9002\u5408\u5904\u7406\u5927\u89c4\u6a21\u6570\u636e\uff0c\u5c24\u5176\u662f\u5728\u9762\u5bf9\u6570\u636e\u5206\u5e03\u4e0d\u5747\u5300\u6216\u566a\u58f0\u8f83\u591a\u7684\u60c5\u51b5\u4e0b\uff1f", "task_step_question_context": [{"ref_id": "454847042436311108", "chunk_id": "4", "score": 0.1982421875, "text": "# 1 I NTRODUCTION\nA common assumption in machine learning is that the training and the test data are independent and identically distributed (i.i.d.) samples. In practice, the unseen test data are often sampled from distributions that are different from the one for training. For example, a camel may appear on the grassland during the test time instead of the desert which is their usual habitat (Rosenfeld et al., 2020). However, models such as overparameterized deep neural networks are prone to rely on spurious correlations that are only applicable to the train distribution and fail to generalize under the distribution shifts of test data (Hashimoto et al., 2018; Sagawa et al., 2019; Arjovsky et al., 2019). It is crucial to ensure the generalizability of ML models, especially in safety-critical applications such as autonomous driving and medical diagnosis (Ahmad et al., 2018; Yurtsever et al., 2020).  \n\nNumerous studies have attempted to improve the generalization of deep learning models regarding distribution shifts from various angles (Zhang et al., 2017; 2022; Nam et al., 2020; Koh et al., 2021; Liu et al., 2021a; Yao et al., 2022; Gao et al., 2023). An important line of work addresses the issue based on having the training data partitioned according to their respective domains/environments. For example, cameras are placed around different environments (near water or land) to collect sample photos of birds. Such additional environment information sparks the rapid development of domain generalization (DG) algorithms (Arjovsky et al., 2019; Sagawa et al., 2019; Krueger et al., 2021). The common goal of these approaches is to discover the invariant representations that are optimal in loss functions for all environments to improve generalization by discouraging the learning of spurious correlations that only work for certain subsets of data. However, Gulrajani & Lopez-Paz (2020) and Ye et al. (2022) have shown that no DG algorithms clearly outperform the standard empirical risk minimization (ERM) (Vapnik, 1999). Subsequently, Kirichenko et al. (2022) and Rosenfeld et al. (2022) have argued that ERM may have learned good enough representations. These studies call for understanding whether, when, and why DG algorithms may indeed outperform ERM.  \n\nIn this work, we investigate under what conditions DG algorithms are superior to ERM. We demonstrate that the success of some DG algorithms can be ascribed to their robustness to label noise under subpopulation shifts. Theoretically, we prove that when using overparameterized models trained with finite samples under label noise, ERM is more prone to converging to suboptimal solutions that mainly exploit spurious correlations. The analysis is supported by experiments on synthetic datasets. In contrast, the implicit noise robustness of some DG algorithms provides an extra layer of performance guarantee. We trace the origin of the label-noise robustness by analyzing the optimization process of a few exemplary DG algorithms, including IRM (Arjovsky et al., 2019), V-REx (Krueger et al., 2021), and GroupDRO (Sagawa et al., 2020), while algorithms including ERM and Mixup (Zhang et al., 2017) without explicit modification to the objective function, do not offer such a benefit. Empirically, our findings suggest that the noise robustness of DG algorithms can be beneficial in certain synthetic circumstances, where label noise is non-negligible and spurious correlation is severe. However, in general cases with noisy real-world data and pretrained models, there is still no clear evidence that DG algorithms yield better performance at the moment.  \n\nWhy should we care about label noise? It can sometimes be argued that label noise is unrealistic in practice, especially when the amount of injected label noise is all but small. However, learning a fully accurate decision boundary from the data may not always be possible (e.g., some critical information is missing from the input). Such kinds of inaccuracy can be alternatively regarded as manifestations of label noise. Furthermore, a common setup for analyzing the failure mode of ERM assumes that the classifier utilizing only the invariant features is only partially predictive (having ${<}100\\%$ accuracy) of the labels (Arjovsky et al., 2019; Sagawa et al., 2020; Shi et al., 2021). These setups are subsumed by our setting where there initially exists a fully predictive invariant classifier for the noise-free data distribution, and some degree of label noise is added afterward. More broadly, we analyze the failure mode of ERM without assuming the spurious features are generated with less variance (hence less noisy) than the invariant features. By looking closer at domain generalization through the lens of noise robustness, we obtain unique understanding of when and why ERM and DG algorithms work.  \n\nAs our main contribution, we propose an inclusive theoretical and empirical framework that analyzes the domain generalization performance of algorithms under the effect of label noise:  \n\n1. We theoretically demonstrate that when trained with ERM on finite samples, the tendency of learning spurious correlations rather than invariant features for overparameterized models is jointly determined by both the degrees of spurious correlation and label noise . (Section 4)   \n2. We show that several DG algorithms possess the noise-robust property and enable the model to learn invariance rather than spurious correlations despite using data with noisy labels. (Section 5)   \n3. We perform extensive experiments to compare DG algorithms across synthetic and real-life datasets injected with label noise but unfortunately find no clear evidence that noise robustness necessarily leads to better performance in general. To address this, we discuss the difficulties of satisfying the theoretical conditions and other potential mitigators in practice. (Section 6, 7)"}, {"ref_id": "454845744505973136", "chunk_id": "6", "score": 0.197265625, "text": "# 6 Conclusion\nLLM-based RL algorithms have shown generalization across multiple tasks and games. We argue that this ability comes from implicit memory that fits a large number of parameters to the training data, which is inefficient in terms of model size. In contrast, we propose a new approach inspired by the concept of \u201cworking memory\u201d called Decision Transformers with Memory (DT-Mem), which stores training experience explicitly in a content-addressable matrix module for later retrieval and use. Evaluation demonstrates that DT-Mem achieves better generalization on Atari games with only $10\\%$ of the model parameters compared to the state-of-the-art method. Furthermore, we demonstrate that fine-tuning DT-Memwith a small amount of data can produce state-of-the-art results on both Atari games and the Meta-World environment, when compared to MDT [22], PDT [37], and HDT [38].  \n\nLimitations The first limitation of our work is the sample efficiency of memory fine-tuning. The $10\\%$ fine-tuning dataset is still sizeable, and we plan to explore more sample-efficient methods in the future. We could for instance consider a setting with more tasks, each one with less data so that the inter-task generalization would be even more crucial to its performance. Additionally, this work does not propose a control strategy for collecting data on a new task. For future work, we plan to investigate online data collection methods, which includes the design and learning of exploration strategies for an efficient fine-tuning on new tasks. Finally, the approach has been intuitively motivated, but it would be valuable to have a theoretical grounding that would show the structural limits of large models and how equipping them with a memory component overcomes them.  \n\nSocietal Impact We do not foresee any significant societal impact resulting from our proposed method. The current algorithm is not designed to interact with humans, nor any realistic environment yet. If one chooses to extend our methods to such situations, caution should be exercised to ensure that any safety and ethical concerns are appropriately addressed. As our work is categorized in the offline-RL domain, it is feasible to supplement its training with a dataset that aligns with human intents and values. However, one must be wary that the way our architecture generalizes across tasks is still not well understood and as a consequence we cannot guarantee the generalization of its desirable features: performance, robustness, fairness, etc. By working towards methods that improve the computational efficiency of large models, we contribute to increase their access and reduce their ecological impact.\n\n\n\n# A Implementation Details\n\n# A.1 DT-Mem network architecture\nTable 3 summarizes the different model configurations used for evaluation. In this section, we describe these model configurations in detail. While Table 3 provides a summary, we will also provide additional information here. DT-Mem, PDT and HDT are all share the same transformer architectures. However, for task-adaptation, HDT utilizes a pre-trained $2.3\\mathbf{M}$ hyper-network, while DT-Mem introduces 147K LoRA parameters. To compare with MDT, we use the same parameter size as reported in [22].  \n\nTable 3: Detailed Model Sizes   \n\n\n<html><body><table><tr><td>Model</td><td>Layers</td><td>Hidden size (d)</td><td>Heads</td><td>Params</td><td>Memory Size</td><td>Memory Module Params</td></tr><tr><td>HDT</td><td>4</td><td>512</td><td>8</td><td>13M</td><td>N.A.</td><td>N.A.</td></tr><tr><td>MDT-200M</td><td>10</td><td>1280</td><td>20</td><td>200M</td><td>N.A.</td><td>N.A.</td></tr><tr><td>DT-Mem</td><td>4</td><td>512</td><td>8</td><td>13M</td><td>559K</td><td>7M</td></tr></table></body></html>\n\n# A.2 Hyper-parameters\nIn this section, we will delve into the specifics of the model parameters. Understanding these parameters is key to understanding the workings of the model. It is worth noting that the source code for this model is publicly available at https://github.com/luciferkonn/DT_Mem/tree/main .This allows for a deeper understanding of the model\u2019s inner workings and may facilitate the replication of its results.  \n\nTable 4: Hyperparameters for DT-Mem training   \n\n\n<html><body><table><tr><td>Hyperparameters</td><td>Value</td></tr><tr><td>K (length of context)</td><td>28</td></tr><tr><td>dropoutrate</td><td>0.1</td></tr><tr><td>maximum epochs</td><td>1000</td></tr><tr><td>steps for each epoch</td><td>1000</td></tr><tr><td>optimizer learning rate</td><td>1e-4</td></tr><tr><td>weight decay</td><td>1e-4</td></tr><tr><td>gradient norm clip</td><td>1.</td></tr><tr><td>data points for each dataset</td><td>500,000</td></tr><tr><td>batch size</td><td>64</td></tr><tr><td>memory slots</td><td>1290</td></tr><tr><td>activation</td><td>GELU</td></tr><tr><td>optimizer</td><td>Adamw</td></tr><tr><td>scheduler</td><td>LambdaLR</td></tr></table></body></html>\n\n# A.3 Training and fine-tuning algorithm\nIn this section, we present the pre-training DT-Memin Appendix A.3 and fine-tuning DT-Mem with LoRA in Appendix 5.5.  \n\nWe pre-train DT-Mem on multiple offline datasets. Each gradient update of the DT-Memmodel considers information from each training task.  \n\nWe fine-tune the memory module to adapt to each downstream task. To achieve this, we fix the pre-trained DT-Mem model parameters and add additional LoRA parameters for the memory module feed-forward neural networks. The fine-tune dataset is used to update these LoRA parameters only.\n\n# Algorithm 1 Pre-train DT-Mem\n1: for T episodes do   \n2: 3: 4: for Sample trajectories Split trajectories into different segments with length K and calculate return-to-go in the Task $\\mathcal{T}_{i}\\in T^{t r a i n}\\;.$ do $\\tau=(s_{0},a_{0},r_{0},\\cdot\\cdot\\cdot\\,,s_{H},a_{H},r_{H})$ m the dataset $\\mathcal{D}_{i}$ .  \ninput sequence.   \n5: Given $\\hat{\\tau}_{t+1:t+K}$ , compute the sequence embedding $e_{s e q}$ .  \n6: Update the working memory and retrieve the relative information as $E_{o u t}$   \n7: Given $E_{o u t}$ , predict actions $\\tilde{a}_{t}$ , reward $\\tilde{r}_{t}$ , and return-to-go ${\\tilde{R}}_{t}$ .  \n8: Compute the loss according to Eqn. 1.   \n9: Update all modules parameters.   \n10: end for   \n11: end for  \n\nAlgorithm 2 Fine-tuning DT-Mem  \n\n$\\hat{B}^{q},\\hat{B}^{k},\\hat{B}^{v},\\hat{A}^{q},\\hat{A}^{k},\\hat{A}^{v},B^{q},A^{q},B^{k},A^{k}$ Require: Fine-tuning dataset $\\mathcal{T}^{i}~\\in~T^{t e s t}$ .dataset $\\mathcal{D}^{i}$ for $\\mathcal{T}^{i}$ .Initialize LoRA parameters 1: for T steps do   \n2: Split trajectories into different segments with length $\\mathbf{K}$ and calculate return-to-go in the input sequence.   \n3: Given $\\hat{\\tau}_{t+1:t+K}$ , compute the sequence embedding $e_{s e q}$ .  \n4: Update working memory using $\\hat{Q}\\,=\\,M(\\hat{W}^{q}+\\hat{B}^{q}\\bar{A}^{q})$ ,$\\hat{K}\\,=\\,M(\\hat{W}^{k}\\,+\\,\\hat{B}^{k}\\hat{A}^{k}),\\hat{V}\\,=$ $M(\\hat{W}^{v}+\\hat{B}^{v}\\hat{A}^{v})$ ,$Q=M(W^{q}+B^{q}A^{q}),K=M(W^{k}+B^{k}A^{k})$   \n5: Retrieve the relative information as $E_{o u t}$   \n6: Given $E_{o u t}$ , predict actions $\\tilde{a}_{t}$ , reward $\\tilde{r}_{t}$ , and return-to-go ${\\tilde{R}}_{t}$ .  \n7: Compute the loss according to Eqn. 1.   \n8: Update LoRA parameters only.   \n9: end for"}, {"ref_id": "455038427552559154", "chunk_id": "6", "score": 0.1875, "text": "# 3 Large Scale Results\nHere, we present the results for large-scale training to illustrate that our model performance scales with data size. We utilize, LAION115M dataset in addition to the 14M dataset used in the paper. Note that, the downloaded dataset using web urls has an approximate 20% miss rate, leading to overall dataset size of $105M$ for large-scale training.  \n\nWe employ the synthetic captions released by BLIP [27] for LAION115M, SBU and Conceptual Caption datasets. We use the Stage-1 model pre-trained with 14M dataset as weight initialization. The LLM alignment stage is trained for an epoch on the large-scale dataset. We experiment with $O P T_{2.7B}$ LLM and $F l a n T5_{X L}$ [8] LLM and present Zero-shot visual question answering results on GQA and OKVQA datasets. $O P T$ is a decoder-only LLM while FlanT 5 is enocder-decoder LLM. Following BLIP-2 [26], for $O P T$ model we train with language modeling loss while FlanT 5 model is trained with prefix language modeling loss i.e., caption is split into two parts: prefix and suffix. The prefix text along with visual representation forms input to LLM encoder and the suffix text is used as generation target for LLM decoder. A random value from start to middle of sentence is picked to divide the caption into two parts.  \n\nTable 10: Zero-shot Visual Question Answering results on GQA and OKVQA datasets with Large scale training. \\* evaluated using official checkpoint   \n\n\n<html><body><table><tr><td>Method</td><td colspan=\"5\">Caption Pre-processing CLIP Caption#Caps/Img Ranking</td><td>GQA OKVQA Acc. Acc.</td></tr><tr><td>Frozen [40]</td><td></td><td></td><td>Dhta</td><td>StageStage2 Data</td><td></td><td>5.9</td></tr><tr><td>VLKD [9]</td><td></td><td></td><td></td><td></td><td></td><td>13.3</td></tr><tr><td>FewVLM [22]</td><td></td><td></td><td></td><td></td><td>29.3</td><td>16.5</td></tr><tr><td>Flamingo3B [2]</td><td></td><td></td><td></td><td></td><td></td><td>41.2</td></tr><tr><td>Flamingo9B [2]</td><td></td><td></td><td></td><td></td><td></td><td>44.7</td></tr><tr><td>Flamingo80B [2]</td><td></td><td></td><td></td><td></td><td></td><td>50.6</td></tr><tr><td>PNP-VQA T03B[38]</td><td></td><td></td><td></td><td></td><td>32.3</td><td>26.6</td></tr><tr><td>PNP-VQA T011B [38]</td><td></td><td></td><td></td><td></td><td>33.4</td><td>30.5</td></tr><tr><td>PNP-VQA UnifiedQAv23B [38]</td><td></td><td></td><td></td><td></td><td>42.3</td><td>34.1</td></tr><tr><td>PNP-VQA UnifiedQAv211B[38]</td><td></td><td></td><td></td><td></td><td>41.9</td><td>35.9</td></tr><tr><td>BLIP-2 OPT2.7B* [26]</td><td></td><td>2</td><td>129M</td><td>129M</td><td>32.5</td><td>31.5</td></tr><tr><td>BLIP-2FlanT5xL*[26]</td><td>\u221a</td><td>2</td><td>129M</td><td>129M</td><td>43.9</td><td>41.2</td></tr><tr><td>BLIP-2 OPT2.7B [26]</td><td>\u00d7</td><td>1</td><td>14M</td><td>105M</td><td>32.2</td><td>25</td></tr><tr><td>X-Former(Ours)OPT2.7B</td><td>\u00d7</td><td>1</td><td>14M</td><td>105M</td><td>34.3</td><td>27.6</td></tr><tr><td>BLIP-2 FlanT5xL [26]</td><td>\u00d7</td><td>1</td><td>14M</td><td>105M</td><td>42.9</td><td>38.2</td></tr><tr><td>X-Former (Ours) FlanT5xL</td><td></td><td>1</td><td>14M</td><td>105M</td><td>44.9</td><td>39.5</td></tr></table></body></html>  \n\nWe demonstrate that our model outperforms BLIP-2 [26] at scale in Table 10. Specifically, our model achieves a $2.1\\%$ gain on GQA dataset and $2.6\\%$ gain on OKVQA dataset respectively with $O P T_{2.7B}$ LLM. We show similar gains using $F l a n T5_{X L}$ LLM as well; our approach improves by $2\\%$ on GQA dataset and $1.3\\%$ on OKVQA dataset respectively. Note that PNP-VQA [38] performance relies heavily on QA model specifically UnifiedQAv2 is a task-specific model pretrained for question answering, and OFA [42] trains visual encoders while we keep it frozen hence we do not compare with it.\n\n# 4 Ablation Analysis\nAblation On CLIP Layers As mentioned in Section 3.3 of main text \u201cLeveraging Early Layer CLIP features\u201d, we present additional results by experimenting with different layers from CLIP. We experiment with the following layers $\\{22,24,26,28,30,32,34,36\\}$ as early layer features from CLIP ViT and report performance trend on GQA dataset. As shown in Figure 7, we observe that the best performance is achieved for layer 26 and layer 30, while utilizing features from layers below 26 leads to a drop in performance. Furthermore, using features from layers beyond 30 also results in a decline in performance.  \n\nOur findings demonstrate that the performance using early layer CLIP features is inferior to that of our model, with a $2\\%$ decrease in performance compared to the best layer.  \n\nMore Ablations We perform comprehensive ablations studies to analyze the impact of different loss components, effect of Horizontal flip augmentation and effect of Self-Attention. Further, we analyze the impact of X-Former training for LLM alignment and importance of MAE to capture detailed visual information complementing the global semantic representation from CLIP-ViT. Note that for these ablations, we use 8-A100swith batch size of 320/272 for stage 1 and stage 2 respectively.  \n\nWe find that ITG significantly affects retrieval more than ITM; without ITC, there is a slight drop in captioning performance. As shown in Table 11, horizontal flip augmentation does not effect overall performance. For comprehensiveness, we analyze the effect of Self-Attention (SA) layer in X-Former as shown in Table 11, row 5. There is a drop in captioning performance when we remove SA layer before the Cross-Attention with MAE.  \n\nTable 11: Ablations Analysis.   \n\n\n<html><body><table><tr><td>Method</td><td>TR5</td><td>TR10</td><td>IR5</td><td>IR10</td><td>B@4</td><td>C</td></tr><tr><td>w/o ITM</td><td>96.6</td><td>98.8</td><td>93.8</td><td>96.7</td><td>35.9</td><td>120.3</td></tr><tr><td>w/o ITG</td><td>84.9</td><td>93.1</td><td>88.2</td><td>92.9</td><td></td><td></td></tr><tr><td>w/o ITC</td><td></td><td></td><td></td><td></td><td>36.2</td><td>120.7</td></tr><tr><td>w/o HFlip</td><td>93.2</td><td>98.4</td><td>93.9</td><td>97.2</td><td>36.3</td><td>122.4</td></tr><tr><td>w/oSA</td><td>95.5</td><td>99</td><td>93.9</td><td>97.1</td><td>35.6</td><td>120</td></tr><tr><td>X-Former</td><td>(suno) 95.8</td><td>99</td><td>94</td><td>96.7</td><td>37</td><td>123.2</td></tr></table></body></html>  \n\nTable 12: Ablation Analysis. \\*: smaller batch size in both stages   \n\n\n<html><body><table><tr><td>Method</td><td>GQA</td><td>OKVQA</td></tr><tr><td>CLIP W1 Recon.</td><td>22.5</td><td>8.1</td></tr><tr><td>X-Former Frozen</td><td>25.5</td><td>15.9</td></tr><tr><td>X-Former (Ours)</td><td>31.9</td><td>25.9</td></tr></table></body></html>  \n\nFor LLM alignment, we follow BLIP-2 protocol and train X-Former in stage2 along with a Fully Connected layer. To analyze the impact of training XFormer for LLM alignment, we experiment with frozen X-Former in stage-2 and report results in Table 12 row 2. To demonstrate the importance of MAE further, we replace MAE encoder with CLIP-ViT and pass masked image to CLIP-ViT which is then optimized for image reconstruction with MAE decoder. As shown in Table 12 row 1, the performance drops significantly by replacing MAE-ViT encoder with CLIP-ViT on both GQA and OKVQA dataset. Thus demonstrating MAE-ViT encoder plays crucial role in learning detailed visual features.  \n\n  \nFig. 7: Zero-shot visual question answering performance on GQA datasets for different layer features from CLIP.\n\n# 5 Qualitative Results\nIn this section, we present qualitative results, including cases where our method did not perform as expected. In Figure 8, we present examples that involve comparing the colors of different objects within the image. As you can see in Figure 8 (a), (b), and (c), our method successfully understands the specified objects in the questions, regardless of their positions in the image, and compares their colors accurately. However, Figure 8 (d) shows a more challenging scenario. Here, the pillow and the bed are not clearly distinguishable, which made it difficult for our model to identify the pillow in the image.  \n\n  \nFig. 8: Qualitative Comparison demonstrating ability to compare colors of specified objects.  \n\nFigure 9 showcases the spatial understanding capabilities of our model in comparison to the BLIP-2. A kitchen scene depicted in images 9(a) and (b), our model accurately identifies the refrigerator behind the countertop and the microwave above it, respectively. In contrast, BLIP-2 erroneously predicts a sink and refrigerator for the same questions. Our model correctly discerns the attire of individuals in 9 (c), recognizing that the person to the left of the one wearing glasses is indeed wearing jeans\u2014a detail that BLIP-2 overlooks. However, 9 (d) presents a more challenging scenario for both models. When asked about the color of the computer to the right of the shelf, our model and BLIP-2 both incorrectly identify a silver laptop as black.  \n\n  \nFig. 9: Qualitative Comparison pertaining to question of spatial understanding.  \n\n  \nFig. 10: Qualitative Comparison pertaining to question of relative object understanding in both background and foreground.  \n\nFigure 10 provides insights into the ability of our model to comprehend the relative positioning of objects within an image, both in the background and foreground. Figure 10 (a) and (b), probe the understanding of background elements, our model demonstrates a clear capacity to correctly identify objects, distinguishing trees on a beach and recognizing the tall, green trees beside a double-decker bus. This is in contrast to BLIP-2, which incorrectly identifies grass instead of trees and fails to acknowledge the verdancy and height of the trees. Further, in Figure 10 (c), which shifts the focus to foreground objects, our model accurately discerns the presence of deer in a grassy field. However, in Figure 10 (d), both our model and BLIP-2 inaccurately detect a fence in front of a tennis player, when, in fact, it is behind the player as shown. Overall, our model shows enhanced understanding of object contexts and positioning.  \n\n  \nFig. 11: Qualitative Comparison for questions relating to absolute image position understanding.  \n\n  \nFig. 12: Qualitative Comparison for samples with objects in close proximity in the scene.  \n\nFigure 11 exemplifies the absolute position reasoning capabilities of our model by assessing its ability to identify objects and their locations within an image, whether they are situated at the top or bottom parts of the image. In Figure 11 (a), (b) and (c), our model accurately determines the position of the fried food, the location of the old men, and the placement of the mirror, respectively, demonstrating better understanding of absolute positions within various contexts. However, Figure 11 (d) introduces a more complex situation involving multiple vehicles parked. Our model encounters difficulty here, incorrectly identifying a van as a car due to its close resemblance to car in this image.  \n\nFigure 12 demonstrates the capacity of our model to discern fine details of objects in close proximity within an image. In the living room scene depicted in Figure 12 (a), when questioned about the presence of a girl to the right of a pillow, our model accurately confirms the absence of a girl, whereas BLIP-2 incorrectly asserts a presence. The image of a food plate, shown in Figure 12(b) and (c), further probe the model\u2019s ability to understand foreground and background distinctions. Our model correctly identifies the fruit next to the cake as a strawberry, where as BLIP-2 incorrectly categorizes it as raspberries. Additionally, our model successfully distinguishes the white color of the plate amidst the various food items placed upon it, indicating a enhanced visual perception capabilities. However Figure 12 (d), presents a complex scenario involving an assessment of an onion\u2019s quality, both our model and BLIP-2 fail to correctly evaluate its healthiness. This highlights the challenge in assessing condition/quality of food which is subject to interpretation.  \n\n  \nFig. 13: Query Diversity: Ours (left), BLIP-2 (right)"}], "task_step_question_answer": "\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\uff0cLayerNorm\u901a\u8fc7\u5176\u7a33\u5b9a\u6027\uff0c\u80fd\u591f\u6709\u6548\u7f13\u89e3\u4e0d\u540c\u5ba2\u6237\u7aef\u6570\u636e\u5206\u5e03\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002\u800cRMSNorm\u5219\u901a\u8fc7\u5176\u9ad8\u6548\u6027\uff0c\u51cf\u5c11\u4e86\u901a\u4fe1\u5f00\u9500\uff0c\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u3002\u5728\u9690\u79c1\u4fdd\u62a4\u65b9\u9762\uff0cLayerNorm\u901a\u8fc7\u5176\u5168\u5c40\u5f52\u4e00\u5316\u7684\u7279\u6027\uff0c\u80fd\u591f\u66f4\u597d\u5730\u4fdd\u62a4\u654f\u611f\u6570\u636e\uff0c\u800cRMSNorm\u5219\u901a\u8fc7\u5176\u9ad8\u6548\u7684\u8ba1\u7b97\u8fc7\u7a0b\uff0c\u51cf\u5c11\u4e86\u9690\u79c1\u6cc4\u9732\u7684\u98ce\u9669\u3002\u672a\u6765\u7684\u7814\u7a76\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63a2\u7d22\u5982\u4f55\u7ed3\u5408LayerNorm\u548cRMSNorm\u7684\u4f18\u70b9\uff0c\u8bbe\u8ba1\u51fa\u66f4\u9ad8\u6548\u7684\u5f52\u4e00\u5316\u65b9\u6cd5\u3002\u4f8b\u5982\uff0c\u53ef\u4ee5\u7814\u7a76\u5982\u4f55\u6839\u636e\u8f93\u5165\u6570\u636e\u7684\u7279\u6027\u52a8\u6001\u9009\u62e9\u4f7f\u7528LayerNorm\u6216RMSNorm\uff0c\u4ee5\u8fdb\u4e00\u6b65\u63d0\u5347\u6a21\u578b\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u3002\u6b64\u5916\uff0c\u8fd8\u53ef\u4ee5\u63a2\u7d22\u8fd9\u4e9b\u65b9\u6cd5\u5728\u65b0\u5174\u7814\u7a76\u9886\u57df\uff08\u5982\u9690\u79c1\u4fdd\u62a4\u3001\u8054\u90a6\u5b66\u4e60\u7b49\uff09\u4e2d\u7684\u5e94\u7528\uff0c\u4ee5\u5e94\u5bf9\u65e5\u76ca\u590d\u6742\u7684\u5b9e\u9645\u9700\u6c42\u3002", "ref_task_step_id": ""}, "__type__": "task_step_node"}, "f2e5ad03-a297-4065-b995-c2dd46e1e9b4": {"__data__": {"id_": "f2e5ad03-a297-4065-b995-c2dd46e1e9b4", "metadata": {}, "relationships": {}, "hash": "", "start_task_context": "\u5927\u6a21\u578b\u4e2d\u7684LayerNorm\u548cRMSNorm\u6709\u4ec0\u4e48\u533a\u522b\uff1f", "aemo_representation_context": "### Step-by-Step Decomposition of Computer Science Research Content\n\n#### 1. \u5206\u6790\u8fd1\u51e0\u5e74\u7814\u7a76\u9886\u57df\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba\n- **\u6280\u672f\u6846\u67b6**\uff1a\u8fd1\u5e74\u6765\uff0c\u6df1\u5ea6\u5b66\u4e60\u3001\u5f3a\u5316\u5b66\u4e60\u3001\u56fe\u795e\u7ecf\u7f51\u7edc\u7b49\u6846\u67b6\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u9886\u57df\u5360\u636e\u4e3b\u5bfc\u5730\u4f4d\u3002\u5177\u4f53\u6846\u67b6\u5982Transformer\u3001BERT\u3001GNN\u7b49\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002\n- **\u65b9\u6cd5\u8bba**\uff1a\u7814\u7a76\u65b9\u6cd5\u9010\u6e10\u4ece\u5355\u4e00\u6a21\u578b\u8f6c\u5411\u591a\u6a21\u578b\u878d\u5408\uff0c\u4ece\u76d1\u7763\u5b66\u4e60\u6269\u5c55\u5230\u65e0\u76d1\u7763\u548c\u81ea\u76d1\u7763\u5b66\u4e60\uff0c\u5f3a\u8c03\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002\n\n#### 2. \u7814\u7a76\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u4e3b\u8981\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0e\u53d8\u4f53\n- **\u5e94\u7528**\uff1a\u4f8b\u5982\uff0cTransformer\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u4e2d\u88ab\u5e7f\u6cdb\u5e94\u7528\uff0cBERT\u53d8\u4f53\u5982RoBERTa\u3001ALBERT\u5728\u6587\u672c\u5206\u7c7b\u3001\u95ee\u7b54\u7cfb\u7edf\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff1bGNN\u5728\u63a8\u8350\u7cfb\u7edf\u3001\u77e5\u8bc6\u56fe\u8c31\u7b49\u9886\u57df\u6709\u663e\u8457\u5e94\u7528\u3002\n- **\u53d8\u4f53**\uff1a\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\uff0c\u7814\u7a76\u8005\u63d0\u51fa\u4e86\u591a\u79cd\u53d8\u4f53\uff0c\u5982\u9488\u5bf9\u957f\u5e8f\u5217\u5904\u7406\u7684Transformer-XL\uff0c\u9488\u5bf9\u591a\u6a21\u6001\u6570\u636e\u7684ViLBERT\u7b49\u3002\n\n#### 3. \u8bc4\u4f30\u5b66\u672f\u754c\u7684\u6280\u672f\u8fdb\u6b65\u4e0e\u5c40\u9650\u6027\n- **\u6280\u672f\u8fdb\u6b65**\uff1a\u6a21\u578b\u6027\u80fd\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\uff0c\u8ba1\u7b97\u6548\u7387\u4e5f\u6709\u6240\u63d0\u9ad8\uff0c\u5982EfficientNet\u5728\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u8868\u73b0\u3002\n- **\u5c40\u9650\u6027**\uff1a\u4f9d\u7136\u5b58\u5728\u6a21\u578b\u504f\u5dee\uff08\u5982\u6027\u522b\u504f\u89c1\uff09\u3001\u6570\u636e\u4f9d\u8d56\uff08\u5982\u5bf9\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\uff09\u7b49\u95ee\u9898\uff0c\u6cdb\u5316\u80fd\u529b\u6709\u5f85\u8fdb\u4e00\u6b65\u63d0\u5347\u3002\n\n#### 4. \u63a2\u8ba8\u8ba1\u7b97\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\n- **\u9002\u7528\u6027**\uff1a\u7814\u7a76\u6a21\u578b\u5728\u4e0d\u540c\u9886\u57df\uff08\u5982\u533b\u7597\u3001\u91d1\u878d\uff09\u548c\u591a\u6a21\u6001\u6570\u636e\uff08\u5982\u6587\u672c+\u56fe\u50cf\uff09\u4e0a\u7684\u8868\u73b0\uff0c\u5982\u591a\u6a21\u6001Transformer\u6a21\u578b\u3002\n- **\u6cdb\u5316\u80fd\u529b**\uff1a\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\u3001\u5143\u5b66\u4e60\u7b49\u65b9\u6cd5\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f7f\u5176\u80fd\u591f\u5e94\u5bf9\u66f4\u591a\u73b0\u5b9e\u5e94\u7528\u573a\u666f\u3002\n\n#### 5. \u5206\u6790\u6700\u65b0\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027\n- **\u7a33\u5b9a\u6027**\uff1a\u7814\u7a76\u7b97\u6cd5\u5728\u590d\u6742\u3001\u52a8\u6001\u73af\u5883\u4e0b\u7684\u8868\u73b0\uff0c\u5982\u5bf9\u6297\u8bad\u7ec3\u3001\u9c81\u68d2\u4f18\u5316\u7b49\u65b9\u6cd5\u7684\u5e94\u7528\u3002\n- **\u5bb9\u9519\u6027**\uff1a\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\u7684\u9002\u5e94\u6027\uff0c\u5982\u5206\u5e03\u5f0f\u8bad\u7ec3\u4e2d\u7684\u68af\u5ea6\u538b\u7f29\u3001\u6a21\u578b\u5e76\u884c\u7b49\u6280\u672f\u3002\n\n#### 6. \u8bc4\u4f30\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218\n- **\u672a\u6765\u65b9\u5411**\uff1a\u5982\u53ef\u89e3\u91caAI\u3001\u8054\u90a6\u5b66\u4e60\u3001\u9690\u79c1\u4fdd\u62a4\u7b49\u65b0\u5174\u7814\u7a76\u65b9\u5411\u3002\n- **\u6311\u6218**\uff1a\u5982\u4f55\u5e73\u8861\u6a21\u578b\u6027\u80fd\u4e0e\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\uff0c\u5982\u4f55\u89e3\u51b3\u6570\u636e\u504f\u89c1\u548c\u4f26\u7406\u95ee\u9898\u7b49\u3002\n\n### \u5927\u6a21\u578b\u4e2d\u7684LayerNorm\u548cRMSNorm\u7684\u533a\u522b\n\n#### LayerNorm\uff08Layer Normalization\uff09\n- **\u5b9a\u4e49**\uff1aLayerNorm\u662f\u5bf9\u6bcf\u4e2a\u6837\u672c\u7684\u6bcf\u4e2a\u7279\u5f81\u8fdb\u884c\u5f52\u4e00\u5316\uff0c\u4f7f\u5176\u5747\u503c\u548c\u65b9\u5dee\u5206\u522b\u63a5\u8fd10\u548c1\u3002\n- **\u516c\u5f0f**\uff1a\n  \\[\n  \\text{LayerNorm}(x) = \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} \\cdot \\gamma + \\beta\n  \\]\n  \u5176\u4e2d\uff0c\\(\\mu\\)\u548c\\(\\sigma^2\\)\u5206\u522b\u662f\u7279\u5f81\u5747\u503c\u548c\u65b9\u5dee\uff0c\\(\\gamma\\)\u548c\\(\\beta\\)\u662f\u53ef\u5b66\u4e60\u53c2\u6570\uff0c\\(\\epsilon\\)\u662f\u5fae\u5c0f\u5e38\u6570\u4ee5\u9632\u6b62\u9664\u96f6\u3002\n- **\u5e94\u7528**\uff1a\u5e7f\u6cdb\u7528\u4e8eTransformer\u6a21\u578b\u4e2d\uff0c\u6709\u52a9\u4e8e\u7f13\u89e3\u68af\u5ea6\u6d88\u5931\u95ee\u9898\uff0c\u63d0\u5347\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002\n\n#### RMSNorm\uff08Root Mean Square Normalization\uff09\n- **\u5b9a\u4e49**\uff1aRMSNorm\u662f\u5bf9\u6bcf\u4e2a\u6837\u672c\u7684\u6bcf\u4e2a\u7279\u5f81\u8fdb\u884c\u5f52\u4e00\u5316\uff0c\u4f46\u53ea\u5f52\u4e00\u5316\u65b9\u5dee\uff0c\u4e0d\u5f52\u4e00\u5316\u5747\u503c\u3002\n- **\u516c\u5f0f**\uff1a\n  \\[\n  \\text{RMSNorm}(x) = \\frac{x}{\\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} x_i^2 + \\epsilon}} \\cdot \\gamma + \\beta\n  \\]\n  \u5176\u4e2d\uff0c\\(n\\)\u662f\u7279\u5f81\u6570\u91cf\uff0c\\(\\gamma\\)\u548c\\(\\beta\\)\u662f\u53ef\u5b66\u4e60\u53c2\u6570\uff0c\\(\\epsilon\\)\u662f\u5fae\u5c0f\u5e38\u6570\u3002\n- **\u5e94\u7528**\uff1a\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0cRMSNorm\u6bd4LayerNorm\u66f4\u7b80\u5355\uff0c\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\uff0c\u5c24\u5176\u5728\u5927\u89c4\u6a21\u6a21\u578b\u8bad\u7ec3\u4e2d\u3002\n\n#### \u4e3b\u8981\u533a\u522b\n1. **\u5f52\u4e00\u5316\u65b9\u5f0f**\uff1a\n   - LayerNorm\u540c\u65f6\u5f52\u4e00\u5316\u5747\u503c\u548c\u65b9\u5dee\u3002\n   - RMSNorm\u53ea\u5f52\u4e00\u5316\u65b9\u5dee\u3002\n2. **\u8ba1\u7b97\u590d\u6742\u5ea6**\uff1a\n   - LayerNorm\u9700\u8981\u8ba1\u7b97\u5747\u503c\u548c\u65b9\u5dee\uff0c\u8ba1\u7b97\u91cf\u7a0d\u5927\u3002\n   - RMSNorm\u53ea\u9700\u8ba1\u7b97\u65b9\u5dee\uff0c\u8ba1\u7b97\u91cf\u8f83\u5c0f\u3002\n3. **\u6548\u679c**\uff1a\n   - LayerNorm\u5728\u8bb8\u591a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u5728NLP\u9886\u57df\u3002\n   - RMSNorm\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u8868\u73b0\u76f8\u4f3c\u751a\u81f3\u66f4\u4f18\uff0c\u4e14\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u3002\n\n\u901a\u8fc7\u4e0a\u8ff0\u5206\u6790\uff0c\u53ef\u4ee5\u66f4\u597d\u5730\u7406\u89e3LayerNorm\u548cRMSNorm\u5728\u5927\u6a21\u578b\u4e2d\u7684\u5e94\u7528\u53ca\u5176\u4f18\u7f3a\u70b9\uff0c\u4ece\u800c\u5728\u5177\u4f53\u4efb\u52a1\u4e2d\u9009\u62e9\u5408\u9002\u7684\u5f52\u4e00\u5316\u65b9\u6cd5\u3002", "task_step_name": "\u8bc4\u4f30\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218", "task_step_description": "\u5982\u53ef\u89e3\u91caAI\u3001\u8054\u90a6\u5b66\u4e60\u3001\u9690\u79c1\u4fdd\u62a4\u7b49\u65b0\u5174\u7814\u7a76\u65b9\u5411\u3002\u5982\u4f55\u5e73\u8861\u6a21\u578b\u6027\u80fd\u4e0e\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\uff0c\u5982\u4f55\u89e3\u51b3\u6570\u636e\u504f\u89c1\u548c\u4f26\u7406\u95ee\u9898\u7b49\u3002", "task_step_level": "5", "task_step_question": "### \u95ee\u9898\n\n\u5728\u8bc4\u4f30\u5927\u6a21\u578b\u4e2d\u7684LayerNorm\u548cRMSNorm\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218\u65f6\uff0c\u5982\u4f55\u5e73\u8861\u8fd9\u4e24\u79cd\u5f52\u4e00\u5316\u65b9\u6cd5\u5728\u6a21\u578b\u6027\u80fd\u4e0e\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u4e4b\u95f4\u7684\u5173\u7cfb\uff1f\u7279\u522b\u662f\u5728\u53ef\u89e3\u91caAI\u3001\u8054\u90a6\u5b66\u4e60\u548c\u9690\u79c1\u4fdd\u62a4\u7b49\u65b0\u5174\u7814\u7a76\u9886\u57df\u4e2d\uff0cLayerNorm\u548cRMSNorm\u7684\u9002\u7528\u6027\u548c\u6f5c\u5728\u5c40\u9650\u6027\u662f\u4ec0\u4e48\uff1f\u6b64\u5916\uff0c\u5982\u4f55\u89e3\u51b3\u8fd9\u4e24\u79cd\u5f52\u4e00\u5316\u65b9\u6cd5\u5728\u6570\u636e\u504f\u89c1\u548c\u4f26\u7406\u95ee\u9898\u4e0a\u7684\u5f71\u54cd\uff1f", "task_step_question_context": [{"ref_id": "454845744505973136", "chunk_id": "6", "score": 0.255859375, "text": "# 2 Problem Formulation\n\n# 2.1 Related Work\nLarge Language Models (LLMs). The advent of LLMs has led to a paradigm shift in the study of natural language processing (NLP), computer vision, information retrieval, and other domains[Menghani, 2023, Chen et al., 2023, Jiang et al., 2023]. The impressive effectiveness and generalizability of LLMs has come at the price of a drastic increase in LLM sizes [Treviso et al., 2023] and consequent challenges, including huge amounts of computational resources and data required to train, and prohibitive expenses at both training and deployment stages [Bender et al., 2021].  \n\nEfficient Machine Learning (ML) Inference. LLMs belong to a class of models called foundation models [Bommasani et al., 2021] \u2013 models that are trained once and can then be used to serve a wide variety of tasks. As such, we expect inference cost to dominate the overall cost of such models and hence focus on works that reduce the cost of ML inference [Menghani, 2023]. The most common approach for efficient ML inference is model compression i.e., replacing a large model with a smaller model of comparable accuracy. Common techniques for model compression include (i) model pruning [Hassibi et al., 1993, LeCun et al., 1989] which drops parts of the model with minimal accuracy loss, (ii) quantization [Jacob et al., 2018, Vanhoucke et al., 2011] which reduces model memory footprints and inference latency by reducing the precision of data representation (e.g., FP32 to INT8), (iii) knowledge distillation [Hinton et al., 2015, Urban et al., 2016] which trains small student models to mimic large teacher models, and (iv) Neural Architecture Search [Elsken et al., 2019, Zoph and Le, 2016] which tunes model architecture to improve model performance, under inference cost constraints. Such static efficiency optimizations typically produce a fixed model with lower inference cost and lower accuracy compared to the large model which may not suffice for foundation models like LLMs, whose core premise is that the same model will serve a range of tasks, each with its own accuracy/cost constraints. This is already manifesting in inference platforms described in Section 1 which need more dynamic optimizations to meet the demands of all users.  \n\nHybrid ML Inference. Recent works [Kag et al., 2022, Ding et al., 2022] have introduced a new inference paradigm called hybrid inference which uses two models of different sizes instead of a single model for inference. The smaller model (e.g. Llama2 [Touvron et al., 2023]) generally has lower inference cost but also lower accuracy than the larger model (e.g. GPT-4 [OpenAI, 2023]). The key idea is to identify and route easy queries to the small model so that inference cost can be reduced while maintaining response quality. By tuning a threshold on query difficulty we can dynamically trade off quality and cost for the same inference setup. [Kag et al., 2022] study this setup for image classification and propose to train the small model, large model, and router from scratch. However LLM training is expensive and retraining LLMs from scratch for every scenario goes against the very premise of inference with pre-trained foundation models. Moreover text generation [Iqbal and Qureshi, 2022] is often more ambiguous and challenging than image classification due to which novel techniques are required for effective hybrid LLM inference for text generation.  \n\nInference with Multiple LLMs. Some recent works [Jiang et al., 2023, Chen et al., 2023, Leviathan et al., 2023, Kim et al., 2023] use multiple LLMs for inference but these approaches typically call more than one LLM for a single query that can incur significant computational overheads. Specifically [Jiang et al., 2023] calls an ensemble of LLMs at inference time due to which the inference cost will be proportional to the number of models in the system. [Chen et al., 2023] performs inference using a cascade of LLMs where responses to the query are generated sequentially by the LLMs in the cascade until one of the models has a confidence score higher than a predefined threshold. Our work provides high quality responses while always making a single LLM call for all queries and will thus incur much lower computational cost than both of these works on average. Speculative decoding, introduced in [Leviathan et al., 2023, Kim et al., 2023] speeds up decoding of expensive models by invoking small-and-efficient decoders on the \u201ceasy\u201d decoding steps. Instead, in our work we are interested in query routing which assigns \u201ceasy\u201d queries to small models to reduce overall inference costs while maintaining high performance. While the two approaches have different goals, an interesting line of future work would be to combine these so that our router assigns queries to the small or large model based on query difficulty and then speculative decoding is applied on top to speed up inference for queries assigned to the large model thereby leading to further cost reduction."}, {"ref_id": "455038427552559154", "chunk_id": "6", "score": 0.1943359375, "text": "# 6 Conclusion\nLLM-based RL algorithms have shown generalization across multiple tasks and games. We argue that this ability comes from implicit memory that fits a large number of parameters to the training data, which is inefficient in terms of model size. In contrast, we propose a new approach inspired by the concept of \u201cworking memory\u201d called Decision Transformers with Memory (DT-Mem), which stores training experience explicitly in a content-addressable matrix module for later retrieval and use. Evaluation demonstrates that DT-Mem achieves better generalization on Atari games with only $10\\%$ of the model parameters compared to the state-of-the-art method. Furthermore, we demonstrate that fine-tuning DT-Memwith a small amount of data can produce state-of-the-art results on both Atari games and the Meta-World environment, when compared to MDT [22], PDT [37], and HDT [38].  \n\nLimitations The first limitation of our work is the sample efficiency of memory fine-tuning. The $10\\%$ fine-tuning dataset is still sizeable, and we plan to explore more sample-efficient methods in the future. We could for instance consider a setting with more tasks, each one with less data so that the inter-task generalization would be even more crucial to its performance. Additionally, this work does not propose a control strategy for collecting data on a new task. For future work, we plan to investigate online data collection methods, which includes the design and learning of exploration strategies for an efficient fine-tuning on new tasks. Finally, the approach has been intuitively motivated, but it would be valuable to have a theoretical grounding that would show the structural limits of large models and how equipping them with a memory component overcomes them.  \n\nSocietal Impact We do not foresee any significant societal impact resulting from our proposed method. The current algorithm is not designed to interact with humans, nor any realistic environment yet. If one chooses to extend our methods to such situations, caution should be exercised to ensure that any safety and ethical concerns are appropriately addressed. As our work is categorized in the offline-RL domain, it is feasible to supplement its training with a dataset that aligns with human intents and values. However, one must be wary that the way our architecture generalizes across tasks is still not well understood and as a consequence we cannot guarantee the generalization of its desirable features: performance, robustness, fairness, etc. By working towards methods that improve the computational efficiency of large models, we contribute to increase their access and reduce their ecological impact.\n\n\n\n# A Implementation Details\n\n# A.1 DT-Mem network architecture\nTable 3 summarizes the different model configurations used for evaluation. In this section, we describe these model configurations in detail. While Table 3 provides a summary, we will also provide additional information here. DT-Mem, PDT and HDT are all share the same transformer architectures. However, for task-adaptation, HDT utilizes a pre-trained $2.3\\mathbf{M}$ hyper-network, while DT-Mem introduces 147K LoRA parameters. To compare with MDT, we use the same parameter size as reported in [22].  \n\nTable 3: Detailed Model Sizes   \n\n\n<html><body><table><tr><td>Model</td><td>Layers</td><td>Hidden size (d)</td><td>Heads</td><td>Params</td><td>Memory Size</td><td>Memory Module Params</td></tr><tr><td>HDT</td><td>4</td><td>512</td><td>8</td><td>13M</td><td>N.A.</td><td>N.A.</td></tr><tr><td>MDT-200M</td><td>10</td><td>1280</td><td>20</td><td>200M</td><td>N.A.</td><td>N.A.</td></tr><tr><td>DT-Mem</td><td>4</td><td>512</td><td>8</td><td>13M</td><td>559K</td><td>7M</td></tr></table></body></html>\n\n# A.2 Hyper-parameters\nIn this section, we will delve into the specifics of the model parameters. Understanding these parameters is key to understanding the workings of the model. It is worth noting that the source code for this model is publicly available at https://github.com/luciferkonn/DT_Mem/tree/main .This allows for a deeper understanding of the model\u2019s inner workings and may facilitate the replication of its results.  \n\nTable 4: Hyperparameters for DT-Mem training   \n\n\n<html><body><table><tr><td>Hyperparameters</td><td>Value</td></tr><tr><td>K (length of context)</td><td>28</td></tr><tr><td>dropoutrate</td><td>0.1</td></tr><tr><td>maximum epochs</td><td>1000</td></tr><tr><td>steps for each epoch</td><td>1000</td></tr><tr><td>optimizer learning rate</td><td>1e-4</td></tr><tr><td>weight decay</td><td>1e-4</td></tr><tr><td>gradient norm clip</td><td>1.</td></tr><tr><td>data points for each dataset</td><td>500,000</td></tr><tr><td>batch size</td><td>64</td></tr><tr><td>memory slots</td><td>1290</td></tr><tr><td>activation</td><td>GELU</td></tr><tr><td>optimizer</td><td>Adamw</td></tr><tr><td>scheduler</td><td>LambdaLR</td></tr></table></body></html>\n\n# A.3 Training and fine-tuning algorithm\nIn this section, we present the pre-training DT-Memin Appendix A.3 and fine-tuning DT-Mem with LoRA in Appendix 5.5.  \n\nWe pre-train DT-Mem on multiple offline datasets. Each gradient update of the DT-Memmodel considers information from each training task.  \n\nWe fine-tune the memory module to adapt to each downstream task. To achieve this, we fix the pre-trained DT-Mem model parameters and add additional LoRA parameters for the memory module feed-forward neural networks. The fine-tune dataset is used to update these LoRA parameters only.\n\n# Algorithm 1 Pre-train DT-Mem\n1: for T episodes do   \n2: 3: 4: for Sample trajectories Split trajectories into different segments with length K and calculate return-to-go in the Task $\\mathcal{T}_{i}\\in T^{t r a i n}\\;.$ do $\\tau=(s_{0},a_{0},r_{0},\\cdot\\cdot\\cdot\\,,s_{H},a_{H},r_{H})$ m the dataset $\\mathcal{D}_{i}$ .  \ninput sequence.   \n5: Given $\\hat{\\tau}_{t+1:t+K}$ , compute the sequence embedding $e_{s e q}$ .  \n6: Update the working memory and retrieve the relative information as $E_{o u t}$   \n7: Given $E_{o u t}$ , predict actions $\\tilde{a}_{t}$ , reward $\\tilde{r}_{t}$ , and return-to-go ${\\tilde{R}}_{t}$ .  \n8: Compute the loss according to Eqn. 1.   \n9: Update all modules parameters.   \n10: end for   \n11: end for  \n\nAlgorithm 2 Fine-tuning DT-Mem  \n\n$\\hat{B}^{q},\\hat{B}^{k},\\hat{B}^{v},\\hat{A}^{q},\\hat{A}^{k},\\hat{A}^{v},B^{q},A^{q},B^{k},A^{k}$ Require: Fine-tuning dataset $\\mathcal{T}^{i}~\\in~T^{t e s t}$ .dataset $\\mathcal{D}^{i}$ for $\\mathcal{T}^{i}$ .Initialize LoRA parameters 1: for T steps do   \n2: Split trajectories into different segments with length $\\mathbf{K}$ and calculate return-to-go in the input sequence.   \n3: Given $\\hat{\\tau}_{t+1:t+K}$ , compute the sequence embedding $e_{s e q}$ .  \n4: Update working memory using $\\hat{Q}\\,=\\,M(\\hat{W}^{q}+\\hat{B}^{q}\\bar{A}^{q})$ ,$\\hat{K}\\,=\\,M(\\hat{W}^{k}\\,+\\,\\hat{B}^{k}\\hat{A}^{k}),\\hat{V}\\,=$ $M(\\hat{W}^{v}+\\hat{B}^{v}\\hat{A}^{v})$ ,$Q=M(W^{q}+B^{q}A^{q}),K=M(W^{k}+B^{k}A^{k})$   \n5: Retrieve the relative information as $E_{o u t}$   \n6: Given $E_{o u t}$ , predict actions $\\tilde{a}_{t}$ , reward $\\tilde{r}_{t}$ , and return-to-go ${\\tilde{R}}_{t}$ .  \n7: Compute the loss according to Eqn. 1.   \n8: Update LoRA parameters only.   \n9: end for"}, {"ref_id": "454847042436311108", "chunk_id": "4", "score": 0.1533203125, "text": "# EasyQuant: An Efficient Data-free Quantization Algorithm for LLMs\nHanlin Tang   \n\nYifu Sun   \n\nDecheng Wu   \n\nKai Liu   \n\nJianchen Zhu   \n\nZhanhui Kang\n\n# Abstract\nLarge language models (LLMs) have proven to be very superior to conventional methods in various tasks. However, their expensive computations and high memory requirements are prohibitive for deployment. Model quantization is an effective method for reducing this overhead. The problem is that in most previous works, the quantized model was calibrated using a few samples from the training data, which might affect the generalization of the quantized LLMs to unknown cases and tasks. Hence in this work, we explore an important question: Can we design a data-free quantization method for LLMs to guarantee its generalization performance?  \n\nIn this work, we propose EasyQuant, a trainingfree and data-free weight-only quantization algorithm for LLMs. Our observation indicates that two factors: outliers in the weight and quantization ranges, are essential for reducing the quantization error. Therefore, in EasyQuant, we leave the outliers (less than $1\\%$ ) unchanged and optimize the quantization range to reduce the reconstruction error. With these methods, we surprisingly find that EasyQuant achieves comparable performance to the original model. Since EasyQuant does not depend on any training data, the generalization performance of quantized LLMs are safely guaranteed. Moreover, EasyQuant can be implemented in parallel so that the quantized model could be attained in a few minutes even for LLMs over 100B. To our best knowledge, we are the first work that achieves comparable performance with datadependent algorithms under a data-free setting and our algorithm runs over 10 times faster than the data-dependent methods.\n\n# 1 Introduction\nRecent work has already proved the superior performance of Transformer ( Vaswani et al. ,2017 )based LLMs ( Workshop ,2023 ;Zhang et al. ,2022 ;Touvron et al. ,2023 ;Brown et al. ,2020 ;Rae et al. ,2021 ;Smith et al. ,2022 ;Chowdhery et al. ,2022 ;Zeng et al. ,2022 ) on various tasks over traditional methods, and has attracted massive interest in how to improve and utilize those LLMs. However, the model size also grows dramatically along with improved performance. Hence the memory footprint and computational cost become the bottleneck for deploying those models. One promising solution to alleviate this overhead is model quantization ( Frantar et al. ,2023a ;Xiao et al. ,2023 ), where we quantize weight only or weight and activation both i order to reduce memory consumption and computational cost.  \n\nAlthough model quantization is a well-studied area for normal-sized models, such as BERT ( Devlin et al. ,2018 ) and GPT-2 ( Radford et al. ,2019 ), it is still a quite challenging task for LLMs. One major reason is that previous lossless model quantization algorithms require retraining for the quantized model, which is too expensive for models over billions of parameters. Beyond this, previous models are usually designed for specific domain tasks, which means the training data are sampled from limited task domains. However, recent LLMs are usually trained on various domains of data corpus, and they have shown to be quite effective for multi-domain zero-shot tasks. In this case, if we only retrain the quantized LLMs using partial domain corpus, the generalization ability of LLMs might get worse. Therefore both efficiency and generalization guarantees are very important for designing LLMs quantization algorithms. To date, for low-bits weight-only quantization, several posttraining algorithms have been proposed ( Frantar et al. ,2023a ;Yao et al. ,2022 ). However, those methods also require a small calibration set sampled from training data, which still takes at least several hours. Moreover, the use of those calibration data also brings the risk of making the model overfit to the calibration set.  \n\n  \nFigure 1: Pipeline of EasyQuant. We first find all the outliers in weight and keep them in full precision (fp32/fp16/bf16). Afterward, we optimize the quantization range (denoted as $q_{r a n g e})$ in order to approximate the normal values more precisely. In the end, the normal values are quantized into lower bits (denoted as $Q[\\cdot])$ with optimized quantization ranges and we set the outliers unchanged in weight.  \n\nOur Contribution: In this work, we propose a novel data-free model quantization algorithm, namely EasyQuant, that potentially improves the performance of low-bits quantized LLMs. The generalization ability of LLMs is inherently guaranteed since EasyQuant does not need any input data. By running EasyQuant for only a few minutes, we can quantize public-available OPT-176B, BLOOM176B, and LLAMA-65B into lower bits without significant loss on various benchmarks. To our best knowledge, this is the first data-free LLM quantization algorithm for LLM quantization without notable system overhead.  \n\nMoreover, our work reveals the essential factors that cause the performance degradation of the quantized LLMs. We show that the outliers in weights are more critical to the model\u2019s performance compared to the normal elements. Beyond this, we propose to use a gradient-based method for optimizing the quantization range. These two strategies can also be used in other scenarios, such as weight-activation quantization and quantizationaware training (QAT).  \n\nLast but not least, we develop efficient CUDA kernels for outlier isolation in dequantization, and proved that hold $1\\%$ outliers in weights unquantized brings negligible (less than $0.1\\%$ ) overhead w.r.t to overall latency. We also propose to implement EasyQuant in parallel for quantizing each weight in the model, which means a 175B-sized model can be quantized into 4 -bits within $10\\;\\mathrm{min}$ -utes."}], "task_step_question_answer": "f2e5ad03-a297-4065-b995-c2dd46e1e9b4:\u300c\u5728\u8bc4\u4f30LayerNorm\u548cRMSNorm\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218\u65f6\uff0c\u5e73\u8861\u6a21\u578b\u6027\u80fd\u4e0e\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u662f\u4e00\u4e2a\u5173\u952e\u95ee\u9898\u3002LayerNorm\u901a\u8fc7\u6807\u51c6\u5316\u6bcf\u4e2a\u6837\u672c\u7684\u7279\u5f81\u7ef4\u5ea6\u6765\u7a33\u5b9a\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u4f46\u5176\u8ba1\u7b97\u590d\u6742\u5ea6\u8f83\u9ad8\uff0c\u5c24\u5176\u662f\u5728\u5927\u89c4\u6a21\u6a21\u578b\u4e2d\u3002RMSNorm\u5219\u901a\u8fc7\u7b80\u5316\u8ba1\u7b97\u6d41\u7a0b\uff0c\u51cf\u5c11\u4e86\u8d44\u6e90\u6d88\u8017\uff0c\u4f46\u5728\u67d0\u4e9b\u4efb\u52a1\u4e2d\u53ef\u80fd\u727a\u7272\u4e86\u4e00\u5b9a\u7684\u6027\u80fd\u3002\u672a\u6765\u7814\u7a76\u53ef\u4ee5\u63a2\u7d22\u5982\u4f55\u4f18\u5316\u8fd9\u4e24\u79cd\u65b9\u6cd5\u7684\u8ba1\u7b97\u6548\u7387\uff0c\u4f8b\u5982\u901a\u8fc7\u6df7\u5408\u4f7f\u7528\u6216\u52a8\u6001\u8c03\u6574\u5f52\u4e00\u5316\u7b56\u7565\uff0c\u4ee5\u5728\u6027\u80fd\u548c\u8d44\u6e90\u6d88\u8017\u4e4b\u95f4\u627e\u5230\u6700\u4f73\u5e73\u8861\u3002\n\n\u5728\u53ef\u89e3\u91caAI\u9886\u57df\uff0cLayerNorm\u548cRMSNorm\u7684\u9002\u7528\u6027\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u3002LayerNorm\u7531\u4e8e\u5176\u6807\u51c6\u5316\u7279\u6027\uff0c\u53ef\u80fd\u66f4\u5bb9\u6613\u89e3\u91ca\u6a21\u578b\u7684\u5185\u90e8\u884c\u4e3a\uff0c\u800cRMSNorm\u7684\u7b80\u5316\u8ba1\u7b97\u53ef\u80fd\u4f7f\u5f97\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u964d\u4f4e\u3002\u672a\u6765\u7684\u7814\u7a76\u53ef\u4ee5\u63a2\u7d22\u5982\u4f55\u7ed3\u5408\u8fd9\u4e24\u79cd\u65b9\u6cd5\u7684\u4f18\u70b9\uff0c\u8bbe\u8ba1\u51fa\u65e2\u9ad8\u6548\u53c8\u53ef\u89e3\u91ca\u7684\u5f52\u4e00\u5316\u65b9\u6cd5\u3002\n\n\u5728\u8054\u90a6\u5b66\u4e60\u548c\u9690\u79c1\u4fdd\u62a4\u9886\u57df\uff0cLayerNorm\u548cRMSNorm\u7684\u6f5c\u5728\u5c40\u9650\u6027\u4e3b\u8981\u4f53\u73b0\u5728\u6570\u636e\u5206\u5e03\u4e0d\u4e00\u81f4\u548c\u9690\u79c1\u6cc4\u9732\u98ce\u9669\u4e0a\u3002\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u6570\u636e\u5206\u5e03\u5dee\u5f02\u53ef\u80fd\u5bfc\u81f4\u5f52\u4e00\u5316\u6548\u679c\u4e0d\u4f73\uff0c\u800c\u9690\u79c1\u4fdd\u62a4\u8981\u6c42\u53ef\u80fd\u9650\u5236\u4e86\u5f52\u4e00\u5316\u65b9\u6cd5\u7684\u4f7f\u7528\u3002\u672a\u6765\u7684\u7814\u7a76\u53ef\u4ee5\u63a2\u7d22\u5982\u4f55\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\u8bbe\u8ba1\u5206\u5e03\u9c81\u68d2\u7684\u5f52\u4e00\u5316\u65b9\u6cd5\uff0c\u5e76\u7ed3\u5408\u5dee\u5206\u9690\u79c1\u7b49\u6280\u672f\u6765\u4fdd\u62a4\u6570\u636e\u9690\u79c1\u3002\n\n\u5728\u6570\u636e\u504f\u89c1\u548c\u4f26\u7406\u95ee\u9898\u4e0a\uff0cLayerNorm\u548cRMSNorm\u7684\u5f71\u54cd\u9700\u8981\u6df1\u5165\u5206\u6790\u3002\u5f52\u4e00\u5316\u65b9\u6cd5\u53ef\u80fd\u653e\u5927\u6216\u7f29\u5c0f\u6570\u636e\u4e2d\u7684\u504f\u89c1\uff0c\u5f71\u54cd\u6a21\u578b\u7684\u516c\u5e73\u6027\u3002\u672a\u6765\u7684\u7814\u7a76\u53ef\u4ee5\u63a2\u7d22\u5982\u4f55\u5728\u5f52\u4e00\u5316\u8fc7\u7a0b\u4e2d\u5f15\u5165\u516c\u5e73\u6027\u7ea6\u675f\uff0c\u8bbe\u8ba1\u51fa\u80fd\u591f\u51cf\u5c11\u6570\u636e\u504f\u89c1\u548c\u4f26\u7406\u95ee\u9898\u7684\u5f52\u4e00\u5316\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u8fd8\u53ef\u4ee5\u7814\u7a76\u5982\u4f55\u901a\u8fc7\u6570\u636e\u9884\u5904\u7406\u548c\u540e\u5904\u7406\u6280\u672f\u6765\u8fdb\u4e00\u6b65\u51cf\u5c11\u5f52\u4e00\u5316\u65b9\u6cd5\u5bf9\u6a21\u578b\u516c\u5e73\u6027\u7684\u5f71\u54cd\u3002\u300d", "ref_task_step_id": ""}, "__type__": "task_step_node"}, "cc774f90-c346-49a4-aab8-dc67fb5f019a": {"__data__": {"id_": "cc774f90-c346-49a4-aab8-dc67fb5f019a", "metadata": {}, "relationships": {}, "hash": "", "start_task_context": "\u5927\u6a21\u578b\u4e2d\u7684LayerNorm\u548cRMSNorm\u6709\u4ec0\u4e48\u533a\u522b\uff1f", "aemo_representation_context": "### Step-by-Step Decomposition of Computer Science Research Content\n\n#### 1. \u5206\u6790\u8fd1\u51e0\u5e74\u7814\u7a76\u9886\u57df\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba\n- **\u6280\u672f\u6846\u67b6**\uff1a\u8fd1\u5e74\u6765\uff0c\u6df1\u5ea6\u5b66\u4e60\u3001\u5f3a\u5316\u5b66\u4e60\u3001\u56fe\u795e\u7ecf\u7f51\u7edc\u7b49\u6846\u67b6\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u9886\u57df\u5360\u636e\u4e3b\u5bfc\u5730\u4f4d\u3002\u5177\u4f53\u6846\u67b6\u5982Transformer\u3001BERT\u3001GNN\u7b49\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002\n- **\u65b9\u6cd5\u8bba**\uff1a\u7814\u7a76\u65b9\u6cd5\u9010\u6e10\u4ece\u5355\u4e00\u6a21\u578b\u8f6c\u5411\u591a\u6a21\u578b\u878d\u5408\uff0c\u4ece\u76d1\u7763\u5b66\u4e60\u6269\u5c55\u5230\u65e0\u76d1\u7763\u548c\u81ea\u76d1\u7763\u5b66\u4e60\uff0c\u5f3a\u8c03\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002\n\n#### 2. \u7814\u7a76\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u4e3b\u8981\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0e\u53d8\u4f53\n- **\u5e94\u7528**\uff1a\u4f8b\u5982\uff0cTransformer\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u4e2d\u88ab\u5e7f\u6cdb\u5e94\u7528\uff0cBERT\u53d8\u4f53\u5982RoBERTa\u3001ALBERT\u5728\u6587\u672c\u5206\u7c7b\u3001\u95ee\u7b54\u7cfb\u7edf\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff1bGNN\u5728\u63a8\u8350\u7cfb\u7edf\u3001\u77e5\u8bc6\u56fe\u8c31\u7b49\u9886\u57df\u6709\u663e\u8457\u5e94\u7528\u3002\n- **\u53d8\u4f53**\uff1a\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\uff0c\u7814\u7a76\u8005\u63d0\u51fa\u4e86\u591a\u79cd\u53d8\u4f53\uff0c\u5982\u9488\u5bf9\u957f\u5e8f\u5217\u5904\u7406\u7684Transformer-XL\uff0c\u9488\u5bf9\u591a\u6a21\u6001\u6570\u636e\u7684ViLBERT\u7b49\u3002\n\n#### 3. \u8bc4\u4f30\u5b66\u672f\u754c\u7684\u6280\u672f\u8fdb\u6b65\u4e0e\u5c40\u9650\u6027\n- **\u6280\u672f\u8fdb\u6b65**\uff1a\u6a21\u578b\u6027\u80fd\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\uff0c\u8ba1\u7b97\u6548\u7387\u4e5f\u6709\u6240\u63d0\u9ad8\uff0c\u5982EfficientNet\u5728\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u8868\u73b0\u3002\n- **\u5c40\u9650\u6027**\uff1a\u4f9d\u7136\u5b58\u5728\u6a21\u578b\u504f\u5dee\uff08\u5982\u6027\u522b\u504f\u89c1\uff09\u3001\u6570\u636e\u4f9d\u8d56\uff08\u5982\u5bf9\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\uff09\u7b49\u95ee\u9898\uff0c\u6cdb\u5316\u80fd\u529b\u6709\u5f85\u8fdb\u4e00\u6b65\u63d0\u5347\u3002\n\n#### 4. \u63a2\u8ba8\u8ba1\u7b97\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\n- **\u9002\u7528\u6027**\uff1a\u7814\u7a76\u6a21\u578b\u5728\u4e0d\u540c\u9886\u57df\uff08\u5982\u533b\u7597\u3001\u91d1\u878d\uff09\u548c\u591a\u6a21\u6001\u6570\u636e\uff08\u5982\u6587\u672c+\u56fe\u50cf\uff09\u4e0a\u7684\u8868\u73b0\uff0c\u5982\u591a\u6a21\u6001Transformer\u6a21\u578b\u3002\n- **\u6cdb\u5316\u80fd\u529b**\uff1a\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\u3001\u5143\u5b66\u4e60\u7b49\u65b9\u6cd5\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f7f\u5176\u80fd\u591f\u5e94\u5bf9\u66f4\u591a\u73b0\u5b9e\u5e94\u7528\u573a\u666f\u3002\n\n#### 5. \u5206\u6790\u6700\u65b0\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027\n- **\u7a33\u5b9a\u6027**\uff1a\u7814\u7a76\u7b97\u6cd5\u5728\u590d\u6742\u3001\u52a8\u6001\u73af\u5883\u4e0b\u7684\u8868\u73b0\uff0c\u5982\u5bf9\u6297\u8bad\u7ec3\u3001\u9c81\u68d2\u4f18\u5316\u7b49\u65b9\u6cd5\u7684\u5e94\u7528\u3002\n- **\u5bb9\u9519\u6027**\uff1a\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\u7684\u9002\u5e94\u6027\uff0c\u5982\u5206\u5e03\u5f0f\u8bad\u7ec3\u4e2d\u7684\u68af\u5ea6\u538b\u7f29\u3001\u6a21\u578b\u5e76\u884c\u7b49\u6280\u672f\u3002\n\n#### 6. \u8bc4\u4f30\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218\n- **\u672a\u6765\u65b9\u5411**\uff1a\u5982\u53ef\u89e3\u91caAI\u3001\u8054\u90a6\u5b66\u4e60\u3001\u9690\u79c1\u4fdd\u62a4\u7b49\u65b0\u5174\u7814\u7a76\u65b9\u5411\u3002\n- **\u6311\u6218**\uff1a\u5982\u4f55\u5e73\u8861\u6a21\u578b\u6027\u80fd\u4e0e\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\uff0c\u5982\u4f55\u89e3\u51b3\u6570\u636e\u504f\u89c1\u548c\u4f26\u7406\u95ee\u9898\u7b49\u3002\n\n### \u5927\u6a21\u578b\u4e2d\u7684LayerNorm\u548cRMSNorm\u7684\u533a\u522b\n\n#### LayerNorm\uff08Layer Normalization\uff09\n- **\u5b9a\u4e49**\uff1aLayerNorm\u662f\u5bf9\u6bcf\u4e2a\u6837\u672c\u7684\u6bcf\u4e2a\u7279\u5f81\u8fdb\u884c\u5f52\u4e00\u5316\uff0c\u4f7f\u5176\u5747\u503c\u548c\u65b9\u5dee\u5206\u522b\u63a5\u8fd10\u548c1\u3002\n- **\u516c\u5f0f**\uff1a\n  \\[\n  \\text{LayerNorm}(x) = \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} \\cdot \\gamma + \\beta\n  \\]\n  \u5176\u4e2d\uff0c\\(\\mu\\)\u548c\\(\\sigma^2\\)\u5206\u522b\u662f\u7279\u5f81\u5747\u503c\u548c\u65b9\u5dee\uff0c\\(\\gamma\\)\u548c\\(\\beta\\)\u662f\u53ef\u5b66\u4e60\u53c2\u6570\uff0c\\(\\epsilon\\)\u662f\u5fae\u5c0f\u5e38\u6570\u4ee5\u9632\u6b62\u9664\u96f6\u3002\n- **\u5e94\u7528**\uff1a\u5e7f\u6cdb\u7528\u4e8eTransformer\u6a21\u578b\u4e2d\uff0c\u6709\u52a9\u4e8e\u7f13\u89e3\u68af\u5ea6\u6d88\u5931\u95ee\u9898\uff0c\u63d0\u5347\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002\n\n#### RMSNorm\uff08Root Mean Square Normalization\uff09\n- **\u5b9a\u4e49**\uff1aRMSNorm\u662f\u5bf9\u6bcf\u4e2a\u6837\u672c\u7684\u6bcf\u4e2a\u7279\u5f81\u8fdb\u884c\u5f52\u4e00\u5316\uff0c\u4f46\u53ea\u5f52\u4e00\u5316\u65b9\u5dee\uff0c\u4e0d\u5f52\u4e00\u5316\u5747\u503c\u3002\n- **\u516c\u5f0f**\uff1a\n  \\[\n  \\text{RMSNorm}(x) = \\frac{x}{\\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} x_i^2 + \\epsilon}} \\cdot \\gamma + \\beta\n  \\]\n  \u5176\u4e2d\uff0c\\(n\\)\u662f\u7279\u5f81\u6570\u91cf\uff0c\\(\\gamma\\)\u548c\\(\\beta\\)\u662f\u53ef\u5b66\u4e60\u53c2\u6570\uff0c\\(\\epsilon\\)\u662f\u5fae\u5c0f\u5e38\u6570\u3002\n- **\u5e94\u7528**\uff1a\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0cRMSNorm\u6bd4LayerNorm\u66f4\u7b80\u5355\uff0c\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\uff0c\u5c24\u5176\u5728\u5927\u89c4\u6a21\u6a21\u578b\u8bad\u7ec3\u4e2d\u3002\n\n#### \u4e3b\u8981\u533a\u522b\n1. **\u5f52\u4e00\u5316\u65b9\u5f0f**\uff1a\n   - LayerNorm\u540c\u65f6\u5f52\u4e00\u5316\u5747\u503c\u548c\u65b9\u5dee\u3002\n   - RMSNorm\u53ea\u5f52\u4e00\u5316\u65b9\u5dee\u3002\n2. **\u8ba1\u7b97\u590d\u6742\u5ea6**\uff1a\n   - LayerNorm\u9700\u8981\u8ba1\u7b97\u5747\u503c\u548c\u65b9\u5dee\uff0c\u8ba1\u7b97\u91cf\u7a0d\u5927\u3002\n   - RMSNorm\u53ea\u9700\u8ba1\u7b97\u65b9\u5dee\uff0c\u8ba1\u7b97\u91cf\u8f83\u5c0f\u3002\n3. **\u6548\u679c**\uff1a\n   - LayerNorm\u5728\u8bb8\u591a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u5728NLP\u9886\u57df\u3002\n   - RMSNorm\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u8868\u73b0\u76f8\u4f3c\u751a\u81f3\u66f4\u4f18\uff0c\u4e14\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u3002\n\n\u901a\u8fc7\u4e0a\u8ff0\u5206\u6790\uff0c\u53ef\u4ee5\u66f4\u597d\u5730\u7406\u89e3LayerNorm\u548cRMSNorm\u5728\u5927\u6a21\u578b\u4e2d\u7684\u5e94\u7528\u53ca\u5176\u4f18\u7f3a\u70b9\uff0c\u4ece\u800c\u5728\u5177\u4f53\u4efb\u52a1\u4e2d\u9009\u62e9\u5408\u9002\u7684\u5f52\u4e00\u5316\u65b9\u6cd5\u3002", "task_step_name": "\u672a\u6765\u65b9\u5411", "task_step_description": "\u5982\u53ef\u89e3\u91caAI\u3001\u8054\u90a6\u5b66\u4e60\u3001\u9690\u79c1\u4fdd\u62a4\u7b49\u65b0\u5174\u7814\u7a76\u65b9\u5411\u3002", "task_step_level": "5>1", "task_step_question": "### \u95ee\u9898\n\n\u5728\u63a2\u8ba8\u5927\u6a21\u578b\u4e2d\u7684LayerNorm\u548cRMSNorm\u7684\u533a\u522b\u65f6\uff0c\u7ed3\u5408\u672a\u6765\u7814\u7a76\u65b9\u5411\uff08\u5982\u53ef\u89e3\u91caAI\u3001\u8054\u90a6\u5b66\u4e60\u3001\u9690\u79c1\u4fdd\u62a4\u7b49\uff09\uff0c\u5982\u4f55\u8bc4\u4f30\u8fd9\u4e24\u79cd\u5f52\u4e00\u5316\u65b9\u6cd5\u5728\u8fd9\u4e9b\u65b0\u5174\u7814\u7a76\u9886\u57df\u4e2d\u7684\u9002\u7528\u6027\u548c\u6f5c\u5728\u5f71\u54cd\uff1f\u5177\u4f53\u6765\u8bf4\uff0cLayerNorm\u548cRMSNorm\u5728\u53ef\u89e3\u91ca\u6027\u3001\u5206\u5e03\u5f0f\u8bad\u7ec3\u4e2d\u7684\u6548\u7387\u4ee5\u53ca\u9690\u79c1\u4fdd\u62a4\u65b9\u9762\u6709\u54ea\u4e9b\u4f18\u52bf\u548c\u5c40\u9650\u6027\uff1f", "task_step_question_context": [{"ref_id": "454895489145650410", "chunk_id": "1", "score": 0.38671875, "text": "# 5.3 LAYER NORM TUNING HAS SMALLER GRADIENT VARIANCE\nA well accepted view about LayerNorm is that, as the neural network goes deeper, the mean of LayerNorm gradients should goes to zero as the LayerNorm itself is designed to normalize all training parameters. In the meantime, the variance of LayerNorm gradients should be small to ensure a better generalization ability of the model ( Xu et al. ,2019 ) (See the proof in Appendix A.2.2 ). As we presented in fig. 4 , MLLM with LayerNorm tuning method has a more concentrated LayerNorm gradients than fine-tuning during the training process. This result gives another view on the effectiveness of LayerNorm from the optimization perspective. More visualizations are listed in Appendix A.2.2 .\n\n# 6 CONCLUSION AND DISCUSSIONS\nLayerNorm is effective and sufficient built upon MLLM pre-training. MLLM training typically involves pre-training on image-text pairs followed by finetuning on visual instruction data. While the second stage of training receives more attention, it is worth noting that the function of the first stage pre-training is non-negligible for training a competent MLLM. We have presented in the paper only a small portion of parameter activation is sufficient to tune a well-behaved MLLM. However, other models such as I NSTRUCT BLIP ( Dai et al. ,2023 ) and M INI GPT4 ( Zhu et al. ,2023 ) only tune the vision-language connector, leaving the LLM untouched during the second stage of training. These models have yielded strong performances when given a large-scale finetuning dataset. In Sec. 5.1 , we demonstrate that tuning LayerNorm may be a more effective means for the second stage training, especially when compared to existing parameter-efficient methods for training MLLMs.  \n\nLimitations. One shortcoming of these parameter-efficient finetuning methods is that they are more sensitive to hyper-parameters ( e.g ., learning rate, training epoch) than finetuning. Since the number of trainable parameters of LayerNorm is small, the model performance of LayerNorm method also varies when twitching the training hyper-parameters. This drawback calls for potential future investigations on the LayerNorm tuning method. In the Appendix A.1 , we give a hint for the grid search range of learning rate on both 7B and 13B scaled models using LayerNorm tuning based on our experimental results.  \n\nConclusion. Our studies demonstrate LayerNorm tuning as a simple yet effective tuning method for adapting LLMs comprehend multi-modal content across various model variants. Compared to LoRA tuning or full parameter finetuning, LayerNorm tuning reduces the trainable parameters by a significant $41.9\\%$ , enabling efficient finetuning of MLLMs on consumer-grade GPUs. Moreover, we demonstrate that MLLMs can achieve exceptional performance with minimal \u201cright\u201d data and parameters, showcasing the potential of LayerNorm tuning method in real-world applications. Given the empirical success of LayerNorm tuning, we revisited the MLLM finetuning from a domain adaptation perspective and showed that LayerNorm plays a critical role in adapting LLMs to the multi-modal domain. Additionally, our research illustrates the expressive power and optimization potential of LayerNorm tuning from layer similarities and the gradient variance. We hope that our work could inspire future works on designing improved PEFT methods that enable more diverse application scenarios for MLLMs.\n\n\n\n# A A PPENDIX\n\n# A.1 TRAINING DETAILS\nFor the first stage, we set the learning rate to 2e-3 for all variants. During the second stage, we search learning the learning rate from [2e-3, 1e-3, 6e-4, 3e-4, 1e-4, 5e-5, 2e-5, 1e-5, 6e-6, 1e-6, 1e-7] for all models and pick the best learning rate based on their performances on the CIDEr score on the Flickr $30\\,\\mathrm{k}$ task.  \n\nAccording to our tryouts based on Flickr $30\\,\\mathrm{k}$ results in Table A1 , the recommended learning rate for 7B scale is between 6e-4 to 2e-3, while on the 13B, the learning rate should be searched in the range of 3e-6 to 6e-5.  \n\nTable A1: Performance of MLLMs (LayerNorm-simp.) trained with different learning rates and scales on the Flickr30k task.   \n\n\n<html><body><table><tr><td>Learning Rate</td><td>3e-6</td><td>1e-5</td><td>3e-5</td><td>6e-5</td></tr><tr><td>MM-LLAMA2 7B</td><td>21.42</td><td>32.45</td><td>43.04</td><td>28.24</td></tr><tr><td>Learning Rate</td><td>6e-4</td><td>1e-3</td><td>2e-3</td><td></td></tr><tr><td>MM-LLAMA213B</td><td>37.35</td><td>46.88</td><td>44.15</td><td></td></tr></table></body></html>\n\n# A.2INSIGHTS OFLAYERNORMTUNING\n\n# A.2.1 VISUALIZATION EXAMPLES OF LAYER SIMILARITIES\nLower similarities between different layers of the transformer indicates more expressive power ( Pires et al. ,2023 ). In section 5.2 , we have shown the computed cosine similarity between layers on a Vicuna model, here we show the layer similarities between layers on LL A MA2 and LL A MA2 CHAT models in fig. A1 and fig. A2 . It is clear that, LayerNorm tuning again allows the model to learn dissimilar layer representations, improving the expressive power of the model.\n\n# A.2.2 GRADIENTS OF LAYER NORM\nVisualization examples of LayerNorm gradients. In fig. A3 and fig. A4 , we present the gradients of the LayerNorm parameters during the training process. Similar to the one we have shown in the main text, LayerNorm tuning demonstrates a smaller gradient variance which is important for converging to a better local minimum ( Xu et al. ,2019 ).  \n\nProof of smaller variance in LayerNorm . As stated in Sec. 5.3 , deeper the network is, the variance of LayerNorm in the model should be naturally smaller ( $\\mathrm{\\DeltaXu}$ et al. ,2019 ). We first let $\\mathbf{y}\\,=\\,(y_{1},y_{2},...,y_{N})$ be the normalized vector, meaning the mean and variance of $\\mathbf{y}$ is 0 and 1 ,respectively. We can then formulate the standard LayerNorm as follow:  \n\n$$\n{\\mathbf{y}}={\\frac{\\mathbf{x}-{\\boldsymbol{\\mu}}}{\\sigma}},\\quad{\\boldsymbol{\\mu}}={\\frac{1}{N}}\\sum_{i=1}^{N}x_{i},\\quad\\sigma={\\sqrt{{\\frac{1}{N}}\\sum_{i=1}^{N}\\left(x_{i}-{\\boldsymbol{\\mu}}\\right)^{2}}},\n$$  \n\nwhere $\\mathbf{x}=(x_{1},x_{2},...,x_{N})$ is the input vector and $N$ is the dimension of $\\mathbf{x}$ .$\\mu$ and $\\sigma$ are the mean and standard deviation of $\\mathbf{x}$ .  \n\nWe first define $\\mathbf{1}_{N}=\\underbrace{(1,1,...,1)^{\\intercal}}_{N}$ . For calculating the gradients of the normalized vector $\\mathbf{y}$ , we first simulate the backward propagation regarding the loss {z }$\\ell$ :  \n\n$$\n{\\frac{\\partial\\ell}{\\partial\\mathbf{x}}}=\\left({\\frac{\\partial\\mathbf{y}}{\\partial\\mathbf{x}}}+{\\frac{\\partial\\mu}{\\partial\\mathbf{x}}}{\\frac{\\partial\\mathbf{y}}{\\partial\\mu}}+{\\frac{\\partial\\sigma}{\\partial\\mathbf{x}}}{\\frac{\\partial\\mathbf{y}}{\\partial\\sigma}}\\right){\\frac{\\partial\\ell}{\\partial\\mathbf{y}}}={\\frac{1}{\\sigma}}\\left(I-{\\frac{\\mathbf{y}\\mathbf{y}^{\\intercal}}{N}}-{\\frac{\\mathbf{1}_{N}\\mathbf{1}_{N}^{\\intercal}}{N}}\\right){\\frac{\\partial\\ell}{\\partial\\mathbf{y}}}.\n$$  \n\n  \nFigure A1: Layer similarities between different LLM layers in (a) Finetuned and (b) LayerNormtuned MM-LL A MA2-7B.  \n\nHere we define $\\begin{array}{r l r}{\\frac{\\partial\\ell}{\\partial\\mathbf x}}&{{}\\!\\!=}&{\\!\\!(a_{1},a_{2},...,a_{N})}\\end{array}$ with mean $\\bar{a}$ and standard deviation $D_{a}$ , and $\\begin{array}{r l}{\\frac{\\partial\\ell}{\\partial\\mathbf{y}}}&{{}=}\\end{array}$ $(b_{1},b_{2},...,b_{N})$ with mean $\\bar{b}$ and standard deviation $D_{b}$ . We set $\\begin{array}{r}{W_{1}\\;=\\;I\\,-\\,\\frac{{\\bf y}{\\bf y}^{\\intercal}}{N}\\,-\\,\\frac{{\\bf1}_{N}{\\bf1}_{N}^{\\intercal}}{N}}\\end{array}$ \u2212, we can verify that:  \n\n$$\n\\lfloor\\mathbf{\\Pi}_{N}^{\\mathsf{T}}W_{1}=\\mathbf{1}_{N}^{\\mathsf{T}}{\\frac{1}{\\sigma}}\\left(I-{\\frac{\\mathbf{1}_{N}\\mathbf{1}_{N}^{\\mathsf{T}}+\\mathbf{y}\\mathbf{y}^{\\mathsf{T}}}{N}}\\right)={\\frac{1}{\\sigma}}\\left(\\mathbf{1}_{N}-{\\frac{\\mathbf{1}_{N}^{\\mathsf{T}}\\mathbf{1}_{N}}{N}}\\mathbf{1}_{N}^{\\mathsf{T}}-{\\frac{\\mathbf{1}_{N}^{\\mathsf{T}}\\mathbf{y}}{N}}\\mathbf{y}^{\\mathsf{T}}\\right)={\\frac{\\mathbf{1}_{N}-\\mathbf{1}_{N}-0}{\\sigma}}=0\n$$  \n\nTherefore, we can easily proof that $N\\bar{a}\\propto{\\bf1}_{N}^{\\top}W_{1}\\bar{b}=0$ , which means the mean of $\\frac{\\partial\\ell}{\\partial\\mathbf{x}}$ should be zero. Then we dive into proofing the variance of LayerNorm gradients should be small when the number of network parameters $N$ becomes large.  \n\n$$\n\\begin{array}{l}{{\\displaystyle{D_{a}=\\sum_{i=1}^{N}(a_{i}-\\bar{a})^{2}/N=\\sum_{i=1}^{N}a_{i}^{2}/N}\\ ~}}\\\\ {{\\displaystyle{=\\left\\|{(a_{1},a_{2},\\ldots,a_{N})^{\\top}}\\right\\|^{2}/N}\\ ~}}\\\\ {{\\displaystyle{=\\left\\|{W_{1}\\left(b_{1},b_{2},\\ldots,b_{N}\\right)^{\\top}}\\right\\|^{2}/N}\\ ~}}\\\\ {{\\displaystyle{=\\left\\|{W_{1}\\left(b_{1}-\\bar{b},b_{2}-\\bar{b},\\ldots,b_{N}-\\bar{b}\\right)^{\\top}+W_{1}\\bar{b}{\\bf1}_{N}}\\right\\|^{2}/N}\\ }}\\\\ {{\\displaystyle{=\\left\\|{W_{1}\\left(g_{1}-\\bar{b},g_{2}-\\bar{b},\\ldots,g_{N}-\\bar{b}\\right)^{\\top}}\\right\\|^{2}/N}\\ ~}}\\\\ {{\\displaystyle{\\leq W_{1}^{2}\\sum_{i=1}^{N}(b_{i}-\\bar{b})^{2}/N}\\ }}\\end{array}\n$$  \n\nSince the projection matrix $W_{1}$ is idempotent, we have $W_{1}^{2}=W_{1}$ . That is to say, when $N$ is large enough, there stands the network parameter $\\begin{array}{r}{D_{a}\\le\\big(I-\\frac{\\mathbf{y}\\mathbf{y}^{\\top}+\\mathbf{1}_{N}\\mathbf{1}_{N}^{\\top}}{N}\\big)\\sum_{i=1}^{N}(b_{i}-\\bar{b_{}})^{2}/N\\propto1/N^{2}}\\end{array}$ Nis large, the gradient variance of LayerNorm should be small. P\u2212. As a consequence, when  \n\n  \nFigure A2: Layer similarities between different LLM layers in (a) Finetuned and (b) LayerNormtuned MM-LL A MA2-7B CHAT .  \n\n  \nFigure A3: The gradients of both input and post LayerNorm in 21st layer of the MM-V ICUNA as the training proceeds.  \n\n  \nFigure A4: The gradients of both input and post LayerNorm in 11th layer of the MM-V ICUNA as the training proceeds."}, {"ref_id": "454847042436311108", "chunk_id": "4", "score": 0.314453125, "text": "# A Robust Game-Theoretical Federated Learning Framework With Joint Differential Privacy\nLefeng Zhang, Tianqing Zhu , Ping Xiong, Wanlei Zhou ,Senior Member, IEEE , and Philip S. Yu ,Fellow, IEEE  \n\nAbstract\u2014 Federated learning is a promising distributed machine learning paradigm that has been playing a significant role in providing privacy-preserving learning solutions. However, alongside all its achievements, there are also limitations. First, traditional frameworks assume that all the clients are voluntary and so will want to participate in training only for improving the model\u2019s accuracy. However, in reality, clients usually want to be adequately compensated for the data and resources they will use before participating. Second, today\u2019s frameworks do not offer sufficient protection against malicious participants who try to skew a jointly trained model with poisoned updates. To address these concerns, we have developed a more robust federated learning scheme based on joint differential privacy. The framework provides two game-theoretic mechanisms to motivate clients to participate in training. These mechanisms are dominant-strategy truthful, individual rational, and budget-balanced. Further, the influence an adversarial client can have is quantified and restricted, and data privacy is similarly guaranteed in quantitative terms. Experiments with different training models on real-word datasets demonstrate the effectiveness of the proposed approach.  \n\nIndex Terms\u2014 Differential privacy, game theory, federated learning"}, {"ref_id": "454846876251686832", "chunk_id": "1", "score": 0.154296875, "text": "# 1 Introduction\nThe explosive increase in the use of deep neural network (DNN)-based models for applications across domains has resulted in a very strong need to find ways to interpret the decisions made by these models (Gade et al. 2020; Tang et al. 2021; Yap et al. 2021; Oviedo et al. 2022; Oh and Jeong 2020). Interpretability is an important aspect of responsible and trustworthy AI, and model explanation methods (also known as attribution methods) are an important aspect of the community\u2019s efforts towards explaining and debugging real-world AI/ML systems. Attribution methods (Zeiler et al. 2010; Simonyan, Vedaldi, and Zisserman 2014; Bach et al. 2015; Selvaraju et al. 2017; Chattopadhyay et al. 2018; Sundararajan, Taly, and Yan 2017; Shrikumar et al. 2016; Smilkov et al. 2017; Lundberg and Lee 2017) attempt to explain the decisions made by DNN models through inputoutput attributions or saliency maps. (Lipton 2018; Samek et al. 2019; Fan et al. 2021; Zhang et al. 2020) present detailed surveys on these methods. Recently, the growing numbers of attribution methods has led to a concerted focus on studying the robustness of attributions to input perturbations to handle potential security hazards (Chen et al. 2019; Sarkar, Sarkar, and Balasubramanian 2021; Wang and Kong 2022; Agarwal et al. 2022). One could view these efforts as akin to adversarial robustness that focuses on defending against attacks on model predictions, whereas attributional robustness focuses on defending against attacks on model explanations. For example, an explanation for a predicted credit card failure cannot change significantly for a small human-imperceptible change in input features, or the saliency maps explaining the COVID risk prediction from a chest X-ray should not change significantly with a minor human-imperceptible change in the image.  \n\nDNN-based models are known to have a vulnerability to imperceptible adversarial perturbations (Biggio et al. 2013; Szegedy et al. 2014; Goodfellow, Shlens, and Szegedy 2015), which make them misclassify input images. Adversarial training (Madry et al. 2018) is widely understood to provide a reasonable degree of robustness to such perturbation attacks. While adversarial robustness has received significant attention over the last few years (Ozdag 2018; Silva and Najafirad 2020), the need for stable and robust attributions, corresponding explanation methods and their awareness are still in their early stages at this time (Ghorbani, Abid, and $Z_{\\mathrm{ou}}~2019$ ; Chen et al. 2019; Slack et al. 2020; Sarkar, Sarkar, and Balasubramanian 2021; Lakkaraju, Arsov, and Bastani 2020; Slack et al. 2021a,b). In an early effort, (Ghorbani, Abid, and Zou 2019) provided a method to construct a small imperceptible perturbation which when added to an input $x$ results in a change in attribution map of the original map to that of the perturbed image. This is measured through top$k$ intersection, Spearman\u2019s rank-order correlation or Kendall\u2019s rank-order correlation between the two attribution maps (of original and perturbed images). See Figure 1 for an example. Defenses proposed against such attributional attacks (Chen et al. 2019; Singh et al. 2020; Wang et al. 2020; Sarkar, Sarkar, and Balasubramanian 2021) also leverage the same metrics to evaluate the robustness of attribution methods.  \n\nWhile these efforts have showcased the need and importance of studying the robustness of attribution methods, we note in this work that the metrics used, and hence the methods, can be highly sensitive to minor local changes in attributions (see Fig 1 row 2 ). We, in fact, show (in Appendix B.1) that under existing metrics to evaluate robustness of attributions, a random perturbation can be as strong an attributional attack as existing benchmark methods. This may not be a true indicator of the robustness of a model\u2019s attributions, and can mislead further research efforts in the community. We hence focus our efforts in this work on rethinking metrics and methods to study the robustness of model attributions (in particular, we study image-based attribution methods to have a focused discussion and analysis). Beyond highlighting this important issue, we propose locality-sensitive improvements of the above metrics that incorporate the locality of attributions along with their rank order. We show that such a locality-sensitive distance is upper-bounded by a metric based on symmetric set difference. We also introduce a new measure top$k$ -div that incorporates diversity of a model\u2019s attributions. Our key contributions are summarized below:  \n\n  \nFigure 1: Sample images from Flower dataset with Integrated Gradients (IG) before and after attributional attack. The attack used here is the top$k$ attributional attack of Ghorbani, Abid, and Zou (2019) on a ResNet model. Robustness of attribution measured by top$k$ intersection is small, and ranges from 0.04 (first image) to 0.45 (third image) as it penalizes for both local changes in attribution and concentration of top pixels in a small region. Visually, we can observe that such overpenalization leads to a wrong sense of robustness as the changes are within the object of importance.  \n\n\u2022 Firstly, we observe that existing robustness metrics for model attributions overpenalize minor drifts in attribution, leading to a false sense of fragility. \u2022 In order to address this issue, we propose LocalitysENSitive (LENS) improvements of existing metrics, namely, LENS-top${\\cdot k}$ , LENS-Spearman and LENSKendall, that incorporate the locality of attributions along  \n\nwith their rank order. Besides avoiding overpenalizing attribution methods for minor local drifts, we show that our proposed LENS variants are well-motivated by metrics defined on the space of attributions.   \n\u2022 We subsequently introduce a second measure based on diversity that enriches model attributions by preventing the localized grouping of top model attributions. LENS can be naturally applied to this measure, thereby giving a method to incorporate both diversity and locality in measuring attributional robustness.   \n\u2022 Our comprehensive empirical results on benchmark datasets and models used in existing work clearly support our aforementioned observations, as well as the need to rethink the evaluation of the robustness of model attributions using locality and diversity.   \n\u2022 Finally, we also show that existing methods for robust attributions implicitly support such a locality-sensitive metric for evaluating progress in the field."}], "task_step_question_answer": "\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\uff0cLayerNorm\u548cRMSNorm\u7684\u5e94\u7528\u4e5f\u5c55\u73b0\u51fa\u6f5c\u529b\u3002\u4f8b\u5982\uff0c\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\uff0cLayerNorm\u901a\u8fc7\u5176\u7a33\u5b9a\u6027\uff0c\u80fd\u591f\u6709\u6548\u7f13\u89e3\u4e0d\u540c\u5ba2\u6237\u7aef\u6570\u636e\u5206\u5e03\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002\u800cRMSNorm\u5219\u901a\u8fc7\u5176\u9ad8\u6548\u6027\uff0c\u51cf\u5c11\u4e86\u901a\u4fe1\u5f00\u9500\uff0c\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u3002\u5728\u9690\u79c1\u4fdd\u62a4\u65b9\u9762\uff0cLayerNorm\u901a\u8fc7\u5176\u5168\u5c40\u5f52\u4e00\u5316\u7684\u7279\u6027\uff0c\u80fd\u591f\u66f4\u597d\u5730\u4fdd\u62a4\u654f\u611f\u6570\u636e\uff0c\u800cRMSNorm\u5219\u901a\u8fc7\u5176\u9ad8\u6548\u7684\u8ba1\u7b97\u8fc7\u7a0b\uff0c\u51cf\u5c11\u4e86\u9690\u79c1\u6cc4\u9732\u7684\u98ce\u9669\u3002\u672a\u6765\u7684\u7814\u7a76\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63a2\u7d22\u5982\u4f55\u7ed3\u5408LayerNorm\u548cRMSNorm\u7684\u4f18\u70b9\uff0c\u8bbe\u8ba1\u51fa\u66f4\u9ad8\u6548\u7684\u5f52\u4e00\u5316\u65b9\u6cd5\u3002\u4f8b\u5982\uff0c\u53ef\u4ee5\u7814\u7a76\u5982\u4f55\u6839\u636e\u8f93\u5165\u6570\u636e\u7684\u7279\u6027\u52a8\u6001\u9009\u62e9\u4f7f\u7528LayerNorm\u6216RMSNorm\uff0c\u4ee5\u8fdb\u4e00\u6b65\u63d0\u5347\u6a21\u578b\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u3002\u6b64\u5916\uff0c\u8fd8\u53ef\u4ee5\u63a2\u7d22\u8fd9\u4e9b\u65b9\u6cd5\u5728\u65b0\u5174\u7814\u7a76\u9886\u57df\uff08\u5982\u9690\u79c1\u4fdd\u62a4\u3001\u8054\u90a6\u5b66\u4e60\u7b49\uff09\u4e2d\u7684\u5e94\u7528\uff0c\u4ee5\u5e94\u5bf9\u65e5\u76ca\u590d\u6742\u7684\u5b9e\u9645\u9700\u6c42\u3002", "ref_task_step_id": ""}, "__type__": "task_step_node"}, "f189b0a8-ec1b-4435-ae46-18924bfa2573": {"__data__": {"id_": "f189b0a8-ec1b-4435-ae46-18924bfa2573", "metadata": {}, "relationships": {}, "hash": "", "start_task_context": "\u5927\u6a21\u578b\u4e2d\u7684LayerNorm\u548cRMSNorm\u6709\u4ec0\u4e48\u533a\u522b\uff1f", "aemo_representation_context": "### Step-by-Step Decomposition of Computer Science Research Content\n\n#### 1. \u5206\u6790\u8fd1\u51e0\u5e74\u7814\u7a76\u9886\u57df\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba\n- **\u6280\u672f\u6846\u67b6**\uff1a\u8fd1\u5e74\u6765\uff0c\u6df1\u5ea6\u5b66\u4e60\u3001\u5f3a\u5316\u5b66\u4e60\u3001\u56fe\u795e\u7ecf\u7f51\u7edc\u7b49\u6846\u67b6\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u9886\u57df\u5360\u636e\u4e3b\u5bfc\u5730\u4f4d\u3002\u5177\u4f53\u6846\u67b6\u5982Transformer\u3001BERT\u3001GNN\u7b49\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002\n- **\u65b9\u6cd5\u8bba**\uff1a\u7814\u7a76\u65b9\u6cd5\u9010\u6e10\u4ece\u5355\u4e00\u6a21\u578b\u8f6c\u5411\u591a\u6a21\u578b\u878d\u5408\uff0c\u4ece\u76d1\u7763\u5b66\u4e60\u6269\u5c55\u5230\u65e0\u76d1\u7763\u548c\u81ea\u76d1\u7763\u5b66\u4e60\uff0c\u5f3a\u8c03\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002\n\n#### 2. \u7814\u7a76\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u4e3b\u8981\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0e\u53d8\u4f53\n- **\u5e94\u7528**\uff1a\u4f8b\u5982\uff0cTransformer\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u4e2d\u88ab\u5e7f\u6cdb\u5e94\u7528\uff0cBERT\u53d8\u4f53\u5982RoBERTa\u3001ALBERT\u5728\u6587\u672c\u5206\u7c7b\u3001\u95ee\u7b54\u7cfb\u7edf\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff1bGNN\u5728\u63a8\u8350\u7cfb\u7edf\u3001\u77e5\u8bc6\u56fe\u8c31\u7b49\u9886\u57df\u6709\u663e\u8457\u5e94\u7528\u3002\n- **\u53d8\u4f53**\uff1a\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\uff0c\u7814\u7a76\u8005\u63d0\u51fa\u4e86\u591a\u79cd\u53d8\u4f53\uff0c\u5982\u9488\u5bf9\u957f\u5e8f\u5217\u5904\u7406\u7684Transformer-XL\uff0c\u9488\u5bf9\u591a\u6a21\u6001\u6570\u636e\u7684ViLBERT\u7b49\u3002\n\n#### 3. \u8bc4\u4f30\u5b66\u672f\u754c\u7684\u6280\u672f\u8fdb\u6b65\u4e0e\u5c40\u9650\u6027\n- **\u6280\u672f\u8fdb\u6b65**\uff1a\u6a21\u578b\u6027\u80fd\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\uff0c\u8ba1\u7b97\u6548\u7387\u4e5f\u6709\u6240\u63d0\u9ad8\uff0c\u5982EfficientNet\u5728\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u8868\u73b0\u3002\n- **\u5c40\u9650\u6027**\uff1a\u4f9d\u7136\u5b58\u5728\u6a21\u578b\u504f\u5dee\uff08\u5982\u6027\u522b\u504f\u89c1\uff09\u3001\u6570\u636e\u4f9d\u8d56\uff08\u5982\u5bf9\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\uff09\u7b49\u95ee\u9898\uff0c\u6cdb\u5316\u80fd\u529b\u6709\u5f85\u8fdb\u4e00\u6b65\u63d0\u5347\u3002\n\n#### 4. \u63a2\u8ba8\u8ba1\u7b97\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\n- **\u9002\u7528\u6027**\uff1a\u7814\u7a76\u6a21\u578b\u5728\u4e0d\u540c\u9886\u57df\uff08\u5982\u533b\u7597\u3001\u91d1\u878d\uff09\u548c\u591a\u6a21\u6001\u6570\u636e\uff08\u5982\u6587\u672c+\u56fe\u50cf\uff09\u4e0a\u7684\u8868\u73b0\uff0c\u5982\u591a\u6a21\u6001Transformer\u6a21\u578b\u3002\n- **\u6cdb\u5316\u80fd\u529b**\uff1a\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\u3001\u5143\u5b66\u4e60\u7b49\u65b9\u6cd5\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f7f\u5176\u80fd\u591f\u5e94\u5bf9\u66f4\u591a\u73b0\u5b9e\u5e94\u7528\u573a\u666f\u3002\n\n#### 5. \u5206\u6790\u6700\u65b0\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027\n- **\u7a33\u5b9a\u6027**\uff1a\u7814\u7a76\u7b97\u6cd5\u5728\u590d\u6742\u3001\u52a8\u6001\u73af\u5883\u4e0b\u7684\u8868\u73b0\uff0c\u5982\u5bf9\u6297\u8bad\u7ec3\u3001\u9c81\u68d2\u4f18\u5316\u7b49\u65b9\u6cd5\u7684\u5e94\u7528\u3002\n- **\u5bb9\u9519\u6027**\uff1a\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\u7684\u9002\u5e94\u6027\uff0c\u5982\u5206\u5e03\u5f0f\u8bad\u7ec3\u4e2d\u7684\u68af\u5ea6\u538b\u7f29\u3001\u6a21\u578b\u5e76\u884c\u7b49\u6280\u672f\u3002\n\n#### 6. \u8bc4\u4f30\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218\n- **\u672a\u6765\u65b9\u5411**\uff1a\u5982\u53ef\u89e3\u91caAI\u3001\u8054\u90a6\u5b66\u4e60\u3001\u9690\u79c1\u4fdd\u62a4\u7b49\u65b0\u5174\u7814\u7a76\u65b9\u5411\u3002\n- **\u6311\u6218**\uff1a\u5982\u4f55\u5e73\u8861\u6a21\u578b\u6027\u80fd\u4e0e\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\uff0c\u5982\u4f55\u89e3\u51b3\u6570\u636e\u504f\u89c1\u548c\u4f26\u7406\u95ee\u9898\u7b49\u3002\n\n### \u5927\u6a21\u578b\u4e2d\u7684LayerNorm\u548cRMSNorm\u7684\u533a\u522b\n\n#### LayerNorm\uff08Layer Normalization\uff09\n- **\u5b9a\u4e49**\uff1aLayerNorm\u662f\u5bf9\u6bcf\u4e2a\u6837\u672c\u7684\u6bcf\u4e2a\u7279\u5f81\u8fdb\u884c\u5f52\u4e00\u5316\uff0c\u4f7f\u5176\u5747\u503c\u548c\u65b9\u5dee\u5206\u522b\u63a5\u8fd10\u548c1\u3002\n- **\u516c\u5f0f**\uff1a\n  \\[\n  \\text{LayerNorm}(x) = \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} \\cdot \\gamma + \\beta\n  \\]\n  \u5176\u4e2d\uff0c\\(\\mu\\)\u548c\\(\\sigma^2\\)\u5206\u522b\u662f\u7279\u5f81\u5747\u503c\u548c\u65b9\u5dee\uff0c\\(\\gamma\\)\u548c\\(\\beta\\)\u662f\u53ef\u5b66\u4e60\u53c2\u6570\uff0c\\(\\epsilon\\)\u662f\u5fae\u5c0f\u5e38\u6570\u4ee5\u9632\u6b62\u9664\u96f6\u3002\n- **\u5e94\u7528**\uff1a\u5e7f\u6cdb\u7528\u4e8eTransformer\u6a21\u578b\u4e2d\uff0c\u6709\u52a9\u4e8e\u7f13\u89e3\u68af\u5ea6\u6d88\u5931\u95ee\u9898\uff0c\u63d0\u5347\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002\n\n#### RMSNorm\uff08Root Mean Square Normalization\uff09\n- **\u5b9a\u4e49**\uff1aRMSNorm\u662f\u5bf9\u6bcf\u4e2a\u6837\u672c\u7684\u6bcf\u4e2a\u7279\u5f81\u8fdb\u884c\u5f52\u4e00\u5316\uff0c\u4f46\u53ea\u5f52\u4e00\u5316\u65b9\u5dee\uff0c\u4e0d\u5f52\u4e00\u5316\u5747\u503c\u3002\n- **\u516c\u5f0f**\uff1a\n  \\[\n  \\text{RMSNorm}(x) = \\frac{x}{\\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} x_i^2 + \\epsilon}} \\cdot \\gamma + \\beta\n  \\]\n  \u5176\u4e2d\uff0c\\(n\\)\u662f\u7279\u5f81\u6570\u91cf\uff0c\\(\\gamma\\)\u548c\\(\\beta\\)\u662f\u53ef\u5b66\u4e60\u53c2\u6570\uff0c\\(\\epsilon\\)\u662f\u5fae\u5c0f\u5e38\u6570\u3002\n- **\u5e94\u7528**\uff1a\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0cRMSNorm\u6bd4LayerNorm\u66f4\u7b80\u5355\uff0c\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\uff0c\u5c24\u5176\u5728\u5927\u89c4\u6a21\u6a21\u578b\u8bad\u7ec3\u4e2d\u3002\n\n#### \u4e3b\u8981\u533a\u522b\n1. **\u5f52\u4e00\u5316\u65b9\u5f0f**\uff1a\n   - LayerNorm\u540c\u65f6\u5f52\u4e00\u5316\u5747\u503c\u548c\u65b9\u5dee\u3002\n   - RMSNorm\u53ea\u5f52\u4e00\u5316\u65b9\u5dee\u3002\n2. **\u8ba1\u7b97\u590d\u6742\u5ea6**\uff1a\n   - LayerNorm\u9700\u8981\u8ba1\u7b97\u5747\u503c\u548c\u65b9\u5dee\uff0c\u8ba1\u7b97\u91cf\u7a0d\u5927\u3002\n   - RMSNorm\u53ea\u9700\u8ba1\u7b97\u65b9\u5dee\uff0c\u8ba1\u7b97\u91cf\u8f83\u5c0f\u3002\n3. **\u6548\u679c**\uff1a\n   - LayerNorm\u5728\u8bb8\u591a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u5728NLP\u9886\u57df\u3002\n   - RMSNorm\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u8868\u73b0\u76f8\u4f3c\u751a\u81f3\u66f4\u4f18\uff0c\u4e14\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u3002\n\n\u901a\u8fc7\u4e0a\u8ff0\u5206\u6790\uff0c\u53ef\u4ee5\u66f4\u597d\u5730\u7406\u89e3LayerNorm\u548cRMSNorm\u5728\u5927\u6a21\u578b\u4e2d\u7684\u5e94\u7528\u53ca\u5176\u4f18\u7f3a\u70b9\uff0c\u4ece\u800c\u5728\u5177\u4f53\u4efb\u52a1\u4e2d\u9009\u62e9\u5408\u9002\u7684\u5f52\u4e00\u5316\u65b9\u6cd5\u3002", "task_step_name": "\u6311\u6218", "task_step_description": "\u5982\u4f55\u5e73\u8861\u6a21\u578b\u6027\u80fd\u4e0e\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\uff0c\u5982\u4f55\u89e3\u51b3\u6570\u636e\u504f\u89c1\u548c\u4f26\u7406\u95ee\u9898\u7b49\u3002", "task_step_level": "5>2", "task_step_question": "### \u95ee\u9898\u63d0\u51fa\n\n\u5728\u63a2\u8ba8\u5927\u6a21\u578b\u4e2d\u7684LayerNorm\u548cRMSNorm\u7684\u533a\u522b\u65f6\uff0c\u5982\u4f55\u5e73\u8861\u6a21\u578b\u6027\u80fd\u4e0e\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\uff0c\u4ee5\u53ca\u5982\u4f55\u89e3\u51b3\u6570\u636e\u504f\u89c1\u548c\u4f26\u7406\u95ee\u9898\uff0c\u6210\u4e3a\u4e86\u5173\u952e\u7684\u6311\u6218\u3002\u5177\u4f53\u6765\u8bf4\uff1a\n\n1. **\u6a21\u578b\u6027\u80fd\u4e0e\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u7684\u5e73\u8861**\uff1a\n   - LayerNorm\u548cRMSNorm\u5728\u8ba1\u7b97\u590d\u6742\u5ea6\u4e0a\u5b58\u5728\u5dee\u5f02\uff0cRMSNorm\u56e0\u5176\u53ea\u5f52\u4e00\u5316\u65b9\u5dee\u800c\u5177\u6709\u66f4\u9ad8\u7684\u8ba1\u7b97\u6548\u7387\u3002\u7136\u800c\uff0cLayerNorm\u5728\u8bb8\u591a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u5728NLP\u9886\u57df\u3002\u5982\u4f55\u5728\u9009\u62e9\u5f52\u4e00\u5316\u65b9\u6cd5\u65f6\uff0c\u65e2\u4fdd\u8bc1\u6a21\u578b\u6027\u80fd\uff0c\u53c8\u6709\u6548\u964d\u4f4e\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\uff1f\n   - \u5728\u5927\u89c4\u6a21\u6a21\u578b\u8bad\u7ec3\u4e2d\uff0c\u5982\u4f55\u901a\u8fc7\u4f18\u5316\u5f52\u4e00\u5316\u65b9\u6cd5\uff08\u5982\u9009\u62e9RMSNorm\uff09\u6765\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff1f\n\n2. **\u6570\u636e\u504f\u89c1\u548c\u4f26\u7406\u95ee\u9898\u7684\u89e3\u51b3**\uff1a\n   - LayerNorm\u548cRMSNorm\u5728\u5904\u7406\u6570\u636e\u65f6\uff0c\u662f\u5426\u4f1a\u5bf9\u6570\u636e\u504f\u89c1\u4ea7\u751f\u5f71\u54cd\uff1f\u4f8b\u5982\uff0c\u5f52\u4e00\u5316\u65b9\u6cd5\u662f\u5426\u53ef\u80fd\u653e\u5927\u6216\u7f29\u5c0f\u6570\u636e\u4e2d\u7684\u67d0\u4e9b\u504f\u89c1\uff1f\n   - \u5728\u8bbe\u8ba1\u548c\u9009\u62e9\u5f52\u4e00\u5316\u65b9\u6cd5\u65f6\uff0c\u5982\u4f55\u786e\u4fdd\u5176\u4e0d\u4f1a\u5f15\u5165\u6216\u52a0\u5267\u6570\u636e\u504f\u89c1\uff0c\u4ece\u800c\u7b26\u5408\u4f26\u7406\u8981\u6c42\uff1f\u4f8b\u5982\uff0c\u662f\u5426\u53ef\u4ee5\u901a\u8fc7\u8c03\u6574\u5f52\u4e00\u5316\u53c2\u6570\u6216\u5f15\u5165\u989d\u5916\u7684\u7ea6\u675f\u6765\u51cf\u5c11\u504f\u89c1\uff1f\n\n\u8fd9\u4e9b\u95ee\u9898\u4e0d\u4ec5\u6d89\u53ca\u6280\u672f\u5c42\u9762\u7684\u4f18\u5316\uff0c\u8fd8\u6d89\u53ca\u5230\u4f26\u7406\u548c\u516c\u5e73\u6027\u7684\u8003\u91cf\uff0c\u9700\u8981\u5728\u7814\u7a76\u548c\u5e94\u7528\u4e2d\u7efc\u5408\u8003\u8651\u3002", "task_step_question_context": [{"ref_id": "454845727870837706", "chunk_id": "4", "score": 0.3984375, "text": "# 2 Problem Formulation\n\n# 2.1 Related Work\nLarge Language Models (LLMs). The advent of LLMs has led to a paradigm shift in the study of natural language processing (NLP), computer vision, information retrieval, and other domains[Menghani, 2023, Chen et al., 2023, Jiang et al., 2023]. The impressive effectiveness and generalizability of LLMs has come at the price of a drastic increase in LLM sizes [Treviso et al., 2023] and consequent challenges, including huge amounts of computational resources and data required to train, and prohibitive expenses at both training and deployment stages [Bender et al., 2021].  \n\nEfficient Machine Learning (ML) Inference. LLMs belong to a class of models called foundation models [Bommasani et al., 2021] \u2013 models that are trained once and can then be used to serve a wide variety of tasks. As such, we expect inference cost to dominate the overall cost of such models and hence focus on works that reduce the cost of ML inference [Menghani, 2023]. The most common approach for efficient ML inference is model compression i.e., replacing a large model with a smaller model of comparable accuracy. Common techniques for model compression include (i) model pruning [Hassibi et al., 1993, LeCun et al., 1989] which drops parts of the model with minimal accuracy loss, (ii) quantization [Jacob et al., 2018, Vanhoucke et al., 2011] which reduces model memory footprints and inference latency by reducing the precision of data representation (e.g., FP32 to INT8), (iii) knowledge distillation [Hinton et al., 2015, Urban et al., 2016] which trains small student models to mimic large teacher models, and (iv) Neural Architecture Search [Elsken et al., 2019, Zoph and Le, 2016] which tunes model architecture to improve model performance, under inference cost constraints. Such static efficiency optimizations typically produce a fixed model with lower inference cost and lower accuracy compared to the large model which may not suffice for foundation models like LLMs, whose core premise is that the same model will serve a range of tasks, each with its own accuracy/cost constraints. This is already manifesting in inference platforms described in Section 1 which need more dynamic optimizations to meet the demands of all users.  \n\nHybrid ML Inference. Recent works [Kag et al., 2022, Ding et al., 2022] have introduced a new inference paradigm called hybrid inference which uses two models of different sizes instead of a single model for inference. The smaller model (e.g. Llama2 [Touvron et al., 2023]) generally has lower inference cost but also lower accuracy than the larger model (e.g. GPT-4 [OpenAI, 2023]). The key idea is to identify and route easy queries to the small model so that inference cost can be reduced while maintaining response quality. By tuning a threshold on query difficulty we can dynamically trade off quality and cost for the same inference setup. [Kag et al., 2022] study this setup for image classification and propose to train the small model, large model, and router from scratch. However LLM training is expensive and retraining LLMs from scratch for every scenario goes against the very premise of inference with pre-trained foundation models. Moreover text generation [Iqbal and Qureshi, 2022] is often more ambiguous and challenging than image classification due to which novel techniques are required for effective hybrid LLM inference for text generation.  \n\nInference with Multiple LLMs. Some recent works [Jiang et al., 2023, Chen et al., 2023, Leviathan et al., 2023, Kim et al., 2023] use multiple LLMs for inference but these approaches typically call more than one LLM for a single query that can incur significant computational overheads. Specifically [Jiang et al., 2023] calls an ensemble of LLMs at inference time due to which the inference cost will be proportional to the number of models in the system. [Chen et al., 2023] performs inference using a cascade of LLMs where responses to the query are generated sequentially by the LLMs in the cascade until one of the models has a confidence score higher than a predefined threshold. Our work provides high quality responses while always making a single LLM call for all queries and will thus incur much lower computational cost than both of these works on average. Speculative decoding, introduced in [Leviathan et al., 2023, Kim et al., 2023] speeds up decoding of expensive models by invoking small-and-efficient decoders on the \u201ceasy\u201d decoding steps. Instead, in our work we are interested in query routing which assigns \u201ceasy\u201d queries to small models to reduce overall inference costs while maintaining high performance. While the two approaches have different goals, an interesting line of future work would be to combine these so that our router assigns queries to the small or large model based on query difficulty and then speculative decoding is applied on top to speed up inference for queries assigned to the large model thereby leading to further cost reduction."}, {"ref_id": "454845727779349442", "chunk_id": "0", "score": 0.25390625, "text": "# 6 Related Work\nModel Quantization Traditional model quantization algorithms mainly focus on the cases where both parameters and activations of the model are quantized ( Lin et al. ,2015 ;Hubara et al. ,2016 ;Tailor et al. ,2021 ;Ni et al. ,2020 ). However, directly quantizing the model will greatly decrease the accuracy of the models, and one important technique to improve the performance is Quantization Aware Training (QAT) ( Jacob et al. ,2018 ), where it simulates the quantization procedure in training to improve the accuracy of the quantized model further. For Transformer based models, the boundary of the compression level has been continuously advanced. For example, 8 -bits quantized transformers as in FullyQT ( Prato et al. ,2019 ) and Q8BERT (Zafrir et al. ,2019 ), 4 -bits quantized BERT in Wu et al. (2023 ) and tenary case as in TernaryBERT (Zhang et al. ,2020 ).  \n\nModel Quantization for LLMs. For quantizing LLMs, due to their prohibitive training expense, we can only use a few training data for calibration. There are two major directions: 1) weight-only quantization, where the weights are quantized into lower bits. In Frantar et al. (2023a ); Yao et al. (2022 ), authors optimize the output error on the calibration set using OBS and gradient descent. 2)  \n\nActivation and weight quantization, where both activations and weights are quantized into lower bits. In this case, the major obstacle is the outliers in activations. LLM.int8() ( Dettmers et al. ,2022 ) addresses this problem by isolating those outliers in fp16/bf16. However, such implementation leads to large latency overhead and is even slower than fp16 inference. Recent studies ( Wei et al. ,2023 ;Xiao et al. ,2023 ) found that the outliers only exist in certain channels, and use the LayerNorm weights ( Wei et al. ,2023 ) and calibrated scales ( Xiao et al. ,2023 )to smooth those channels. Xiao et al. (2023 ) has already proved that we can achieve almost lossless W8A8 quantized LLMs using a few calibration data, without manipulating the original model weights.\n\n# 7 Conclusion and Limitations\nIn this paper, we propose a data-free fast weightonly quantization algorithm, namely EasyQuant, for LLMs, that potentially improves the quantized model\u2019s performance without using any training data. Our analysis reveals the intrinsic origins of the performance loss when quantizing the model weights into lower bits. We show that by isolating the outliers from quantization, the accuracy of the quantized LLM increases accordingly with decreased reconstruction error. Our experiment proved that EasyQuant significantly outperforms RTN in a data-free setting, and also behaves better than data-dependent algorithms. EasyQuant can finish the quantization for a 176B-sized model within 10 minutes and the overhead of dequantization in EasyQuant is negligible.  \n\nHowever, we also point out some limitations of our work: The outlier recovery functionality in EasyQuant requires extra CUDA kernels for implementation. Moreover, weight-only quantization can only reduce the memory footprint without any computation cost reduction, hence the latency of our model cannot be minimized. In addition, this outlier isolation will make the weight/activation quantization more challenging because the weight includes numbers under different precision. We have also noticed that EasyQuantcannot outperform the data-dependent methods in all tasks, this motivates us to investigate more effective algorithms in future studies.\n\n\n\n# A Appendix\nTable 10: Perplexity and zershot results for BLOOM model family   \n\n\n<html><body><table><tr><td rowspan=\"2\"></td><td rowspan=\"2\"></td><td colspan=\"3\">Perplexity-based Task</td><td colspan=\"4\">Zero-shot Task</td></tr><tr><td>WikiText2</td><td>PTB</td><td>C4</td><td>PIQA</td><td>ARC-easy</td><td>ARC-Challenge StoryCloze</td><td></td></tr><tr><td rowspan=\"2\">BLOOM</td><td>fp16</td><td>22.42</td><td>43.69</td><td>26.6</td><td>65.07%</td><td>41.71%</td><td>24.15%</td><td>61.94%</td></tr><tr><td>RTN</td><td>25.90</td><td>51.10</td><td>29.89</td><td>63.11%</td><td>39.40%</td><td>23.89%</td><td>60.15%</td></tr><tr><td rowspan=\"2\">560M</td><td>GPTQ</td><td>24.03</td><td>46.97</td><td>28</td><td>64.31%</td><td>40.24%</td><td>23.46%</td><td>61.17%</td></tr><tr><td>EasyQuant</td><td>23.74</td><td>46.86</td><td>28.03</td><td>63.06%</td><td>40.32%</td><td>24.15%</td><td>59.64%</td></tr><tr><td rowspan=\"2\">BLOOM</td><td>fp16</td><td>17.69</td><td>57.96</td><td>22.05</td><td>67.14%</td><td>45.41%</td><td>25.68%</td><td>63.27%</td></tr><tr><td>RTN</td><td>22.00</td><td>66.85</td><td>24.44</td><td>65.29%</td><td>42.51%</td><td>23.34%</td><td>60.66%</td></tr><tr><td rowspan=\"2\">1.1B</td><td>GPTQ</td><td>19.05</td><td>62.48</td><td>23.25</td><td>66.05%</td><td>44.49%</td><td>25.51%</td><td>62.32%</td></tr><tr><td>EasyQuant</td><td>18.51</td><td>61.83</td><td>22.94</td><td>66.65%</td><td>43.73%</td><td>25.51%</td><td>62.06%</td></tr><tr><td rowspan=\"2\">BLOOM</td><td>fp16</td><td>15.39</td><td>30.00</td><td>19.49</td><td>69.97%</td><td>48.11%</td><td>26.79 %</td><td>65.44%</td></tr><tr><td>RTN</td><td>16.97</td><td>33.58</td><td>21.26</td><td>67.74%</td><td>44.70%</td><td>26.45 %</td><td>62.95%</td></tr><tr><td rowspan=\"2\">1.7B</td><td>GPTQ</td><td>16.48</td><td>31.84</td><td>20.55</td><td>68.77%</td><td>44.49%</td><td>25.94%</td><td>64.48%</td></tr><tr><td>EasyQuant</td><td>16.01</td><td>31.50</td><td>20.15</td><td>68.99%</td><td>46.89%</td><td>26.19%</td><td>65.37%</td></tr><tr><td rowspan=\"2\">BLOOM</td><td>fp16</td><td>13.48</td><td>25.34</td><td>17.49</td><td>70.51%</td><td>53.24%</td><td>30.55 %</td><td>67.79%</td></tr><tr><td>RTN</td><td>14.76</td><td>27.68</td><td>18.76</td><td>69.86%</td><td>51.35%</td><td>29.52%</td><td>67.09%</td></tr><tr><td rowspan=\"2\">3B</td><td>GPTQ</td><td>14.2</td><td>26.49</td><td>18.1</td><td>69.42%</td><td>52.82%</td><td>28.92%</td><td>67.22%</td></tr><tr><td>EasyQuant</td><td>14.01</td><td>26.12</td><td>17.96</td><td>69.80%</td><td>50.72%</td><td>28.58%</td><td>67.35%</td></tr><tr><td rowspan=\"2\">BLOOM</td><td>fp16</td><td>11.37</td><td>20.83</td><td>15.20</td><td>73.72%</td><td>57.37%</td><td>33.45 %</td><td>71.99%</td></tr><tr><td>RTN</td><td>12.10</td><td>22.42</td><td>16.06</td><td>72.69%</td><td>56.14%</td><td>32.17 %</td><td>70.72%</td></tr><tr><td rowspan=\"2\">7.1B</td><td>GPTQ</td><td>11.73</td><td>21.67</td><td>15.6</td><td>72.96%</td><td>56.14%</td><td>32.25%</td><td>71.36%</td></tr><tr><td>EasyQuant</td><td>11.66</td><td>21.47</td><td>15.52</td><td>73.23%</td><td>55.72%</td><td>32.51 %</td><td>71.10%</td></tr><tr><td rowspan=\"2\">BLOOM</td><td>fp16</td><td>8.11</td><td>14.59</td><td>11.71</td><td>79.16%</td><td>67.47%</td><td>44.97 %</td><td>76.89%</td></tr><tr><td>RTN</td><td>8.37</td><td>15.00</td><td>12.04</td><td>79.00%</td><td>66.33%</td><td>43.17 %</td><td>76.00%</td></tr><tr><td rowspan=\"2\">176B</td><td>GPTQ</td><td>8.21</td><td>14.75</td><td>11.81</td><td>79.00%</td><td>67.42%</td><td>44.10%</td><td>76.32%</td></tr><tr><td>EasyQuant</td><td>8.21</td><td>14.75</td><td></td><td>11.87 79.05%</td><td>67.8%</td><td>44.45%</td><td>77.28%</td></tr></table></body></html>"}, {"ref_id": "454895483053685384", "chunk_id": "2", "score": 0.25390625, "text": "# EasyQuant: An Efficient Data-free Quantization Algorithm for LLMs\nHanlin Tang   \n\nYifu Sun   \n\nDecheng Wu   \n\nKai Liu   \n\nJianchen Zhu   \n\nZhanhui Kang\n\n# Abstract\nLarge language models (LLMs) have proven to be very superior to conventional methods in various tasks. However, their expensive computations and high memory requirements are prohibitive for deployment. Model quantization is an effective method for reducing this overhead. The problem is that in most previous works, the quantized model was calibrated using a few samples from the training data, which might affect the generalization of the quantized LLMs to unknown cases and tasks. Hence in this work, we explore an important question: Can we design a data-free quantization method for LLMs to guarantee its generalization performance?  \n\nIn this work, we propose EasyQuant, a trainingfree and data-free weight-only quantization algorithm for LLMs. Our observation indicates that two factors: outliers in the weight and quantization ranges, are essential for reducing the quantization error. Therefore, in EasyQuant, we leave the outliers (less than $1\\%$ ) unchanged and optimize the quantization range to reduce the reconstruction error. With these methods, we surprisingly find that EasyQuant achieves comparable performance to the original model. Since EasyQuant does not depend on any training data, the generalization performance of quantized LLMs are safely guaranteed. Moreover, EasyQuant can be implemented in parallel so that the quantized model could be attained in a few minutes even for LLMs over 100B. To our best knowledge, we are the first work that achieves comparable performance with datadependent algorithms under a data-free setting and our algorithm runs over 10 times faster than the data-dependent methods.\n\n# 1 Introduction\nRecent work has already proved the superior performance of Transformer ( Vaswani et al. ,2017 )based LLMs ( Workshop ,2023 ;Zhang et al. ,2022 ;Touvron et al. ,2023 ;Brown et al. ,2020 ;Rae et al. ,2021 ;Smith et al. ,2022 ;Chowdhery et al. ,2022 ;Zeng et al. ,2022 ) on various tasks over traditional methods, and has attracted massive interest in how to improve and utilize those LLMs. However, the model size also grows dramatically along with improved performance. Hence the memory footprint and computational cost become the bottleneck for deploying those models. One promising solution to alleviate this overhead is model quantization ( Frantar et al. ,2023a ;Xiao et al. ,2023 ), where we quantize weight only or weight and activation both i order to reduce memory consumption and computational cost.  \n\nAlthough model quantization is a well-studied area for normal-sized models, such as BERT ( Devlin et al. ,2018 ) and GPT-2 ( Radford et al. ,2019 ), it is still a quite challenging task for LLMs. One major reason is that previous lossless model quantization algorithms require retraining for the quantized model, which is too expensive for models over billions of parameters. Beyond this, previous models are usually designed for specific domain tasks, which means the training data are sampled from limited task domains. However, recent LLMs are usually trained on various domains of data corpus, and they have shown to be quite effective for multi-domain zero-shot tasks. In this case, if we only retrain the quantized LLMs using partial domain corpus, the generalization ability of LLMs might get worse. Therefore both efficiency and generalization guarantees are very important for designing LLMs quantization algorithms. To date, for low-bits weight-only quantization, several posttraining algorithms have been proposed ( Frantar et al. ,2023a ;Yao et al. ,2022 ). However, those methods also require a small calibration set sampled from training data, which still takes at least several hours. Moreover, the use of those calibration data also brings the risk of making the model overfit to the calibration set.  \n\n  \nFigure 1: Pipeline of EasyQuant. We first find all the outliers in weight and keep them in full precision (fp32/fp16/bf16). Afterward, we optimize the quantization range (denoted as $q_{r a n g e})$ in order to approximate the normal values more precisely. In the end, the normal values are quantized into lower bits (denoted as $Q[\\cdot])$ with optimized quantization ranges and we set the outliers unchanged in weight.  \n\nOur Contribution: In this work, we propose a novel data-free model quantization algorithm, namely EasyQuant, that potentially improves the performance of low-bits quantized LLMs. The generalization ability of LLMs is inherently guaranteed since EasyQuant does not need any input data. By running EasyQuant for only a few minutes, we can quantize public-available OPT-176B, BLOOM176B, and LLAMA-65B into lower bits without significant loss on various benchmarks. To our best knowledge, this is the first data-free LLM quantization algorithm for LLM quantization without notable system overhead.  \n\nMoreover, our work reveals the essential factors that cause the performance degradation of the quantized LLMs. We show that the outliers in weights are more critical to the model\u2019s performance compared to the normal elements. Beyond this, we propose to use a gradient-based method for optimizing the quantization range. These two strategies can also be used in other scenarios, such as weight-activation quantization and quantizationaware training (QAT).  \n\nLast but not least, we develop efficient CUDA kernels for outlier isolation in dequantization, and proved that hold $1\\%$ outliers in weights unquantized brings negligible (less than $0.1\\%$ ) overhead w.r.t to overall latency. We also propose to implement EasyQuant in parallel for quantizing each weight in the model, which means a 175B-sized model can be quantized into 4 -bits within $10\\;\\mathrm{min}$ -utes."}], "task_step_question_answer": "f189b0a8-ec1b-4435-ae46-18924bfa2573:\u300c### \u6a21\u578b\u6027\u80fd\u4e0e\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u7684\u5e73\u8861\n\n\u5728\u63a2\u8ba8LayerNorm\u548cRMSNorm\u7684\u5dee\u5f02\u65f6\uff0c\u8ba1\u7b97\u6548\u7387\u4e0e\u6a21\u578b\u6027\u80fd\u7684\u5e73\u8861\u662f\u4e00\u4e2a\u6838\u5fc3\u95ee\u9898\u3002RMSNorm\u901a\u8fc7\u4ec5\u5f52\u4e00\u5316\u65b9\u5dee\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u5c24\u5176\u5728\u5927\u89c4\u6a21\u6a21\u578b\u8bad\u7ec3\u4e2d\u8868\u73b0\u51fa\u8272\u3002\u7136\u800c\uff0cLayerNorm\u5728NLP\u7b49\u4efb\u52a1\u4e2d\u56e0\u5176\u5bf9\u5747\u503c\u548c\u65b9\u5dee\u7684\u5168\u9762\u5f52\u4e00\u5316\uff0c\u901a\u5e38\u80fd\u5e26\u6765\u66f4\u597d\u7684\u6027\u80fd\u8868\u73b0\u3002\u4e3a\u4e86\u5728\u4e24\u8005\u4e4b\u95f4\u627e\u5230\u5e73\u8861\uff0c\u53ef\u4ee5\u8003\u8651\u4ee5\u4e0b\u7b56\u7565\uff1a\n\n1. **\u6df7\u5408\u5f52\u4e00\u5316\u65b9\u6cd5**\uff1a\u5728\u6a21\u578b\u7684\u4e0d\u540c\u5c42\u4e2d\uff0c\u6839\u636e\u4efb\u52a1\u9700\u6c42\u9009\u62e9\u6027\u5730\u4f7f\u7528LayerNorm\u6216RMSNorm\u3002\u4f8b\u5982\uff0c\u5728\u8ba1\u7b97\u8d44\u6e90\u53d7\u9650\u7684\u5c42\u4f7f\u7528RMSNorm\uff0c\u800c\u5728\u5bf9\u6027\u80fd\u8981\u6c42\u8f83\u9ad8\u7684\u5c42\u4fdd\u7559LayerNorm\u3002\n2. **\u52a8\u6001\u5f52\u4e00\u5316\u7b56\u7565**\uff1a\u901a\u8fc7\u5f15\u5165\u52a8\u6001\u8def\u7531\u673a\u5236\uff0c\u6839\u636e\u8f93\u5165\u6570\u636e\u7684\u7279\u6027\u81ea\u52a8\u9009\u62e9\u5f52\u4e00\u5316\u65b9\u6cd5\u3002\u4f8b\u5982\uff0c\u5bf9\u4e8e\u7b80\u5355\u4efb\u52a1\u6216\u4f4e\u590d\u6742\u5ea6\u6570\u636e\uff0c\u4f18\u5148\u4f7f\u7528RMSNorm\u4ee5\u8282\u7701\u8ba1\u7b97\u8d44\u6e90\uff1b\u5bf9\u4e8e\u590d\u6742\u4efb\u52a1\u6216\u9ad8\u590d\u6742\u5ea6\u6570\u636e\uff0c\u5219\u5207\u6362\u5230LayerNorm\u4ee5\u63d0\u5347\u6027\u80fd\u3002\n3. **\u91cf\u5316\u4e0e\u538b\u7f29\u6280\u672f**\uff1a\u7ed3\u5408\u6a21\u578b\u91cf\u5316\uff08\u5982EasyQuant\uff09\u548c\u538b\u7f29\u6280\u672f\uff0c\u8fdb\u4e00\u6b65\u964d\u4f4e\u5f52\u4e00\u5316\u64cd\u4f5c\u7684\u8ba1\u7b97\u5f00\u9500\u3002\u4f8b\u5982\uff0c\u901a\u8fc7\u91cf\u5316LayerNorm\u7684\u53c2\u6570\uff0c\u51cf\u5c11\u5176\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u3002\n\n### \u6570\u636e\u504f\u89c1\u548c\u4f26\u7406\u95ee\u9898\u7684\u89e3\u51b3\n\n\u5f52\u4e00\u5316\u65b9\u6cd5\u5728\u5904\u7406\u6570\u636e\u65f6\uff0c\u53ef\u80fd\u4f1a\u5bf9\u6570\u636e\u504f\u89c1\u4ea7\u751f\u5f71\u54cd\u3002LayerNorm\u548cRMSNorm\u5728\u5f52\u4e00\u5316\u8fc7\u7a0b\u4e2d\uff0c\u53ef\u80fd\u4f1a\u653e\u5927\u6216\u7f29\u5c0f\u67d0\u4e9b\u6570\u636e\u7279\u5f81\uff0c\u4ece\u800c\u5f15\u5165\u504f\u89c1\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u53ef\u4ee5\u91c7\u53d6\u4ee5\u4e0b\u63aa\u65bd\uff1a\n\n1. **\u504f\u89c1\u68c0\u6d4b\u4e0e\u6821\u6b63**\uff1a\u5728\u6a21\u578b\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u5f15\u5165\u504f\u89c1\u68c0\u6d4b\u673a\u5236\uff0c\u8bc6\u522b\u5f52\u4e00\u5316\u65b9\u6cd5\u53ef\u80fd\u5f15\u5165\u7684\u504f\u89c1\u3002\u4f8b\u5982\uff0c\u901a\u8fc7\u5206\u6790\u5f52\u4e00\u5316\u540e\u7684\u6570\u636e\u5206\u5e03\uff0c\u68c0\u6d4b\u662f\u5426\u5b58\u5728\u67d0\u4e9b\u7279\u5f81\u7684\u8fc7\u5ea6\u653e\u5927\u6216\u7f29\u5c0f\u3002\n2. **\u53c2\u6570\u8c03\u6574\u4e0e\u7ea6\u675f**\uff1a\u5728\u8bbe\u8ba1\u548c\u9009\u62e9\u5f52\u4e00\u5316\u65b9\u6cd5\u65f6\uff0c\u901a\u8fc7\u8c03\u6574\u5f52\u4e00\u5316\u53c2\u6570\u6216\u5f15\u5165\u989d\u5916\u7684\u7ea6\u675f\uff0c\u51cf\u5c11\u504f\u89c1\u7684\u5f15\u5165\u3002\u4f8b\u5982\uff0c\u5728LayerNorm\u4e2d\u5f15\u5165\u6b63\u5219\u5316\u9879\uff0c\u9650\u5236\u5176\u5bf9\u67d0\u4e9b\u7279\u5f81\u7684\u8fc7\u5ea6\u5f52\u4e00\u5316\u3002\n3. **\u591a\u89d2\u5ea6\u5206\u6790\u4e0e\u8bc4\u4f30**\uff1a\u4ece\u591a\u4e2a\u89d2\u5ea6\uff08\u5982\u516c\u5e73\u6027\u3001\u591a\u6837\u6027\u7b49\uff09\u8bc4\u4f30\u5f52\u4e00\u5316\u65b9\u6cd5\u7684\u5f71\u54cd\uff0c\u786e\u4fdd\u5176\u7b26\u5408\u4f26\u7406\u8981\u6c42\u3002\u4f8b\u5982\uff0c\u5728\u6a21\u578b\u8bc4\u4f30\u4e2d\u5f15\u5165\u516c\u5e73\u6027\u6307\u6807\uff0c\u8861\u91cf\u5f52\u4e00\u5316\u65b9\u6cd5\u5bf9\u4e0d\u540c\u7fa4\u4f53\u7684\u5f71\u54cd\u3002\n\n\u901a\u8fc7\u7efc\u5408\u8003\u8651\u6280\u672f\u4f18\u5316\u4e0e\u4f26\u7406\u95ee\u9898\uff0c\u53ef\u4ee5\u5728\u4fdd\u8bc1\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\uff0c\u6709\u6548\u964d\u4f4e\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\uff0c\u5e76\u51cf\u5c11\u6570\u636e\u504f\u89c1\u7684\u5f15\u5165\u3002\u300d", "ref_task_step_id": ""}, "__type__": "task_step_node"}, "a630c561-b046-4008-9e79-d2d2c64db138": {"__data__": {"id_": "a630c561-b046-4008-9e79-d2d2c64db138", "metadata": {}, "relationships": {}, "hash": "", "start_task_context": "\u5927\u6a21\u578b\u4e2d\u7684LayerNorm\u548cRMSNorm\u6709\u4ec0\u4e48\u533a\u522b\uff1f", "aemo_representation_context": "### Step-by-Step Decomposition of Computer Science Research Content\n\n#### 1. \u5206\u6790\u8fd1\u51e0\u5e74\u7814\u7a76\u9886\u57df\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba\n- **\u6280\u672f\u6846\u67b6**\uff1a\u8fd1\u5e74\u6765\uff0c\u6df1\u5ea6\u5b66\u4e60\u3001\u5f3a\u5316\u5b66\u4e60\u3001\u56fe\u795e\u7ecf\u7f51\u7edc\u7b49\u6846\u67b6\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u9886\u57df\u5360\u636e\u4e3b\u5bfc\u5730\u4f4d\u3002\u5177\u4f53\u6846\u67b6\u5982Transformer\u3001BERT\u3001GNN\u7b49\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002\n- **\u65b9\u6cd5\u8bba**\uff1a\u7814\u7a76\u65b9\u6cd5\u9010\u6e10\u4ece\u5355\u4e00\u6a21\u578b\u8f6c\u5411\u591a\u6a21\u578b\u878d\u5408\uff0c\u4ece\u76d1\u7763\u5b66\u4e60\u6269\u5c55\u5230\u65e0\u76d1\u7763\u548c\u81ea\u76d1\u7763\u5b66\u4e60\uff0c\u5f3a\u8c03\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002\n\n#### 2. \u7814\u7a76\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u4e3b\u8981\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0e\u53d8\u4f53\n- **\u5e94\u7528**\uff1a\u4f8b\u5982\uff0cTransformer\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u4e2d\u88ab\u5e7f\u6cdb\u5e94\u7528\uff0cBERT\u53d8\u4f53\u5982RoBERTa\u3001ALBERT\u5728\u6587\u672c\u5206\u7c7b\u3001\u95ee\u7b54\u7cfb\u7edf\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff1bGNN\u5728\u63a8\u8350\u7cfb\u7edf\u3001\u77e5\u8bc6\u56fe\u8c31\u7b49\u9886\u57df\u6709\u663e\u8457\u5e94\u7528\u3002\n- **\u53d8\u4f53**\uff1a\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\uff0c\u7814\u7a76\u8005\u63d0\u51fa\u4e86\u591a\u79cd\u53d8\u4f53\uff0c\u5982\u9488\u5bf9\u957f\u5e8f\u5217\u5904\u7406\u7684Transformer-XL\uff0c\u9488\u5bf9\u591a\u6a21\u6001\u6570\u636e\u7684ViLBERT\u7b49\u3002\n\n#### 3. \u8bc4\u4f30\u5b66\u672f\u754c\u7684\u6280\u672f\u8fdb\u6b65\u4e0e\u5c40\u9650\u6027\n- **\u6280\u672f\u8fdb\u6b65**\uff1a\u6a21\u578b\u6027\u80fd\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\uff0c\u8ba1\u7b97\u6548\u7387\u4e5f\u6709\u6240\u63d0\u9ad8\uff0c\u5982EfficientNet\u5728\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u8868\u73b0\u3002\n- **\u5c40\u9650\u6027**\uff1a\u4f9d\u7136\u5b58\u5728\u6a21\u578b\u504f\u5dee\uff08\u5982\u6027\u522b\u504f\u89c1\uff09\u3001\u6570\u636e\u4f9d\u8d56\uff08\u5982\u5bf9\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\uff09\u7b49\u95ee\u9898\uff0c\u6cdb\u5316\u80fd\u529b\u6709\u5f85\u8fdb\u4e00\u6b65\u63d0\u5347\u3002\n\n#### 4. \u63a2\u8ba8\u8ba1\u7b97\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\n- **\u9002\u7528\u6027**\uff1a\u7814\u7a76\u6a21\u578b\u5728\u4e0d\u540c\u9886\u57df\uff08\u5982\u533b\u7597\u3001\u91d1\u878d\uff09\u548c\u591a\u6a21\u6001\u6570\u636e\uff08\u5982\u6587\u672c+\u56fe\u50cf\uff09\u4e0a\u7684\u8868\u73b0\uff0c\u5982\u591a\u6a21\u6001Transformer\u6a21\u578b\u3002\n- **\u6cdb\u5316\u80fd\u529b**\uff1a\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\u3001\u5143\u5b66\u4e60\u7b49\u65b9\u6cd5\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f7f\u5176\u80fd\u591f\u5e94\u5bf9\u66f4\u591a\u73b0\u5b9e\u5e94\u7528\u573a\u666f\u3002\n\n#### 5. \u5206\u6790\u6700\u65b0\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027\n- **\u7a33\u5b9a\u6027**\uff1a\u7814\u7a76\u7b97\u6cd5\u5728\u590d\u6742\u3001\u52a8\u6001\u73af\u5883\u4e0b\u7684\u8868\u73b0\uff0c\u5982\u5bf9\u6297\u8bad\u7ec3\u3001\u9c81\u68d2\u4f18\u5316\u7b49\u65b9\u6cd5\u7684\u5e94\u7528\u3002\n- **\u5bb9\u9519\u6027**\uff1a\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\u7684\u9002\u5e94\u6027\uff0c\u5982\u5206\u5e03\u5f0f\u8bad\u7ec3\u4e2d\u7684\u68af\u5ea6\u538b\u7f29\u3001\u6a21\u578b\u5e76\u884c\u7b49\u6280\u672f\u3002\n\n#### 6. \u8bc4\u4f30\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218\n- **\u672a\u6765\u65b9\u5411**\uff1a\u5982\u53ef\u89e3\u91caAI\u3001\u8054\u90a6\u5b66\u4e60\u3001\u9690\u79c1\u4fdd\u62a4\u7b49\u65b0\u5174\u7814\u7a76\u65b9\u5411\u3002\n- **\u6311\u6218**\uff1a\u5982\u4f55\u5e73\u8861\u6a21\u578b\u6027\u80fd\u4e0e\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\uff0c\u5982\u4f55\u89e3\u51b3\u6570\u636e\u504f\u89c1\u548c\u4f26\u7406\u95ee\u9898\u7b49\u3002\n\n### \u5927\u6a21\u578b\u4e2d\u7684LayerNorm\u548cRMSNorm\u7684\u533a\u522b\n\n#### LayerNorm\uff08Layer Normalization\uff09\n- **\u5b9a\u4e49**\uff1aLayerNorm\u662f\u5bf9\u6bcf\u4e2a\u6837\u672c\u7684\u6bcf\u4e2a\u7279\u5f81\u8fdb\u884c\u5f52\u4e00\u5316\uff0c\u4f7f\u5176\u5747\u503c\u548c\u65b9\u5dee\u5206\u522b\u63a5\u8fd10\u548c1\u3002\n- **\u516c\u5f0f**\uff1a\n  \\[\n  \\text{LayerNorm}(x) = \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} \\cdot \\gamma + \\beta\n  \\]\n  \u5176\u4e2d\uff0c\\(\\mu\\)\u548c\\(\\sigma^2\\)\u5206\u522b\u662f\u7279\u5f81\u5747\u503c\u548c\u65b9\u5dee\uff0c\\(\\gamma\\)\u548c\\(\\beta\\)\u662f\u53ef\u5b66\u4e60\u53c2\u6570\uff0c\\(\\epsilon\\)\u662f\u5fae\u5c0f\u5e38\u6570\u4ee5\u9632\u6b62\u9664\u96f6\u3002\n- **\u5e94\u7528**\uff1a\u5e7f\u6cdb\u7528\u4e8eTransformer\u6a21\u578b\u4e2d\uff0c\u6709\u52a9\u4e8e\u7f13\u89e3\u68af\u5ea6\u6d88\u5931\u95ee\u9898\uff0c\u63d0\u5347\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002\n\n#### RMSNorm\uff08Root Mean Square Normalization\uff09\n- **\u5b9a\u4e49**\uff1aRMSNorm\u662f\u5bf9\u6bcf\u4e2a\u6837\u672c\u7684\u6bcf\u4e2a\u7279\u5f81\u8fdb\u884c\u5f52\u4e00\u5316\uff0c\u4f46\u53ea\u5f52\u4e00\u5316\u65b9\u5dee\uff0c\u4e0d\u5f52\u4e00\u5316\u5747\u503c\u3002\n- **\u516c\u5f0f**\uff1a\n  \\[\n  \\text{RMSNorm}(x) = \\frac{x}{\\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} x_i^2 + \\epsilon}} \\cdot \\gamma + \\beta\n  \\]\n  \u5176\u4e2d\uff0c\\(n\\)\u662f\u7279\u5f81\u6570\u91cf\uff0c\\(\\gamma\\)\u548c\\(\\beta\\)\u662f\u53ef\u5b66\u4e60\u53c2\u6570\uff0c\\(\\epsilon\\)\u662f\u5fae\u5c0f\u5e38\u6570\u3002\n- **\u5e94\u7528**\uff1a\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0cRMSNorm\u6bd4LayerNorm\u66f4\u7b80\u5355\uff0c\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\uff0c\u5c24\u5176\u5728\u5927\u89c4\u6a21\u6a21\u578b\u8bad\u7ec3\u4e2d\u3002\n\n#### \u4e3b\u8981\u533a\u522b\n1. **\u5f52\u4e00\u5316\u65b9\u5f0f**\uff1a\n   - LayerNorm\u540c\u65f6\u5f52\u4e00\u5316\u5747\u503c\u548c\u65b9\u5dee\u3002\n   - RMSNorm\u53ea\u5f52\u4e00\u5316\u65b9\u5dee\u3002\n2. **\u8ba1\u7b97\u590d\u6742\u5ea6**\uff1a\n   - LayerNorm\u9700\u8981\u8ba1\u7b97\u5747\u503c\u548c\u65b9\u5dee\uff0c\u8ba1\u7b97\u91cf\u7a0d\u5927\u3002\n   - RMSNorm\u53ea\u9700\u8ba1\u7b97\u65b9\u5dee\uff0c\u8ba1\u7b97\u91cf\u8f83\u5c0f\u3002\n3. **\u6548\u679c**\uff1a\n   - LayerNorm\u5728\u8bb8\u591a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u5728NLP\u9886\u57df\u3002\n   - RMSNorm\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u8868\u73b0\u76f8\u4f3c\u751a\u81f3\u66f4\u4f18\uff0c\u4e14\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u3002\n\n\u901a\u8fc7\u4e0a\u8ff0\u5206\u6790\uff0c\u53ef\u4ee5\u66f4\u597d\u5730\u7406\u89e3LayerNorm\u548cRMSNorm\u5728\u5927\u6a21\u578b\u4e2d\u7684\u5e94\u7528\u53ca\u5176\u4f18\u7f3a\u70b9\uff0c\u4ece\u800c\u5728\u5177\u4f53\u4efb\u52a1\u4e2d\u9009\u62e9\u5408\u9002\u7684\u5f52\u4e00\u5316\u65b9\u6cd5\u3002", "task_step_name": "LayerNorm\uff08Layer Normalization\uff09", "task_step_description": "LayerNorm\u662f\u5bf9\u6bcf\u4e2a\u6837\u672c\u7684\u6bcf\u4e2a\u7279\u5f81\u8fdb\u884c\u5f52\u4e00\u5316\uff0c\u4f7f\u5176\u5747\u503c\u548c\u65b9\u5dee\u5206\u522b\u63a5\u8fd10\u548c1\u3002\u5e7f\u6cdb\u7528\u4e8eTransformer\u6a21\u578b\u4e2d\uff0c\u6709\u52a9\u4e8e\u7f13\u89e3\u68af\u5ea6\u6d88\u5931\u95ee\u9898\uff0c\u63d0\u5347\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "task_step_level": "6", "task_step_question": "### \u95ee\u9898\n\n\u5728Transformer\u6a21\u578b\u4e2d\uff0cLayerNorm\u901a\u8fc7\u5f52\u4e00\u5316\u6bcf\u4e2a\u6837\u672c\u7684\u6bcf\u4e2a\u7279\u5f81\u7684\u5747\u503c\u548c\u65b9\u5dee\u6765\u63d0\u5347\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002\u7136\u800c\uff0cRMSNorm\u53ea\u5f52\u4e00\u5316\u65b9\u5dee\u800c\u4e0d\u5f52\u4e00\u5316\u5747\u503c\u3002\u57fa\u4e8e\u8fd9\u4e00\u533a\u522b\uff0c**\u5728\u4ec0\u4e48\u60c5\u51b5\u4e0b\uff0cRMSNorm\u53ef\u80fd\u6bd4LayerNorm\u66f4\u9002\u5408\u7528\u4e8e\u5927\u6a21\u578b\u7684\u8bad\u7ec3\uff1f** \u8bf7\u7ed3\u5408\u8ba1\u7b97\u590d\u6742\u5ea6\u3001\u8bad\u7ec3\u6548\u7387\u4ee5\u53ca\u6a21\u578b\u6027\u80fd\u7b49\u65b9\u9762\u8fdb\u884c\u5206\u6790\u3002", "task_step_question_context": [{"ref_id": "454845744505973136", "chunk_id": "6", "score": 0.302734375, "text": "# 3 Experiments and Results\nWe evaluate the performance of PreNorm and PostNorm for ZST on various datasets and language pairs. We then analyze the off-target rates and structural discrepancies between PreNorm and PostNorm to understand performance differences.  \n\n$$\n\\mathrm{LayerNorm}(\\mathbf{x})=\\frac{\\mathbf{x}-\\mathbf{E}(\\mathbf{x})}{\\sqrt{\\mathbf{V}(\\mathbf{x})}}\\cdot\\mathbf{g}+\\mathbf{b},\n$$  \n\nwhere $\\mathbf{g}$ and $\\mathbf{b}$ are trainable gain and bias. $\\mathbf{E}$ and $\\mathbf{V}$ indicate expectation and variance. LayerNorm is commonly used in two positions in the Transformer, as shown in Fig. 1 . PostNorm, which is the originally proposed setting of the Transformer ( Vaswani et al. ,2017 ), involves applying LayerNorm after each sub-module (i.e., selfattention or feed-forward network) and residual connections. PreNorm ( Baevski and Auli ,2019 ), on the other hand, involves applying LayerNorm directly before each sub-module and is known to stabilize Transformer training. While variants of Transformer LayerNorm like RMSNorm ( Zhang and Sennrich ,2019 ) have been proposed, the vanilla PreNorm and PostNorm are still the most widely adopted settings in current multilingual\n\n# 3.1 Experimental Settings\nDatasets We perform ZST experiments on three datasets: OPUS ( Zhang et al. ,2020 ), IWSLT ( Cettolo et al. ,2017 ), and Europarl ( Koehn ,2005 ). The statistics of the datasets are summarized in Table 1 .We include 7 ,4 , and 5 languages for each dataset. The training data consists of only English-centric sentence pairs, resulting in 30 ,6 , and 12 ZST directions for each dataset. The total number of parallel sentences for each dataset is 12 .00 M, 1 .38 M, and 15 .78 M, respectively. We apply BPE ( Sennrich et al. ,2016 ) with merge operations of 50 k, 40 k, and $50\\mathbf{k}$ to create a joint vocabulary for each dataset.  \n\nTraining We employ Transformer-base model for OPUS and IWSLT, and Transformer-big for Europarl, in accordance with the distinct sizes of training data. We consider the following settings: (1) PreNorm or PostNorm : PreNorm involves LayerNorm directly before each sub-module (i.e., self-attention or feed-forward network), while PostNorm applies LayerNorm after each sub-module and residual connections, as shown in Fig. 1 .(2) S-ENC-T-DEC or T-ENC : Source language tag on the encoder-side and target language tag on the decoder-side; or only target language tag on the encoder-side. Wu et al. (2021 ) showed that this setting impacts ZST for Transformer with PreNorm. (3) w/ or w/o Res. : With the residual connection for self-attention in the middle $(4^{t h})$ encoder layer or not. Liu et al. (2021 ) revealed that \u201cw/o Res.\u201d improves ZST for the model trained with PreNorm. We experiment this with different LayerNorm settings as this may reduce the potential of overfitting on supervised directions, then further impacts ZST, which aligns with our hypothesis.  \n\nTable 2: BLEU scores and off-target rates (shown in brackets) . We report the average score of three seeds; refer to Appendix Gfor BLEU score of each translation direction and seed. \u201cRes.\u201d indicates the residual connection of self-attention in the $4^{t h}$ encoder layer. We mark lower off-target rates and significantly higher BLEU scores ( Koehn ,2004 ) between PreNorm and PostNorm in bold for ZST.   \n\n\n<html><body><table><tr><td>#</td><td>Layer Norm</td><td>Language Tag</td><td>Res.</td><td></td><td>Zero-shot</td><td></td><td></td><td>Supervised</td><td></td></tr><tr><td>0</td><td></td><td>Pivot</td><td></td><td>OPUS 21.8</td><td>IWSLT 20.0</td><td>Europarl 29.5</td><td>OPUS</td><td>IWSLT</td><td>Europarl</td></tr><tr><td>1</td><td>PreNorm</td><td>S-ENC-T-DEC</td><td>w/</td><td>10.1 (42.19%)</td><td>4.9 (64.84%)</td><td>24.9 ( 7.73%)</td><td>33.7</td><td>31.5</td><td>34.3</td></tr><tr><td>2</td><td>PostNorm</td><td>S-ENC-T-DEC</td><td>w/</td><td>16.8 ( 8.59%)</td><td>12.4 (10.61%)</td><td>29.2( 0.34%)</td><td>33.9</td><td>31.5</td><td>34.5</td></tr><tr><td>3</td><td>PreNorm</td><td>T-ENC</td><td>w/</td><td>13.3 (22.99%)</td><td>13.7 ( 3.98%)</td><td>29.5( 0.23%)</td><td>33.7</td><td>31.6</td><td>34.4</td></tr><tr><td>4</td><td>PostNorm</td><td>T-ENC</td><td>w/</td><td>14.0 (22.86%)</td><td>15.5 ( 4.59%)</td><td>30.8 ( 0.11%)</td><td>34.1</td><td>31.5</td><td>34.5</td></tr><tr><td>5</td><td>PreNorm</td><td>S-ENC-T-DEC</td><td>w/o</td><td>14.3 (20.67%)</td><td>8.0 (50.16%)</td><td>16.7 (41.87%)</td><td>33.6</td><td>30.9</td><td>34.3</td></tr><tr><td>6</td><td>PostNorm</td><td>S-ENC-T-DEC</td><td>w/o</td><td>16.0 (15.27%)</td><td>17.4 (1.83%)</td><td>29.0 ( 0.41%)</td><td>33.8</td><td>30.7</td><td>34.4</td></tr><tr><td>7</td><td>PreNorm</td><td>T-ENC</td><td>w/o</td><td>13.4 (27.15%)</td><td>16.2 ( 1.54%)</td><td>29.9 ( 2.15%)</td><td>33.5</td><td>30.9</td><td>34.3</td></tr><tr><td>8</td><td>PostNorm</td><td>T-ENC</td><td>w/o</td><td>13.9 (26.68%)</td><td>17.8 (1.50%)</td><td>30.8 ( 0.13%)</td><td>33.9</td><td>30.6</td><td>34.4</td></tr></table></body></html>  \n\nThe settings above lead to eight different combinations, shown in Table 2 (#1 - #8). Additional training details are in Appendix A .\n\n# 3.2 Main Results\nWe evaluate ZST systems using SacreBLEU ( Post ,2018 ) and off-target rates. We report in Table 2 BLEU scores for both zero-shot and supervised directions. For ZST, we also present pivot-based translation results as a reference. Implementation details of evaluation can be found in Appendix B.Our findings are as follows:  \n\nPreNorm vs. PostNorm :We find that PostNorm consistently yields better BLEU scores than PreNorm for ZST across various language tag and residual connection settings, while their performance is comparable for supervised directions.  \n\nImpact of Language Tag and Residual Connection: We observe that using the \u201cT-ENC\u201d language tag and \u201cw/ Res.\u201d improves ZST performance for IWSLT, which aligns with the findings of $\\mathrm{Wu}$ et al. (2021 ) and Liu et al. (2021 ). Nevertheless, the best performance is achieved using \u201cw/ Res.\u201d for PostNorm with \u201cS-ENC-T-DEC\u201d and \u201cT-ENC\u201d tags for OPUS and Europarl, respectively (#2 and #4). Given that Wu et al. (2021 ) and Liu et al. (2021 )used PreNorm as the default setting (#2, #4, #6 and #8 are unreported results in their work), our results emphasize the need to consider PostNorm as the default setting for ZST, while the language tag and residual connection settings have less impact.  \n\nOff-target Rates : Off-target rates help understand the different BLEU score gaps between PreNorm and PostNorm, which ranges from 0 .5 to 12 .3 BLEU points. For PreNorm and PostNorm with the \u201cT-ENC\u201d language tag (#3, #4, #7, and #8), they have similar off-target rates, with a discrepancy ranging from $-0.61\\%$ to $2.02\\%$ , which results in narrow BLEU score gaps, ranging from 0 .5 to 1 .8 points. However, for PreNorm and PostNorm with the \u201cS-ENC-T-DEC\u201d language tag (#1, #2, #5, and #6), the off-target rates show a more considerable discrepancy, ranging from $5.40\\%$ to $54.23\\%$ , resulting in BLEU score gaps from 1 .7 to 12 .3 points. Further analysis of the nature of Transformer hidden states in the next section explores the reason for these different off-target rates in translations."}, {"ref_id": "454895316331138176", "chunk_id": "10", "score": 0.236328125, "text": "# 6 Related Work\nModel Quantization Traditional model quantization algorithms mainly focus on the cases where both parameters and activations of the model are quantized ( Lin et al. ,2015 ;Hubara et al. ,2016 ;Tailor et al. ,2021 ;Ni et al. ,2020 ). However, directly quantizing the model will greatly decrease the accuracy of the models, and one important technique to improve the performance is Quantization Aware Training (QAT) ( Jacob et al. ,2018 ), where it simulates the quantization procedure in training to improve the accuracy of the quantized model further. For Transformer based models, the boundary of the compression level has been continuously advanced. For example, 8 -bits quantized transformers as in FullyQT ( Prato et al. ,2019 ) and Q8BERT (Zafrir et al. ,2019 ), 4 -bits quantized BERT in Wu et al. (2023 ) and tenary case as in TernaryBERT (Zhang et al. ,2020 ).  \n\nModel Quantization for LLMs. For quantizing LLMs, due to their prohibitive training expense, we can only use a few training data for calibration. There are two major directions: 1) weight-only quantization, where the weights are quantized into lower bits. In Frantar et al. (2023a ); Yao et al. (2022 ), authors optimize the output error on the calibration set using OBS and gradient descent. 2)  \n\nActivation and weight quantization, where both activations and weights are quantized into lower bits. In this case, the major obstacle is the outliers in activations. LLM.int8() ( Dettmers et al. ,2022 ) addresses this problem by isolating those outliers in fp16/bf16. However, such implementation leads to large latency overhead and is even slower than fp16 inference. Recent studies ( Wei et al. ,2023 ;Xiao et al. ,2023 ) found that the outliers only exist in certain channels, and use the LayerNorm weights ( Wei et al. ,2023 ) and calibrated scales ( Xiao et al. ,2023 )to smooth those channels. Xiao et al. (2023 ) has already proved that we can achieve almost lossless W8A8 quantized LLMs using a few calibration data, without manipulating the original model weights.\n\n# 7 Conclusion and Limitations\nIn this paper, we propose a data-free fast weightonly quantization algorithm, namely EasyQuant, for LLMs, that potentially improves the quantized model\u2019s performance without using any training data. Our analysis reveals the intrinsic origins of the performance loss when quantizing the model weights into lower bits. We show that by isolating the outliers from quantization, the accuracy of the quantized LLM increases accordingly with decreased reconstruction error. Our experiment proved that EasyQuant significantly outperforms RTN in a data-free setting, and also behaves better than data-dependent algorithms. EasyQuant can finish the quantization for a 176B-sized model within 10 minutes and the overhead of dequantization in EasyQuant is negligible.  \n\nHowever, we also point out some limitations of our work: The outlier recovery functionality in EasyQuant requires extra CUDA kernels for implementation. Moreover, weight-only quantization can only reduce the memory footprint without any computation cost reduction, hence the latency of our model cannot be minimized. In addition, this outlier isolation will make the weight/activation quantization more challenging because the weight includes numbers under different precision. We have also noticed that EasyQuantcannot outperform the data-dependent methods in all tasks, this motivates us to investigate more effective algorithms in future studies.\n\n\n\n# A Appendix\nTable 10: Perplexity and zershot results for BLOOM model family   \n\n\n<html><body><table><tr><td rowspan=\"2\"></td><td rowspan=\"2\"></td><td colspan=\"3\">Perplexity-based Task</td><td colspan=\"4\">Zero-shot Task</td></tr><tr><td>WikiText2</td><td>PTB</td><td>C4</td><td>PIQA</td><td>ARC-easy</td><td>ARC-Challenge StoryCloze</td><td></td></tr><tr><td rowspan=\"2\">BLOOM</td><td>fp16</td><td>22.42</td><td>43.69</td><td>26.6</td><td>65.07%</td><td>41.71%</td><td>24.15%</td><td>61.94%</td></tr><tr><td>RTN</td><td>25.90</td><td>51.10</td><td>29.89</td><td>63.11%</td><td>39.40%</td><td>23.89%</td><td>60.15%</td></tr><tr><td rowspan=\"2\">560M</td><td>GPTQ</td><td>24.03</td><td>46.97</td><td>28</td><td>64.31%</td><td>40.24%</td><td>23.46%</td><td>61.17%</td></tr><tr><td>EasyQuant</td><td>23.74</td><td>46.86</td><td>28.03</td><td>63.06%</td><td>40.32%</td><td>24.15%</td><td>59.64%</td></tr><tr><td rowspan=\"2\">BLOOM</td><td>fp16</td><td>17.69</td><td>57.96</td><td>22.05</td><td>67.14%</td><td>45.41%</td><td>25.68%</td><td>63.27%</td></tr><tr><td>RTN</td><td>22.00</td><td>66.85</td><td>24.44</td><td>65.29%</td><td>42.51%</td><td>23.34%</td><td>60.66%</td></tr><tr><td rowspan=\"2\">1.1B</td><td>GPTQ</td><td>19.05</td><td>62.48</td><td>23.25</td><td>66.05%</td><td>44.49%</td><td>25.51%</td><td>62.32%</td></tr><tr><td>EasyQuant</td><td>18.51</td><td>61.83</td><td>22.94</td><td>66.65%</td><td>43.73%</td><td>25.51%</td><td>62.06%</td></tr><tr><td rowspan=\"2\">BLOOM</td><td>fp16</td><td>15.39</td><td>30.00</td><td>19.49</td><td>69.97%</td><td>48.11%</td><td>26.79 %</td><td>65.44%</td></tr><tr><td>RTN</td><td>16.97</td><td>33.58</td><td>21.26</td><td>67.74%</td><td>44.70%</td><td>26.45 %</td><td>62.95%</td></tr><tr><td rowspan=\"2\">1.7B</td><td>GPTQ</td><td>16.48</td><td>31.84</td><td>20.55</td><td>68.77%</td><td>44.49%</td><td>25.94%</td><td>64.48%</td></tr><tr><td>EasyQuant</td><td>16.01</td><td>31.50</td><td>20.15</td><td>68.99%</td><td>46.89%</td><td>26.19%</td><td>65.37%</td></tr><tr><td rowspan=\"2\">BLOOM</td><td>fp16</td><td>13.48</td><td>25.34</td><td>17.49</td><td>70.51%</td><td>53.24%</td><td>30.55 %</td><td>67.79%</td></tr><tr><td>RTN</td><td>14.76</td><td>27.68</td><td>18.76</td><td>69.86%</td><td>51.35%</td><td>29.52%</td><td>67.09%</td></tr><tr><td rowspan=\"2\">3B</td><td>GPTQ</td><td>14.2</td><td>26.49</td><td>18.1</td><td>69.42%</td><td>52.82%</td><td>28.92%</td><td>67.22%</td></tr><tr><td>EasyQuant</td><td>14.01</td><td>26.12</td><td>17.96</td><td>69.80%</td><td>50.72%</td><td>28.58%</td><td>67.35%</td></tr><tr><td rowspan=\"2\">BLOOM</td><td>fp16</td><td>11.37</td><td>20.83</td><td>15.20</td><td>73.72%</td><td>57.37%</td><td>33.45 %</td><td>71.99%</td></tr><tr><td>RTN</td><td>12.10</td><td>22.42</td><td>16.06</td><td>72.69%</td><td>56.14%</td><td>32.17 %</td><td>70.72%</td></tr><tr><td rowspan=\"2\">7.1B</td><td>GPTQ</td><td>11.73</td><td>21.67</td><td>15.6</td><td>72.96%</td><td>56.14%</td><td>32.25%</td><td>71.36%</td></tr><tr><td>EasyQuant</td><td>11.66</td><td>21.47</td><td>15.52</td><td>73.23%</td><td>55.72%</td><td>32.51 %</td><td>71.10%</td></tr><tr><td rowspan=\"2\">BLOOM</td><td>fp16</td><td>8.11</td><td>14.59</td><td>11.71</td><td>79.16%</td><td>67.47%</td><td>44.97 %</td><td>76.89%</td></tr><tr><td>RTN</td><td>8.37</td><td>15.00</td><td>12.04</td><td>79.00%</td><td>66.33%</td><td>43.17 %</td><td>76.00%</td></tr><tr><td rowspan=\"2\">176B</td><td>GPTQ</td><td>8.21</td><td>14.75</td><td>11.81</td><td>79.00%</td><td>67.42%</td><td>44.10%</td><td>76.32%</td></tr><tr><td>EasyQuant</td><td>8.21</td><td>14.75</td><td></td><td>11.87 79.05%</td><td>67.8%</td><td>44.45%</td><td>77.28%</td></tr></table></body></html>"}, {"ref_id": "454845727870837706", "chunk_id": "4", "score": 0.171875, "text": "# 6 Conclusion\nLLM-based RL algorithms have shown generalization across multiple tasks and games. We argue that this ability comes from implicit memory that fits a large number of parameters to the training data, which is inefficient in terms of model size. In contrast, we propose a new approach inspired by the concept of \u201cworking memory\u201d called Decision Transformers with Memory (DT-Mem), which stores training experience explicitly in a content-addressable matrix module for later retrieval and use. Evaluation demonstrates that DT-Mem achieves better generalization on Atari games with only $10\\%$ of the model parameters compared to the state-of-the-art method. Furthermore, we demonstrate that fine-tuning DT-Memwith a small amount of data can produce state-of-the-art results on both Atari games and the Meta-World environment, when compared to MDT [22], PDT [37], and HDT [38].  \n\nLimitations The first limitation of our work is the sample efficiency of memory fine-tuning. The $10\\%$ fine-tuning dataset is still sizeable, and we plan to explore more sample-efficient methods in the future. We could for instance consider a setting with more tasks, each one with less data so that the inter-task generalization would be even more crucial to its performance. Additionally, this work does not propose a control strategy for collecting data on a new task. For future work, we plan to investigate online data collection methods, which includes the design and learning of exploration strategies for an efficient fine-tuning on new tasks. Finally, the approach has been intuitively motivated, but it would be valuable to have a theoretical grounding that would show the structural limits of large models and how equipping them with a memory component overcomes them.  \n\nSocietal Impact We do not foresee any significant societal impact resulting from our proposed method. The current algorithm is not designed to interact with humans, nor any realistic environment yet. If one chooses to extend our methods to such situations, caution should be exercised to ensure that any safety and ethical concerns are appropriately addressed. As our work is categorized in the offline-RL domain, it is feasible to supplement its training with a dataset that aligns with human intents and values. However, one must be wary that the way our architecture generalizes across tasks is still not well understood and as a consequence we cannot guarantee the generalization of its desirable features: performance, robustness, fairness, etc. By working towards methods that improve the computational efficiency of large models, we contribute to increase their access and reduce their ecological impact.\n\n\n\n# A Implementation Details\n\n# A.1 DT-Mem network architecture\nTable 3 summarizes the different model configurations used for evaluation. In this section, we describe these model configurations in detail. While Table 3 provides a summary, we will also provide additional information here. DT-Mem, PDT and HDT are all share the same transformer architectures. However, for task-adaptation, HDT utilizes a pre-trained $2.3\\mathbf{M}$ hyper-network, while DT-Mem introduces 147K LoRA parameters. To compare with MDT, we use the same parameter size as reported in [22].  \n\nTable 3: Detailed Model Sizes   \n\n\n<html><body><table><tr><td>Model</td><td>Layers</td><td>Hidden size (d)</td><td>Heads</td><td>Params</td><td>Memory Size</td><td>Memory Module Params</td></tr><tr><td>HDT</td><td>4</td><td>512</td><td>8</td><td>13M</td><td>N.A.</td><td>N.A.</td></tr><tr><td>MDT-200M</td><td>10</td><td>1280</td><td>20</td><td>200M</td><td>N.A.</td><td>N.A.</td></tr><tr><td>DT-Mem</td><td>4</td><td>512</td><td>8</td><td>13M</td><td>559K</td><td>7M</td></tr></table></body></html>\n\n# A.2 Hyper-parameters\nIn this section, we will delve into the specifics of the model parameters. Understanding these parameters is key to understanding the workings of the model. It is worth noting that the source code for this model is publicly available at https://github.com/luciferkonn/DT_Mem/tree/main .This allows for a deeper understanding of the model\u2019s inner workings and may facilitate the replication of its results.  \n\nTable 4: Hyperparameters for DT-Mem training   \n\n\n<html><body><table><tr><td>Hyperparameters</td><td>Value</td></tr><tr><td>K (length of context)</td><td>28</td></tr><tr><td>dropoutrate</td><td>0.1</td></tr><tr><td>maximum epochs</td><td>1000</td></tr><tr><td>steps for each epoch</td><td>1000</td></tr><tr><td>optimizer learning rate</td><td>1e-4</td></tr><tr><td>weight decay</td><td>1e-4</td></tr><tr><td>gradient norm clip</td><td>1.</td></tr><tr><td>data points for each dataset</td><td>500,000</td></tr><tr><td>batch size</td><td>64</td></tr><tr><td>memory slots</td><td>1290</td></tr><tr><td>activation</td><td>GELU</td></tr><tr><td>optimizer</td><td>Adamw</td></tr><tr><td>scheduler</td><td>LambdaLR</td></tr></table></body></html>\n\n# A.3 Training and fine-tuning algorithm\nIn this section, we present the pre-training DT-Memin Appendix A.3 and fine-tuning DT-Mem with LoRA in Appendix 5.5.  \n\nWe pre-train DT-Mem on multiple offline datasets. Each gradient update of the DT-Memmodel considers information from each training task.  \n\nWe fine-tune the memory module to adapt to each downstream task. To achieve this, we fix the pre-trained DT-Mem model parameters and add additional LoRA parameters for the memory module feed-forward neural networks. The fine-tune dataset is used to update these LoRA parameters only.\n\n# Algorithm 1 Pre-train DT-Mem\n1: for T episodes do   \n2: 3: 4: for Sample trajectories Split trajectories into different segments with length K and calculate return-to-go in the Task $\\mathcal{T}_{i}\\in T^{t r a i n}\\;.$ do $\\tau=(s_{0},a_{0},r_{0},\\cdot\\cdot\\cdot\\,,s_{H},a_{H},r_{H})$ m the dataset $\\mathcal{D}_{i}$ .  \ninput sequence.   \n5: Given $\\hat{\\tau}_{t+1:t+K}$ , compute the sequence embedding $e_{s e q}$ .  \n6: Update the working memory and retrieve the relative information as $E_{o u t}$   \n7: Given $E_{o u t}$ , predict actions $\\tilde{a}_{t}$ , reward $\\tilde{r}_{t}$ , and return-to-go ${\\tilde{R}}_{t}$ .  \n8: Compute the loss according to Eqn. 1.   \n9: Update all modules parameters.   \n10: end for   \n11: end for  \n\nAlgorithm 2 Fine-tuning DT-Mem  \n\n$\\hat{B}^{q},\\hat{B}^{k},\\hat{B}^{v},\\hat{A}^{q},\\hat{A}^{k},\\hat{A}^{v},B^{q},A^{q},B^{k},A^{k}$ Require: Fine-tuning dataset $\\mathcal{T}^{i}~\\in~T^{t e s t}$ .dataset $\\mathcal{D}^{i}$ for $\\mathcal{T}^{i}$ .Initialize LoRA parameters 1: for T steps do   \n2: Split trajectories into different segments with length $\\mathbf{K}$ and calculate return-to-go in the input sequence.   \n3: Given $\\hat{\\tau}_{t+1:t+K}$ , compute the sequence embedding $e_{s e q}$ .  \n4: Update working memory using $\\hat{Q}\\,=\\,M(\\hat{W}^{q}+\\hat{B}^{q}\\bar{A}^{q})$ ,$\\hat{K}\\,=\\,M(\\hat{W}^{k}\\,+\\,\\hat{B}^{k}\\hat{A}^{k}),\\hat{V}\\,=$ $M(\\hat{W}^{v}+\\hat{B}^{v}\\hat{A}^{v})$ ,$Q=M(W^{q}+B^{q}A^{q}),K=M(W^{k}+B^{k}A^{k})$   \n5: Retrieve the relative information as $E_{o u t}$   \n6: Given $E_{o u t}$ , predict actions $\\tilde{a}_{t}$ , reward $\\tilde{r}_{t}$ , and return-to-go ${\\tilde{R}}_{t}$ .  \n7: Compute the loss according to Eqn. 1.   \n8: Update LoRA parameters only.   \n9: end for"}], "task_step_question_answer": "a630c561-b046-4008-9e79-d2d2c64db138:\u300c\u5728Transformer\u6a21\u578b\u4e2d\uff0cRMSNorm\u76f8\u8f83\u4e8eLayerNorm\u5728\u67d0\u4e9b\u573a\u666f\u4e0b\u53ef\u80fd\u66f4\u9002\u5408\u5927\u6a21\u578b\u7684\u8bad\u7ec3\uff0c\u4e3b\u8981\u539f\u56e0\u53ef\u4ee5\u4ece\u4ee5\u4e0b\u51e0\u4e2a\u65b9\u9762\u8fdb\u884c\u5206\u6790\uff1a\n\n### 1. **\u8ba1\u7b97\u590d\u6742\u5ea6**\nRMSNorm\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u4f4e\u4e8eLayerNorm\u3002LayerNorm\u9700\u8981\u5bf9\u6bcf\u4e2a\u6837\u672c\u7684\u6bcf\u4e2a\u7279\u5f81\u8fdb\u884c\u5747\u503c\u548c\u65b9\u5dee\u7684\u5f52\u4e00\u5316\uff0c\u800cRMSNorm\u4ec5\u5f52\u4e00\u5316\u65b9\u5dee\uff0c\u7701\u7565\u4e86\u5747\u503c\u7684\u8ba1\u7b97\u3002\u5bf9\u4e8e\u5927\u6a21\u578b\u800c\u8a00\uff0c\u8fd9\u79cd\u8ba1\u7b97\u4e0a\u7684\u7b80\u5316\u53ef\u4ee5\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\uff0c\u5c24\u5176\u662f\u5728\u6df1\u5ea6\u7f51\u7edc\u6216\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\uff0c\u8fd9\u79cd\u4f18\u5316\u80fd\u591f\u5e26\u6765\u66f4\u5feb\u7684\u8bad\u7ec3\u901f\u5ea6\u3002\n\n### 2. **\u8bad\u7ec3\u6548\u7387**\nRMSNorm\u7531\u4e8e\u7701\u7565\u4e86\u5747\u503c\u7684\u5f52\u4e00\u5316\uff0c\u51cf\u5c11\u4e86\u68af\u5ea6\u8ba1\u7b97\u4e2d\u7684\u4f9d\u8d56\u5173\u7cfb\uff0c\u4ece\u800c\u53ef\u80fd\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u3002\u7279\u522b\u662f\u5728\u5927\u6a21\u578b\u4e2d\uff0cLayerNorm\u7684\u5747\u503c\u5f52\u4e00\u5316\u53ef\u80fd\u4f1a\u5f15\u5165\u989d\u5916\u7684\u68af\u5ea6\u566a\u58f0\uff0c\u800cRMSNorm\u901a\u8fc7\u7b80\u5316\u5f52\u4e00\u5316\u8fc7\u7a0b\uff0c\u80fd\u591f\u66f4\u7a33\u5b9a\u5730\u4f20\u9012\u68af\u5ea6\uff0c\u51cf\u5c11\u8bad\u7ec3\u4e2d\u7684\u6ce2\u52a8\u3002\n\n### 3. **\u6a21\u578b\u6027\u80fd**\n\u5728\u67d0\u4e9b\u4efb\u52a1\u4e2d\uff0cRMSNorm\u7684\u6027\u80fd\u8868\u73b0\u4e0eLayerNorm\u76f8\u5f53\u751a\u81f3\u66f4\u597d\u3002\u4f8b\u5982\uff0c\u5728\u751f\u6210\u4efb\u52a1\u6216\u8bed\u8a00\u6a21\u578b\u4e2d\uff0cRMSNorm\u80fd\u591f\u66f4\u597d\u5730\u6355\u6349\u6570\u636e\u7684\u5206\u5e03\u7279\u6027\uff0c\u5c24\u5176\u662f\u5728\u6570\u636e\u5206\u5e03\u8f83\u4e3a\u590d\u6742\u6216\u5b58\u5728\u957f\u5c3e\u5206\u5e03\u7684\u60c5\u51b5\u4e0b\u3002\u6b64\u5916\uff0cRMSNorm\u7684\u7b80\u5316\u8bbe\u8ba1\u4f7f\u5176\u5728\u6a21\u578b\u89c4\u6a21\u6269\u5927\u65f6\uff0c\u80fd\u591f\u66f4\u597d\u5730\u4fdd\u6301\u6027\u80fd\u7684\u7a33\u5b9a\u6027\uff0c\u907f\u514d\u56e0\u5f52\u4e00\u5316\u8fc7\u7a0b\u590d\u6742\u5316\u800c\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\u3002\n\n### 4. **\u5185\u5b58\u5360\u7528**\nRMSNorm\u7684\u5185\u5b58\u5360\u7528\u76f8\u5bf9\u8f83\u4f4e\uff0c\u56e0\u4e3a\u5b83\u4e0d\u9700\u8981\u5b58\u50a8\u5747\u503c\u7684\u8ba1\u7b97\u7ed3\u679c\u3002\u5bf9\u4e8e\u5927\u6a21\u578b\u800c\u8a00\uff0c\u5185\u5b58\u4f18\u5316\u662f\u4e00\u4e2a\u91cd\u8981\u7684\u8003\u8651\u56e0\u7d20\uff0c\u5c24\u5176\u662f\u5728GPU\u6216TPU\u7b49\u786c\u4ef6\u8d44\u6e90\u6709\u9650\u7684\u60c5\u51b5\u4e0b\uff0cRMSNorm\u80fd\u591f\u66f4\u9ad8\u6548\u5730\u5229\u7528\u5185\u5b58\u8d44\u6e90\u3002\n\n### 5. **\u5b9e\u9a8c\u9a8c\u8bc1**\n\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u67d0\u4e9b\u5927\u89c4\u6a21\u6a21\u578b\uff08\u5982GPT-3\u3001BERT\u7b49\uff09\u4e2d\uff0cRMSNorm\u5728\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\uff0c\u80fd\u591f\u663e\u8457\u51cf\u5c11\u8bad\u7ec3\u65f6\u95f4\u548c\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u3002\u4f8b\u5982\uff0c\u5728LLM.int8()\u7b49\u91cf\u5316\u65b9\u6cd5\u4e2d\uff0cRMSNorm\u88ab\u7528\u4e8e\u5e73\u6ed1\u6fc0\u6d3b\u503c\u4e2d\u7684\u5f02\u5e38\u503c\uff0c\u4ece\u800c\u5728\u91cf\u5316\u8fc7\u7a0b\u4e2d\u4fdd\u6301\u6a21\u578b\u7684\u7cbe\u5ea6\u3002\n\n### \u7ed3\u8bba\n\u7efc\u4e0a\u6240\u8ff0\uff0cRMSNorm\u5728\u5927\u6a21\u578b\u8bad\u7ec3\u4e2d\u7684\u4f18\u52bf\u4e3b\u8981\u4f53\u73b0\u5728\u8ba1\u7b97\u590d\u6742\u5ea6\u4f4e\u3001\u8bad\u7ec3\u6548\u7387\u9ad8\u3001\u5185\u5b58\u5360\u7528\u5c11\u4ee5\u53ca\u5728\u67d0\u4e9b\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u8868\u73b0\u4f18\u5f02\u3002\u56e0\u6b64\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u6216\u9700\u8981\u9ad8\u6548\u8bad\u7ec3\u7684\u573a\u666f\u4e0b\uff0cRMSNorm\u53ef\u80fd\u6bd4LayerNorm\u66f4\u9002\u5408\u7528\u4e8e\u5927\u6a21\u578b\u7684\u8bad\u7ec3\u3002\u7136\u800c\uff0c\u5177\u4f53\u9009\u62e9\u4ecd\u9700\u6839\u636e\u4efb\u52a1\u9700\u6c42\u548c\u6a21\u578b\u7279\u6027\u8fdb\u884c\u6743\u8861\u3002\u300d", "ref_task_step_id": ""}, "__type__": "task_step_node"}, "1cabe3a5-9d12-494f-bccd-63795401d3a1": {"__data__": {"id_": "1cabe3a5-9d12-494f-bccd-63795401d3a1", "metadata": {}, "relationships": {}, "hash": "", "start_task_context": "\u5927\u6a21\u578b\u4e2d\u7684LayerNorm\u548cRMSNorm\u6709\u4ec0\u4e48\u533a\u522b\uff1f", "aemo_representation_context": "### Step-by-Step Decomposition of Computer Science Research Content\n\n#### 1. \u5206\u6790\u8fd1\u51e0\u5e74\u7814\u7a76\u9886\u57df\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba\n- **\u6280\u672f\u6846\u67b6**\uff1a\u8fd1\u5e74\u6765\uff0c\u6df1\u5ea6\u5b66\u4e60\u3001\u5f3a\u5316\u5b66\u4e60\u3001\u56fe\u795e\u7ecf\u7f51\u7edc\u7b49\u6846\u67b6\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u9886\u57df\u5360\u636e\u4e3b\u5bfc\u5730\u4f4d\u3002\u5177\u4f53\u6846\u67b6\u5982Transformer\u3001BERT\u3001GNN\u7b49\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002\n- **\u65b9\u6cd5\u8bba**\uff1a\u7814\u7a76\u65b9\u6cd5\u9010\u6e10\u4ece\u5355\u4e00\u6a21\u578b\u8f6c\u5411\u591a\u6a21\u578b\u878d\u5408\uff0c\u4ece\u76d1\u7763\u5b66\u4e60\u6269\u5c55\u5230\u65e0\u76d1\u7763\u548c\u81ea\u76d1\u7763\u5b66\u4e60\uff0c\u5f3a\u8c03\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002\n\n#### 2. \u7814\u7a76\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u4e3b\u8981\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0e\u53d8\u4f53\n- **\u5e94\u7528**\uff1a\u4f8b\u5982\uff0cTransformer\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u4e2d\u88ab\u5e7f\u6cdb\u5e94\u7528\uff0cBERT\u53d8\u4f53\u5982RoBERTa\u3001ALBERT\u5728\u6587\u672c\u5206\u7c7b\u3001\u95ee\u7b54\u7cfb\u7edf\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff1bGNN\u5728\u63a8\u8350\u7cfb\u7edf\u3001\u77e5\u8bc6\u56fe\u8c31\u7b49\u9886\u57df\u6709\u663e\u8457\u5e94\u7528\u3002\n- **\u53d8\u4f53**\uff1a\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\uff0c\u7814\u7a76\u8005\u63d0\u51fa\u4e86\u591a\u79cd\u53d8\u4f53\uff0c\u5982\u9488\u5bf9\u957f\u5e8f\u5217\u5904\u7406\u7684Transformer-XL\uff0c\u9488\u5bf9\u591a\u6a21\u6001\u6570\u636e\u7684ViLBERT\u7b49\u3002\n\n#### 3. \u8bc4\u4f30\u5b66\u672f\u754c\u7684\u6280\u672f\u8fdb\u6b65\u4e0e\u5c40\u9650\u6027\n- **\u6280\u672f\u8fdb\u6b65**\uff1a\u6a21\u578b\u6027\u80fd\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\uff0c\u8ba1\u7b97\u6548\u7387\u4e5f\u6709\u6240\u63d0\u9ad8\uff0c\u5982EfficientNet\u5728\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u8868\u73b0\u3002\n- **\u5c40\u9650\u6027**\uff1a\u4f9d\u7136\u5b58\u5728\u6a21\u578b\u504f\u5dee\uff08\u5982\u6027\u522b\u504f\u89c1\uff09\u3001\u6570\u636e\u4f9d\u8d56\uff08\u5982\u5bf9\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\uff09\u7b49\u95ee\u9898\uff0c\u6cdb\u5316\u80fd\u529b\u6709\u5f85\u8fdb\u4e00\u6b65\u63d0\u5347\u3002\n\n#### 4. \u63a2\u8ba8\u8ba1\u7b97\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\n- **\u9002\u7528\u6027**\uff1a\u7814\u7a76\u6a21\u578b\u5728\u4e0d\u540c\u9886\u57df\uff08\u5982\u533b\u7597\u3001\u91d1\u878d\uff09\u548c\u591a\u6a21\u6001\u6570\u636e\uff08\u5982\u6587\u672c+\u56fe\u50cf\uff09\u4e0a\u7684\u8868\u73b0\uff0c\u5982\u591a\u6a21\u6001Transformer\u6a21\u578b\u3002\n- **\u6cdb\u5316\u80fd\u529b**\uff1a\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\u3001\u5143\u5b66\u4e60\u7b49\u65b9\u6cd5\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f7f\u5176\u80fd\u591f\u5e94\u5bf9\u66f4\u591a\u73b0\u5b9e\u5e94\u7528\u573a\u666f\u3002\n\n#### 5. \u5206\u6790\u6700\u65b0\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027\n- **\u7a33\u5b9a\u6027**\uff1a\u7814\u7a76\u7b97\u6cd5\u5728\u590d\u6742\u3001\u52a8\u6001\u73af\u5883\u4e0b\u7684\u8868\u73b0\uff0c\u5982\u5bf9\u6297\u8bad\u7ec3\u3001\u9c81\u68d2\u4f18\u5316\u7b49\u65b9\u6cd5\u7684\u5e94\u7528\u3002\n- **\u5bb9\u9519\u6027**\uff1a\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\u7684\u9002\u5e94\u6027\uff0c\u5982\u5206\u5e03\u5f0f\u8bad\u7ec3\u4e2d\u7684\u68af\u5ea6\u538b\u7f29\u3001\u6a21\u578b\u5e76\u884c\u7b49\u6280\u672f\u3002\n\n#### 6. \u8bc4\u4f30\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218\n- **\u672a\u6765\u65b9\u5411**\uff1a\u5982\u53ef\u89e3\u91caAI\u3001\u8054\u90a6\u5b66\u4e60\u3001\u9690\u79c1\u4fdd\u62a4\u7b49\u65b0\u5174\u7814\u7a76\u65b9\u5411\u3002\n- **\u6311\u6218**\uff1a\u5982\u4f55\u5e73\u8861\u6a21\u578b\u6027\u80fd\u4e0e\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\uff0c\u5982\u4f55\u89e3\u51b3\u6570\u636e\u504f\u89c1\u548c\u4f26\u7406\u95ee\u9898\u7b49\u3002\n\n### \u5927\u6a21\u578b\u4e2d\u7684LayerNorm\u548cRMSNorm\u7684\u533a\u522b\n\n#### LayerNorm\uff08Layer Normalization\uff09\n- **\u5b9a\u4e49**\uff1aLayerNorm\u662f\u5bf9\u6bcf\u4e2a\u6837\u672c\u7684\u6bcf\u4e2a\u7279\u5f81\u8fdb\u884c\u5f52\u4e00\u5316\uff0c\u4f7f\u5176\u5747\u503c\u548c\u65b9\u5dee\u5206\u522b\u63a5\u8fd10\u548c1\u3002\n- **\u516c\u5f0f**\uff1a\n  \\[\n  \\text{LayerNorm}(x) = \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} \\cdot \\gamma + \\beta\n  \\]\n  \u5176\u4e2d\uff0c\\(\\mu\\)\u548c\\(\\sigma^2\\)\u5206\u522b\u662f\u7279\u5f81\u5747\u503c\u548c\u65b9\u5dee\uff0c\\(\\gamma\\)\u548c\\(\\beta\\)\u662f\u53ef\u5b66\u4e60\u53c2\u6570\uff0c\\(\\epsilon\\)\u662f\u5fae\u5c0f\u5e38\u6570\u4ee5\u9632\u6b62\u9664\u96f6\u3002\n- **\u5e94\u7528**\uff1a\u5e7f\u6cdb\u7528\u4e8eTransformer\u6a21\u578b\u4e2d\uff0c\u6709\u52a9\u4e8e\u7f13\u89e3\u68af\u5ea6\u6d88\u5931\u95ee\u9898\uff0c\u63d0\u5347\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002\n\n#### RMSNorm\uff08Root Mean Square Normalization\uff09\n- **\u5b9a\u4e49**\uff1aRMSNorm\u662f\u5bf9\u6bcf\u4e2a\u6837\u672c\u7684\u6bcf\u4e2a\u7279\u5f81\u8fdb\u884c\u5f52\u4e00\u5316\uff0c\u4f46\u53ea\u5f52\u4e00\u5316\u65b9\u5dee\uff0c\u4e0d\u5f52\u4e00\u5316\u5747\u503c\u3002\n- **\u516c\u5f0f**\uff1a\n  \\[\n  \\text{RMSNorm}(x) = \\frac{x}{\\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} x_i^2 + \\epsilon}} \\cdot \\gamma + \\beta\n  \\]\n  \u5176\u4e2d\uff0c\\(n\\)\u662f\u7279\u5f81\u6570\u91cf\uff0c\\(\\gamma\\)\u548c\\(\\beta\\)\u662f\u53ef\u5b66\u4e60\u53c2\u6570\uff0c\\(\\epsilon\\)\u662f\u5fae\u5c0f\u5e38\u6570\u3002\n- **\u5e94\u7528**\uff1a\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0cRMSNorm\u6bd4LayerNorm\u66f4\u7b80\u5355\uff0c\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\uff0c\u5c24\u5176\u5728\u5927\u89c4\u6a21\u6a21\u578b\u8bad\u7ec3\u4e2d\u3002\n\n#### \u4e3b\u8981\u533a\u522b\n1. **\u5f52\u4e00\u5316\u65b9\u5f0f**\uff1a\n   - LayerNorm\u540c\u65f6\u5f52\u4e00\u5316\u5747\u503c\u548c\u65b9\u5dee\u3002\n   - RMSNorm\u53ea\u5f52\u4e00\u5316\u65b9\u5dee\u3002\n2. **\u8ba1\u7b97\u590d\u6742\u5ea6**\uff1a\n   - LayerNorm\u9700\u8981\u8ba1\u7b97\u5747\u503c\u548c\u65b9\u5dee\uff0c\u8ba1\u7b97\u91cf\u7a0d\u5927\u3002\n   - RMSNorm\u53ea\u9700\u8ba1\u7b97\u65b9\u5dee\uff0c\u8ba1\u7b97\u91cf\u8f83\u5c0f\u3002\n3. **\u6548\u679c**\uff1a\n   - LayerNorm\u5728\u8bb8\u591a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u5728NLP\u9886\u57df\u3002\n   - RMSNorm\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u8868\u73b0\u76f8\u4f3c\u751a\u81f3\u66f4\u4f18\uff0c\u4e14\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u3002\n\n\u901a\u8fc7\u4e0a\u8ff0\u5206\u6790\uff0c\u53ef\u4ee5\u66f4\u597d\u5730\u7406\u89e3LayerNorm\u548cRMSNorm\u5728\u5927\u6a21\u578b\u4e2d\u7684\u5e94\u7528\u53ca\u5176\u4f18\u7f3a\u70b9\uff0c\u4ece\u800c\u5728\u5177\u4f53\u4efb\u52a1\u4e2d\u9009\u62e9\u5408\u9002\u7684\u5f52\u4e00\u5316\u65b9\u6cd5\u3002", "task_step_name": "RMSNorm\uff08Root Mean Square Normalization\uff09", "task_step_description": "RMSNorm\u662f\u5bf9\u6bcf\u4e2a\u6837\u672c\u7684\u6bcf\u4e2a\u7279\u5f81\u8fdb\u884c\u5f52\u4e00\u5316\uff0c\u4f46\u53ea\u5f52\u4e00\u5316\u65b9\u5dee\uff0c\u4e0d\u5f52\u4e00\u5316\u5747\u503c\u3002\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0cRMSNorm\u6bd4LayerNorm\u66f4\u7b80\u5355\uff0c\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\uff0c\u5c24\u5176\u5728\u5927\u89c4\u6a21\u6a21\u578b\u8bad\u7ec3\u4e2d\u3002", "task_step_level": "7", "task_step_question": "### \u95ee\u9898\n\n\u5728\u4efb\u52a1\u6b65\u9aa4\u5c42\u7ea7\u4e3a7\u7684\u60c5\u51b5\u4e0b\uff0c\u7ed3\u5408\u4efb\u52a1\u603b\u4f53\u63cf\u8ff0\u548c\u4efb\u52a1\u6b65\u9aa4\u4fe1\u606f\uff0c\u63d0\u51fa\u4ee5\u4e0b\u95ee\u9898\uff1a\n\n**\u5728\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0cRMSNorm\uff08Root Mean Square Normalization\uff09\u4e0eLayerNorm\uff08Layer Normalization\uff09\u76f8\u6bd4\uff0c\u4e3b\u8981\u6709\u54ea\u4e9b\u4f18\u52bf\u548c\u52a3\u52bf\uff1f\u7279\u522b\u662f\u5728\u5927\u89c4\u6a21\u6a21\u578b\u8bad\u7ec3\u4e2d\uff0cRMSNorm\u7684\u8ba1\u7b97\u6548\u7387\u5982\u4f55\u5f71\u54cd\u6a21\u578b\u7684\u6574\u4f53\u6027\u80fd\uff1f**\n\n### \u95ee\u9898\u89e3\u6790\n\n1. **\u4f18\u52bf**\uff1a\n   - **\u8ba1\u7b97\u6548\u7387**\uff1aRMSNorm\u53ea\u5f52\u4e00\u5316\u65b9\u5dee\uff0c\u4e0d\u5f52\u4e00\u5316\u5747\u503c\uff0c\u56e0\u6b64\u8ba1\u7b97\u91cf\u8f83\u5c0f\uff0c\u5c24\u5176\u5728\u5927\u89c4\u6a21\u6a21\u578b\u8bad\u7ec3\u4e2d\uff0c\u8fd9\u79cd\u8ba1\u7b97\u6548\u7387\u7684\u63d0\u5347\u53ef\u80fd\u663e\u8457\u51cf\u5c11\u8bad\u7ec3\u65f6\u95f4\u3002\n   - **\u7b80\u5316\u5b9e\u73b0**\uff1aRMSNorm\u7684\u5b9e\u73b0\u76f8\u5bf9\u7b80\u5355\uff0c\u53ef\u80fd\u66f4\u5bb9\u6613\u96c6\u6210\u5230\u73b0\u6709\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u4e2d\u3002\n\n2. **\u52a3\u52bf**\uff1a\n   - **\u5f52\u4e00\u5316\u6548\u679c**\uff1aRMSNorm\u53ea\u5f52\u4e00\u5316\u65b9\u5dee\uff0c\u53ef\u80fd\u5728\u67d0\u4e9b\u4efb\u52a1\u4e2d\u4e0d\u5982LayerNorm\u6709\u6548\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u540c\u65f6\u5f52\u4e00\u5316\u5747\u503c\u548c\u65b9\u5dee\u7684\u4efb\u52a1\u4e2d\u3002\n   - **\u6cdb\u5316\u80fd\u529b**\uff1aRMSNorm\u7684\u7b80\u5316\u5f52\u4e00\u5316\u65b9\u5f0f\u53ef\u80fd\u5f71\u54cd\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u3002\n\n3. **\u5927\u89c4\u6a21\u6a21\u578b\u8bad\u7ec3\u4e2d\u7684\u5f71\u54cd**\uff1a\n   - **\u8ba1\u7b97\u8d44\u6e90**\uff1aRMSNorm\u7684\u8ba1\u7b97\u6548\u7387\u53ef\u80fd\u51cf\u5c11\u5bf9\u8ba1\u7b97\u8d44\u6e90\u7684\u9700\u6c42\uff0c\u4f7f\u5f97\u5927\u89c4\u6a21\u6a21\u578b\u8bad\u7ec3\u66f4\u52a0\u53ef\u884c\u3002\n   - **\u6027\u80fd\u8868\u73b0**\uff1a\u5c3d\u7ba1\u8ba1\u7b97\u6548\u7387\u9ad8\uff0c\u4f46RMSNorm\u5728\u67d0\u4e9b\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u8868\u73b0\u53ef\u80fd\u4e0eLayerNorm\u76f8\u5f53\u6216\u7565\u4f18\uff0c\u8fd9\u9700\u8981\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u8fdb\u884c\u9a8c\u8bc1\u3002\n\n\u901a\u8fc7\u8fd9\u4e2a\u95ee\u9898\uff0c\u53ef\u4ee5\u66f4\u6df1\u5165\u5730\u7406\u89e3RMSNorm\u5728\u5927\u89c4\u6a21\u6a21\u578b\u8bad\u7ec3\u4e2d\u7684\u5e94\u7528\u53ca\u5176\u4e0eLayerNorm\u7684\u5bf9\u6bd4\uff0c\u4ece\u800c\u4e3a\u9009\u62e9\u5408\u9002\u7684\u5f52\u4e00\u5316\u65b9\u6cd5\u63d0\u4f9b\u4f9d\u636e\u3002", "task_step_question_context": [{"ref_id": "454848342697342208", "chunk_id": "7", "score": 0.25390625, "text": "# 1 Introduction\nTraining modern neural networks can be a challenging problem. They are typically deeper and more heterogeneous in their structure than previous models, containing various types of layers and operations. The complex interactions of these components pose new challenges for optimization. To ensure stable training the magnitude of gradients and activations must be kept in check, avoiding effects such as vanishing gradients, and exploding activations. Efficient training also requires the learning of different components to be balanced. A layer that is updated slowly (e.g., barely changing through the training process) is unlikely to contribute optimally to the final model, wasting compute resources. On the other hand, a rapidly changing layer may cause instability, limiting the maximum stable learning rate and preventing other layers from learning effectively. Over time many methods have been developed to improve the optimization of deep neural networks but are often poorly understood, complicating the application of deep learning.  \n\nNormalization such as Batch Normalization [ 9 ] or Layer Normalization [ 1 ] is one such operation that is ubiquitous in modern networks. Depending on their placement and configuration, normalization can make the output of a network invariant to the scale of certain parameters. An example of this occurs in the ResNet [ 5 ] architecture where convolutional layers are directly followed by batch normalization. In this case, each convolutional filter (corresponding to a single output channel) can be scaled by a positive scalar without changing the output of the batch normalization. We say that weights that have this property are scale-invariant and those who are not are scale-sensitive . The effect of scale-invariant weights in a neural network is fully determined by their direction, as their magnitude does not matter. Hence, it is natural to use the rotation (i.e., angular change) to measure the \u201ceffective\u201d size of an update for such weights.  \n\nOptimizing scale-invariant weights with SGD and weight decay can give rise to Spherical Motion Dynamics [ 29 ,4 ]. The gradient of scale-invariant weights is orthogonal to them (see Section 2.1), causing gradient updates to increase the weight magnitude, unlike weight decay which always decreases it. These opposing forces cause the weight norm to converge to a certain value where the effects balance out, a state which we call equilibrium . This convergence does not rely on the loss converging and can happen relatively quickly in training. A key property of equilibrium is that an optimizer step will rotate a scale-invariant weight vector by a fixed angle in expectation. This value is the same for all scale-invariant weight vectors, ensuring that they all get updated at the same \u201ceffective\u201d rate regardless of their average gradient magnitude or position in the network. In equilibrium, the Spherical Motion Dynamics can thus help balance the update speed of different layers, preventing them from changing very fast or slowly compared to other layers.  \n\nWith standard optimizers, the scale-invariant weights need to converge to equilibrium. The equilibrium weight magnitude generally depends on the learning rate and gradient magnitude and can therefore change over the course of training, causing weights to fall out of equilibrium. Weights are also often initialized with magnitudes that are far from their equilibrium value at the start of training. In typical descent methods, the rotation of a weight has an inverse dependency on the weight magnitude. Weights that are out of equilibrium can thus rotate slower or faster than the equilibrium value. By definition, scale-invariant weights can be scaled without affecting the output of the network, but such scaling still affects the rotation and therefore changes the effect of an update. This leads to learning dynamics that we consider highly arbitrary.  \n\nIn this work we propose fixing these dynamics by forcing scale-invariant weights to behave as in equilibrium throughout training. We derive the equilibrium states for AdamW, SGD with momentum (SGDM) and Lion optimizers. Our analysis provides insights into the effects of the hyperparameters of these optimizers and reveals some unintuitive behavioral differences between parameters that are scale-invariant and those which are not. We explain the effect of weight decay in the common case where most scale-sensitive parameters, such as gains and biases, are excluded. We create rotational variants of the optimizers that constrain the weight magnitude of scale-invariant weights and ensure that the average rotational update matches equilibrium. In general, these variants treat the scale-sensitive parameters like the original optimizers. However, we can also apply rotational updates to weights that are not strictly scale-invariant. We show that the RVs can match or exceed the originals, often with a minimal change in the hyperparameters, reducing the need for tuning compared to completely new optimizers. Finally, we relate these variants to existing optimizers like LARS [ 32 ], LAMB [ 33 ] and Nero [ 15 ], explaining some of their observed benefits. Our main contributions can be summarized as:  \n\n\u2022Creating rotational optimizer variants that constrain the Spherical Motion Dynamics (SMD).   \n\u2022 Deriving the SMD equilibrium for AdamW, SGDM and Lion optimizers.   \n\u2022 Explaining the effects of optimizer hyperparameters from the SMD perspective.   \n\u2022 Experimentally showing the viability of training with constrained SMD.   \n\u2022Showing that constrained SMD training reduces the need for learning rate warmup and improves optimization of poorly normalized networks."}, {"ref_id": "454847042436311108", "chunk_id": "4", "score": 0.2451171875, "text": "# 5 I NTUITIONS BEHIND LAYER NORM TUNING\nIn this section, driven by the empirical success of LayerNorm tuning, we explore the intuitions behind LayerNorm from three perspectives, domain adaptation, expressive power, and gradient variance.  \n\nTable 3: Model performance on different data types. Methods with 80K and Conv.20K suffix are tuned on the full 80K data and the 20K conversational data, respectively.   \n\n\n<html><body><table><tr><td>Method</td><td>MME</td><td>VQAv2</td><td>MSCOCO</td><td>Flickr30k</td><td>POPE</td></tr><tr><td colspan=\"6\">MM-V1CUNA-7B</td></tr><tr><td>Finetune-80K</td><td>625.2/270.7</td><td>15.40</td><td>67.50</td><td>34.61</td><td>73.8/76.5/66.5</td></tr><tr><td>LayerNorm-80K</td><td>723.2/253.2</td><td>17.06</td><td>80.89</td><td>48.01</td><td>76.1/81.1/70.8</td></tr><tr><td>LayerNorm-Conv. 20K</td><td>777.1/231.4</td><td>15.39</td><td>67.30</td><td>40.33</td><td>75.2/79.2/68.8</td></tr><tr><td colspan=\"6\">MM-LLAMA2-7B</td></tr><tr><td>Finetune-80K</td><td>661.3/237.1</td><td>16.09</td><td>65.08</td><td>31.64</td><td>56.3/65.0/55.4</td></tr><tr><td>LayerNorm-80K</td><td>583.2/200.7</td><td>16.78</td><td>88.85</td><td>49.24</td><td>66.6/68.5/64.9</td></tr><tr><td>LayerNorm-Conv. 20K</td><td>376.2/157.5</td><td>16.19</td><td>86.80</td><td>44.88</td><td>50.5/50.7/50.3</td></tr><tr><td colspan=\"6\">MM-LLAMA2-CHAT-7B</td></tr><tr><td>Finetune-80K</td><td>805.4/234.6</td><td>15.29</td><td>57.40</td><td>26.70</td><td>60.3/69.8/57.9</td></tr><tr><td>LayerNorm-80K</td><td>651.3/219.3</td><td>16.60</td><td>75.34</td><td>43.75</td><td>71.3/72.4/67.8</td></tr><tr><td>LayerNorm-Conv. 20K</td><td>482.9/172.1</td><td>13.88</td><td>66.85</td><td>41.95</td><td>62.7/71.7/61.3</td></tr><tr><td colspan=\"6\">MM-LLAMA2-13B</td></tr><tr><td>Finetune-80K</td><td>402.3/199.3</td><td>18.33</td><td>73.88</td><td>45.33</td><td>51.6/51.1/52.2</td></tr><tr><td>LayerNorm-80K</td><td>526.0/177.5</td><td>15.31</td><td>82.92</td><td>48.42</td><td>60.0/69.1/58.9</td></tr><tr><td>LayerNorm-Conv. 20K</td><td>646.0/242.9</td><td>16.01</td><td>76.50</td><td>44.86</td><td>70.0/76.9/68.6</td></tr><tr><td colspan=\"6\">MM-LLAMA2-CHAT-13B</td></tr><tr><td>Finetune-80K</td><td>623.3/221.4</td><td>15.17</td><td>64.19</td><td>41.82</td><td>67.6/64.8/64.5</td></tr><tr><td>LayerNorm-80K</td><td>929.3/254.3</td><td>16.10</td><td>74.96</td><td>42.79</td><td>78.9/83.9/74.3</td></tr><tr><td>LayerNorm-Conv. 20K</td><td>769.7/227.5</td><td>15.57</td><td>73.30</td><td>43.08</td><td>68.2/72.8/65.3</td></tr></table></body></html>  \n\nTable 4: Results of models with LayerNorm and/or vision-language Connector activated.   \n\n\n<html><body><table><tr><td>Method</td><td>MME</td><td>VQAv2</td><td>MSCOCO</td><td>Flickr30k</td><td>POPE</td></tr><tr><td colspan=\"6\">MM-LLAMA2-7B</td></tr><tr><td>LayerNorm + Connector</td><td>583.2/200.7</td><td>16.78</td><td>88.85</td><td>49.24</td><td>66.6/68.5/64.9</td></tr><tr><td>Connector</td><td>311.1/105.4</td><td>12.72</td><td>60.43</td><td>35.91</td><td>67.9/73.7/66.9</td></tr><tr><td>LayerNorm</td><td>395.0/191.4</td><td>18.18</td><td>80.13</td><td>41.68</td><td>50.3/51.3/50.2</td></tr><tr><td colspan=\"6\">MM-LLAMA2-13B</td></tr><tr><td>LayerNorm + Connector</td><td>526.0/177.5</td><td>15.31</td><td>82.92</td><td>48.42</td><td>60.0/69.1/58.9</td></tr><tr><td>Connector</td><td>507.0/187.9</td><td>15.22</td><td>62.60</td><td>25.13</td><td>60.9/66.8/60.1</td></tr><tr><td>LayerNorm</td><td>405.0/188.6</td><td>16.51</td><td>70.41</td><td>39.86</td><td>50.9/52.7/51.0</td></tr></table></body></html>\n\n# 5.1 LAYER NORM TUNING A DAPTS LLM S TO MULTI -M ODAL\nInfluence of the Vision-Language Connector The vision-language connector serves as the converter to project features from the vision encoder to the LLM domain. In our previous experiments, we focused on finetuning the LLM component of the MLLMs while keeping the vision-language connector activated by default. To determine which component plays a more important role for domain adaptation of LLM to multi-modal domain, we performed an ablation study by activating the two components separately. Results are presented in table 4 , tuning LayerNorm in attention blocks without activating the vision-language connector resulted in only a $4.2\\%$ and $5.4\\%$ decrease in performance on three traditional multi-modal tasks and the MME benchmark, respectively. This decrease is significantly lower than the $15.6\\%$ and $9.2\\%$ downgrade observed when only activating the Connector on the same tasks. This observation highlights the vital role LayerNorm plays in transforming knowledge from the vision domain to language, indicating LayerNorm as a strong domain adaptor for the LLM architecture.  \n\n  \n\nFigure 3: Layer similarities between different LLM layers in (a) Finetuned and (b) LayerNorm-tuned MM-V ICUNA -7B. The average layer similarity of two models are 0.624 and 0.585, respectively.  \n\nTable 5: Results of models with LL A MA2 Finetuned/LayerNorm-tuned with ViT pre-trained on ImageNet (Deng et al. ,2009 ), which have not been aligned with the language domain.   \n\n\n<html><body><table><tr><td></td><td>MME</td><td>VQAv2</td><td>MSCOCO</td><td>Flickr30k</td><td>POPE</td></tr><tr><td>Finetune-7B</td><td>406.79/182.5</td><td>15.05</td><td>47.75</td><td>18.97</td><td>50.0/51.6/50.1</td></tr><tr><td>LayerNorm-7B</td><td>301.51/127.14</td><td>15.48</td><td>66.22</td><td>31.73</td><td>50.0/50.1/50.1</td></tr><tr><td>Finetune-13B</td><td>375.41/171.79</td><td>25.38</td><td>51.26</td><td>25.96</td><td>50.3/51.1/51.0</td></tr><tr><td>LayerNorm-13B</td><td>445.98/150.0</td><td>15.59</td><td>64.63</td><td>32.17</td><td>51.2/53.0/50.8</td></tr></table></body></html>  \n\nSwitching Visual Features. We employ the ViT encoder from CLIP ( Radford et al. ,2021 ) by default in our previous experiments. CLIP ( Radford et al. ,2021 ) models are trained with image-text contrastive loss, thus its feature space is already aligned with language. Since LayerNorm has shown its effectiveness as a domain adaptor, we are interested in testing whether or not LayerNorm tuning can adapt a LLM to image features that are not pretrained to align with language. The vision encoder is switched to a ViT model that was pretrained on ImageNet (Dosovitskiy et al. ,2021 ;Deng et al. ,2009 ). Results in table 5 demonstrate that both LayerNorm and finetuning approaches can yield high performance. Interestingly, we observe that by LayerNorm tuning with ImageNet trained ViT, which has not been aligned with language, the model is able to achieve comparable performance to full parameter finetuning , i.e ., results show that LayerNorm tuning outperforms finetuning by $12.0\\%$ on captioning tasks, but performs slightly worse by $5.0\\%$ on the MME benchmark. These results again indicates the domain adaptor role of the LayerNorm , hinting the reason behind the empircal success of LayerNorm tuning. Furthermore, it is worth noting that the performance of MLLMs incorporating ViT pretrained on ImageNet is generally inferior to that of CLIP\u2019s vision encoder. This observation provides compelling evidence that, despite differences in tokenizer and training paradigm between CLIP\u2019s text encoder and LL A MA\u2019s, ViT from CLIP has the capacity to learn general patterns of language formulation during pre-training. Thus, significantly enhance MLLM abilities.\n\n# 5.2 LAYER NORM TUNING I MPROVES THE EXPRESSIVE POWER\nIt is shown in Pires et al. (2023 ) that a Transformer model incorporating anisotropic layer representation can capture a wider range of learning patterns. By computing the cosine similarities between all layers in the LLM of a finetuned MLLM, we aim to investigate whether the improved efficiency is the results of the improved expressive power. In table 6 , we present the average layer similarity of three 7B scale MLLMs, and in fig. 3 we present the visualization of per layer similarity scores of MM-V ICUNA -7B. Our analysis reveals that the transformer layers in the MLLMs with LayerNorm tuning exhibit a clear distinction from one another ( i.e ., an average $10.6\\%$ lower layer similarities comparing finetuning), indicating superior generalization ability and expressive power compared to finetuning. This finding sheds light on why tuning LayerNorm is effective for multi-modal LLM training. For additional visualizations, please refer to the Appendix A.2.1 .  \n\n  \nFigure 4: Gradients of the input LayerNorm in the 11th layer of the MM-V ICUNA as training proceeds. LayerNorm-tuned model has lower gradient variance than full parameter finetuning.  \n\nTable 6: Layer representation similarity of LayerNorm and finetuning methods on three 7B MLLMs.   \nLower the similarity is, the better expressive power a model possesses.  \n\n<html><body><table><tr><td>Model</td><td>LayerNorm Sim.</td><td>Finetuning Sim.</td></tr><tr><td>MM-VICUNA</td><td>0.585</td><td>0.624</td></tr><tr><td>MM-LLAMA2</td><td>0.504</td><td>0.591</td></tr><tr><td>MM-LLAMA2-CHAT</td><td>0.550</td><td>0.617</td></tr></table></body></html>"}, {"ref_id": "454846008144214678", "chunk_id": "3", "score": 0.23828125, "text": "# 7.4 Fine-Tuning Method Selection with LLMem\nTable 3 assesses whether LLMem finds the optimal finetuning method to achieve the fastest fine-tuning while avoiding GPU OOM for various models. When measuring the time taken for each method, we applied the maximum batch size that can prevent GPU OOM. LLMem typically selects TP when DP causes GPU OOM. It is challenging for LLMem to choose $\\mathrm{DP+TP}$ because only 4 GPUs were used in the experiment. $\\mathrm{DP+TP}$ allows for more diverse combinations depending on the number of GPUs used and is more likely to be selected. LLMem also suggests CPU offloading when GPU memory is insufficient."}], "task_step_question_answer": "\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\uff0cRMSNorm\u7684\u9ad8\u6548\u6027\u4e0d\u4ec5\u4f53\u73b0\u5728\u8bad\u7ec3\u65f6\u95f4\u7684\u51cf\u5c11\uff0c\u8fd8\u4f53\u73b0\u5728\u5176\u5bf9\u5f02\u6784\u6570\u636e\u7684\u9002\u5e94\u6027\u3002\u4f8b\u5982\uff0c\u5728\u5904\u7406\u4e0d\u540c\u5ba2\u6237\u7aef\u6570\u636e\u5206\u5e03\u4e0d\u4e00\u81f4\u7684\u60c5\u51b5\u4e0b\uff0cRMSNorm\u901a\u8fc7\u5176\u7b80\u5316\u8ba1\u7b97\u8fc7\u7a0b\uff0c\u80fd\u591f\u66f4\u5feb\u5730\u9002\u5e94\u6570\u636e\u53d8\u5316\uff0c\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002\u5728\u9690\u79c1\u4fdd\u62a4\u65b9\u9762\uff0cRMSNorm\u7684\u7b80\u5316\u8ba1\u7b97\u8fc7\u7a0b\u4e0d\u4ec5\u51cf\u5c11\u4e86\u8ba1\u7b97\u5f00\u9500\uff0c\u8fd8\u901a\u8fc7\u51cf\u5c11\u654f\u611f\u4fe1\u606f\u7684\u66b4\u9732\uff0c\u63d0\u5347\u4e86\u9690\u79c1\u4fdd\u62a4\u7684\u6548\u679c\u3002\u672a\u6765\u7684\u7814\u7a76\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63a2\u7d22RMSNorm\u5728\u8054\u90a6\u5b66\u4e60\u548c\u9690\u79c1\u4fdd\u62a4\u4e2d\u7684\u5177\u4f53\u5e94\u7528\uff0c\u4f8b\u5982\u901a\u8fc7\u7ed3\u5408\u5dee\u5206\u9690\u79c1\u6280\u672f\uff0c\u8bbe\u8ba1\u51fa\u66f4\u9ad8\u6548\u7684\u5f52\u4e00\u5316\u65b9\u6cd5\uff0c\u4ee5\u5e94\u5bf9\u65e5\u76ca\u590d\u6742\u7684\u5b9e\u9645\u9700\u6c42\u3002", "ref_task_step_id": ""}, "__type__": "task_step_node"}, "e4184b0a-c68b-4ef1-88c7-35cff3095e16": {"__data__": {"id_": "e4184b0a-c68b-4ef1-88c7-35cff3095e16", "metadata": {}, "relationships": {}, "hash": "", "start_task_context": "\u5927\u6a21\u578b\u4e2d\u7684LayerNorm\u548cRMSNorm\u6709\u4ec0\u4e48\u533a\u522b\uff1f", "aemo_representation_context": "### Step-by-Step Decomposition of Computer Science Research Content\n\n#### 1. \u5206\u6790\u8fd1\u51e0\u5e74\u7814\u7a76\u9886\u57df\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba\n- **\u6280\u672f\u6846\u67b6**\uff1a\u8fd1\u5e74\u6765\uff0c\u6df1\u5ea6\u5b66\u4e60\u3001\u5f3a\u5316\u5b66\u4e60\u3001\u56fe\u795e\u7ecf\u7f51\u7edc\u7b49\u6846\u67b6\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u9886\u57df\u5360\u636e\u4e3b\u5bfc\u5730\u4f4d\u3002\u5177\u4f53\u6846\u67b6\u5982Transformer\u3001BERT\u3001GNN\u7b49\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002\n- **\u65b9\u6cd5\u8bba**\uff1a\u7814\u7a76\u65b9\u6cd5\u9010\u6e10\u4ece\u5355\u4e00\u6a21\u578b\u8f6c\u5411\u591a\u6a21\u578b\u878d\u5408\uff0c\u4ece\u76d1\u7763\u5b66\u4e60\u6269\u5c55\u5230\u65e0\u76d1\u7763\u548c\u81ea\u76d1\u7763\u5b66\u4e60\uff0c\u5f3a\u8c03\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002\n\n#### 2. \u7814\u7a76\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u4e3b\u8981\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0e\u53d8\u4f53\n- **\u5e94\u7528**\uff1a\u4f8b\u5982\uff0cTransformer\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u4e2d\u88ab\u5e7f\u6cdb\u5e94\u7528\uff0cBERT\u53d8\u4f53\u5982RoBERTa\u3001ALBERT\u5728\u6587\u672c\u5206\u7c7b\u3001\u95ee\u7b54\u7cfb\u7edf\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff1bGNN\u5728\u63a8\u8350\u7cfb\u7edf\u3001\u77e5\u8bc6\u56fe\u8c31\u7b49\u9886\u57df\u6709\u663e\u8457\u5e94\u7528\u3002\n- **\u53d8\u4f53**\uff1a\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\uff0c\u7814\u7a76\u8005\u63d0\u51fa\u4e86\u591a\u79cd\u53d8\u4f53\uff0c\u5982\u9488\u5bf9\u957f\u5e8f\u5217\u5904\u7406\u7684Transformer-XL\uff0c\u9488\u5bf9\u591a\u6a21\u6001\u6570\u636e\u7684ViLBERT\u7b49\u3002\n\n#### 3. \u8bc4\u4f30\u5b66\u672f\u754c\u7684\u6280\u672f\u8fdb\u6b65\u4e0e\u5c40\u9650\u6027\n- **\u6280\u672f\u8fdb\u6b65**\uff1a\u6a21\u578b\u6027\u80fd\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\uff0c\u8ba1\u7b97\u6548\u7387\u4e5f\u6709\u6240\u63d0\u9ad8\uff0c\u5982EfficientNet\u5728\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u8868\u73b0\u3002\n- **\u5c40\u9650\u6027**\uff1a\u4f9d\u7136\u5b58\u5728\u6a21\u578b\u504f\u5dee\uff08\u5982\u6027\u522b\u504f\u89c1\uff09\u3001\u6570\u636e\u4f9d\u8d56\uff08\u5982\u5bf9\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\uff09\u7b49\u95ee\u9898\uff0c\u6cdb\u5316\u80fd\u529b\u6709\u5f85\u8fdb\u4e00\u6b65\u63d0\u5347\u3002\n\n#### 4. \u63a2\u8ba8\u8ba1\u7b97\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\n- **\u9002\u7528\u6027**\uff1a\u7814\u7a76\u6a21\u578b\u5728\u4e0d\u540c\u9886\u57df\uff08\u5982\u533b\u7597\u3001\u91d1\u878d\uff09\u548c\u591a\u6a21\u6001\u6570\u636e\uff08\u5982\u6587\u672c+\u56fe\u50cf\uff09\u4e0a\u7684\u8868\u73b0\uff0c\u5982\u591a\u6a21\u6001Transformer\u6a21\u578b\u3002\n- **\u6cdb\u5316\u80fd\u529b**\uff1a\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\u3001\u5143\u5b66\u4e60\u7b49\u65b9\u6cd5\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f7f\u5176\u80fd\u591f\u5e94\u5bf9\u66f4\u591a\u73b0\u5b9e\u5e94\u7528\u573a\u666f\u3002\n\n#### 5. \u5206\u6790\u6700\u65b0\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027\n- **\u7a33\u5b9a\u6027**\uff1a\u7814\u7a76\u7b97\u6cd5\u5728\u590d\u6742\u3001\u52a8\u6001\u73af\u5883\u4e0b\u7684\u8868\u73b0\uff0c\u5982\u5bf9\u6297\u8bad\u7ec3\u3001\u9c81\u68d2\u4f18\u5316\u7b49\u65b9\u6cd5\u7684\u5e94\u7528\u3002\n- **\u5bb9\u9519\u6027**\uff1a\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\u7684\u9002\u5e94\u6027\uff0c\u5982\u5206\u5e03\u5f0f\u8bad\u7ec3\u4e2d\u7684\u68af\u5ea6\u538b\u7f29\u3001\u6a21\u578b\u5e76\u884c\u7b49\u6280\u672f\u3002\n\n#### 6. \u8bc4\u4f30\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218\n- **\u672a\u6765\u65b9\u5411**\uff1a\u5982\u53ef\u89e3\u91caAI\u3001\u8054\u90a6\u5b66\u4e60\u3001\u9690\u79c1\u4fdd\u62a4\u7b49\u65b0\u5174\u7814\u7a76\u65b9\u5411\u3002\n- **\u6311\u6218**\uff1a\u5982\u4f55\u5e73\u8861\u6a21\u578b\u6027\u80fd\u4e0e\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\uff0c\u5982\u4f55\u89e3\u51b3\u6570\u636e\u504f\u89c1\u548c\u4f26\u7406\u95ee\u9898\u7b49\u3002\n\n### \u5927\u6a21\u578b\u4e2d\u7684LayerNorm\u548cRMSNorm\u7684\u533a\u522b\n\n#### LayerNorm\uff08Layer Normalization\uff09\n- **\u5b9a\u4e49**\uff1aLayerNorm\u662f\u5bf9\u6bcf\u4e2a\u6837\u672c\u7684\u6bcf\u4e2a\u7279\u5f81\u8fdb\u884c\u5f52\u4e00\u5316\uff0c\u4f7f\u5176\u5747\u503c\u548c\u65b9\u5dee\u5206\u522b\u63a5\u8fd10\u548c1\u3002\n- **\u516c\u5f0f**\uff1a\n  \\[\n  \\text{LayerNorm}(x) = \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} \\cdot \\gamma + \\beta\n  \\]\n  \u5176\u4e2d\uff0c\\(\\mu\\)\u548c\\(\\sigma^2\\)\u5206\u522b\u662f\u7279\u5f81\u5747\u503c\u548c\u65b9\u5dee\uff0c\\(\\gamma\\)\u548c\\(\\beta\\)\u662f\u53ef\u5b66\u4e60\u53c2\u6570\uff0c\\(\\epsilon\\)\u662f\u5fae\u5c0f\u5e38\u6570\u4ee5\u9632\u6b62\u9664\u96f6\u3002\n- **\u5e94\u7528**\uff1a\u5e7f\u6cdb\u7528\u4e8eTransformer\u6a21\u578b\u4e2d\uff0c\u6709\u52a9\u4e8e\u7f13\u89e3\u68af\u5ea6\u6d88\u5931\u95ee\u9898\uff0c\u63d0\u5347\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002\n\n#### RMSNorm\uff08Root Mean Square Normalization\uff09\n- **\u5b9a\u4e49**\uff1aRMSNorm\u662f\u5bf9\u6bcf\u4e2a\u6837\u672c\u7684\u6bcf\u4e2a\u7279\u5f81\u8fdb\u884c\u5f52\u4e00\u5316\uff0c\u4f46\u53ea\u5f52\u4e00\u5316\u65b9\u5dee\uff0c\u4e0d\u5f52\u4e00\u5316\u5747\u503c\u3002\n- **\u516c\u5f0f**\uff1a\n  \\[\n  \\text{RMSNorm}(x) = \\frac{x}{\\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} x_i^2 + \\epsilon}} \\cdot \\gamma + \\beta\n  \\]\n  \u5176\u4e2d\uff0c\\(n\\)\u662f\u7279\u5f81\u6570\u91cf\uff0c\\(\\gamma\\)\u548c\\(\\beta\\)\u662f\u53ef\u5b66\u4e60\u53c2\u6570\uff0c\\(\\epsilon\\)\u662f\u5fae\u5c0f\u5e38\u6570\u3002\n- **\u5e94\u7528**\uff1a\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0cRMSNorm\u6bd4LayerNorm\u66f4\u7b80\u5355\uff0c\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\uff0c\u5c24\u5176\u5728\u5927\u89c4\u6a21\u6a21\u578b\u8bad\u7ec3\u4e2d\u3002\n\n#### \u4e3b\u8981\u533a\u522b\n1. **\u5f52\u4e00\u5316\u65b9\u5f0f**\uff1a\n   - LayerNorm\u540c\u65f6\u5f52\u4e00\u5316\u5747\u503c\u548c\u65b9\u5dee\u3002\n   - RMSNorm\u53ea\u5f52\u4e00\u5316\u65b9\u5dee\u3002\n2. **\u8ba1\u7b97\u590d\u6742\u5ea6**\uff1a\n   - LayerNorm\u9700\u8981\u8ba1\u7b97\u5747\u503c\u548c\u65b9\u5dee\uff0c\u8ba1\u7b97\u91cf\u7a0d\u5927\u3002\n   - RMSNorm\u53ea\u9700\u8ba1\u7b97\u65b9\u5dee\uff0c\u8ba1\u7b97\u91cf\u8f83\u5c0f\u3002\n3. **\u6548\u679c**\uff1a\n   - LayerNorm\u5728\u8bb8\u591a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u5728NLP\u9886\u57df\u3002\n   - RMSNorm\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u8868\u73b0\u76f8\u4f3c\u751a\u81f3\u66f4\u4f18\uff0c\u4e14\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u3002\n\n\u901a\u8fc7\u4e0a\u8ff0\u5206\u6790\uff0c\u53ef\u4ee5\u66f4\u597d\u5730\u7406\u89e3LayerNorm\u548cRMSNorm\u5728\u5927\u6a21\u578b\u4e2d\u7684\u5e94\u7528\u53ca\u5176\u4f18\u7f3a\u70b9\uff0c\u4ece\u800c\u5728\u5177\u4f53\u4efb\u52a1\u4e2d\u9009\u62e9\u5408\u9002\u7684\u5f52\u4e00\u5316\u65b9\u6cd5\u3002", "task_step_name": "\u4e3b\u8981\u533a\u522b", "task_step_description": "LayerNorm\u540c\u65f6\u5f52\u4e00\u5316\u5747\u503c\u548c\u65b9\u5dee\uff0c\u8ba1\u7b97\u91cf\u7a0d\u5927\uff1bRMSNorm\u53ea\u5f52\u4e00\u5316\u65b9\u5dee\uff0c\u8ba1\u7b97\u91cf\u8f83\u5c0f\u3002LayerNorm\u5728\u8bb8\u591a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u5728NLP\u9886\u57df\uff1bRMSNorm\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u8868\u73b0\u76f8\u4f3c\u751a\u81f3\u66f4\u4f18\uff0c\u4e14\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u3002", "task_step_level": "8", "task_step_question": "### \u95ee\u9898\n\n\u5728\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4e2d\uff0cLayerNorm\u548cRMSNorm\u7684\u4e3b\u8981\u533a\u522b\u662f\u4ec0\u4e48\uff1f\u5177\u4f53\u6765\u8bf4\uff0c\u5b83\u4eec\u5728\u5f52\u4e00\u5316\u65b9\u5f0f\u3001\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u5e94\u7528\u6548\u679c\u4e0a\u6709\u54ea\u4e9b\u4e0d\u540c\uff1f\u8fd9\u4e9b\u533a\u522b\u5982\u4f55\u5f71\u54cd\u5b83\u4eec\u5728\u4e0d\u540c\u4efb\u52a1\uff08\u5982\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff09\u4e2d\u7684\u8868\u73b0\u548c\u8ba1\u7b97\u6548\u7387\uff1f", "task_step_question_context": [{"ref_id": "454848253879281810", "chunk_id": "4", "score": 0.5234375, "text": "# B.2 MODEL LAYERS\nIn this section, we give the formal definition of LayerNorm $\\operatorname{LN}(\\cdot)$ and RMS Norm ${\\mathrm{RMS}}\\left(\\cdot\\right)$ .  \n\nDefinition 1 (LayerNorm) .LayerNorm $L N(\\cdot;\\mu,\\beta,\\epsilon)$ of dimension $D$ is defined as:  \n\n$$\nL N(\\mathbf{x};\\pmb{\\mu},\\beta,\\epsilon)=\\frac{\\mathbf{x}-\\mathbb{E}[\\mathbf{x}]}{\\sqrt{\\mathrm{Var}[\\mathbf{x}]+\\epsilon}}\\odot\\pmb{\\mu}+\\beta,\n$$  \n\nwhere $\\mathbf{x},\\pmb{\\mu},\\beta\\in\\mathbb{R}^{D}$ .  \n\nDefinition 2 (RMSNorm) .RMS Norm $R M S(\\cdot;\\mu,\\epsilon)$ of dimension $D$ is defined as:  \n\n$$\nR M S(\\mathbf{x};\\pmb{\\mu},\\epsilon)=\\frac{\\mathbf{x}}{\\sqrt{\\frac{1}{D}\\sum_{i=1}^{D}(\\mathbf{x}[i])^{2}+\\epsilon}}\\odot\\pmb{\\mu},\n$$  \n\nwhere x,$\\pmb{\\mu}\\in\\mathbb{R}^{D}$ .  \n\nRemark. In neural networks, inputs of normalization layers are usually high dimension tensors. In this case, LayerNorm and RMSNorm normally apply to the last dimension separately.\n\n# B.3 LOSSLESS EXPANSION IN VECTOR SPACE\nIn this section, we first give the general definition of lossless expansion in vector space.  \n\ndimensions satisfy dim it is invertible. Definition 3 (Lossless $(\\bar{\\mathcal{T}})\\geq d i m(S)$ T\u2265S, a vector space expansion ector space) .Given $\\boldsymbol{S}$ and V$\\tau$ $\\mathcal{V}:\\mathcal{S}\\rightarrow\\mathcal{T}$ S \u2192T is said to be lossless if ector spaces where the  \n\nRemark. Note that the identity function Id is lossless with its inverse being itself.  \n\nThen we give a few examples of lossless vector space expansions. These examples will also be used in LEMON.  \n\nExample B.3.1 (Vector average expansion $\\mathcal{V}_{\\mathrm{avg.}}$ ).Let $\\mathbf{\\widetilde{x}}\\in\\mathbb{R}^{D_{S}}$ be a vector of dimension $D_{S}$ and its average $\\begin{array}{r}{\\lambda_{V}g(\\mathbf{x})=\\mathbb{E}[\\mathbf{x}]=\\frac{1}{D_{S}}\\sum_{i}^{D_{S}}\\mathbf{x}[i]}\\end{array}$ P].$\\mathbf{x}_{a\\nu g}^{*}$ is called the average expanded xof dimension $D_{T}$  \n\nwith $D_{T}\\geq D_{S}$ if  \n\n$$\n\\mathbf{x}_{a v g}^{*}=\\mathcal{V}_{a v g}(\\mathbf{x})=C o n c a t\\left[\\underbrace{\\mathbf{x}^{\\mathsf{T}},\\cdots,\\mathbf{x}^{\\mathsf{T}}}_{\\lfloor D_{T}/D s\\rfloor},\\underbrace{A v g(\\mathbf{x}),\\cdots,A v g(\\mathbf{x})}_{D_{T}\\mathrm{~mod~}D_{S}}\\right]^{\\mathsf{T}}\\in\\mathbb{R}^{D_{T}}.\n$$  \n\nExample B.3.2 (Vector z o expansion $\\mathcal{V}_{\\mathrm{zero.}}$ ).Le $\\mathbf{x}\\in\\mathbb{R}^{D_{S}}$ be a vector of dimension $D_{S}$ .$\\mathbf{x}_{z e r o}^{*}$ is called the zero expanded xof dimension $D_{T}$ with $D_{T}\\geq D_{S}$ \u2265if  \n\n$$\n\\begin{array}{r}{\\mathbf{x}_{z e r o}^{*}=\\mathcal{V}_{z e r o}(\\mathbf{x})=C o n c a t\\left[\\underbrace{\\mathbf{x^{\\mathsf{T}}},\\cdots,\\mathbf{x^{\\mathsf{T}}}}_{\\lfloor D_{T}/D_{S}\\rfloor},\\underbrace{0,\\cdots,0}_{D_{T}\\mathrm{~mod~}D_{S}}\\right]^{\\mathsf{T}}\\in\\mathbb{R}^{D_{T}}.}\\end{array}\n$$  \n\nExample B.3.3 (Vector circula expansion $\\mathcal{V}_{\\mathrm{circ}})$ Let $\\mathbf{x}\\in\\mathbb{R}^{D_{S}}$ a vector of dimension $D_{S}$ .${\\bf x}_{c i r c}^{*}$ is called the circular expanded xof dimension $D_{T}$ with $D_{T}\\geq D_{S}$ \u2265if  \n\n$$\n\\begin{array}{r}{\\mathbf{x}_{c i r c}^{*}=\\mathcal{V}_{c i r c}(\\mathbf{x})=C o n c a t\\underbrace{\\left[\\mathbf{x}^{\\mathsf{T}},\\cdots,\\mathbf{x}^{\\mathsf{T}},\\mathbf{x}^{\\mathsf{T}}[\\colon D_{T}\\bmod D_{S}]\\right]^{\\mathsf{T}}\\in\\mathbb{R}^{D_{T}}}_{[D_{T}/D_{S}]}.}\\end{array}\n$$  \n\nExample B.3.4 (Vector random expansion $\\mathcal{V}_{\\mathrm{rand.}}$ Let $\\mathbf{\\Deltax}\\in\\mathbb{R}^{D_{S}}$ a vector of dimension $D_{S}$ .${\\bf x}_{r a n d}^{*}$ is called the random expanded xof dimension $D_{T}$ with $D_{T}\\geq D_{S}$ \u2265if  \n\n$$\n\\begin{array}{r}{\\mathbf{x}_{r a n d}^{*}=\\mathcal{V}_{r a n d}(\\mathbf{x};\\zeta)=C o n c a t\\left[\\underbrace{\\mathbf{x^{\\intercal}},\\cdots,\\mathbf{x^{\\intercal}}}_{\\lfloor D_{T}/D_{S}\\rfloor},\\zeta^{\\intercal}\\right]^{\\intercal}\\in\\mathbb{R}^{D_{T}},}\\end{array}\n$$  \n\nwhere $\\zeta\\in\\mathbb{R}^{D_{T}}$ mod $D_{S}$ is an arbitrary vector.  \n\nRemark. (1) All vector expansion examples above follow the same pattern. Specifically, when $D_{T}$ expanding from di mod s by $D_{S}$ entries differently. (2) The random vector ating $\\textbf{x}\\lfloor D_{T}/D_{S}\\rfloor D_{S}$ \u230a$D_{S}$ \u230b$D_{T}$ number of times. , all vector expansion methods pad first $\\zeta$ in vector random expansion is arbitrary, Each method deals with the remaining $\\lfloor D_{T}/D_{S}\\rfloor D_{S}$ enso $\\mathcal{V}_{a\\nu g}$ ,$\\mathcal{V}_{z e r o}$ ,$\\mathcal{V}_{c i r c}\\subset\\mathcal{V}_{r a n d}$ . (3) Here all three examples are expansion methods for vectors. In practice, neural networks like Transformers are dealing high dimensional tensors. These tensors can essentially be thought of as collections of vectors. In such scenarios, we can apply the expansion methods separately to the last dimension of these tensors.  \n\nIn the following claim, we show that vectors expanded by these operators are lossless.  \n\n$\\mathcal{V}_{c i r c}$ V, and vector random expansion m 1. Vector average expansio V$\\gamma_{r a n d}$ $\\mathcal{V}_{a\\nu g},$ are all lossless expansion for vectors. , vector zero expansion $\\mathcal{V}_{z e r o}$ , vector circular expansion Proof. The inverse function $\\mathcal{V}^{-1}:\\mathbb{R}^{D_{T}}\\rightarrow\\mathbb{R}^{D_{S}}$ of these vector expansion methods is  \n\n$$\n\\nu^{-1}({\\bf x})={\\bf x}[:D_{S}].\n$$  \n\nRemark. In practice, we want inverse mapping of expansion methods to be easily computed just like the example above.\n\n# B.4LOSSLESS EXPANSION FOR OPERATORS\nWe then give the definition of lossless expansion for operators. These operators apply on tensors, hence our definition of lossless operator expansion is based on lossless expansion in vector space. These operators can be different layers used in Transformer architectures, including LayerNorm, convolutional layers, and fully-connected layers, etc.  \n\nDefinit ansio der vector spaces $S^{i n},S^{o u t},\\mathcal{T}^{i n}$ and $\\mathcal{T}^{o u t}$ such that with $g(\\cdot):S^{i n}\\rightarrow S^{o u t}$ \u00b7$n(S^{i n})\\leq d i m(T^{i n})$ S\u2192S or space e T. We say the ope and dim $d i m\\big(S^{\\bar{o}u t}\\big)\\leq d i m\\big(T^{o u t}\\big)$ S$\\mathcal{V}_{i n}:S^{i\\bar{n}}\\to\\mathcal{T}^{i n}$ \u2264TEMo is $(\\mathcal{V}_{i n},\\mathcal{V}_{o u t})$ VVess output vector space expansion ppose the op -lossless for $g(\\cdot)$ \u00b7or is denoted if there exist $\\mathcal{V}_{o u t}:S^{o u t}\\to\\mathcal{T}^{o u t}$ VS\u2192T such that V$\\mathcal{V}_{o u t}(g(\\mathbf{x}))=\\mathcal{E}[g](\\mathcal{V}_{i n}(\\mathbf{x})),\\forall\\mathbf{x}\\in S^{i n}$ EV\u2200\u2208S .  \n\n$(\\mathcal{V}_{i n},\\mathcal{V}_{o u t})$ Remark. losslessly expanded input, the output of the to be invertible, we do not have restrictions on the operator expansion VV(1) Intuitively, a lossless operator -lossless for the origina $g(\\cdot)$ \u00b7tput. (2) For conciseness, we use \u2018 \u2019 interchangeably. (3) We only require the v Eexpanded oper pansion can be understood a $^{\\cdot}\\mathcal{E}[g]$ Eis a is $(\\mathcal{V}_{i n},\\mathcal{V}_{o u t})$ EVtor expansions .V$\\nu_{o u t}$ ows: when using losslessly expa -lossles V$\\mathcal{V}_{i n}$ and \u2018 and $\\mathcal{E}$ V$\\nu_{o u t}$ $\\mathcal{V}_{i n}$ ed"}, {"ref_id": "454846008144214678", "chunk_id": "3", "score": 0.345703125, "text": "# 3.3 A TRANSFORMATION PER BLOCK\nNow that every LayerNorm in the transformer has been converted to RMSNorm, we can select any $\\mathbf{Q}$ to modify the model. Our initial plan was to collect signals from the model, construct an orthogonal matrix using those signals and to delete parts of the network. We quickly saw that the signals at different blocks of the network were not aligned, and that we would need to apply a different orthogonal matrix at each block, $\\mathbf{Q}_{\\ell}$ .  \n\nAllowing the orthogonal matrix used in each block to differ can be shown to leave the model unchanged using the same proof as Theorem 1 ,  \n\n  \nFigure 3: Converting a transformer network from LayerNorm to RMSNorm: the scale matrix diag $(\\alpha)$ is absorbed into the subsequent matrix $\\mathbf{W}_{\\mathrm{in}}$ . Figure shows the block in combined colors. We use $(\\alpha)$ for brevity. The mean-subtraction matrix $\\mathbf{M}$ is applied to each matrix $\\mathbf{W}_{\\mathrm{out}}$ . Layernorm becomes RMSNorm, up to a constant $\\bar{\\sqrt{D}}$ (not shown). Here, the scaling $(\\alpha^{\\prime})$ comes from the previous block.  \n\n  \nFigure 4: With the network converted to RMSNorm (see Figure 3 ), we apply the computational-invariance idea. The input weight matrices $\\mathrm{diag}(\\alpha)\\mathbf{W}_{\\mathrm{in}}$ are pre-multiplied by $\\mathbf{Q}^{\\top}$ . The output matrices $\\mathbf{W}_{\\mathrm{out}}\\mathbf{M}$ are post-multiplied by $\\mathbf{Q}$ . In the skip-connection, a new linear layer is added $\\mathbf{Q}_{\\ell}^{\\top}\\mathbf{Q}_{\\ell+1}$ . After these modifications, the matrices can be sliced (hatched areas).  \n\nwith the exception of line 5 of Algorithm 1 . Here we see that the residual connection and the output of the block must have the same rotation. To fix this, we modify the residual connection by applying the linear transformation applied to different blocks with the additional linear operation in the residual connection. Unlike the $\\mathbf{Q}_{\\ell-1}^{\\top}\\mathbf{Q}_{\\ell}$ \u2212to the residual. Figure 4 shows how different rotations can be modifications to the weight matrices, these additional operations cannot be pre-computed and add a small $(D\\times D)$ overhead to the model. Nonetheless, they are needed to allow slicing the model (Section 3.4 ) and we see real speedup overall (Section 4 ).  \n\nTo compute the matrices $\\mathbf{Q}_{\\ell}$ , we use PCA. We select a calibration dataset from the training set, run it through the model (after converting LayerNorm operations into RMSNorm), and extract the orthogonal matrix of the layer. We use the output of the transformed network to calculate the orthogonal matrices of the next layers. More precisely, if $\\mathbf{X}_{\\ell,i}$ is the output of the $\\ell^{\\mathrm{th}}$ RMSNorm block for the $i^{\\mathrm{th}}$ sequence in the calibration dataset, we compute  \n\n$$\n\\mathbf{C}_{\\ell}=\\sum_{i}\\mathbf{X}_{\\ell,i}^{\\top}\\mathbf{X}_{\\ell,i}\n$$  \n\nand set $\\mathbf{Q}_{\\ell}$ to the be the eigenvectors of $\\mathbf{C}_{\\ell}$ , sorted by decreasing eigenvalues.\n\n# 3.4 SLICING\nThe goal of Principal Component Analysis is usually to take a data matrix $\\mathbf{X}$ and compute a lower dimensional representation $\\mathbf{Z}$ , and an approximate reconstruction $\\tilde{\\mathbf{X}}$ :  \n\n$$\n\\mathbf{Z}=\\mathbf{X}\\mathbf{Q}\\mathbf{D}\\,,\\qquad\\tilde{\\mathbf{X}}=\\mathbf{Z}\\mathbf{D}^{\\top}\\mathbf{Q}^{\\top}\\,.\n$$  \n\nwhere $\\mathbf{Q}$ is the ectors of ${\\bf X}^{\\top}{\\bf X}$ , and $\\mathbf{D}$ is a $D\\times D_{\\mathrm{small}}$ deletion matrix (containing $D_{\\mathrm{small}}$ The reconstruction is columns of the $D\\times D$ \u00d7$L_{2}$ identity matrix), which removes some of the columns of the matrix to the left. optimal, in the sense that QD is a linear mapping that minimizes $\\lVert\\mathbf{X}-\\tilde{\\mathbf{X}}\\rVert^{2}$ .  \n\nWhen we apply PCA to the signal matrix $\\mathbf{X}$ bween blocks, we never materialize the $N\\times D$ signal matrix, but we apply the deletion matrix Dto the operations preceding and succeeding the construction of that matrix, which have already been multiplied by $\\mathbf{Q}$ in the above. We delete rows of $\\mathbf{W}_{\\mathrm{in}}$ that we have inserted into the residual connection (see Figure and columns of $\\mathbf{W}_{\\mathrm{out}}$ and $\\mathbf{W}_{\\mathrm{embd}}$ . We also delete both rows 4 ). and columns of the matrix $\\mathbf{Q}_{\\ell-1}^{\\top}\\mathbf{Q}_{\\ell}$ \u2212\n\n# 4 EXPERIMENTAL VALIDATION\nSetup We use HuggingFace Transformers ( Wolf et al. ,2019 ) to implement our code with PyTorch (Paszke et al. ,2019 ). The computation of $\\mathbf{Q}$ is performed on a single H100 GPU with 80GB of memory, taking approximately 3.5 hours to complete for the L LAMA -2 70B model. During the PCA calculation, we use double precision for computing the eigenvectors of the covariance matrix. We find that using single precision for eigenvector calculations in PyTorch leads to a discrepancy in the final accuracy, as detailed in Appendix A.2 .  \n\nWe experiment with two different calibration sets: 1024 samples from the WikiText-2 training dataset ( Merity et al. ,2016 ) and 5000 samples from the Alpaca training dataset ( Taori et al. ,2023 ). Sequence lengths are chosen as the maximum of each language model. An ablation study on the calibration set size and sequence length is presented in Appendix A.3 .  \n\nModels, Tasks, and GPUs We evaluate all our experiments on OPT ( Zhang et al. ,2022 ), L LAMA -2 (Touvron et al. ,2023 ) model families, and additionally evaluate Phi-2 (in our zero-shot task) experiments. We exclude OPT 175B, as it is outperformed by smaller L LAMA -2 models. Nonetheless, we anticipate that this larger model will yield improved results, as larger models typically offer more promising opportunities for compression (see Section 4.1 ). We evaluate our scheme on both language generation as well as popular zero-shot tasks. To demonstrate the comprehensive speedup achieved by SliceGPT we use: Quadro RTX6000 GPUs with 24GB of memory as a representative example of consumer-level GPUs; 40GB A100s and 80GB H100s to provide datacenter-level benchmarks.  \n\nBaseline Setup We initially planned to compare our results against a scheme that pruned columns (or rows) with the smallest norm but found that this baseline was very poor, with the perplexity of the model soaring into the 1000s after pruning just a few columns. Instead, we compare SliceGPT against SparseGPT ( Frantar & Alistarh ,2023 ) employing a 2:4 sparsity ratio, as this is the only sparsity scheme which achieves speedup ( Mishra et al. ,2021 )."}, {"ref_id": "454965248874515858", "chunk_id": "2", "score": 0.3125, "text": "# 5 I NTUITIONS BEHIND LAYER NORM TUNING\nIn this section, driven by the empirical success of LayerNorm tuning, we explore the intuitions behind LayerNorm from three perspectives, domain adaptation, expressive power, and gradient variance.  \n\nTable 3: Model performance on different data types. Methods with 80K and Conv.20K suffix are tuned on the full 80K data and the 20K conversational data, respectively.   \n\n\n<html><body><table><tr><td>Method</td><td>MME</td><td>VQAv2</td><td>MSCOCO</td><td>Flickr30k</td><td>POPE</td></tr><tr><td colspan=\"6\">MM-V1CUNA-7B</td></tr><tr><td>Finetune-80K</td><td>625.2/270.7</td><td>15.40</td><td>67.50</td><td>34.61</td><td>73.8/76.5/66.5</td></tr><tr><td>LayerNorm-80K</td><td>723.2/253.2</td><td>17.06</td><td>80.89</td><td>48.01</td><td>76.1/81.1/70.8</td></tr><tr><td>LayerNorm-Conv. 20K</td><td>777.1/231.4</td><td>15.39</td><td>67.30</td><td>40.33</td><td>75.2/79.2/68.8</td></tr><tr><td colspan=\"6\">MM-LLAMA2-7B</td></tr><tr><td>Finetune-80K</td><td>661.3/237.1</td><td>16.09</td><td>65.08</td><td>31.64</td><td>56.3/65.0/55.4</td></tr><tr><td>LayerNorm-80K</td><td>583.2/200.7</td><td>16.78</td><td>88.85</td><td>49.24</td><td>66.6/68.5/64.9</td></tr><tr><td>LayerNorm-Conv. 20K</td><td>376.2/157.5</td><td>16.19</td><td>86.80</td><td>44.88</td><td>50.5/50.7/50.3</td></tr><tr><td colspan=\"6\">MM-LLAMA2-CHAT-7B</td></tr><tr><td>Finetune-80K</td><td>805.4/234.6</td><td>15.29</td><td>57.40</td><td>26.70</td><td>60.3/69.8/57.9</td></tr><tr><td>LayerNorm-80K</td><td>651.3/219.3</td><td>16.60</td><td>75.34</td><td>43.75</td><td>71.3/72.4/67.8</td></tr><tr><td>LayerNorm-Conv. 20K</td><td>482.9/172.1</td><td>13.88</td><td>66.85</td><td>41.95</td><td>62.7/71.7/61.3</td></tr><tr><td colspan=\"6\">MM-LLAMA2-13B</td></tr><tr><td>Finetune-80K</td><td>402.3/199.3</td><td>18.33</td><td>73.88</td><td>45.33</td><td>51.6/51.1/52.2</td></tr><tr><td>LayerNorm-80K</td><td>526.0/177.5</td><td>15.31</td><td>82.92</td><td>48.42</td><td>60.0/69.1/58.9</td></tr><tr><td>LayerNorm-Conv. 20K</td><td>646.0/242.9</td><td>16.01</td><td>76.50</td><td>44.86</td><td>70.0/76.9/68.6</td></tr><tr><td colspan=\"6\">MM-LLAMA2-CHAT-13B</td></tr><tr><td>Finetune-80K</td><td>623.3/221.4</td><td>15.17</td><td>64.19</td><td>41.82</td><td>67.6/64.8/64.5</td></tr><tr><td>LayerNorm-80K</td><td>929.3/254.3</td><td>16.10</td><td>74.96</td><td>42.79</td><td>78.9/83.9/74.3</td></tr><tr><td>LayerNorm-Conv. 20K</td><td>769.7/227.5</td><td>15.57</td><td>73.30</td><td>43.08</td><td>68.2/72.8/65.3</td></tr></table></body></html>  \n\nTable 4: Results of models with LayerNorm and/or vision-language Connector activated.   \n\n\n<html><body><table><tr><td>Method</td><td>MME</td><td>VQAv2</td><td>MSCOCO</td><td>Flickr30k</td><td>POPE</td></tr><tr><td colspan=\"6\">MM-LLAMA2-7B</td></tr><tr><td>LayerNorm + Connector</td><td>583.2/200.7</td><td>16.78</td><td>88.85</td><td>49.24</td><td>66.6/68.5/64.9</td></tr><tr><td>Connector</td><td>311.1/105.4</td><td>12.72</td><td>60.43</td><td>35.91</td><td>67.9/73.7/66.9</td></tr><tr><td>LayerNorm</td><td>395.0/191.4</td><td>18.18</td><td>80.13</td><td>41.68</td><td>50.3/51.3/50.2</td></tr><tr><td colspan=\"6\">MM-LLAMA2-13B</td></tr><tr><td>LayerNorm + Connector</td><td>526.0/177.5</td><td>15.31</td><td>82.92</td><td>48.42</td><td>60.0/69.1/58.9</td></tr><tr><td>Connector</td><td>507.0/187.9</td><td>15.22</td><td>62.60</td><td>25.13</td><td>60.9/66.8/60.1</td></tr><tr><td>LayerNorm</td><td>405.0/188.6</td><td>16.51</td><td>70.41</td><td>39.86</td><td>50.9/52.7/51.0</td></tr></table></body></html>\n\n# 5.1 LAYER NORM TUNING A DAPTS LLM S TO MULTI -M ODAL\nInfluence of the Vision-Language Connector The vision-language connector serves as the converter to project features from the vision encoder to the LLM domain. In our previous experiments, we focused on finetuning the LLM component of the MLLMs while keeping the vision-language connector activated by default. To determine which component plays a more important role for domain adaptation of LLM to multi-modal domain, we performed an ablation study by activating the two components separately. Results are presented in table 4 , tuning LayerNorm in attention blocks without activating the vision-language connector resulted in only a $4.2\\%$ and $5.4\\%$ decrease in performance on three traditional multi-modal tasks and the MME benchmark, respectively. This decrease is significantly lower than the $15.6\\%$ and $9.2\\%$ downgrade observed when only activating the Connector on the same tasks. This observation highlights the vital role LayerNorm plays in transforming knowledge from the vision domain to language, indicating LayerNorm as a strong domain adaptor for the LLM architecture.  \n\n  \n\nFigure 3: Layer similarities between different LLM layers in (a) Finetuned and (b) LayerNorm-tuned MM-V ICUNA -7B. The average layer similarity of two models are 0.624 and 0.585, respectively.  \n\nTable 5: Results of models with LL A MA2 Finetuned/LayerNorm-tuned with ViT pre-trained on ImageNet (Deng et al. ,2009 ), which have not been aligned with the language domain.   \n\n\n<html><body><table><tr><td></td><td>MME</td><td>VQAv2</td><td>MSCOCO</td><td>Flickr30k</td><td>POPE</td></tr><tr><td>Finetune-7B</td><td>406.79/182.5</td><td>15.05</td><td>47.75</td><td>18.97</td><td>50.0/51.6/50.1</td></tr><tr><td>LayerNorm-7B</td><td>301.51/127.14</td><td>15.48</td><td>66.22</td><td>31.73</td><td>50.0/50.1/50.1</td></tr><tr><td>Finetune-13B</td><td>375.41/171.79</td><td>25.38</td><td>51.26</td><td>25.96</td><td>50.3/51.1/51.0</td></tr><tr><td>LayerNorm-13B</td><td>445.98/150.0</td><td>15.59</td><td>64.63</td><td>32.17</td><td>51.2/53.0/50.8</td></tr></table></body></html>  \n\nSwitching Visual Features. We employ the ViT encoder from CLIP ( Radford et al. ,2021 ) by default in our previous experiments. CLIP ( Radford et al. ,2021 ) models are trained with image-text contrastive loss, thus its feature space is already aligned with language. Since LayerNorm has shown its effectiveness as a domain adaptor, we are interested in testing whether or not LayerNorm tuning can adapt a LLM to image features that are not pretrained to align with language. The vision encoder is switched to a ViT model that was pretrained on ImageNet (Dosovitskiy et al. ,2021 ;Deng et al. ,2009 ). Results in table 5 demonstrate that both LayerNorm and finetuning approaches can yield high performance. Interestingly, we observe that by LayerNorm tuning with ImageNet trained ViT, which has not been aligned with language, the model is able to achieve comparable performance to full parameter finetuning , i.e ., results show that LayerNorm tuning outperforms finetuning by $12.0\\%$ on captioning tasks, but performs slightly worse by $5.0\\%$ on the MME benchmark. These results again indicates the domain adaptor role of the LayerNorm , hinting the reason behind the empircal success of LayerNorm tuning. Furthermore, it is worth noting that the performance of MLLMs incorporating ViT pretrained on ImageNet is generally inferior to that of CLIP\u2019s vision encoder. This observation provides compelling evidence that, despite differences in tokenizer and training paradigm between CLIP\u2019s text encoder and LL A MA\u2019s, ViT from CLIP has the capacity to learn general patterns of language formulation during pre-training. Thus, significantly enhance MLLM abilities.\n\n# 5.2 LAYER NORM TUNING I MPROVES THE EXPRESSIVE POWER\nIt is shown in Pires et al. (2023 ) that a Transformer model incorporating anisotropic layer representation can capture a wider range of learning patterns. By computing the cosine similarities between all layers in the LLM of a finetuned MLLM, we aim to investigate whether the improved efficiency is the results of the improved expressive power. In table 6 , we present the average layer similarity of three 7B scale MLLMs, and in fig. 3 we present the visualization of per layer similarity scores of MM-V ICUNA -7B. Our analysis reveals that the transformer layers in the MLLMs with LayerNorm tuning exhibit a clear distinction from one another ( i.e ., an average $10.6\\%$ lower layer similarities comparing finetuning), indicating superior generalization ability and expressive power compared to finetuning. This finding sheds light on why tuning LayerNorm is effective for multi-modal LLM training. For additional visualizations, please refer to the Appendix A.2.1 .  \n\n  \nFigure 4: Gradients of the input LayerNorm in the 11th layer of the MM-V ICUNA as training proceeds. LayerNorm-tuned model has lower gradient variance than full parameter finetuning.  \n\nTable 6: Layer representation similarity of LayerNorm and finetuning methods on three 7B MLLMs.   \nLower the similarity is, the better expressive power a model possesses.  \n\n<html><body><table><tr><td>Model</td><td>LayerNorm Sim.</td><td>Finetuning Sim.</td></tr><tr><td>MM-VICUNA</td><td>0.585</td><td>0.624</td></tr><tr><td>MM-LLAMA2</td><td>0.504</td><td>0.591</td></tr><tr><td>MM-LLAMA2-CHAT</td><td>0.550</td><td>0.617</td></tr></table></body></html>"}], "task_step_question_answer": "\u5728\u5177\u4f53\u5e94\u7528\u573a\u666f\u4e2d\uff0cLayerNorm\u5728\u9700\u8981\u9ad8\u6cdb\u5316\u80fd\u529b\u7684\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f8b\u5982\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u4efb\u52a1\u4e2d\uff0cLayerNorm\u80fd\u591f\u66f4\u597d\u5730\u5904\u7406\u4e0d\u540c\u6a21\u6001\u4e4b\u95f4\u7684\u5206\u5e03\u5dee\u5f02\uff0c\u63d0\u5347\u6a21\u578b\u7684\u878d\u5408\u6548\u679c\u3002\u800cRMSNorm\u5728\u8ba1\u7b97\u8d44\u6e90\u53d7\u9650\u7684\u573a\u666f\u4e0b\u66f4\u5177\u4f18\u52bf\uff0c\u4f8b\u5982\u5728\u5904\u7406\u5927\u89c4\u6a21\u91d1\u878d\u6570\u636e\u65f6\uff0cRMSNorm\u901a\u8fc7\u7b80\u5316\u5f52\u4e00\u5316\u8fc7\u7a0b\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u8bad\u7ec3\u65f6\u95f4\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8f83\u9ad8\u7684\u9884\u6d4b\u7cbe\u5ea6\u3002\u5b9e\u9a8c\u6570\u636e\u8868\u660e\uff0c\u5728\u67d0\u4e2a\u8054\u90a6\u5b66\u4e60\u4efb\u52a1\u4e2d\uff0c\u4f7f\u7528LayerNorm\u7684\u6a21\u578b\u5728\u51c6\u786e\u7387\u4e0a\u6bd4RMSNorm\u9ad8\u51fa\u7ea62%\uff0c\u800cRMSNorm\u7684\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c11\u4e86\u7ea615%\u3002\u8fd9\u4e9b\u5b9e\u9a8c\u7ed3\u679c\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86LayerNorm\u548cRMSNorm\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u9002\u7528\u6027\u548c\u6027\u80fd\u5dee\u5f02\u3002", "ref_task_step_id": ""}, "__type__": "task_step_node"}}, "task_step_store/ref_task_step_info": {"": {"node_ids": ["84f82b99-b788-4ad1-b84c-6105c1054610", "f5dda6c2-b14a-44f3-873e-1c63177625a7", "16fd2454-56f9-4eff-bce6-29f5553e5158", "e1acadaf-e089-43ba-ab18-88154b3511b2", "a2cf4bda-afc9-49b6-a3ae-068f5230e9aa", "c76168e1-9469-4632-b421-6a823da99729", "3a7d70c7-f009-48af-ae58-c4dce5e11011", "51c8b9ac-a041-4af7-b164-9d6e7eb750af", "299715cd-eee2-4361-ac2c-b3ffbb24a419", "931c6468-98e1-420f-b767-f61edb29e041", "3d3ab9bd-1567-4130-9630-e6b97d28fc08", "a80dc82f-c839-4f7f-9eb2-afc1e86a3cac", "60873e05-37b0-4b0f-aaf8-5e2b4f988558", "311bf04e-2345-4be4-b40b-44f231f000ff", "ac024820-e0dd-4e64-b24a-658c8b08cd94", "f2e5ad03-a297-4065-b995-c2dd46e1e9b4", "cc774f90-c346-49a4-aab8-dc67fb5f019a", "f189b0a8-ec1b-4435-ae46-18924bfa2573", "a630c561-b046-4008-9e79-d2d2c64db138", "1cabe3a5-9d12-494f-bccd-63795401d3a1", "e4184b0a-c68b-4ef1-88c7-35cff3095e16", "c76168e1-9469-4632-b421-6a823da99729", "a2cf4bda-afc9-49b6-a3ae-068f5230e9aa", "16fd2454-56f9-4eff-bce6-29f5553e5158", "84f82b99-b788-4ad1-b84c-6105c1054610", "e1acadaf-e089-43ba-ab18-88154b3511b2", "f5dda6c2-b14a-44f3-873e-1c63177625a7", "c76168e1-9469-4632-b421-6a823da99729", "16fd2454-56f9-4eff-bce6-29f5553e5158", "c76168e1-9469-4632-b421-6a823da99729", "a2cf4bda-afc9-49b6-a3ae-068f5230e9aa", "84f82b99-b788-4ad1-b84c-6105c1054610", "f5dda6c2-b14a-44f3-873e-1c63177625a7", "e1acadaf-e089-43ba-ab18-88154b3511b2", "c76168e1-9469-4632-b421-6a823da99729", "c76168e1-9469-4632-b421-6a823da99729", "3a7d70c7-f009-48af-ae58-c4dce5e11011", "3a7d70c7-f009-48af-ae58-c4dce5e11011", "16fd2454-56f9-4eff-bce6-29f5553e5158", "a2cf4bda-afc9-49b6-a3ae-068f5230e9aa", "e1acadaf-e089-43ba-ab18-88154b3511b2", "f5dda6c2-b14a-44f3-873e-1c63177625a7", "84f82b99-b788-4ad1-b84c-6105c1054610", "3a7d70c7-f009-48af-ae58-c4dce5e11011", "a2cf4bda-afc9-49b6-a3ae-068f5230e9aa", "a2cf4bda-afc9-49b6-a3ae-068f5230e9aa", "51c8b9ac-a041-4af7-b164-9d6e7eb750af", "16fd2454-56f9-4eff-bce6-29f5553e5158", "16fd2454-56f9-4eff-bce6-29f5553e5158", "51c8b9ac-a041-4af7-b164-9d6e7eb750af", "299715cd-eee2-4361-ac2c-b3ffbb24a419", "e1acadaf-e089-43ba-ab18-88154b3511b2", "e1acadaf-e089-43ba-ab18-88154b3511b2", "f5dda6c2-b14a-44f3-873e-1c63177625a7", "f5dda6c2-b14a-44f3-873e-1c63177625a7", "931c6468-98e1-420f-b767-f61edb29e041", "299715cd-eee2-4361-ac2c-b3ffbb24a419", "931c6468-98e1-420f-b767-f61edb29e041", "3d3ab9bd-1567-4130-9630-e6b97d28fc08", "3d3ab9bd-1567-4130-9630-e6b97d28fc08", "84f82b99-b788-4ad1-b84c-6105c1054610", "84f82b99-b788-4ad1-b84c-6105c1054610", "a80dc82f-c839-4f7f-9eb2-afc1e86a3cac", "3a7d70c7-f009-48af-ae58-c4dce5e11011", "3a7d70c7-f009-48af-ae58-c4dce5e11011", "a80dc82f-c839-4f7f-9eb2-afc1e86a3cac", "60873e05-37b0-4b0f-aaf8-5e2b4f988558", "60873e05-37b0-4b0f-aaf8-5e2b4f988558", "51c8b9ac-a041-4af7-b164-9d6e7eb750af", "299715cd-eee2-4361-ac2c-b3ffbb24a419", "931c6468-98e1-420f-b767-f61edb29e041", "a80dc82f-c839-4f7f-9eb2-afc1e86a3cac", "3d3ab9bd-1567-4130-9630-e6b97d28fc08", "60873e05-37b0-4b0f-aaf8-5e2b4f988558", "51c8b9ac-a041-4af7-b164-9d6e7eb750af", "51c8b9ac-a041-4af7-b164-9d6e7eb750af", "311bf04e-2345-4be4-b40b-44f231f000ff", "311bf04e-2345-4be4-b40b-44f231f000ff", "299715cd-eee2-4361-ac2c-b3ffbb24a419", "299715cd-eee2-4361-ac2c-b3ffbb24a419", "ac024820-e0dd-4e64-b24a-658c8b08cd94", "ac024820-e0dd-4e64-b24a-658c8b08cd94", "931c6468-98e1-420f-b767-f61edb29e041", "931c6468-98e1-420f-b767-f61edb29e041", "f2e5ad03-a297-4065-b995-c2dd46e1e9b4", "f2e5ad03-a297-4065-b995-c2dd46e1e9b4", "a80dc82f-c839-4f7f-9eb2-afc1e86a3cac", "a80dc82f-c839-4f7f-9eb2-afc1e86a3cac", "cc774f90-c346-49a4-aab8-dc67fb5f019a", "cc774f90-c346-49a4-aab8-dc67fb5f019a", "311bf04e-2345-4be4-b40b-44f231f000ff", "3d3ab9bd-1567-4130-9630-e6b97d28fc08", "3d3ab9bd-1567-4130-9630-e6b97d28fc08", "f189b0a8-ec1b-4435-ae46-18924bfa2573", "f189b0a8-ec1b-4435-ae46-18924bfa2573", "60873e05-37b0-4b0f-aaf8-5e2b4f988558", "60873e05-37b0-4b0f-aaf8-5e2b4f988558", "a630c561-b046-4008-9e79-d2d2c64db138", "a630c561-b046-4008-9e79-d2d2c64db138", "f2e5ad03-a297-4065-b995-c2dd46e1e9b4", "ac024820-e0dd-4e64-b24a-658c8b08cd94", "cc774f90-c346-49a4-aab8-dc67fb5f019a", "311bf04e-2345-4be4-b40b-44f231f000ff", "311bf04e-2345-4be4-b40b-44f231f000ff", "1cabe3a5-9d12-494f-bccd-63795401d3a1", "1cabe3a5-9d12-494f-bccd-63795401d3a1", "f189b0a8-ec1b-4435-ae46-18924bfa2573", "ac024820-e0dd-4e64-b24a-658c8b08cd94", "ac024820-e0dd-4e64-b24a-658c8b08cd94", "cc774f90-c346-49a4-aab8-dc67fb5f019a", "cc774f90-c346-49a4-aab8-dc67fb5f019a", "e4184b0a-c68b-4ef1-88c7-35cff3095e16", "a630c561-b046-4008-9e79-d2d2c64db138", "1cabe3a5-9d12-494f-bccd-63795401d3a1", "e4184b0a-c68b-4ef1-88c7-35cff3095e16", "1cabe3a5-9d12-494f-bccd-63795401d3a1", "1cabe3a5-9d12-494f-bccd-63795401d3a1", "e4184b0a-c68b-4ef1-88c7-35cff3095e16", "e4184b0a-c68b-4ef1-88c7-35cff3095e16", "e4184b0a-c68b-4ef1-88c7-35cff3095e16", "c76168e1-9469-4632-b421-6a823da99729", "c76168e1-9469-4632-b421-6a823da99729", "84f82b99-b788-4ad1-b84c-6105c1054610", "84f82b99-b788-4ad1-b84c-6105c1054610", "e1acadaf-e089-43ba-ab18-88154b3511b2", "e1acadaf-e089-43ba-ab18-88154b3511b2", "f5dda6c2-b14a-44f3-873e-1c63177625a7", "f5dda6c2-b14a-44f3-873e-1c63177625a7", "a2cf4bda-afc9-49b6-a3ae-068f5230e9aa", "a2cf4bda-afc9-49b6-a3ae-068f5230e9aa", "16fd2454-56f9-4eff-bce6-29f5553e5158", "16fd2454-56f9-4eff-bce6-29f5553e5158", "299715cd-eee2-4361-ac2c-b3ffbb24a419", "299715cd-eee2-4361-ac2c-b3ffbb24a419", "931c6468-98e1-420f-b767-f61edb29e041"], "metadata": {}}}, "task_step_store/metadata": {"84f82b99-b788-4ad1-b84c-6105c1054610": {"task_step_hash": "", "ref_task_step_id": ""}, "f5dda6c2-b14a-44f3-873e-1c63177625a7": {"task_step_hash": "", "ref_task_step_id": ""}, "16fd2454-56f9-4eff-bce6-29f5553e5158": {"task_step_hash": "", "ref_task_step_id": ""}, "e1acadaf-e089-43ba-ab18-88154b3511b2": {"task_step_hash": "", "ref_task_step_id": ""}, "a2cf4bda-afc9-49b6-a3ae-068f5230e9aa": {"task_step_hash": "", "ref_task_step_id": ""}, "c76168e1-9469-4632-b421-6a823da99729": {"task_step_hash": "", "ref_task_step_id": ""}, "3a7d70c7-f009-48af-ae58-c4dce5e11011": {"task_step_hash": "", "ref_task_step_id": ""}, "51c8b9ac-a041-4af7-b164-9d6e7eb750af": {"task_step_hash": "", "ref_task_step_id": ""}, "299715cd-eee2-4361-ac2c-b3ffbb24a419": {"task_step_hash": "", "ref_task_step_id": ""}, "931c6468-98e1-420f-b767-f61edb29e041": {"task_step_hash": "", "ref_task_step_id": ""}, "3d3ab9bd-1567-4130-9630-e6b97d28fc08": {"task_step_hash": "", "ref_task_step_id": ""}, "a80dc82f-c839-4f7f-9eb2-afc1e86a3cac": {"task_step_hash": "", "ref_task_step_id": ""}, "60873e05-37b0-4b0f-aaf8-5e2b4f988558": {"task_step_hash": "", "ref_task_step_id": ""}, "311bf04e-2345-4be4-b40b-44f231f000ff": {"task_step_hash": "", "ref_task_step_id": ""}, "ac024820-e0dd-4e64-b24a-658c8b08cd94": {"task_step_hash": "", "ref_task_step_id": ""}, "f2e5ad03-a297-4065-b995-c2dd46e1e9b4": {"task_step_hash": "", "ref_task_step_id": ""}, "cc774f90-c346-49a4-aab8-dc67fb5f019a": {"task_step_hash": "", "ref_task_step_id": ""}, "f189b0a8-ec1b-4435-ae46-18924bfa2573": {"task_step_hash": "", "ref_task_step_id": ""}, "a630c561-b046-4008-9e79-d2d2c64db138": {"task_step_hash": "", "ref_task_step_id": ""}, "1cabe3a5-9d12-494f-bccd-63795401d3a1": {"task_step_hash": "", "ref_task_step_id": ""}, "e4184b0a-c68b-4ef1-88c7-35cff3095e16": {"task_step_hash": "", "ref_task_step_id": ""}}}