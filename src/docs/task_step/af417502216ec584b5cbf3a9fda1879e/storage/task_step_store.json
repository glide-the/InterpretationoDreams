{"task_step_store/data": {"d753c815-d196-4a48-b95c-c565926fb392": {"__data__": {"id_": "d753c815-d196-4a48-b95c-c565926fb392", "metadata": {}, "relationships": {}, "hash": "", "start_task_context": "\u5927\u6a21\u578b\u4e2d\u7684LayerNorm\u548cRMSNorm\u6709\u4ec0\u4e48\u533a\u522b\uff1f", "aemo_representation_context": "### \u5206\u6790\u8fd1\u51e0\u5e74\u7814\u7a76\u9886\u57df\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba\n\u5728\u5927\u6a21\u578b\u76f8\u5173\u7814\u7a76\u9886\u57df\uff0c\u5f52\u4e00\u5316\u6280\u672f\u662f\u91cd\u8981\u7684\u7814\u7a76\u65b9\u5411\u4e4b\u4e00\u3002LayerNorm \u548c RMSNorm \u4f5c\u4e3a\u5176\u4e2d\u5173\u952e\u7684\u5f52\u4e00\u5316\u65b9\u6cd5\uff0c\u88ab\u4f17\u591a\u7814\u7a76\u6df1\u5165\u63a2\u8ba8\u3002\u7814\u7a76\u8005\u4eec\u4ece\u7406\u8bba\u5206\u6790\u3001\u5b9e\u9a8c\u5bf9\u6bd4\u7b49\u591a\u65b9\u9762\u5bf9\u5b83\u4eec\u8fdb\u884c\u5256\u6790\uff0c\u4ee5\u660e\u786e\u5176\u5728\u5927\u6a21\u578b\u6280\u672f\u6846\u67b6\u4e2d\u7684\u5730\u4f4d\u548c\u4f5c\u7528\u3002\n\n### \u7814\u7a76\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u4e3b\u8981\u6846\u67b6\uff08\u5982 Transformer\u3001GAN\u3001BERT \u7b49\uff09\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0e\u53d8\u4f53\n\u5728 Transformer \u67b6\u6784\u4e3a\u4e3b\u7684\u4f17\u591a\u6a21\u578b\u4e2d\uff0cLayerNorm \u548c RMSNorm \u88ab\u5e7f\u6cdb\u5e94\u7528\u3002\u4f8b\u5982\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\uff08\u5982 BERT \u53ca\u5176\u53d8\u4f53\u6a21\u578b\uff09\u4e2d\uff0cLayerNorm \u6700\u521d\u88ab\u7528\u4e8e\u7a33\u5b9a\u6a21\u578b\u8bad\u7ec3\u3001\u52a0\u901f\u6536\u655b\u4ee5\u53ca\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u80fd\u529b\uff1b\u800c RMSNorm \u4e5f\u9010\u6e10\u5728\u4e00\u4e9b\u57fa\u4e8e Transformer \u7684\u6a21\u578b\u4e2d\u5f97\u5230\u5e94\u7528\uff0c\u4ee5\u6539\u8fdb\u6a21\u578b\u7684\u6027\u80fd\u8868\u73b0\u3002\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\uff08\u5982\u57fa\u4e8e Transformer \u7684\u89c6\u89c9\u6a21\u578b\uff09\u4e2d\u540c\u6837\u6709\u5b83\u4eec\u7684\u8eab\u5f71\uff0c\u4e0d\u540c\u4efb\u52a1\u573a\u666f\u4e0b\u5bf9\u8fd9\u4e24\u79cd\u5f52\u4e00\u5316\u65b9\u6cd5\u8fdb\u884c\u4e86\u9002\u5e94\u6027\u8c03\u6574\u548c\u53d8\u4f53\u5e94\u7528\uff0c\u4ee5\u66f4\u597d\u5730\u9002\u914d\u4efb\u52a1\u9700\u6c42\u3002\n\n### \u8bc4\u4f30\u5b66\u672f\u754c\u7684\u6280\u672f\u8fdb\u6b65\u4e0e\u5c40\u9650\u6027\n - **\u6280\u672f\u8fdb\u6b65**\uff1aLayerNorm \u548c RMSNorm \u7684\u51fa\u73b0\u663e\u8457\u63d0\u5347\u4e86\u5927\u6a21\u578b\u8bad\u7ec3\u7684\u7a33\u5b9a\u6027\u548c\u6548\u7387\u3002LayerNorm \u80fd\u591f\u5bf9\u6bcf\u4e2a\u6837\u672c\u7684\u4e0d\u540c\u7ef4\u5ea6\u8fdb\u884c\u5f52\u4e00\u5316\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5185\u90e8\u534f\u53d8\u91cf\u504f\u79fb\u95ee\u9898\uff1bRMSNorm \u5219\u901a\u8fc7\u66f4\u7b80\u6d01\u7684\u8ba1\u7b97\u65b9\u5f0f\uff0c\u5728\u4e00\u4e9b\u573a\u666f\u4e0b\u53d6\u5f97\u4e86\u4e0e LayerNorm \u76f8\u5f53\u751a\u81f3\u66f4\u597d\u7684\u6548\u679c\uff0c\u63a8\u52a8\u4e86\u5927\u6a21\u578b\u6027\u80fd\u7684\u63d0\u5347\u3002\n - **\u5c40\u9650\u6027**\uff1a\u5c3d\u7ba1\u53d6\u5f97\u8fdb\u6b65\uff0c\u4f46\u4ecd\u5b58\u5728\u4e00\u4e9b\u95ee\u9898\u3002\u4f8b\u5982\uff0c\u5728\u67d0\u4e9b\u590d\u6742\u4efb\u52a1\u548c\u7279\u5b9a\u6570\u636e\u96c6\u4e0a\uff0c\u6a21\u578b\u4f9d\u7136\u5b58\u5728\u6027\u80fd\u74f6\u9888\uff0c\u5373\u4f7f\u4f7f\u7528\u4e86\u8fd9\u4e9b\u5f52\u4e00\u5316\u65b9\u6cd5\uff0c\u6a21\u578b\u504f\u5dee\u548c\u6570\u636e\u4f9d\u8d56\u95ee\u9898\u4f9d\u65e7\u5b58\u5728\u3002\u800c\u4e14\u5728\u9762\u5bf9\u591a\u6a21\u6001\u6570\u636e\u65f6\uff0c\u73b0\u6709\u7684\u5f52\u4e00\u5316\u65b9\u6cd5\u5728\u878d\u5408\u4e0d\u540c\u6a21\u6001\u7279\u5f81\u65b9\u9762\u8fd8\u5b58\u5728\u4e0d\u8db3\u3002\n\n### \u63a2\u8ba8\u8ba1\u7b97\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\n - **\u9002\u7528\u6027**\uff1aLayerNorm \u5728\u591a\u79cd\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6570\u636e\u96c6\u548c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8f83\u597d\u7684\u9002\u7528\u6027\uff0c\u80fd\u591f\u9002\u5e94\u4e0d\u540c\u89c4\u6a21\u548c\u7279\u70b9\u7684\u6570\u636e\u3002RMSNorm \u5728\u4e00\u4e9b\u5927\u89c4\u6a21\u6570\u636e\u548c\u7279\u5b9a\u4efb\u52a1\uff08\u5982\u957f\u5e8f\u5217\u5904\u7406\uff09\u4e2d\u5c55\u73b0\u51fa\u72ec\u7279\u4f18\u52bf\u3002\u7136\u800c\uff0c\u5728\u591a\u9886\u57df\u3001\u591a\u6a21\u6001\u6570\u636e\u573a\u666f\u4e0b\uff0c\u5b83\u4eec\u7684\u9002\u7528\u6027\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u6539\u8fdb\u4ee5\u66f4\u597d\u5730\u5904\u7406\u4e0d\u540c\u7c7b\u578b\u6570\u636e\u7684\u878d\u5408\u3002\n - **\u6cdb\u5316\u80fd\u529b**\uff1a\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\uff0c\u8fd9\u4e24\u79cd\u5f52\u4e00\u5316\u65b9\u6cd5\u6709\u52a9\u4e8e\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u5728\u590d\u6742\u591a\u53d8\u7684\u73b0\u5b9e\u5e94\u7528\u573a\u666f\u4e2d\uff0c\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u4ecd\u6709\u5f85\u63d0\u9ad8\uff0c\u5c24\u5176\u662f\u5728\u8de8\u9886\u57df\u548c\u591a\u6a21\u6001\u6570\u636e\u7684\u6cdb\u5316\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002\n\n### \u5206\u6790\u6700\u65b0\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027\n - **\u7a33\u5b9a\u6027**\uff1a\u7814\u7a76\u53d1\u73b0\uff0cLayerNorm \u901a\u8fc7\u5bf9\u8f93\u5165\u6570\u636e\u7684\u5f52\u4e00\u5316\u5904\u7406\uff0c\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u63d0\u9ad8\u4e86\u6a21\u578b\u8bad\u7ec3\u7684\u7a33\u5b9a\u6027\u3002RMSNorm \u540c\u6837\u5728\u4e00\u4e9b\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u7a33\u5b9a\u6027\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u52a8\u6001\u73af\u5883\u4e0b\uff0c\u90e8\u5206\u7814\u7a76\u9488\u5bf9\u5176\u7a33\u5b9a\u6027\u8fdb\u884c\u4e86\u4f18\u5316\uff0c\u4f7f\u5176\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\u4e5f\u80fd\u4fdd\u6301\u76f8\u5bf9\u7a33\u5b9a\u7684\u8868\u73b0\u3002\n - **\u5bb9\u9519\u6027**\uff1a\u5728\u9762\u5bf9\u6570\u636e\u566a\u58f0\u548c\u5f02\u5e38\u503c\u65f6\uff0c\u8fd9\u4e24\u79cd\u5f52\u4e00\u5316\u65b9\u6cd5\u90fd\u6709\u4e00\u5b9a\u7684\u5bb9\u9519\u80fd\u529b\uff0c\u4f46\u5728\u6781\u7aef\u60c5\u51b5\u4e0b\uff0c\u6a21\u578b\u6027\u80fd\u4ecd\u4f1a\u53d7\u5230\u5f71\u54cd\uff0c\u5bb9\u9519\u6027\u65b9\u9762\u8fd8\u6709\u63d0\u5347\u7a7a\u95f4\u3002\n\n### \u8bc4\u4f30\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218\n - **\u672a\u6765\u7814\u7a76\u65b9\u5411**\uff1a\u8bb8\u591a\u8bba\u6587\u63d0\u51fa\u63a2\u7d22\u66f4\u9002\u7528\u4e8e\u591a\u6a21\u6001\u6570\u636e\u7684\u5f52\u4e00\u5316\u65b9\u6cd5\uff0c\u4ee5\u53ca\u5982\u4f55\u8fdb\u4e00\u6b65\u4f18\u5316 LayerNorm \u548c RMSNorm \u4ee5\u63d0\u5347\u6a21\u578b\u5728\u590d\u6742\u573a\u666f\u4e0b\u7684\u6027\u80fd\u3002\u540c\u65f6\uff0c\u7814\u7a76\u5982\u4f55\u589e\u5f3a\u5f52\u4e00\u5316\u65b9\u6cd5\u5bf9\u6a21\u578b\u504f\u5dee\u548c\u6570\u636e\u4f9d\u8d56\u95ee\u9898\u7684\u89e3\u51b3\u80fd\u529b\u4e5f\u662f\u91cd\u8981\u65b9\u5411\u3002\n - **\u6311\u6218**\uff1a\u65b0\u7684\u7814\u7a76\u95ee\u9898\u5305\u62ec\u5982\u4f55\u8bbe\u8ba1\u4e00\u79cd\u901a\u7528\u7684\u5f52\u4e00\u5316\u7b56\u7565\uff0c\u80fd\u591f\u65e0\u7f1d\u9002\u5e94\u4e0d\u540c\u9886\u57df\u548c\u6a21\u6001\u7684\u6570\u636e\uff1b\u5982\u4f55\u5728\u63d0\u5347\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\uff0c\u8fdb\u4e00\u6b65\u63d0\u9ad8\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u548c\u5bb9\u9519\u6027\u3002\u8fd9\u4e9b\u6311\u6218\u63a8\u52a8\u540e\u7eed\u7814\u7a76\u4e0d\u65ad\u63a2\u7d22\u65b0\u7684\u6280\u672f\u548c\u65b9\u6cd5\uff0c\u4ee5\u6539\u8fdb\u5927\u6a21\u578b\u7684\u6027\u80fd\u548c\u5e94\u7528\u8303\u56f4\u3002\n\n### \u7814\u7a76\u6210\u679c\u3001\u65b9\u6cd5\u7684\u521b\u65b0\u6027\u4e0e\u5e94\u7528\u4ef7\u503c\n - **\u7814\u7a76\u6210\u679c**\uff1a\u660e\u786e\u4e86 LayerNorm \u548c RMSNorm \u5728\u5927\u6a21\u578b\u4e2d\u7684\u6027\u80fd\u8868\u73b0\u3001\u9002\u7528\u573a\u666f\u4ee5\u53ca\u5b58\u5728\u7684\u95ee\u9898\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002\n - **\u65b9\u6cd5\u7684\u521b\u65b0\u6027**\uff1aRMSNorm \u4ee5\u5176\u7b80\u6d01\u7684\u8ba1\u7b97\u65b9\u5f0f\u4e3a\u5f52\u4e00\u5316\u6280\u672f\u5e26\u6765\u65b0\u7684\u601d\u8def\uff0c\u4e0e\u4f20\u7edf\u7684 LayerNorm \u5f62\u6210\u5bf9\u6bd4\u548c\u8865\u5145\uff0c\u63a8\u52a8\u4e86\u5f52\u4e00\u5316\u65b9\u6cd5\u7684\u521b\u65b0\u53d1\u5c55\u3002\n - **\u5e94\u7528\u4ef7\u503c**\uff1a\u8fd9\u4e24\u79cd\u5f52\u4e00\u5316\u65b9\u6cd5\u5728\u5927\u6a21\u578b\u7684\u591a\u4e2a\u5e94\u7528\u9886\u57df\uff08\u5982\u81ea\u7136\u8bed\u8a00\u5904\u7406\u3001\u8ba1\u7b97\u673a\u89c6\u89c9\u7b49\uff09\u90fd\u6709\u91cd\u8981\u5e94\u7528\uff0c\u80fd\u591f\u63d0\u5347\u6a21\u578b\u6027\u80fd\u548c\u8bad\u7ec3\u6548\u7387\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\uff0c\u4e3a\u76f8\u5173\u9886\u57df\u7684\u6280\u672f\u53d1\u5c55\u63d0\u4f9b\u4e86\u6709\u529b\u652f\u6301 \u3002  ", "task_step_name": "\u5206\u6790\u8fd1\u51e0\u5e74\u7814\u7a76\u9886\u57df\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba", "task_step_description": "\u5728\u5927\u6a21\u578b\u76f8\u5173\u7814\u7a76\u9886\u57df\uff0c\u5f52\u4e00\u5316\u6280\u672f\u662f\u91cd\u8981\u7684\u7814\u7a76\u65b9\u5411\u4e4b\u4e00\u3002LayerNorm\u548cRMSNorm\u4f5c\u4e3a\u5176\u4e2d\u5173\u952e\u7684\u5f52\u4e00\u5316\u65b9\u6cd5\uff0c\u88ab\u4f17\u591a\u7814\u7a76\u6df1\u5165\u63a2\u8ba8\u3002\u7814\u7a76\u8005\u4eec\u4ece\u7406\u8bba\u5206\u6790\u3001\u5b9e\u9a8c\u5bf9\u6bd4\u7b49\u591a\u65b9\u9762\u5bf9\u5b83\u4eec\u8fdb\u884c\u5256\u6790\uff0c\u4ee5\u660e\u786e\u5176\u5728\u5927\u6a21\u578b\u6280\u672f\u6846\u67b6\u4e2d\u7684\u5730\u4f4d\u548c\u4f5c\u7528\u3002", "task_step_level": "0", "task_step_question": "\u8fd1\u51e0\u5e74\u5728\u5927\u6a21\u578b\u76f8\u5173\u7814\u7a76\u9886\u57df\uff0c\u7814\u7a76\u8005\u4ece\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u5bf9\u6bd4\u7b49\u591a\u65b9\u9762\u5bf9LayerNorm\u548cRMSNorm\u8fdb\u884c\u5256\u6790\u540e\uff0c\u5b83\u4eec\u5728\u5927\u6a21\u578b\u6280\u672f\u6846\u67b6\u4e2d\u5206\u522b\u5904\u4e8e\u4ec0\u4e48\u6837\u7684\u5730\u4f4d\u3001\u53d1\u6325\u7740\u600e\u6837\u7684\u4f5c\u7528\uff1f ", "task_step_question_context": [{"ref_id": "454845924254196540", "chunk_id": "7", "score": 0.48046875, "text": "# B.2 MODEL LAYERS\nIn this section, we give the formal definition of LayerNorm $\\operatorname{LN}(\\cdot)$ and RMS Norm ${\\mathrm{RMS}}\\left(\\cdot\\right)$ .  \n\nDefinition 1 (LayerNorm) .LayerNorm $L N(\\cdot;\\mu,\\beta,\\epsilon)$ of dimension $D$ is defined as:  \n\n$$\nL N(\\mathbf{x};\\pmb{\\mu},\\beta,\\epsilon)=\\frac{\\mathbf{x}-\\mathbb{E}[\\mathbf{x}]}{\\sqrt{\\mathrm{Var}[\\mathbf{x}]+\\epsilon}}\\odot\\pmb{\\mu}+\\beta,\n$$  \n\nwhere $\\mathbf{x},\\pmb{\\mu},\\beta\\in\\mathbb{R}^{D}$ .  \n\nDefinition 2 (RMSNorm) .RMS Norm $R M S(\\cdot;\\mu,\\epsilon)$ of dimension $D$ is defined as:  \n\n$$\nR M S(\\mathbf{x};\\pmb{\\mu},\\epsilon)=\\frac{\\mathbf{x}}{\\sqrt{\\frac{1}{D}\\sum_{i=1}^{D}(\\mathbf{x}[i])^{2}+\\epsilon}}\\odot\\pmb{\\mu},\n$$  \n\nwhere x,$\\pmb{\\mu}\\in\\mathbb{R}^{D}$ .  \n\nRemark. In neural networks, inputs of normalization layers are usually high dimension tensors. In this case, LayerNorm and RMSNorm normally apply to the last dimension separately.\n\n# B.3 LOSSLESS EXPANSION IN VECTOR SPACE\nIn this section, we first give the general definition of lossless expansion in vector space.  \n\ndimensions satisfy dim it is invertible. Definition 3 (Lossless $(\\bar{\\mathcal{T}})\\geq d i m(S)$ T\u2265S, a vector space expansion ector space) .Given $\\boldsymbol{S}$ and V$\\tau$ $\\mathcal{V}:\\mathcal{S}\\rightarrow\\mathcal{T}$ S \u2192T is said to be lossless if ector spaces where the  \n\nRemark. Note that the identity function Id is lossless with its inverse being itself.  \n\nThen we give a few examples of lossless vector space expansions. These examples will also be used in LEMON.  \n\nExample B.3.1 (Vector average expansion $\\mathcal{V}_{\\mathrm{avg.}}$ ).Let $\\mathbf{\\widetilde{x}}\\in\\mathbb{R}^{D_{S}}$ be a vector of dimension $D_{S}$ and its average $\\begin{array}{r}{\\lambda_{V}g(\\mathbf{x})=\\mathbb{E}[\\mathbf{x}]=\\frac{1}{D_{S}}\\sum_{i}^{D_{S}}\\mathbf{x}[i]}\\end{array}$ P].$\\mathbf{x}_{a\\nu g}^{*}$ is called the average expanded xof dimension $D_{T}$  \n\nwith $D_{T}\\geq D_{S}$ if  \n\n$$\n\\mathbf{x}_{a v g}^{*}=\\mathcal{V}_{a v g}(\\mathbf{x})=C o n c a t\\left[\\underbrace{\\mathbf{x}^{\\mathsf{T}},\\cdots,\\mathbf{x}^{\\mathsf{T}}}_{\\lfloor D_{T}/D s\\rfloor},\\underbrace{A v g(\\mathbf{x}),\\cdots,A v g(\\mathbf{x})}_{D_{T}\\mathrm{~mod~}D_{S}}\\right]^{\\mathsf{T}}\\in\\mathbb{R}^{D_{T}}.\n$$  \n\nExample B.3.2 (Vector z o expansion $\\mathcal{V}_{\\mathrm{zero.}}$ ).Le $\\mathbf{x}\\in\\mathbb{R}^{D_{S}}$ be a vector of dimension $D_{S}$ .$\\mathbf{x}_{z e r o}^{*}$ is called the zero expanded xof dimension $D_{T}$ with $D_{T}\\geq D_{S}$ \u2265if  \n\n$$\n\\begin{array}{r}{\\mathbf{x}_{z e r o}^{*}=\\mathcal{V}_{z e r o}(\\mathbf{x})=C o n c a t\\left[\\underbrace{\\mathbf{x^{\\mathsf{T}}},\\cdots,\\mathbf{x^{\\mathsf{T}}}}_{\\lfloor D_{T}/D_{S}\\rfloor},\\underbrace{0,\\cdots,0}_{D_{T}\\mathrm{~mod~}D_{S}}\\right]^{\\mathsf{T}}\\in\\mathbb{R}^{D_{T}}.}\\end{array}\n$$  \n\nExample B.3.3 (Vector circula expansion $\\mathcal{V}_{\\mathrm{circ}})$ Let $\\mathbf{x}\\in\\mathbb{R}^{D_{S}}$ a vector of dimension $D_{S}$ .${\\bf x}_{c i r c}^{*}$ is called the circular expanded xof dimension $D_{T}$ with $D_{T}\\geq D_{S}$ \u2265if  \n\n$$\n\\begin{array}{r}{\\mathbf{x}_{c i r c}^{*}=\\mathcal{V}_{c i r c}(\\mathbf{x})=C o n c a t\\underbrace{\\left[\\mathbf{x}^{\\mathsf{T}},\\cdots,\\mathbf{x}^{\\mathsf{T}},\\mathbf{x}^{\\mathsf{T}}[\\colon D_{T}\\bmod D_{S}]\\right]^{\\mathsf{T}}\\in\\mathbb{R}^{D_{T}}}_{[D_{T}/D_{S}]}.}\\end{array}\n$$  \n\nExample B.3.4 (Vector random expansion $\\mathcal{V}_{\\mathrm{rand.}}$ Let $\\mathbf{\\Deltax}\\in\\mathbb{R}^{D_{S}}$ a vector of dimension $D_{S}$ .${\\bf x}_{r a n d}^{*}$ is called the random expanded xof dimension $D_{T}$ with $D_{T}\\geq D_{S}$ \u2265if  \n\n$$\n\\begin{array}{r}{\\mathbf{x}_{r a n d}^{*}=\\mathcal{V}_{r a n d}(\\mathbf{x};\\zeta)=C o n c a t\\left[\\underbrace{\\mathbf{x^{\\intercal}},\\cdots,\\mathbf{x^{\\intercal}}}_{\\lfloor D_{T}/D_{S}\\rfloor},\\zeta^{\\intercal}\\right]^{\\intercal}\\in\\mathbb{R}^{D_{T}},}\\end{array}\n$$  \n\nwhere $\\zeta\\in\\mathbb{R}^{D_{T}}$ mod $D_{S}$ is an arbitrary vector.  \n\nRemark. (1) All vector expansion examples above follow the same pattern. Specifically, when $D_{T}$ expanding from di mod s by $D_{S}$ entries differently. (2) The random vector ating $\\textbf{x}\\lfloor D_{T}/D_{S}\\rfloor D_{S}$ \u230a$D_{S}$ \u230b$D_{T}$ number of times. , all vector expansion methods pad first $\\zeta$ in vector random expansion is arbitrary, Each method deals with the remaining $\\lfloor D_{T}/D_{S}\\rfloor D_{S}$ enso $\\mathcal{V}_{a\\nu g}$ ,$\\mathcal{V}_{z e r o}$ ,$\\mathcal{V}_{c i r c}\\subset\\mathcal{V}_{r a n d}$ . (3) Here all three examples are expansion methods for vectors. In practice, neural networks like Transformers are dealing high dimensional tensors. These tensors can essentially be thought of as collections of vectors. In such scenarios, we can apply the expansion methods separately to the last dimension of these tensors.  \n\nIn the following claim, we show that vectors expanded by these operators are lossless.  \n\n$\\mathcal{V}_{c i r c}$ V, and vector random expansion m 1. Vector average expansio V$\\gamma_{r a n d}$ $\\mathcal{V}_{a\\nu g},$ are all lossless expansion for vectors. , vector zero expansion $\\mathcal{V}_{z e r o}$ , vector circular expansion Proof. The inverse function $\\mathcal{V}^{-1}:\\mathbb{R}^{D_{T}}\\rightarrow\\mathbb{R}^{D_{S}}$ of these vector expansion methods is  \n\n$$\n\\nu^{-1}({\\bf x})={\\bf x}[:D_{S}].\n$$  \n\nRemark. In practice, we want inverse mapping of expansion methods to be easily computed just like the example above.\n\n# B.4LOSSLESS EXPANSION FOR OPERATORS\nWe then give the definition of lossless expansion for operators. These operators apply on tensors, hence our definition of lossless operator expansion is based on lossless expansion in vector space. These operators can be different layers used in Transformer architectures, including LayerNorm, convolutional layers, and fully-connected layers, etc.  \n\nDefinit ansio der vector spaces $S^{i n},S^{o u t},\\mathcal{T}^{i n}$ and $\\mathcal{T}^{o u t}$ such that with $g(\\cdot):S^{i n}\\rightarrow S^{o u t}$ \u00b7$n(S^{i n})\\leq d i m(T^{i n})$ S\u2192S or space e T. We say the ope and dim $d i m\\big(S^{\\bar{o}u t}\\big)\\leq d i m\\big(T^{o u t}\\big)$ S$\\mathcal{V}_{i n}:S^{i\\bar{n}}\\to\\mathcal{T}^{i n}$ \u2264TEMo is $(\\mathcal{V}_{i n},\\mathcal{V}_{o u t})$ VVess output vector space expansion ppose the op -lossless for $g(\\cdot)$ \u00b7or is denoted if there exist $\\mathcal{V}_{o u t}:S^{o u t}\\to\\mathcal{T}^{o u t}$ VS\u2192T such that V$\\mathcal{V}_{o u t}(g(\\mathbf{x}))=\\mathcal{E}[g](\\mathcal{V}_{i n}(\\mathbf{x})),\\forall\\mathbf{x}\\in S^{i n}$ EV\u2200\u2208S .  \n\n$(\\mathcal{V}_{i n},\\mathcal{V}_{o u t})$ Remark. losslessly expanded input, the output of the to be invertible, we do not have restrictions on the operator expansion VV(1) Intuitively, a lossless operator -lossless for the origina $g(\\cdot)$ \u00b7tput. (2) For conciseness, we use \u2018 \u2019 interchangeably. (3) We only require the v Eexpanded oper pansion can be understood a $^{\\cdot}\\mathcal{E}[g]$ Eis a is $(\\mathcal{V}_{i n},\\mathcal{V}_{o u t})$ EVtor expansions .V$\\nu_{o u t}$ ows: when using losslessly expa -lossles V$\\mathcal{V}_{i n}$ and \u2018 and $\\mathcal{E}$ V$\\nu_{o u t}$ $\\mathcal{V}_{i n}$ ed"}, {"ref_id": "454847819065993190", "chunk_id": "1", "score": 0.42578125, "text": "# 3.3 A TRANSFORMATION PER BLOCK\nNow that every LayerNorm in the transformer has been converted to RMSNorm, we can select any $\\mathbf{Q}$ to modify the model. Our initial plan was to collect signals from the model, construct an orthogonal matrix using those signals and to delete parts of the network. We quickly saw that the signals at different blocks of the network were not aligned, and that we would need to apply a different orthogonal matrix at each block, $\\mathbf{Q}_{\\ell}$ .  \n\nAllowing the orthogonal matrix used in each block to differ can be shown to leave the model unchanged using the same proof as Theorem 1 ,  \n\n  \nFigure 3: Converting a transformer network from LayerNorm to RMSNorm: the scale matrix diag $(\\alpha)$ is absorbed into the subsequent matrix $\\mathbf{W}_{\\mathrm{in}}$ . Figure shows the block in combined colors. We use $(\\alpha)$ for brevity. The mean-subtraction matrix $\\mathbf{M}$ is applied to each matrix $\\mathbf{W}_{\\mathrm{out}}$ . Layernorm becomes RMSNorm, up to a constant $\\bar{\\sqrt{D}}$ (not shown). Here, the scaling $(\\alpha^{\\prime})$ comes from the previous block.  \n\n  \nFigure 4: With the network converted to RMSNorm (see Figure 3 ), we apply the computational-invariance idea. The input weight matrices $\\mathrm{diag}(\\alpha)\\mathbf{W}_{\\mathrm{in}}$ are pre-multiplied by $\\mathbf{Q}^{\\top}$ . The output matrices $\\mathbf{W}_{\\mathrm{out}}\\mathbf{M}$ are post-multiplied by $\\mathbf{Q}$ . In the skip-connection, a new linear layer is added $\\mathbf{Q}_{\\ell}^{\\top}\\mathbf{Q}_{\\ell+1}$ . After these modifications, the matrices can be sliced (hatched areas).  \n\nwith the exception of line 5 of Algorithm 1 . Here we see that the residual connection and the output of the block must have the same rotation. To fix this, we modify the residual connection by applying the linear transformation applied to different blocks with the additional linear operation in the residual connection. Unlike the $\\mathbf{Q}_{\\ell-1}^{\\top}\\mathbf{Q}_{\\ell}$ \u2212to the residual. Figure 4 shows how different rotations can be modifications to the weight matrices, these additional operations cannot be pre-computed and add a small $(D\\times D)$ overhead to the model. Nonetheless, they are needed to allow slicing the model (Section 3.4 ) and we see real speedup overall (Section 4 ).  \n\nTo compute the matrices $\\mathbf{Q}_{\\ell}$ , we use PCA. We select a calibration dataset from the training set, run it through the model (after converting LayerNorm operations into RMSNorm), and extract the orthogonal matrix of the layer. We use the output of the transformed network to calculate the orthogonal matrices of the next layers. More precisely, if $\\mathbf{X}_{\\ell,i}$ is the output of the $\\ell^{\\mathrm{th}}$ RMSNorm block for the $i^{\\mathrm{th}}$ sequence in the calibration dataset, we compute  \n\n$$\n\\mathbf{C}_{\\ell}=\\sum_{i}\\mathbf{X}_{\\ell,i}^{\\top}\\mathbf{X}_{\\ell,i}\n$$  \n\nand set $\\mathbf{Q}_{\\ell}$ to the be the eigenvectors of $\\mathbf{C}_{\\ell}$ , sorted by decreasing eigenvalues.\n\n# 3.4 SLICING\nThe goal of Principal Component Analysis is usually to take a data matrix $\\mathbf{X}$ and compute a lower dimensional representation $\\mathbf{Z}$ , and an approximate reconstruction $\\tilde{\\mathbf{X}}$ :  \n\n$$\n\\mathbf{Z}=\\mathbf{X}\\mathbf{Q}\\mathbf{D}\\,,\\qquad\\tilde{\\mathbf{X}}=\\mathbf{Z}\\mathbf{D}^{\\top}\\mathbf{Q}^{\\top}\\,.\n$$  \n\nwhere $\\mathbf{Q}$ is the ectors of ${\\bf X}^{\\top}{\\bf X}$ , and $\\mathbf{D}$ is a $D\\times D_{\\mathrm{small}}$ deletion matrix (containing $D_{\\mathrm{small}}$ The reconstruction is columns of the $D\\times D$ \u00d7$L_{2}$ identity matrix), which removes some of the columns of the matrix to the left. optimal, in the sense that QD is a linear mapping that minimizes $\\lVert\\mathbf{X}-\\tilde{\\mathbf{X}}\\rVert^{2}$ .  \n\nWhen we apply PCA to the signal matrix $\\mathbf{X}$ bween blocks, we never materialize the $N\\times D$ signal matrix, but we apply the deletion matrix Dto the operations preceding and succeeding the construction of that matrix, which have already been multiplied by $\\mathbf{Q}$ in the above. We delete rows of $\\mathbf{W}_{\\mathrm{in}}$ that we have inserted into the residual connection (see Figure and columns of $\\mathbf{W}_{\\mathrm{out}}$ and $\\mathbf{W}_{\\mathrm{embd}}$ . We also delete both rows 4 ). and columns of the matrix $\\mathbf{Q}_{\\ell-1}^{\\top}\\mathbf{Q}_{\\ell}$ \u2212\n\n# 4 EXPERIMENTAL VALIDATION\nSetup We use HuggingFace Transformers ( Wolf et al. ,2019 ) to implement our code with PyTorch (Paszke et al. ,2019 ). The computation of $\\mathbf{Q}$ is performed on a single H100 GPU with 80GB of memory, taking approximately 3.5 hours to complete for the L LAMA -2 70B model. During the PCA calculation, we use double precision for computing the eigenvectors of the covariance matrix. We find that using single precision for eigenvector calculations in PyTorch leads to a discrepancy in the final accuracy, as detailed in Appendix A.2 .  \n\nWe experiment with two different calibration sets: 1024 samples from the WikiText-2 training dataset ( Merity et al. ,2016 ) and 5000 samples from the Alpaca training dataset ( Taori et al. ,2023 ). Sequence lengths are chosen as the maximum of each language model. An ablation study on the calibration set size and sequence length is presented in Appendix A.3 .  \n\nModels, Tasks, and GPUs We evaluate all our experiments on OPT ( Zhang et al. ,2022 ), L LAMA -2 (Touvron et al. ,2023 ) model families, and additionally evaluate Phi-2 (in our zero-shot task) experiments. We exclude OPT 175B, as it is outperformed by smaller L LAMA -2 models. Nonetheless, we anticipate that this larger model will yield improved results, as larger models typically offer more promising opportunities for compression (see Section 4.1 ). We evaluate our scheme on both language generation as well as popular zero-shot tasks. To demonstrate the comprehensive speedup achieved by SliceGPT we use: Quadro RTX6000 GPUs with 24GB of memory as a representative example of consumer-level GPUs; 40GB A100s and 80GB H100s to provide datacenter-level benchmarks.  \n\nBaseline Setup We initially planned to compare our results against a scheme that pruned columns (or rows) with the smallest norm but found that this baseline was very poor, with the perplexity of the model soaring into the 1000s after pruning just a few columns. Instead, we compare SliceGPT against SparseGPT ( Frantar & Alistarh ,2023 ) employing a 2:4 sparsity ratio, as this is the only sparsity scheme which achieves speedup ( Mishra et al. ,2021 )."}, {"ref_id": "454895409734360760", "chunk_id": "3", "score": 0.34375, "text": "# 3 Experiments and Results\nWe evaluate the performance of PreNorm and PostNorm for ZST on various datasets and language pairs. We then analyze the off-target rates and structural discrepancies between PreNorm and PostNorm to understand performance differences.  \n\n$$\n\\mathrm{LayerNorm}(\\mathbf{x})=\\frac{\\mathbf{x}-\\mathbf{E}(\\mathbf{x})}{\\sqrt{\\mathbf{V}(\\mathbf{x})}}\\cdot\\mathbf{g}+\\mathbf{b},\n$$  \n\nwhere $\\mathbf{g}$ and $\\mathbf{b}$ are trainable gain and bias. $\\mathbf{E}$ and $\\mathbf{V}$ indicate expectation and variance. LayerNorm is commonly used in two positions in the Transformer, as shown in Fig. 1 . PostNorm, which is the originally proposed setting of the Transformer ( Vaswani et al. ,2017 ), involves applying LayerNorm after each sub-module (i.e., selfattention or feed-forward network) and residual connections. PreNorm ( Baevski and Auli ,2019 ), on the other hand, involves applying LayerNorm directly before each sub-module and is known to stabilize Transformer training. While variants of Transformer LayerNorm like RMSNorm ( Zhang and Sennrich ,2019 ) have been proposed, the vanilla PreNorm and PostNorm are still the most widely adopted settings in current multilingual\n\n# 3.1 Experimental Settings\nDatasets We perform ZST experiments on three datasets: OPUS ( Zhang et al. ,2020 ), IWSLT ( Cettolo et al. ,2017 ), and Europarl ( Koehn ,2005 ). The statistics of the datasets are summarized in Table 1 .We include 7 ,4 , and 5 languages for each dataset. The training data consists of only English-centric sentence pairs, resulting in 30 ,6 , and 12 ZST directions for each dataset. The total number of parallel sentences for each dataset is 12 .00 M, 1 .38 M, and 15 .78 M, respectively. We apply BPE ( Sennrich et al. ,2016 ) with merge operations of 50 k, 40 k, and $50\\mathbf{k}$ to create a joint vocabulary for each dataset.  \n\nTraining We employ Transformer-base model for OPUS and IWSLT, and Transformer-big for Europarl, in accordance with the distinct sizes of training data. We consider the following settings: (1) PreNorm or PostNorm : PreNorm involves LayerNorm directly before each sub-module (i.e., self-attention or feed-forward network), while PostNorm applies LayerNorm after each sub-module and residual connections, as shown in Fig. 1 .(2) S-ENC-T-DEC or T-ENC : Source language tag on the encoder-side and target language tag on the decoder-side; or only target language tag on the encoder-side. Wu et al. (2021 ) showed that this setting impacts ZST for Transformer with PreNorm. (3) w/ or w/o Res. : With the residual connection for self-attention in the middle $(4^{t h})$ encoder layer or not. Liu et al. (2021 ) revealed that \u201cw/o Res.\u201d improves ZST for the model trained with PreNorm. We experiment this with different LayerNorm settings as this may reduce the potential of overfitting on supervised directions, then further impacts ZST, which aligns with our hypothesis.  \n\nTable 2: BLEU scores and off-target rates (shown in brackets) . We report the average score of three seeds; refer to Appendix Gfor BLEU score of each translation direction and seed. \u201cRes.\u201d indicates the residual connection of self-attention in the $4^{t h}$ encoder layer. We mark lower off-target rates and significantly higher BLEU scores ( Koehn ,2004 ) between PreNorm and PostNorm in bold for ZST.   \n\n\n<html><body><table><tr><td>#</td><td>Layer Norm</td><td>Language Tag</td><td>Res.</td><td></td><td>Zero-shot</td><td></td><td></td><td>Supervised</td><td></td></tr><tr><td>0</td><td></td><td>Pivot</td><td></td><td>OPUS 21.8</td><td>IWSLT 20.0</td><td>Europarl 29.5</td><td>OPUS</td><td>IWSLT</td><td>Europarl</td></tr><tr><td>1</td><td>PreNorm</td><td>S-ENC-T-DEC</td><td>w/</td><td>10.1 (42.19%)</td><td>4.9 (64.84%)</td><td>24.9 ( 7.73%)</td><td>33.7</td><td>31.5</td><td>34.3</td></tr><tr><td>2</td><td>PostNorm</td><td>S-ENC-T-DEC</td><td>w/</td><td>16.8 ( 8.59%)</td><td>12.4 (10.61%)</td><td>29.2( 0.34%)</td><td>33.9</td><td>31.5</td><td>34.5</td></tr><tr><td>3</td><td>PreNorm</td><td>T-ENC</td><td>w/</td><td>13.3 (22.99%)</td><td>13.7 ( 3.98%)</td><td>29.5( 0.23%)</td><td>33.7</td><td>31.6</td><td>34.4</td></tr><tr><td>4</td><td>PostNorm</td><td>T-ENC</td><td>w/</td><td>14.0 (22.86%)</td><td>15.5 ( 4.59%)</td><td>30.8 ( 0.11%)</td><td>34.1</td><td>31.5</td><td>34.5</td></tr><tr><td>5</td><td>PreNorm</td><td>S-ENC-T-DEC</td><td>w/o</td><td>14.3 (20.67%)</td><td>8.0 (50.16%)</td><td>16.7 (41.87%)</td><td>33.6</td><td>30.9</td><td>34.3</td></tr><tr><td>6</td><td>PostNorm</td><td>S-ENC-T-DEC</td><td>w/o</td><td>16.0 (15.27%)</td><td>17.4 (1.83%)</td><td>29.0 ( 0.41%)</td><td>33.8</td><td>30.7</td><td>34.4</td></tr><tr><td>7</td><td>PreNorm</td><td>T-ENC</td><td>w/o</td><td>13.4 (27.15%)</td><td>16.2 ( 1.54%)</td><td>29.9 ( 2.15%)</td><td>33.5</td><td>30.9</td><td>34.3</td></tr><tr><td>8</td><td>PostNorm</td><td>T-ENC</td><td>w/o</td><td>13.9 (26.68%)</td><td>17.8 (1.50%)</td><td>30.8 ( 0.13%)</td><td>33.9</td><td>30.6</td><td>34.4</td></tr></table></body></html>  \n\nThe settings above lead to eight different combinations, shown in Table 2 (#1 - #8). Additional training details are in Appendix A .\n\n# 3.2 Main Results\nWe evaluate ZST systems using SacreBLEU ( Post ,2018 ) and off-target rates. We report in Table 2 BLEU scores for both zero-shot and supervised directions. For ZST, we also present pivot-based translation results as a reference. Implementation details of evaluation can be found in Appendix B.Our findings are as follows:  \n\nPreNorm vs. PostNorm :We find that PostNorm consistently yields better BLEU scores than PreNorm for ZST across various language tag and residual connection settings, while their performance is comparable for supervised directions.  \n\nImpact of Language Tag and Residual Connection: We observe that using the \u201cT-ENC\u201d language tag and \u201cw/ Res.\u201d improves ZST performance for IWSLT, which aligns with the findings of $\\mathrm{Wu}$ et al. (2021 ) and Liu et al. (2021 ). Nevertheless, the best performance is achieved using \u201cw/ Res.\u201d for PostNorm with \u201cS-ENC-T-DEC\u201d and \u201cT-ENC\u201d tags for OPUS and Europarl, respectively (#2 and #4). Given that Wu et al. (2021 ) and Liu et al. (2021 )used PreNorm as the default setting (#2, #4, #6 and #8 are unreported results in their work), our results emphasize the need to consider PostNorm as the default setting for ZST, while the language tag and residual connection settings have less impact.  \n\nOff-target Rates : Off-target rates help understand the different BLEU score gaps between PreNorm and PostNorm, which ranges from 0 .5 to 12 .3 BLEU points. For PreNorm and PostNorm with the \u201cT-ENC\u201d language tag (#3, #4, #7, and #8), they have similar off-target rates, with a discrepancy ranging from $-0.61\\%$ to $2.02\\%$ , which results in narrow BLEU score gaps, ranging from 0 .5 to 1 .8 points. However, for PreNorm and PostNorm with the \u201cS-ENC-T-DEC\u201d language tag (#1, #2, #5, and #6), the off-target rates show a more considerable discrepancy, ranging from $5.40\\%$ to $54.23\\%$ , resulting in BLEU score gaps from 1 .7 to 12 .3 points. Further analysis of the nature of Transformer hidden states in the next section explores the reason for these different off-target rates in translations."}], "task_step_question_answer": "\u968f\u7740\u5bf9\u8fd9\u4e9b\u5f52\u4e00\u5316\u65b9\u6cd5\u7814\u7a76\u7684\u4e0d\u65ad\u6df1\u5165\uff0c\u7814\u7a76\u4eba\u5458\u5f00\u59cb\u5c1d\u8bd5\u5c06 LayerNorm \u548c RMSNorm \u4e0e\u5176\u4ed6\u65b0\u5174\u6280\u672f\u8fdb\u884c\u878d\u5408\u3002\u4f8b\u5982\uff0c\u4e0e\u5f3a\u5316\u5b66\u4e60\u76f8\u7ed3\u5408\uff0c\u901a\u8fc7\u667a\u80fd\u8c03\u6574\u5f52\u4e00\u5316\u53c2\u6570\uff0c\u4f7f\u6a21\u578b\u5728\u52a8\u6001\u73af\u5883\u4e2d\u80fd\u591f\u66f4\u597d\u5730\u9002\u5e94\u6570\u636e\u53d8\u5316\u3002\u5728\u5f3a\u5316\u5b66\u4e60\u7684\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u667a\u80fd\u4f53\u53ef\u4ee5\u6839\u636e\u73af\u5883\u53cd\u9988\u6765\u52a8\u6001\u8c03\u6574\u5f52\u4e00\u5316\u7684\u53c2\u6570\uff0c\u4ee5\u4f18\u5316\u6a21\u578b\u7684\u6027\u80fd\u3002\u8fd9\u79cd\u878d\u5408\u65b9\u5f0f\u4e3a\u89e3\u51b3\u590d\u6742\u52a8\u6001\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u3002\u540c\u65f6\uff0c\u7814\u7a76\u4eba\u5458\u4e5f\u5728\u63a2\u7d22\u5c06\u5176\u5e94\u7528\u4e8e\u91cf\u5b50\u8ba1\u7b97\u9886\u57df\u3002\u91cf\u5b50\u8ba1\u7b97\u5177\u6709\u72ec\u7279\u7684\u8ba1\u7b97\u7279\u6027\uff0c\u4f20\u7edf\u7684\u5f52\u4e00\u5316\u65b9\u6cd5\u5728\u91cf\u5b50\u8ba1\u7b97\u73af\u5883\u4e2d\u53ef\u80fd\u9700\u8981\u8fdb\u884c\u9002\u5e94\u6027\u8c03\u6574\u3002LayerNorm \u548c RMSNorm \u80fd\u5426\u5728\u91cf\u5b50\u8ba1\u7b97\u4e2d\u53d1\u6325\u4f5c\u7528\uff0c\u4ee5\u53ca\u5982\u4f55\u8fdb\u884c\u6539\u8fdb\u4ee5\u9002\u5e94\u91cf\u5b50\u8ba1\u7b97\u7684\u9700\u6c42\uff0c\u6210\u4e3a\u4e86\u65b0\u7684\u7814\u7a76\u70ed\u70b9\u3002", "ref_task_step_id": ""}, "__type__": "task_step_node"}, "e408c292-33f5-459e-9724-9942a9d19d1d": {"__data__": {"id_": "e408c292-33f5-459e-9724-9942a9d19d1d", "metadata": {}, "relationships": {}, "hash": "", "start_task_context": "\u5927\u6a21\u578b\u4e2d\u7684LayerNorm\u548cRMSNorm\u6709\u4ec0\u4e48\u533a\u522b\uff1f", "aemo_representation_context": "### \u5206\u6790\u8fd1\u51e0\u5e74\u7814\u7a76\u9886\u57df\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba\n\u5728\u5927\u6a21\u578b\u76f8\u5173\u7814\u7a76\u9886\u57df\uff0c\u5f52\u4e00\u5316\u6280\u672f\u662f\u91cd\u8981\u7684\u7814\u7a76\u65b9\u5411\u4e4b\u4e00\u3002LayerNorm \u548c RMSNorm \u4f5c\u4e3a\u5176\u4e2d\u5173\u952e\u7684\u5f52\u4e00\u5316\u65b9\u6cd5\uff0c\u88ab\u4f17\u591a\u7814\u7a76\u6df1\u5165\u63a2\u8ba8\u3002\u7814\u7a76\u8005\u4eec\u4ece\u7406\u8bba\u5206\u6790\u3001\u5b9e\u9a8c\u5bf9\u6bd4\u7b49\u591a\u65b9\u9762\u5bf9\u5b83\u4eec\u8fdb\u884c\u5256\u6790\uff0c\u4ee5\u660e\u786e\u5176\u5728\u5927\u6a21\u578b\u6280\u672f\u6846\u67b6\u4e2d\u7684\u5730\u4f4d\u548c\u4f5c\u7528\u3002\n\n### \u7814\u7a76\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u4e3b\u8981\u6846\u67b6\uff08\u5982 Transformer\u3001GAN\u3001BERT \u7b49\uff09\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0e\u53d8\u4f53\n\u5728 Transformer \u67b6\u6784\u4e3a\u4e3b\u7684\u4f17\u591a\u6a21\u578b\u4e2d\uff0cLayerNorm \u548c RMSNorm \u88ab\u5e7f\u6cdb\u5e94\u7528\u3002\u4f8b\u5982\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\uff08\u5982 BERT \u53ca\u5176\u53d8\u4f53\u6a21\u578b\uff09\u4e2d\uff0cLayerNorm \u6700\u521d\u88ab\u7528\u4e8e\u7a33\u5b9a\u6a21\u578b\u8bad\u7ec3\u3001\u52a0\u901f\u6536\u655b\u4ee5\u53ca\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u80fd\u529b\uff1b\u800c RMSNorm \u4e5f\u9010\u6e10\u5728\u4e00\u4e9b\u57fa\u4e8e Transformer \u7684\u6a21\u578b\u4e2d\u5f97\u5230\u5e94\u7528\uff0c\u4ee5\u6539\u8fdb\u6a21\u578b\u7684\u6027\u80fd\u8868\u73b0\u3002\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\uff08\u5982\u57fa\u4e8e Transformer \u7684\u89c6\u89c9\u6a21\u578b\uff09\u4e2d\u540c\u6837\u6709\u5b83\u4eec\u7684\u8eab\u5f71\uff0c\u4e0d\u540c\u4efb\u52a1\u573a\u666f\u4e0b\u5bf9\u8fd9\u4e24\u79cd\u5f52\u4e00\u5316\u65b9\u6cd5\u8fdb\u884c\u4e86\u9002\u5e94\u6027\u8c03\u6574\u548c\u53d8\u4f53\u5e94\u7528\uff0c\u4ee5\u66f4\u597d\u5730\u9002\u914d\u4efb\u52a1\u9700\u6c42\u3002\n\n### \u8bc4\u4f30\u5b66\u672f\u754c\u7684\u6280\u672f\u8fdb\u6b65\u4e0e\u5c40\u9650\u6027\n - **\u6280\u672f\u8fdb\u6b65**\uff1aLayerNorm \u548c RMSNorm \u7684\u51fa\u73b0\u663e\u8457\u63d0\u5347\u4e86\u5927\u6a21\u578b\u8bad\u7ec3\u7684\u7a33\u5b9a\u6027\u548c\u6548\u7387\u3002LayerNorm \u80fd\u591f\u5bf9\u6bcf\u4e2a\u6837\u672c\u7684\u4e0d\u540c\u7ef4\u5ea6\u8fdb\u884c\u5f52\u4e00\u5316\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5185\u90e8\u534f\u53d8\u91cf\u504f\u79fb\u95ee\u9898\uff1bRMSNorm \u5219\u901a\u8fc7\u66f4\u7b80\u6d01\u7684\u8ba1\u7b97\u65b9\u5f0f\uff0c\u5728\u4e00\u4e9b\u573a\u666f\u4e0b\u53d6\u5f97\u4e86\u4e0e LayerNorm \u76f8\u5f53\u751a\u81f3\u66f4\u597d\u7684\u6548\u679c\uff0c\u63a8\u52a8\u4e86\u5927\u6a21\u578b\u6027\u80fd\u7684\u63d0\u5347\u3002\n - **\u5c40\u9650\u6027**\uff1a\u5c3d\u7ba1\u53d6\u5f97\u8fdb\u6b65\uff0c\u4f46\u4ecd\u5b58\u5728\u4e00\u4e9b\u95ee\u9898\u3002\u4f8b\u5982\uff0c\u5728\u67d0\u4e9b\u590d\u6742\u4efb\u52a1\u548c\u7279\u5b9a\u6570\u636e\u96c6\u4e0a\uff0c\u6a21\u578b\u4f9d\u7136\u5b58\u5728\u6027\u80fd\u74f6\u9888\uff0c\u5373\u4f7f\u4f7f\u7528\u4e86\u8fd9\u4e9b\u5f52\u4e00\u5316\u65b9\u6cd5\uff0c\u6a21\u578b\u504f\u5dee\u548c\u6570\u636e\u4f9d\u8d56\u95ee\u9898\u4f9d\u65e7\u5b58\u5728\u3002\u800c\u4e14\u5728\u9762\u5bf9\u591a\u6a21\u6001\u6570\u636e\u65f6\uff0c\u73b0\u6709\u7684\u5f52\u4e00\u5316\u65b9\u6cd5\u5728\u878d\u5408\u4e0d\u540c\u6a21\u6001\u7279\u5f81\u65b9\u9762\u8fd8\u5b58\u5728\u4e0d\u8db3\u3002\n\n### \u63a2\u8ba8\u8ba1\u7b97\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\n - **\u9002\u7528\u6027**\uff1aLayerNorm \u5728\u591a\u79cd\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6570\u636e\u96c6\u548c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8f83\u597d\u7684\u9002\u7528\u6027\uff0c\u80fd\u591f\u9002\u5e94\u4e0d\u540c\u89c4\u6a21\u548c\u7279\u70b9\u7684\u6570\u636e\u3002RMSNorm \u5728\u4e00\u4e9b\u5927\u89c4\u6a21\u6570\u636e\u548c\u7279\u5b9a\u4efb\u52a1\uff08\u5982\u957f\u5e8f\u5217\u5904\u7406\uff09\u4e2d\u5c55\u73b0\u51fa\u72ec\u7279\u4f18\u52bf\u3002\u7136\u800c\uff0c\u5728\u591a\u9886\u57df\u3001\u591a\u6a21\u6001\u6570\u636e\u573a\u666f\u4e0b\uff0c\u5b83\u4eec\u7684\u9002\u7528\u6027\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u6539\u8fdb\u4ee5\u66f4\u597d\u5730\u5904\u7406\u4e0d\u540c\u7c7b\u578b\u6570\u636e\u7684\u878d\u5408\u3002\n - **\u6cdb\u5316\u80fd\u529b**\uff1a\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\uff0c\u8fd9\u4e24\u79cd\u5f52\u4e00\u5316\u65b9\u6cd5\u6709\u52a9\u4e8e\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u5728\u590d\u6742\u591a\u53d8\u7684\u73b0\u5b9e\u5e94\u7528\u573a\u666f\u4e2d\uff0c\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u4ecd\u6709\u5f85\u63d0\u9ad8\uff0c\u5c24\u5176\u662f\u5728\u8de8\u9886\u57df\u548c\u591a\u6a21\u6001\u6570\u636e\u7684\u6cdb\u5316\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002\n\n### \u5206\u6790\u6700\u65b0\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027\n - **\u7a33\u5b9a\u6027**\uff1a\u7814\u7a76\u53d1\u73b0\uff0cLayerNorm \u901a\u8fc7\u5bf9\u8f93\u5165\u6570\u636e\u7684\u5f52\u4e00\u5316\u5904\u7406\uff0c\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u63d0\u9ad8\u4e86\u6a21\u578b\u8bad\u7ec3\u7684\u7a33\u5b9a\u6027\u3002RMSNorm \u540c\u6837\u5728\u4e00\u4e9b\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u7a33\u5b9a\u6027\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u52a8\u6001\u73af\u5883\u4e0b\uff0c\u90e8\u5206\u7814\u7a76\u9488\u5bf9\u5176\u7a33\u5b9a\u6027\u8fdb\u884c\u4e86\u4f18\u5316\uff0c\u4f7f\u5176\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\u4e5f\u80fd\u4fdd\u6301\u76f8\u5bf9\u7a33\u5b9a\u7684\u8868\u73b0\u3002\n - **\u5bb9\u9519\u6027**\uff1a\u5728\u9762\u5bf9\u6570\u636e\u566a\u58f0\u548c\u5f02\u5e38\u503c\u65f6\uff0c\u8fd9\u4e24\u79cd\u5f52\u4e00\u5316\u65b9\u6cd5\u90fd\u6709\u4e00\u5b9a\u7684\u5bb9\u9519\u80fd\u529b\uff0c\u4f46\u5728\u6781\u7aef\u60c5\u51b5\u4e0b\uff0c\u6a21\u578b\u6027\u80fd\u4ecd\u4f1a\u53d7\u5230\u5f71\u54cd\uff0c\u5bb9\u9519\u6027\u65b9\u9762\u8fd8\u6709\u63d0\u5347\u7a7a\u95f4\u3002\n\n### \u8bc4\u4f30\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218\n - **\u672a\u6765\u7814\u7a76\u65b9\u5411**\uff1a\u8bb8\u591a\u8bba\u6587\u63d0\u51fa\u63a2\u7d22\u66f4\u9002\u7528\u4e8e\u591a\u6a21\u6001\u6570\u636e\u7684\u5f52\u4e00\u5316\u65b9\u6cd5\uff0c\u4ee5\u53ca\u5982\u4f55\u8fdb\u4e00\u6b65\u4f18\u5316 LayerNorm \u548c RMSNorm \u4ee5\u63d0\u5347\u6a21\u578b\u5728\u590d\u6742\u573a\u666f\u4e0b\u7684\u6027\u80fd\u3002\u540c\u65f6\uff0c\u7814\u7a76\u5982\u4f55\u589e\u5f3a\u5f52\u4e00\u5316\u65b9\u6cd5\u5bf9\u6a21\u578b\u504f\u5dee\u548c\u6570\u636e\u4f9d\u8d56\u95ee\u9898\u7684\u89e3\u51b3\u80fd\u529b\u4e5f\u662f\u91cd\u8981\u65b9\u5411\u3002\n - **\u6311\u6218**\uff1a\u65b0\u7684\u7814\u7a76\u95ee\u9898\u5305\u62ec\u5982\u4f55\u8bbe\u8ba1\u4e00\u79cd\u901a\u7528\u7684\u5f52\u4e00\u5316\u7b56\u7565\uff0c\u80fd\u591f\u65e0\u7f1d\u9002\u5e94\u4e0d\u540c\u9886\u57df\u548c\u6a21\u6001\u7684\u6570\u636e\uff1b\u5982\u4f55\u5728\u63d0\u5347\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\uff0c\u8fdb\u4e00\u6b65\u63d0\u9ad8\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u548c\u5bb9\u9519\u6027\u3002\u8fd9\u4e9b\u6311\u6218\u63a8\u52a8\u540e\u7eed\u7814\u7a76\u4e0d\u65ad\u63a2\u7d22\u65b0\u7684\u6280\u672f\u548c\u65b9\u6cd5\uff0c\u4ee5\u6539\u8fdb\u5927\u6a21\u578b\u7684\u6027\u80fd\u548c\u5e94\u7528\u8303\u56f4\u3002\n\n### \u7814\u7a76\u6210\u679c\u3001\u65b9\u6cd5\u7684\u521b\u65b0\u6027\u4e0e\u5e94\u7528\u4ef7\u503c\n - **\u7814\u7a76\u6210\u679c**\uff1a\u660e\u786e\u4e86 LayerNorm \u548c RMSNorm \u5728\u5927\u6a21\u578b\u4e2d\u7684\u6027\u80fd\u8868\u73b0\u3001\u9002\u7528\u573a\u666f\u4ee5\u53ca\u5b58\u5728\u7684\u95ee\u9898\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002\n - **\u65b9\u6cd5\u7684\u521b\u65b0\u6027**\uff1aRMSNorm \u4ee5\u5176\u7b80\u6d01\u7684\u8ba1\u7b97\u65b9\u5f0f\u4e3a\u5f52\u4e00\u5316\u6280\u672f\u5e26\u6765\u65b0\u7684\u601d\u8def\uff0c\u4e0e\u4f20\u7edf\u7684 LayerNorm \u5f62\u6210\u5bf9\u6bd4\u548c\u8865\u5145\uff0c\u63a8\u52a8\u4e86\u5f52\u4e00\u5316\u65b9\u6cd5\u7684\u521b\u65b0\u53d1\u5c55\u3002\n - **\u5e94\u7528\u4ef7\u503c**\uff1a\u8fd9\u4e24\u79cd\u5f52\u4e00\u5316\u65b9\u6cd5\u5728\u5927\u6a21\u578b\u7684\u591a\u4e2a\u5e94\u7528\u9886\u57df\uff08\u5982\u81ea\u7136\u8bed\u8a00\u5904\u7406\u3001\u8ba1\u7b97\u673a\u89c6\u89c9\u7b49\uff09\u90fd\u6709\u91cd\u8981\u5e94\u7528\uff0c\u80fd\u591f\u63d0\u5347\u6a21\u578b\u6027\u80fd\u548c\u8bad\u7ec3\u6548\u7387\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\uff0c\u4e3a\u76f8\u5173\u9886\u57df\u7684\u6280\u672f\u53d1\u5c55\u63d0\u4f9b\u4e86\u6709\u529b\u652f\u6301 \u3002  ", "task_step_name": "\u7814\u7a76\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u4e3b\u8981\u6846\u67b6\uff08\u5982Transformer\u3001GAN\u3001BERT\u7b49\uff09\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0e\u53d8\u4f53", "task_step_description": "\u5728Transformer\u67b6\u6784\u4e3a\u4e3b\u7684\u4f17\u591a\u6a21\u578b\u4e2d\uff0cLayerNorm\u548cRMSNorm\u88ab\u5e7f\u6cdb\u5e94\u7528\u3002\u4f8b\u5982\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\uff08\u5982BERT\u53ca\u5176\u53d8\u4f53\u6a21\u578b\uff09\u4e2d\uff0cLayerNorm\u6700\u521d\u88ab\u7528\u4e8e\u7a33\u5b9a\u6a21\u578b\u8bad\u7ec3\u3001\u52a0\u901f\u6536\u655b\u4ee5\u53ca\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u80fd\u529b\uff1b\u800cRMSNorm\u4e5f\u9010\u6e10\u5728\u4e00\u4e9b\u57fa\u4e8eTransformer\u7684\u6a21\u578b\u4e2d\u5f97\u5230\u5e94\u7528\uff0c\u4ee5\u6539\u8fdb\u6a21\u578b\u7684\u6027\u80fd\u8868\u73b0\u3002\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\uff08\u5982\u57fa\u4e8eTransformer\u7684\u89c6\u89c9\u6a21\u578b\uff09\u4e2d\u540c\u6837\u6709\u5b83\u4eec\u7684\u8eab\u5f71\uff0c\u4e0d\u540c\u4efb\u52a1\u573a\u666f\u4e0b\u5bf9\u8fd9\u4e24\u79cd\u5f52\u4e00\u5316\u65b9\u6cd5\u8fdb\u884c\u4e86\u9002\u5e94\u6027\u8c03\u6574\u548c\u53d8\u4f53\u5e94\u7528\uff0c\u4ee5\u66f4\u597d\u5730\u9002\u914d\u4efb\u52a1\u9700\u6c42\u3002", "task_step_level": "1", "task_step_question": "\u5728\u57fa\u4e8eTransformer\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\u4e2d\uff0cLayerNorm\u548cRMSNorm\u5177\u4f53\u8fdb\u884c\u4e86\u54ea\u4e9b\u9002\u5e94\u6027\u8c03\u6574\u548c\u53d8\u4f53\u5e94\u7528\u6765\u9002\u914d\u89c6\u89c9\u4efb\u52a1\u9700\u6c42\uff1f ", "task_step_question_context": [{"ref_id": "454895409734360760", "chunk_id": "3", "score": 0.37109375, "text": "# 3.3 A TRANSFORMATION PER BLOCK\nNow that every LayerNorm in the transformer has been converted to RMSNorm, we can select any $\\mathbf{Q}$ to modify the model. Our initial plan was to collect signals from the model, construct an orthogonal matrix using those signals and to delete parts of the network. We quickly saw that the signals at different blocks of the network were not aligned, and that we would need to apply a different orthogonal matrix at each block, $\\mathbf{Q}_{\\ell}$ .  \n\nAllowing the orthogonal matrix used in each block to differ can be shown to leave the model unchanged using the same proof as Theorem 1 ,  \n\n  \nFigure 3: Converting a transformer network from LayerNorm to RMSNorm: the scale matrix diag $(\\alpha)$ is absorbed into the subsequent matrix $\\mathbf{W}_{\\mathrm{in}}$ . Figure shows the block in combined colors. We use $(\\alpha)$ for brevity. The mean-subtraction matrix $\\mathbf{M}$ is applied to each matrix $\\mathbf{W}_{\\mathrm{out}}$ . Layernorm becomes RMSNorm, up to a constant $\\bar{\\sqrt{D}}$ (not shown). Here, the scaling $(\\alpha^{\\prime})$ comes from the previous block.  \n\n  \nFigure 4: With the network converted to RMSNorm (see Figure 3 ), we apply the computational-invariance idea. The input weight matrices $\\mathrm{diag}(\\alpha)\\mathbf{W}_{\\mathrm{in}}$ are pre-multiplied by $\\mathbf{Q}^{\\top}$ . The output matrices $\\mathbf{W}_{\\mathrm{out}}\\mathbf{M}$ are post-multiplied by $\\mathbf{Q}$ . In the skip-connection, a new linear layer is added $\\mathbf{Q}_{\\ell}^{\\top}\\mathbf{Q}_{\\ell+1}$ . After these modifications, the matrices can be sliced (hatched areas).  \n\nwith the exception of line 5 of Algorithm 1 . Here we see that the residual connection and the output of the block must have the same rotation. To fix this, we modify the residual connection by applying the linear transformation applied to different blocks with the additional linear operation in the residual connection. Unlike the $\\mathbf{Q}_{\\ell-1}^{\\top}\\mathbf{Q}_{\\ell}$ \u2212to the residual. Figure 4 shows how different rotations can be modifications to the weight matrices, these additional operations cannot be pre-computed and add a small $(D\\times D)$ overhead to the model. Nonetheless, they are needed to allow slicing the model (Section 3.4 ) and we see real speedup overall (Section 4 ).  \n\nTo compute the matrices $\\mathbf{Q}_{\\ell}$ , we use PCA. We select a calibration dataset from the training set, run it through the model (after converting LayerNorm operations into RMSNorm), and extract the orthogonal matrix of the layer. We use the output of the transformed network to calculate the orthogonal matrices of the next layers. More precisely, if $\\mathbf{X}_{\\ell,i}$ is the output of the $\\ell^{\\mathrm{th}}$ RMSNorm block for the $i^{\\mathrm{th}}$ sequence in the calibration dataset, we compute  \n\n$$\n\\mathbf{C}_{\\ell}=\\sum_{i}\\mathbf{X}_{\\ell,i}^{\\top}\\mathbf{X}_{\\ell,i}\n$$  \n\nand set $\\mathbf{Q}_{\\ell}$ to the be the eigenvectors of $\\mathbf{C}_{\\ell}$ , sorted by decreasing eigenvalues.\n\n# 3.4 SLICING\nThe goal of Principal Component Analysis is usually to take a data matrix $\\mathbf{X}$ and compute a lower dimensional representation $\\mathbf{Z}$ , and an approximate reconstruction $\\tilde{\\mathbf{X}}$ :  \n\n$$\n\\mathbf{Z}=\\mathbf{X}\\mathbf{Q}\\mathbf{D}\\,,\\qquad\\tilde{\\mathbf{X}}=\\mathbf{Z}\\mathbf{D}^{\\top}\\mathbf{Q}^{\\top}\\,.\n$$  \n\nwhere $\\mathbf{Q}$ is the ectors of ${\\bf X}^{\\top}{\\bf X}$ , and $\\mathbf{D}$ is a $D\\times D_{\\mathrm{small}}$ deletion matrix (containing $D_{\\mathrm{small}}$ The reconstruction is columns of the $D\\times D$ \u00d7$L_{2}$ identity matrix), which removes some of the columns of the matrix to the left. optimal, in the sense that QD is a linear mapping that minimizes $\\lVert\\mathbf{X}-\\tilde{\\mathbf{X}}\\rVert^{2}$ .  \n\nWhen we apply PCA to the signal matrix $\\mathbf{X}$ bween blocks, we never materialize the $N\\times D$ signal matrix, but we apply the deletion matrix Dto the operations preceding and succeeding the construction of that matrix, which have already been multiplied by $\\mathbf{Q}$ in the above. We delete rows of $\\mathbf{W}_{\\mathrm{in}}$ that we have inserted into the residual connection (see Figure and columns of $\\mathbf{W}_{\\mathrm{out}}$ and $\\mathbf{W}_{\\mathrm{embd}}$ . We also delete both rows 4 ). and columns of the matrix $\\mathbf{Q}_{\\ell-1}^{\\top}\\mathbf{Q}_{\\ell}$ \u2212\n\n# 4 EXPERIMENTAL VALIDATION\nSetup We use HuggingFace Transformers ( Wolf et al. ,2019 ) to implement our code with PyTorch (Paszke et al. ,2019 ). The computation of $\\mathbf{Q}$ is performed on a single H100 GPU with 80GB of memory, taking approximately 3.5 hours to complete for the L LAMA -2 70B model. During the PCA calculation, we use double precision for computing the eigenvectors of the covariance matrix. We find that using single precision for eigenvector calculations in PyTorch leads to a discrepancy in the final accuracy, as detailed in Appendix A.2 .  \n\nWe experiment with two different calibration sets: 1024 samples from the WikiText-2 training dataset ( Merity et al. ,2016 ) and 5000 samples from the Alpaca training dataset ( Taori et al. ,2023 ). Sequence lengths are chosen as the maximum of each language model. An ablation study on the calibration set size and sequence length is presented in Appendix A.3 .  \n\nModels, Tasks, and GPUs We evaluate all our experiments on OPT ( Zhang et al. ,2022 ), L LAMA -2 (Touvron et al. ,2023 ) model families, and additionally evaluate Phi-2 (in our zero-shot task) experiments. We exclude OPT 175B, as it is outperformed by smaller L LAMA -2 models. Nonetheless, we anticipate that this larger model will yield improved results, as larger models typically offer more promising opportunities for compression (see Section 4.1 ). We evaluate our scheme on both language generation as well as popular zero-shot tasks. To demonstrate the comprehensive speedup achieved by SliceGPT we use: Quadro RTX6000 GPUs with 24GB of memory as a representative example of consumer-level GPUs; 40GB A100s and 80GB H100s to provide datacenter-level benchmarks.  \n\nBaseline Setup We initially planned to compare our results against a scheme that pruned columns (or rows) with the smallest norm but found that this baseline was very poor, with the perplexity of the model soaring into the 1000s after pruning just a few columns. Instead, we compare SliceGPT against SparseGPT ( Frantar & Alistarh ,2023 ) employing a 2:4 sparsity ratio, as this is the only sparsity scheme which achieves speedup ( Mishra et al. ,2021 )."}, {"ref_id": "454847819065993190", "chunk_id": "1", "score": 0.3203125, "text": "# 4.2 HE-Friendly Normalization\nTo enhance training stability, transformers rely on LayerNorm , which is formulated as follows:  \n\n$$\n\\operatorname{LayerNorm}(x)={\\frac{x-\\mu}{\\sqrt{\\sigma^{2}}}}\\cdot\\gamma+\\beta\n$$  \n\nwhere $x$ is the input vector, $\\mu$ is the mean of $x,\\sigma^{2}$ is the variance, and $\\gamma$ and $\\beta$ are learnable parameters. Computing LayerNorm over HE requires calculating the inverse square root, which is not a polynomial operation. A common practice in designing neural networks for secure inference over HE is to replace LayerNorm with BatchNorm , as it can be implemented by a straightforward constant affine transformation at inference time. Therefore, we attempted to train transformers using $\\sigma$ -attention and BatchNorm . We observed that these models were highly unstable, performing poorly on vision tasks and failing to converge in NLP tasks. Consequently, we adopt two distinct approaches for vision and NLP tasks.  \n\nNormalization for Vision Transformers For vision transformers, to improve performance and mitigate training instability, we add two components: (i) Additional BatchNorm in the MLP of ViT, which is proposed in ( Yao et al. ,2021 ) as a stabilizer for ViT training, and (ii) additional BatchNorm within the $\\sigma$ -attention, which normalizes values across different attention heads, since we observe that those are the sources of instability. The resulting $\\sigma$ -attention variant is:  \n\n$$\n{\\frac{1}{\\mathrm{S}(L)}}\\sigma\\left({\\mathrm{BatchNorm~}}2\\mathrm{D}\\left({\\frac{Q K^{T}}{\\sqrt{d_{k}}}}\\right)\\right)V\n$$  \n\nNormalization for NLP Transformers For NLP, models with $\\sigma$ -attention and BatchNorm completely failed, even when augmented by stabilizing factors from the literature, such as ( Wang et al. ,2022 ). Consequently, we had to confront the challenge of approximating LayerNorm by polynomials, which entails approximating the inverse square root function. Empirically, we found that the values of the variance in trained transformers (with $\\sigma$ -attention) ranged between 1 and $10^{9}$ , causing approximation challenges due to the extremely large domain. To solve this problem, we first focus on narrowing the domain of the variance, which then makes it easier to approximate the inverse square root over this restricted domain. The method is similar to ( Baruch et al. ,2023 ), which introduces an additional loss function that encourages the model to minimize the range of the input to activation layers. We apply this technique on the variance at each layer via the following objective:  \n\n$$\n\\mathbb{L}_{\\mathrm{Variance\\;Minimization}}:=\\Sigma_{m=1}^{L_{N}}\\operatorname*{max}_{c\\in C,x_{i}\\in X}\\left(\\mathrm{var}_{m,c}^{i}\\right)\n$$  \n\nwhere we denote the number of layers by $N_{L}$ , the number of channels by $C$ , and the train dataset by $X:=[x_{1},x_{2},..]$ . Furthermore, we denote the variance at layer $m$ and channel $c$ , when the model processes the $x_{i}$ example by $\\mathrm{var}_{m,c}^{i}$ . For reasons of efficiency, we compute the loss over each batch rather than the whole training set $X$ . By extending this method to operate on layer normalization instead of activations, we succeed in reducing the variance range to a smaller domain. This reduction makes it feasible to use well-known approximations, such as the technique described in ( Baruch et al. ,2023 ), for the inverse square root.\n\n# 4.3 A Recipe for a Polynomial Transformer\nFig. 2 illustrates the entire method, which comprises three stages: (i) First we modify the architecture from the original transformer architecture (first column) to a HE-friendly architecture (second column), namely, an architecture that can eventually be converted into a polynomial form. Then we train the modified model from scratch with the same hyperparameters. (ii) In the second stage, we perform a supplementary training procedure to obtain a model with HE-friendly weights ,which means that each non-polynomial component will only operate on specific and restricted domains. To do so, we add a loss function that minimizes the range of inputs to non-polynomial layers. For the activations (standard activations and attention-activations), we directly apply the method from ( Baruch et al. ,2023 ), which defines the range loss for activations. For the LayerNorm layers, we use the loss defined in Eq. 11 . The whole training objective $\\mathbb{L}$ is defined by:\n\n# \u03b1LRange Minimization +\u03b2LVariance Minimization +Loriginal\nwhere $\\alpha$ and $\\beta$ are hyperparameters. (iii) Finally, each non-polynomial layer is directly replaced with its polynomial approximation, resulting in a polynomial model . Appendix A contains details on the approximation we used. Those approximations are accurate for the HE-friendly architecture & weights obtained from earlier stages.\n\n# 5 Experiments\nWe evaluate the polynomial models generated by our method in Section 5.1 , focusing on language modeling with the Wikitext-103 dataset and image classification using standard benchmarks, including Tiny-ImageNet and CIFAR-10. Section 5.2 justifies our methodological choices, specifically the use of scaled $\\sigma$ -attention and an additional training phase designed to manipulate the input values of non-polynomial layers. Furthermore, that section contains several ablation studies to assess the impact of each method component on the overall performance degradation. Section 5.3 discusses the accuracy and latency implications of applying our models over FHE. The experimental setup is detailed in Appendix B.\n\n# 5.1 Polynomial Models\nPolynomial Language Modeling We evaluated our BERT-like transformer model for language modeling as our NLP task. Specifically, we trained on Wikitext-103 with a self-supervised scheme for Next Token Prediction (NTP). The results in Table 1 show that after architectural and training modifications, we achieved a fully polynomial model with competitive perplexity scores. In particular, the perplexity increased by 0.91 compared to a vanilla transformer of the same size, from 18.98 to 19.89 for a 6-layer transformer (53.3M parameters), and by 2.02 from 16.89 to 18.91 for a 12- layer model (95.8 Mparameters). Considering that at least $80\\%$ of the gap between the vanilla transformer and our corresponding polynomial model is caused in the last stage where polynomial approximations are used (0.74 for 6 layers model and 1.76 for 12 layers), we hypothesize that more accurate polynomials can mitigate most of the performance gap.  \n\n<html><body><table><tr><td>Depth</td><td>Original</td><td>P</td><td>P+MR</td><td>Poly</td></tr><tr><td>6</td><td>18.98</td><td>19.07</td><td>19.15</td><td>19.89</td></tr><tr><td>12</td><td>16.89</td><td>16.98</td><td>17.15</td><td>18.91</td></tr></table></body></html>  \n\nTable 1: NLP Results: Perplexity results of a polynomial BERT-like transformer on the Wikitext-103 benchmark. \u2018Depth\u2019 indicates the number of transformer layers. \u2018Original\u2019 denotes the perplexity of the vanilla Softmax-based transformer of equivalent size. \u2018P\u2019 represents models utilizing scaled $\\sigma$ -attention, while $\\mathbf{\\dot{P}+M R}$ \u2019shows perplexity at the end of the range minimization training. \u2018Poly\u2019 details the final performance after substituting LayerNorm and activation functions with polynomial approximations.  \n\nPolynomial Image Classification We evaluated our vision models on two image classification benchmarks: Tiny-ImageNet and CIFAR-100. The results, presented in Table 2 , indicate that our vision models, which are converted to polynomial form by our methods, remain competitive. Specifically, for ViT on CIFAR-100, the original ViT (denoted as $\\mathbf{\\omega}^{\\bullet}\\mathbf{O}^{\\bullet}$ ) achieved a score of $73.4\\%$ , whereas our HE-friendly alternative $(\\mathrm{P+BN+QK+A})$ ), achieved a score of $71.1\\%$ . The HE-friendly alternative employs BatchNorm as the normalization layer, includes additional stabilizers described in 4.2 and is based on scaled$\\sigma$ attention. After applying our range-aware training procedure, the accuracy of our model decreased marginally by $0.1\\%$ to $71.0\\%$ , and it further decreased to $70.8\\%$ after approximating nonpolynomial components. For the Swin Transformer on Tiny-ImageNet, the original model achieved $59.4\\%$ , and transitioning to the HE-friendly architecture resulted in a performance decrease of $0.3\\%$ to $59.1\\%$ . After employing range-aware training to obtain HE-friendly weights, the performance further degraded by $0.2\\%$ to $58.9\\%$ , while the final performance of the polynomial model remained the same. In conclusion, the performance gap between the polynomial models and the original architectures is less than $4\\%$ , demonstrating the practicality of our methods in this domain.  \n\n<html><body><table><tr><td colspan=\"3\">Model Dataset 0 P+BN+QK+A MR Poly</td></tr><tr><td>ViT CIFAR-100</td><td>73.4 71.1</td><td>71.0 70.8</td></tr><tr><td>Swin</td><td>Tiny-ImgNet 59.4</td><td>59.1 58.9 58.9</td></tr></table></body></html>  \n\nTable 2: Vision Results: Test accuracy results of a polynomial ViT. \u2018O\u2019 represents the original vanilla model, $\\mathrm{{^{\\circ}\\mathrm{{P}+\\mathrm{{BN}+\\mathrm{{QK}+\\mathrm{{A}^{\\circ}}}}}}}$ represents scaled$\\boldsymbol{\\sigma}$ attention-based ViT trained with BatchNorm as the normalization function instead of LayerNorm , and contains the additional stabilizers described in 4.2 . \u2018MR\u2019 refers to the accuracy at the end of the range minimization training, and \u2018Poly\u2019 details the final performance after substituting polynomial approximations."}, {"ref_id": "454895289196085862", "chunk_id": "3", "score": 0.271484375, "text": "# 3 Experiments and Results\nWe evaluate the performance of PreNorm and PostNorm for ZST on various datasets and language pairs. We then analyze the off-target rates and structural discrepancies between PreNorm and PostNorm to understand performance differences.  \n\n$$\n\\mathrm{LayerNorm}(\\mathbf{x})=\\frac{\\mathbf{x}-\\mathbf{E}(\\mathbf{x})}{\\sqrt{\\mathbf{V}(\\mathbf{x})}}\\cdot\\mathbf{g}+\\mathbf{b},\n$$  \n\nwhere $\\mathbf{g}$ and $\\mathbf{b}$ are trainable gain and bias. $\\mathbf{E}$ and $\\mathbf{V}$ indicate expectation and variance. LayerNorm is commonly used in two positions in the Transformer, as shown in Fig. 1 . PostNorm, which is the originally proposed setting of the Transformer ( Vaswani et al. ,2017 ), involves applying LayerNorm after each sub-module (i.e., selfattention or feed-forward network) and residual connections. PreNorm ( Baevski and Auli ,2019 ), on the other hand, involves applying LayerNorm directly before each sub-module and is known to stabilize Transformer training. While variants of Transformer LayerNorm like RMSNorm ( Zhang and Sennrich ,2019 ) have been proposed, the vanilla PreNorm and PostNorm are still the most widely adopted settings in current multilingual\n\n# 3.1 Experimental Settings\nDatasets We perform ZST experiments on three datasets: OPUS ( Zhang et al. ,2020 ), IWSLT ( Cettolo et al. ,2017 ), and Europarl ( Koehn ,2005 ). The statistics of the datasets are summarized in Table 1 .We include 7 ,4 , and 5 languages for each dataset. The training data consists of only English-centric sentence pairs, resulting in 30 ,6 , and 12 ZST directions for each dataset. The total number of parallel sentences for each dataset is 12 .00 M, 1 .38 M, and 15 .78 M, respectively. We apply BPE ( Sennrich et al. ,2016 ) with merge operations of 50 k, 40 k, and $50\\mathbf{k}$ to create a joint vocabulary for each dataset.  \n\nTraining We employ Transformer-base model for OPUS and IWSLT, and Transformer-big for Europarl, in accordance with the distinct sizes of training data. We consider the following settings: (1) PreNorm or PostNorm : PreNorm involves LayerNorm directly before each sub-module (i.e., self-attention or feed-forward network), while PostNorm applies LayerNorm after each sub-module and residual connections, as shown in Fig. 1 .(2) S-ENC-T-DEC or T-ENC : Source language tag on the encoder-side and target language tag on the decoder-side; or only target language tag on the encoder-side. Wu et al. (2021 ) showed that this setting impacts ZST for Transformer with PreNorm. (3) w/ or w/o Res. : With the residual connection for self-attention in the middle $(4^{t h})$ encoder layer or not. Liu et al. (2021 ) revealed that \u201cw/o Res.\u201d improves ZST for the model trained with PreNorm. We experiment this with different LayerNorm settings as this may reduce the potential of overfitting on supervised directions, then further impacts ZST, which aligns with our hypothesis.  \n\nTable 2: BLEU scores and off-target rates (shown in brackets) . We report the average score of three seeds; refer to Appendix Gfor BLEU score of each translation direction and seed. \u201cRes.\u201d indicates the residual connection of self-attention in the $4^{t h}$ encoder layer. We mark lower off-target rates and significantly higher BLEU scores ( Koehn ,2004 ) between PreNorm and PostNorm in bold for ZST.   \n\n\n<html><body><table><tr><td>#</td><td>Layer Norm</td><td>Language Tag</td><td>Res.</td><td></td><td>Zero-shot</td><td></td><td></td><td>Supervised</td><td></td></tr><tr><td>0</td><td></td><td>Pivot</td><td></td><td>OPUS 21.8</td><td>IWSLT 20.0</td><td>Europarl 29.5</td><td>OPUS</td><td>IWSLT</td><td>Europarl</td></tr><tr><td>1</td><td>PreNorm</td><td>S-ENC-T-DEC</td><td>w/</td><td>10.1 (42.19%)</td><td>4.9 (64.84%)</td><td>24.9 ( 7.73%)</td><td>33.7</td><td>31.5</td><td>34.3</td></tr><tr><td>2</td><td>PostNorm</td><td>S-ENC-T-DEC</td><td>w/</td><td>16.8 ( 8.59%)</td><td>12.4 (10.61%)</td><td>29.2( 0.34%)</td><td>33.9</td><td>31.5</td><td>34.5</td></tr><tr><td>3</td><td>PreNorm</td><td>T-ENC</td><td>w/</td><td>13.3 (22.99%)</td><td>13.7 ( 3.98%)</td><td>29.5( 0.23%)</td><td>33.7</td><td>31.6</td><td>34.4</td></tr><tr><td>4</td><td>PostNorm</td><td>T-ENC</td><td>w/</td><td>14.0 (22.86%)</td><td>15.5 ( 4.59%)</td><td>30.8 ( 0.11%)</td><td>34.1</td><td>31.5</td><td>34.5</td></tr><tr><td>5</td><td>PreNorm</td><td>S-ENC-T-DEC</td><td>w/o</td><td>14.3 (20.67%)</td><td>8.0 (50.16%)</td><td>16.7 (41.87%)</td><td>33.6</td><td>30.9</td><td>34.3</td></tr><tr><td>6</td><td>PostNorm</td><td>S-ENC-T-DEC</td><td>w/o</td><td>16.0 (15.27%)</td><td>17.4 (1.83%)</td><td>29.0 ( 0.41%)</td><td>33.8</td><td>30.7</td><td>34.4</td></tr><tr><td>7</td><td>PreNorm</td><td>T-ENC</td><td>w/o</td><td>13.4 (27.15%)</td><td>16.2 ( 1.54%)</td><td>29.9 ( 2.15%)</td><td>33.5</td><td>30.9</td><td>34.3</td></tr><tr><td>8</td><td>PostNorm</td><td>T-ENC</td><td>w/o</td><td>13.9 (26.68%)</td><td>17.8 (1.50%)</td><td>30.8 ( 0.13%)</td><td>33.9</td><td>30.6</td><td>34.4</td></tr></table></body></html>  \n\nThe settings above lead to eight different combinations, shown in Table 2 (#1 - #8). Additional training details are in Appendix A .\n\n# 3.2 Main Results\nWe evaluate ZST systems using SacreBLEU ( Post ,2018 ) and off-target rates. We report in Table 2 BLEU scores for both zero-shot and supervised directions. For ZST, we also present pivot-based translation results as a reference. Implementation details of evaluation can be found in Appendix B.Our findings are as follows:  \n\nPreNorm vs. PostNorm :We find that PostNorm consistently yields better BLEU scores than PreNorm for ZST across various language tag and residual connection settings, while their performance is comparable for supervised directions.  \n\nImpact of Language Tag and Residual Connection: We observe that using the \u201cT-ENC\u201d language tag and \u201cw/ Res.\u201d improves ZST performance for IWSLT, which aligns with the findings of $\\mathrm{Wu}$ et al. (2021 ) and Liu et al. (2021 ). Nevertheless, the best performance is achieved using \u201cw/ Res.\u201d for PostNorm with \u201cS-ENC-T-DEC\u201d and \u201cT-ENC\u201d tags for OPUS and Europarl, respectively (#2 and #4). Given that Wu et al. (2021 ) and Liu et al. (2021 )used PreNorm as the default setting (#2, #4, #6 and #8 are unreported results in their work), our results emphasize the need to consider PostNorm as the default setting for ZST, while the language tag and residual connection settings have less impact.  \n\nOff-target Rates : Off-target rates help understand the different BLEU score gaps between PreNorm and PostNorm, which ranges from 0 .5 to 12 .3 BLEU points. For PreNorm and PostNorm with the \u201cT-ENC\u201d language tag (#3, #4, #7, and #8), they have similar off-target rates, with a discrepancy ranging from $-0.61\\%$ to $2.02\\%$ , which results in narrow BLEU score gaps, ranging from 0 .5 to 1 .8 points. However, for PreNorm and PostNorm with the \u201cS-ENC-T-DEC\u201d language tag (#1, #2, #5, and #6), the off-target rates show a more considerable discrepancy, ranging from $5.40\\%$ to $54.23\\%$ , resulting in BLEU score gaps from 1 .7 to 12 .3 points. Further analysis of the nature of Transformer hidden states in the next section explores the reason for these different off-target rates in translations."}], "task_step_question_answer": "\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u9886\u57df\uff0cBERT\u6a21\u578b\u901a\u8fc7LayerNorm\u5728\u6bcf\u4e00\u5c42\u7684\u8f93\u51fa\u4e0a\u8fdb\u884c\u5f52\u4e00\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002\u7c7b\u4f3c\u5730\uff0c\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\uff0cVision Transformer\uff08ViT\uff09\u901a\u8fc7\u5f15\u5165LayerNorm\u6765\u9002\u5e94\u56fe\u50cf\u6570\u636e\u7684\u7279\u6027\uff0c\u4ece\u800c\u5728\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u4e0e\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u76f8\u5ab2\u7f8e\u7684\u6027\u80fd\u3002\u5728\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GAN\uff09\u4e2d\uff0cRMSNorm\u88ab\u7528\u4e8e\u751f\u6210\u5668\u7684\u6bcf\u4e00\u5c42\uff0c\u4ee5\u6539\u8fdb\u751f\u6210\u56fe\u50cf\u7684\u8d28\u91cf\u548c\u591a\u6837\u6027\u3002\u5728\u8bed\u97f3\u8bc6\u522b\u4efb\u52a1\u4e2d\uff0cRMSNorm\u901a\u8fc7\u8c03\u6574\u7279\u5f81\u5206\u5e03\u7684\u5747\u503c\u548c\u65b9\u5dee\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u8bc6\u522b\u51c6\u786e\u7387\u3002\u5728\u591a\u6a21\u6001\u6570\u636e\u5904\u7406\u65b9\u9762\uff0c\u6700\u65b0\u7684\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u7ed3\u5408\u81ea\u9002\u5e94\u5f52\u4e00\u5316\u7b56\u7565\u548c\u6ce8\u610f\u529b\u673a\u5236\uff0c\u53ef\u4ee5\u5728\u6587\u672c-\u56fe\u50cf\u8054\u5408\u5efa\u6a21\u4efb\u52a1\u4e2d\u5b9e\u73b0\u66f4\u597d\u7684\u7279\u5f81\u5bf9\u9f50\u548c\u878d\u5408\u3002\u4f8b\u5982\uff0cCLIP\u6a21\u578b\u901a\u8fc7\u7ed3\u5408LayerNorm\u548c\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5728\u56fe\u50cf-\u6587\u672c\u5339\u914d\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6548\u679c\u3002\u672a\u6765\u7684\u7814\u7a76\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63a2\u7d22\u5982\u4f55\u8bbe\u8ba1\u4e00\u79cd\u901a\u7528\u7684\u5f52\u4e00\u5316\u7b56\u7565\uff0c\u80fd\u591f\u65e0\u7f1d\u9002\u5e94\u4e0d\u540c\u9886\u57df\u548c\u6a21\u6001\u7684\u6570\u636e\uff0c\u5e76\u89e3\u51b3\u5176\u6f5c\u5728\u7684\u6280\u672f\u6311\u6218\u3002", "ref_task_step_id": ""}, "__type__": "task_step_node"}, "cf852a98-8ebb-40f0-bf13-6ea29fc63dd5": {"__data__": {"id_": "cf852a98-8ebb-40f0-bf13-6ea29fc63dd5", "metadata": {}, "relationships": {}, "hash": "", "start_task_context": "\u5927\u6a21\u578b\u4e2d\u7684LayerNorm\u548cRMSNorm\u6709\u4ec0\u4e48\u533a\u522b\uff1f", "aemo_representation_context": "### \u5206\u6790\u8fd1\u51e0\u5e74\u7814\u7a76\u9886\u57df\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba\n\u5728\u5927\u6a21\u578b\u76f8\u5173\u7814\u7a76\u9886\u57df\uff0c\u5f52\u4e00\u5316\u6280\u672f\u662f\u91cd\u8981\u7684\u7814\u7a76\u65b9\u5411\u4e4b\u4e00\u3002LayerNorm \u548c RMSNorm \u4f5c\u4e3a\u5176\u4e2d\u5173\u952e\u7684\u5f52\u4e00\u5316\u65b9\u6cd5\uff0c\u88ab\u4f17\u591a\u7814\u7a76\u6df1\u5165\u63a2\u8ba8\u3002\u7814\u7a76\u8005\u4eec\u4ece\u7406\u8bba\u5206\u6790\u3001\u5b9e\u9a8c\u5bf9\u6bd4\u7b49\u591a\u65b9\u9762\u5bf9\u5b83\u4eec\u8fdb\u884c\u5256\u6790\uff0c\u4ee5\u660e\u786e\u5176\u5728\u5927\u6a21\u578b\u6280\u672f\u6846\u67b6\u4e2d\u7684\u5730\u4f4d\u548c\u4f5c\u7528\u3002\n\n### \u7814\u7a76\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u4e3b\u8981\u6846\u67b6\uff08\u5982 Transformer\u3001GAN\u3001BERT \u7b49\uff09\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0e\u53d8\u4f53\n\u5728 Transformer \u67b6\u6784\u4e3a\u4e3b\u7684\u4f17\u591a\u6a21\u578b\u4e2d\uff0cLayerNorm \u548c RMSNorm \u88ab\u5e7f\u6cdb\u5e94\u7528\u3002\u4f8b\u5982\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\uff08\u5982 BERT \u53ca\u5176\u53d8\u4f53\u6a21\u578b\uff09\u4e2d\uff0cLayerNorm \u6700\u521d\u88ab\u7528\u4e8e\u7a33\u5b9a\u6a21\u578b\u8bad\u7ec3\u3001\u52a0\u901f\u6536\u655b\u4ee5\u53ca\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u80fd\u529b\uff1b\u800c RMSNorm \u4e5f\u9010\u6e10\u5728\u4e00\u4e9b\u57fa\u4e8e Transformer \u7684\u6a21\u578b\u4e2d\u5f97\u5230\u5e94\u7528\uff0c\u4ee5\u6539\u8fdb\u6a21\u578b\u7684\u6027\u80fd\u8868\u73b0\u3002\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\uff08\u5982\u57fa\u4e8e Transformer \u7684\u89c6\u89c9\u6a21\u578b\uff09\u4e2d\u540c\u6837\u6709\u5b83\u4eec\u7684\u8eab\u5f71\uff0c\u4e0d\u540c\u4efb\u52a1\u573a\u666f\u4e0b\u5bf9\u8fd9\u4e24\u79cd\u5f52\u4e00\u5316\u65b9\u6cd5\u8fdb\u884c\u4e86\u9002\u5e94\u6027\u8c03\u6574\u548c\u53d8\u4f53\u5e94\u7528\uff0c\u4ee5\u66f4\u597d\u5730\u9002\u914d\u4efb\u52a1\u9700\u6c42\u3002\n\n### \u8bc4\u4f30\u5b66\u672f\u754c\u7684\u6280\u672f\u8fdb\u6b65\u4e0e\u5c40\u9650\u6027\n - **\u6280\u672f\u8fdb\u6b65**\uff1aLayerNorm \u548c RMSNorm \u7684\u51fa\u73b0\u663e\u8457\u63d0\u5347\u4e86\u5927\u6a21\u578b\u8bad\u7ec3\u7684\u7a33\u5b9a\u6027\u548c\u6548\u7387\u3002LayerNorm \u80fd\u591f\u5bf9\u6bcf\u4e2a\u6837\u672c\u7684\u4e0d\u540c\u7ef4\u5ea6\u8fdb\u884c\u5f52\u4e00\u5316\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5185\u90e8\u534f\u53d8\u91cf\u504f\u79fb\u95ee\u9898\uff1bRMSNorm \u5219\u901a\u8fc7\u66f4\u7b80\u6d01\u7684\u8ba1\u7b97\u65b9\u5f0f\uff0c\u5728\u4e00\u4e9b\u573a\u666f\u4e0b\u53d6\u5f97\u4e86\u4e0e LayerNorm \u76f8\u5f53\u751a\u81f3\u66f4\u597d\u7684\u6548\u679c\uff0c\u63a8\u52a8\u4e86\u5927\u6a21\u578b\u6027\u80fd\u7684\u63d0\u5347\u3002\n - **\u5c40\u9650\u6027**\uff1a\u5c3d\u7ba1\u53d6\u5f97\u8fdb\u6b65\uff0c\u4f46\u4ecd\u5b58\u5728\u4e00\u4e9b\u95ee\u9898\u3002\u4f8b\u5982\uff0c\u5728\u67d0\u4e9b\u590d\u6742\u4efb\u52a1\u548c\u7279\u5b9a\u6570\u636e\u96c6\u4e0a\uff0c\u6a21\u578b\u4f9d\u7136\u5b58\u5728\u6027\u80fd\u74f6\u9888\uff0c\u5373\u4f7f\u4f7f\u7528\u4e86\u8fd9\u4e9b\u5f52\u4e00\u5316\u65b9\u6cd5\uff0c\u6a21\u578b\u504f\u5dee\u548c\u6570\u636e\u4f9d\u8d56\u95ee\u9898\u4f9d\u65e7\u5b58\u5728\u3002\u800c\u4e14\u5728\u9762\u5bf9\u591a\u6a21\u6001\u6570\u636e\u65f6\uff0c\u73b0\u6709\u7684\u5f52\u4e00\u5316\u65b9\u6cd5\u5728\u878d\u5408\u4e0d\u540c\u6a21\u6001\u7279\u5f81\u65b9\u9762\u8fd8\u5b58\u5728\u4e0d\u8db3\u3002\n\n### \u63a2\u8ba8\u8ba1\u7b97\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\n - **\u9002\u7528\u6027**\uff1aLayerNorm \u5728\u591a\u79cd\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6570\u636e\u96c6\u548c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8f83\u597d\u7684\u9002\u7528\u6027\uff0c\u80fd\u591f\u9002\u5e94\u4e0d\u540c\u89c4\u6a21\u548c\u7279\u70b9\u7684\u6570\u636e\u3002RMSNorm \u5728\u4e00\u4e9b\u5927\u89c4\u6a21\u6570\u636e\u548c\u7279\u5b9a\u4efb\u52a1\uff08\u5982\u957f\u5e8f\u5217\u5904\u7406\uff09\u4e2d\u5c55\u73b0\u51fa\u72ec\u7279\u4f18\u52bf\u3002\u7136\u800c\uff0c\u5728\u591a\u9886\u57df\u3001\u591a\u6a21\u6001\u6570\u636e\u573a\u666f\u4e0b\uff0c\u5b83\u4eec\u7684\u9002\u7528\u6027\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u6539\u8fdb\u4ee5\u66f4\u597d\u5730\u5904\u7406\u4e0d\u540c\u7c7b\u578b\u6570\u636e\u7684\u878d\u5408\u3002\n - **\u6cdb\u5316\u80fd\u529b**\uff1a\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\uff0c\u8fd9\u4e24\u79cd\u5f52\u4e00\u5316\u65b9\u6cd5\u6709\u52a9\u4e8e\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u5728\u590d\u6742\u591a\u53d8\u7684\u73b0\u5b9e\u5e94\u7528\u573a\u666f\u4e2d\uff0c\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u4ecd\u6709\u5f85\u63d0\u9ad8\uff0c\u5c24\u5176\u662f\u5728\u8de8\u9886\u57df\u548c\u591a\u6a21\u6001\u6570\u636e\u7684\u6cdb\u5316\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002\n\n### \u5206\u6790\u6700\u65b0\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027\n - **\u7a33\u5b9a\u6027**\uff1a\u7814\u7a76\u53d1\u73b0\uff0cLayerNorm \u901a\u8fc7\u5bf9\u8f93\u5165\u6570\u636e\u7684\u5f52\u4e00\u5316\u5904\u7406\uff0c\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u63d0\u9ad8\u4e86\u6a21\u578b\u8bad\u7ec3\u7684\u7a33\u5b9a\u6027\u3002RMSNorm \u540c\u6837\u5728\u4e00\u4e9b\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u7a33\u5b9a\u6027\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u52a8\u6001\u73af\u5883\u4e0b\uff0c\u90e8\u5206\u7814\u7a76\u9488\u5bf9\u5176\u7a33\u5b9a\u6027\u8fdb\u884c\u4e86\u4f18\u5316\uff0c\u4f7f\u5176\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\u4e5f\u80fd\u4fdd\u6301\u76f8\u5bf9\u7a33\u5b9a\u7684\u8868\u73b0\u3002\n - **\u5bb9\u9519\u6027**\uff1a\u5728\u9762\u5bf9\u6570\u636e\u566a\u58f0\u548c\u5f02\u5e38\u503c\u65f6\uff0c\u8fd9\u4e24\u79cd\u5f52\u4e00\u5316\u65b9\u6cd5\u90fd\u6709\u4e00\u5b9a\u7684\u5bb9\u9519\u80fd\u529b\uff0c\u4f46\u5728\u6781\u7aef\u60c5\u51b5\u4e0b\uff0c\u6a21\u578b\u6027\u80fd\u4ecd\u4f1a\u53d7\u5230\u5f71\u54cd\uff0c\u5bb9\u9519\u6027\u65b9\u9762\u8fd8\u6709\u63d0\u5347\u7a7a\u95f4\u3002\n\n### \u8bc4\u4f30\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218\n - **\u672a\u6765\u7814\u7a76\u65b9\u5411**\uff1a\u8bb8\u591a\u8bba\u6587\u63d0\u51fa\u63a2\u7d22\u66f4\u9002\u7528\u4e8e\u591a\u6a21\u6001\u6570\u636e\u7684\u5f52\u4e00\u5316\u65b9\u6cd5\uff0c\u4ee5\u53ca\u5982\u4f55\u8fdb\u4e00\u6b65\u4f18\u5316 LayerNorm \u548c RMSNorm \u4ee5\u63d0\u5347\u6a21\u578b\u5728\u590d\u6742\u573a\u666f\u4e0b\u7684\u6027\u80fd\u3002\u540c\u65f6\uff0c\u7814\u7a76\u5982\u4f55\u589e\u5f3a\u5f52\u4e00\u5316\u65b9\u6cd5\u5bf9\u6a21\u578b\u504f\u5dee\u548c\u6570\u636e\u4f9d\u8d56\u95ee\u9898\u7684\u89e3\u51b3\u80fd\u529b\u4e5f\u662f\u91cd\u8981\u65b9\u5411\u3002\n - **\u6311\u6218**\uff1a\u65b0\u7684\u7814\u7a76\u95ee\u9898\u5305\u62ec\u5982\u4f55\u8bbe\u8ba1\u4e00\u79cd\u901a\u7528\u7684\u5f52\u4e00\u5316\u7b56\u7565\uff0c\u80fd\u591f\u65e0\u7f1d\u9002\u5e94\u4e0d\u540c\u9886\u57df\u548c\u6a21\u6001\u7684\u6570\u636e\uff1b\u5982\u4f55\u5728\u63d0\u5347\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\uff0c\u8fdb\u4e00\u6b65\u63d0\u9ad8\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u548c\u5bb9\u9519\u6027\u3002\u8fd9\u4e9b\u6311\u6218\u63a8\u52a8\u540e\u7eed\u7814\u7a76\u4e0d\u65ad\u63a2\u7d22\u65b0\u7684\u6280\u672f\u548c\u65b9\u6cd5\uff0c\u4ee5\u6539\u8fdb\u5927\u6a21\u578b\u7684\u6027\u80fd\u548c\u5e94\u7528\u8303\u56f4\u3002\n\n### \u7814\u7a76\u6210\u679c\u3001\u65b9\u6cd5\u7684\u521b\u65b0\u6027\u4e0e\u5e94\u7528\u4ef7\u503c\n - **\u7814\u7a76\u6210\u679c**\uff1a\u660e\u786e\u4e86 LayerNorm \u548c RMSNorm \u5728\u5927\u6a21\u578b\u4e2d\u7684\u6027\u80fd\u8868\u73b0\u3001\u9002\u7528\u573a\u666f\u4ee5\u53ca\u5b58\u5728\u7684\u95ee\u9898\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002\n - **\u65b9\u6cd5\u7684\u521b\u65b0\u6027**\uff1aRMSNorm \u4ee5\u5176\u7b80\u6d01\u7684\u8ba1\u7b97\u65b9\u5f0f\u4e3a\u5f52\u4e00\u5316\u6280\u672f\u5e26\u6765\u65b0\u7684\u601d\u8def\uff0c\u4e0e\u4f20\u7edf\u7684 LayerNorm \u5f62\u6210\u5bf9\u6bd4\u548c\u8865\u5145\uff0c\u63a8\u52a8\u4e86\u5f52\u4e00\u5316\u65b9\u6cd5\u7684\u521b\u65b0\u53d1\u5c55\u3002\n - **\u5e94\u7528\u4ef7\u503c**\uff1a\u8fd9\u4e24\u79cd\u5f52\u4e00\u5316\u65b9\u6cd5\u5728\u5927\u6a21\u578b\u7684\u591a\u4e2a\u5e94\u7528\u9886\u57df\uff08\u5982\u81ea\u7136\u8bed\u8a00\u5904\u7406\u3001\u8ba1\u7b97\u673a\u89c6\u89c9\u7b49\uff09\u90fd\u6709\u91cd\u8981\u5e94\u7528\uff0c\u80fd\u591f\u63d0\u5347\u6a21\u578b\u6027\u80fd\u548c\u8bad\u7ec3\u6548\u7387\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\uff0c\u4e3a\u76f8\u5173\u9886\u57df\u7684\u6280\u672f\u53d1\u5c55\u63d0\u4f9b\u4e86\u6709\u529b\u652f\u6301 \u3002  ", "task_step_name": "\u8bc4\u4f30\u5b66\u672f\u754c\u7684\u6280\u672f\u8fdb\u6b65\u4e0e\u5c40\u9650\u6027", "task_step_description": "- **\u6280\u672f\u8fdb\u6b65**\uff1aLayerNorm\u548cRMSNorm\u7684\u51fa\u73b0\u663e\u8457\u63d0\u5347\u4e86\u5927\u6a21\u578b\u8bad\u7ec3\u7684\u7a33\u5b9a\u6027\u548c\u6548\u7387\u3002LayerNorm\u80fd\u591f\u5bf9\u6bcf\u4e2a\u6837\u672c\u7684\u4e0d\u540c\u7ef4\u5ea6\u8fdb\u884c\u5f52\u4e00\u5316\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5185\u90e8\u534f\u53d8\u91cf\u504f\u79fb\u95ee\u9898\uff1bRMSNorm\u5219\u901a\u8fc7\u66f4\u7b80\u6d01\u7684\u8ba1\u7b97\u65b9\u5f0f\uff0c\u5728\u4e00\u4e9b\u573a\u666f\u4e0b\u53d6\u5f97\u4e86\u4e0eLayerNorm\u76f8\u5f53\u751a\u81f3\u66f4\u597d\u7684\u6548\u679c\uff0c\u63a8\u52a8\u4e86\u5927\u6a21\u578b\u6027\u80fd\u7684\u63d0\u5347\u3002 - **\u5c40\u9650\u6027**\uff1a\u5c3d\u7ba1\u53d6\u5f97\u8fdb\u6b65\uff0c\u4f46\u4ecd\u5b58\u5728\u4e00\u4e9b\u95ee\u9898\u3002\u4f8b\u5982\uff0c\u5728\u67d0\u4e9b\u590d\u6742\u4efb\u52a1\u548c\u7279\u5b9a\u6570\u636e\u96c6\u4e0a\uff0c\u6a21\u578b\u4f9d\u7136\u5b58\u5728\u6027\u80fd\u74f6\u9888\uff0c\u5373\u4f7f\u4f7f\u7528\u4e86\u8fd9\u4e9b\u5f52\u4e00\u5316\u65b9\u6cd5\uff0c\u6a21\u578b\u504f\u5dee\u548c\u6570\u636e\u4f9d\u8d56\u95ee\u9898\u4f9d\u65e7\u5b58\u5728\u3002\u800c\u4e14\u5728\u9762\u5bf9\u591a\u6a21\u6001\u6570\u636e\u65f6\uff0c\u73b0\u6709\u7684\u5f52\u4e00\u5316\u65b9\u6cd5\u5728\u878d\u5408\u4e0d\u540c\u6a21\u6001\u7279\u5f81\u65b9\u9762\u8fd8\u5b58\u5728\u4e0d\u8db3\u3002", "task_step_level": "2", "task_step_question": "LayerNorm \u548c RMSNorm \u5728\u63d0\u5347\u5927\u6a21\u578b\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u6548\u7387\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u6b65\uff0c\u4f46\u5728\u9762\u5bf9\u590d\u6742\u4efb\u52a1\u3001\u7279\u5b9a\u6570\u636e\u96c6\u4ee5\u53ca\u591a\u6a21\u6001\u6570\u636e\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u90a3\u4e48\u5982\u4f55\u9488\u5bf9\u8fd9\u4e9b\u5c40\u9650\u6027\u8fdb\u884c\u6539\u8fdb\u4ee5\u8fdb\u4e00\u6b65\u63a8\u52a8\u5927\u6a21\u578b\u6280\u672f\u53d1\u5c55\uff1f ", "task_step_question_context": [{"ref_id": "454984230824843304", "chunk_id": "1", "score": 0.486328125, "text": "# Related Work\nLLM Optimization. As most LLMs are based on Transformer (Vaswani et al. 2017), which is a typical memoryintensive architecture. The inference bottleneck lies more in the GPU\u2019s memory bandwidth, hence reducing its memory access can significantly improve the inference speed. FlashAttention (Dao et al. 2022), DeepSpeed (Aminabadi et al. 2022), and FlexGen (Sheng et al. 2023) propose optimized transformer implementations or efficient memory management to improve the throughput of LLMs. Others achieve this goal through model pruning, such as LoSparse (Li et al. 2023), SparseGPT (Frantar and Alistarh 2023), and LLM-Pruner (Ma, Fang, and Wang 2023). MiniMoE (Zhang et al. 2023) obtains smaller models with high performance through distillation.  \n\nPost-training Quantization. Weight-only quantization schemes like GPTQ (Frantar et al. 2022) compresses and stores weight parameters, and decompresses them to FP16 for inference during calculation. This approach can effectively reduce the proportion of memory access time during inference while maintaining model accuracy. LLM.int8() (Dettmers et al. 2022) proposes to use float calculation or to adjust the multiplication operations of LayerNorm to reduce quantization loss. Smoothquant (Xiao et al. 2023) proposes a method to reduce the activation ranges by equivalently transferring the multiplication factors in weights and activations. GPTQ (Frantar et al. 2022) reconstruct weights based on the method in OBS (Hassibi, Stork, and Wolff 1993) via Hessian matrix to reduce quantization error. GPTQ has been widely applied in many scenarios where some LLMs could achieve high precision at 4-bit quantization. RPTQ (Yuan et al. 2023) and AWQ (Lin et al. 2023) further improve this method.  \n\nQuantization-aware Training. Another method to improve the performance of the quantized models is quantization-aware training (QAT), which is to fine-tune the quantized models to match the original float models. QAT is widely studied in convolutional networks, but it encounters significant setbacks in large language model quantization. As the training process of LLMs consumes a huge amount of text data (usually in the order of trillions of tokens), how to efficiently fine-tune the quantized LLMs while maintaining their general knowledge and generalization ability remains an open question. To name a few attempts, LLM-QAT (Liu et al. 2023) requires the update the whole parameters of the LLMs on a set of at least $100\\mathrm{k}$ sampled data. ZeroQuantV2 (Yao et al. 2023) introduces a Low Rank Compensation to achieve parameter-efficient fine-tuning, but this approach neither eliminates the need for a large amount of training data nor avoids the introduction of additional parameters.\n\n# Method\n\n# Motivation\nBased on the observation shown in Figure 1, the difference between the output tensors of each layer in the quantized model and its floating counterpart accumulates, while the output of the quantized model gradually deviates from the quantization-friendly zero-mean distribution. This is somewhat expected since LayerNorm magnifies the outlier (Xiao et al. 2023) and no measure is taken to deal with this effect. Hence when we iteratively update the quantized weights of each layer using GPTQ, it inevitably disrupts the zero-mean distribution of the current layer and increases the deviation.  \n\nTo this end, we aim to improve the quantized model\u2019s performance by adjusting its output distribution to approach that of its float counterpart. Complete fine-tuning of the quantized model through QAT is a direct approach, but the large number of parameters in the LLM model and the huge amount of required training data make QAT unacceptable. In order to achieve high performance the quantized model within the time constraint, we are driven to improve current PTQ methods. As LayerNorm is very handy to manipulate distribution, we choose to adjust this layer to achieve the goal. It is also economical to update its weight considering the small number of parameters. Furthermore, nearly all mainstream LLMs use LayerNorm or similar operators, so that the method can be applied universally to a variety of large language models. Therefore, our core objective can be summarized as adjusting the parameters of LayerNorm to make the output distribution of the quantized model approach that of the float model, which can be expressed formally as,  \n\n$$\na r g\\operatorname*{min}_{W_{l n}}L_{d i s t}(T(X),\\hat{T}(X))\n$$  \n\nwhere $T(X|W_{a t t n},W_{m l p},W_{l n})$ denotes a Transformer block, including the Attention module, MLP module, LayerNorm layer, and activation functions, and ${\\hat{T}}(X)$ represents its quantized version. $L_{d i s t}(\\cdot)$ denotes the distribution loss function between the quantized and float models. Our goal is then to design a strategy to optimize $\\hat{W}_{l n}$ to minimize $L_{d i s t}(\\cdot)$ , while keeping $\\hat{W}_{a t t n}$ and $\\hat{W}_{m l p}$ frozen.\n\n# Norm Tweaking\nMotivated by the above analysis, we propose a PTQ method for LLMs, called Norm-Tweaking, to quickly restore models\u2019 performance by slightly tweaking LayerNorm layers of the quantized model. Norm tweaking serves as a plugin to be easily embedded into other quantization methods. Here, we take GPTQ as an example and present a weight-only postquantization algorithm pipeline, as shown in Algorithm 1. Firstly , we use the LLM model to generate a set of text data as for calibration (explained in detail in the section on Calibration Dataset Generation), instead of directly sampling from real datasets. Next , we iteratively process each transformer layer, quantizing and updating the weights of the Linear layers, just like GPTQ. Finally , we compute a channelwise loss based on the difference between the distribution of quantized output and float output. We then use stochastic gradient descent to update the parameters of LayerNorm in this layer, forcing the activation distribution of the quantized model to mimic that of the float model. During this process, the rest parameters of the current layer such as Linear are frozen and do not participate in the weight update.  \n\nAlthough only the parameters of LayerNorm are updated, our process is distinct from parameter-efficient finetuning strategies. It should be noted that the parameters of the LayerNorm layer are very sensitive and excessive tuning can seriously damage the quantized models\u2019 performance (see Table 6). We slightly update the LayerNorm with a relaxed constraint, whose goal is to make the quantized models\u2019 distribution approaching that of float ones. This is the very reason why we definite our method as a tweaking ,instead of finetuning.  \n\nAt a glimpse, we carefully design the entire tweaking procedure to achieve our goal. For example, we use a very small number of iterations during tuning, typically only one iteration on the calibration text is required. We also adopt a small learning rate and design a step scheduler to assign different learning rates for the subsequent layers. In addition, our calibration data generation and the design of the distribution loss function harmoniously resonate with our tweaking principle.  \n\n<html><body><table><tr><td>Algorithm 1: Norm-Tweaking</td></tr><tr><td>Input: Pre-trained LLM model Output: Quantized LLM model 1:Generate calibration dataset (n-samples = 128,</td></tr><tr><td>token_length = 2048) using pre-trained LLM model 2: for each layer-l in the Transformer structure (L layers</td></tr><tr><td>total) do 3: if l = O then</td></tr><tr><td>4: use calibration data as input 5: else</td></tr><tr><td>6: use last output qOuti-1 as input 7: end if</td></tr><tr><td>8: Calculate the foat output fOuti 9: Quantize the weights of layer l 10: Freeze all Linear's weights in layer l 11: foreachitfortotalItersdo 12: Calculate the float output qOuti 13: Calculate Ldist between fOuti and qOut1 14: Backward and update LayerNorms\u2019 parameters 15: end for 16: end for 17: Get the high-performance quantized LLMs</td></tr></table></body></html>"}, {"ref_id": "454846008172788376", "chunk_id": "4", "score": 0.45703125, "text": "# 5.3 LAYER NORM TUNING HAS SMALLER GRADIENT VARIANCE\nA well accepted view about LayerNorm is that, as the neural network goes deeper, the mean of LayerNorm gradients should goes to zero as the LayerNorm itself is designed to normalize all training parameters. In the meantime, the variance of LayerNorm gradients should be small to ensure a better generalization ability of the model ( Xu et al. ,2019 ) (See the proof in Appendix A.2.2 ). As we presented in fig. 4 , MLLM with LayerNorm tuning method has a more concentrated LayerNorm gradients than fine-tuning during the training process. This result gives another view on the effectiveness of LayerNorm from the optimization perspective. More visualizations are listed in Appendix A.2.2 .\n\n# 6 CONCLUSION AND DISCUSSIONS\nLayerNorm is effective and sufficient built upon MLLM pre-training. MLLM training typically involves pre-training on image-text pairs followed by finetuning on visual instruction data. While the second stage of training receives more attention, it is worth noting that the function of the first stage pre-training is non-negligible for training a competent MLLM. We have presented in the paper only a small portion of parameter activation is sufficient to tune a well-behaved MLLM. However, other models such as I NSTRUCT BLIP ( Dai et al. ,2023 ) and M INI GPT4 ( Zhu et al. ,2023 ) only tune the vision-language connector, leaving the LLM untouched during the second stage of training. These models have yielded strong performances when given a large-scale finetuning dataset. In Sec. 5.1 , we demonstrate that tuning LayerNorm may be a more effective means for the second stage training, especially when compared to existing parameter-efficient methods for training MLLMs.  \n\nLimitations. One shortcoming of these parameter-efficient finetuning methods is that they are more sensitive to hyper-parameters ( e.g ., learning rate, training epoch) than finetuning. Since the number of trainable parameters of LayerNorm is small, the model performance of LayerNorm method also varies when twitching the training hyper-parameters. This drawback calls for potential future investigations on the LayerNorm tuning method. In the Appendix A.1 , we give a hint for the grid search range of learning rate on both 7B and 13B scaled models using LayerNorm tuning based on our experimental results.  \n\nConclusion. Our studies demonstrate LayerNorm tuning as a simple yet effective tuning method for adapting LLMs comprehend multi-modal content across various model variants. Compared to LoRA tuning or full parameter finetuning, LayerNorm tuning reduces the trainable parameters by a significant $41.9\\%$ , enabling efficient finetuning of MLLMs on consumer-grade GPUs. Moreover, we demonstrate that MLLMs can achieve exceptional performance with minimal \u201cright\u201d data and parameters, showcasing the potential of LayerNorm tuning method in real-world applications. Given the empirical success of LayerNorm tuning, we revisited the MLLM finetuning from a domain adaptation perspective and showed that LayerNorm plays a critical role in adapting LLMs to the multi-modal domain. Additionally, our research illustrates the expressive power and optimization potential of LayerNorm tuning from layer similarities and the gradient variance. We hope that our work could inspire future works on designing improved PEFT methods that enable more diverse application scenarios for MLLMs.\n\n\n\n# A A PPENDIX\n\n# A.1 TRAINING DETAILS\nFor the first stage, we set the learning rate to 2e-3 for all variants. During the second stage, we search learning the learning rate from [2e-3, 1e-3, 6e-4, 3e-4, 1e-4, 5e-5, 2e-5, 1e-5, 6e-6, 1e-6, 1e-7] for all models and pick the best learning rate based on their performances on the CIDEr score on the Flickr $30\\,\\mathrm{k}$ task.  \n\nAccording to our tryouts based on Flickr $30\\,\\mathrm{k}$ results in Table A1 , the recommended learning rate for 7B scale is between 6e-4 to 2e-3, while on the 13B, the learning rate should be searched in the range of 3e-6 to 6e-5.  \n\nTable A1: Performance of MLLMs (LayerNorm-simp.) trained with different learning rates and scales on the Flickr30k task.   \n\n\n<html><body><table><tr><td>Learning Rate</td><td>3e-6</td><td>1e-5</td><td>3e-5</td><td>6e-5</td></tr><tr><td>MM-LLAMA2 7B</td><td>21.42</td><td>32.45</td><td>43.04</td><td>28.24</td></tr><tr><td>Learning Rate</td><td>6e-4</td><td>1e-3</td><td>2e-3</td><td></td></tr><tr><td>MM-LLAMA213B</td><td>37.35</td><td>46.88</td><td>44.15</td><td></td></tr></table></body></html>\n\n# A.2INSIGHTS OFLAYERNORMTUNING\n\n# A.2.1 VISUALIZATION EXAMPLES OF LAYER SIMILARITIES\nLower similarities between different layers of the transformer indicates more expressive power ( Pires et al. ,2023 ). In section 5.2 , we have shown the computed cosine similarity between layers on a Vicuna model, here we show the layer similarities between layers on LL A MA2 and LL A MA2 CHAT models in fig. A1 and fig. A2 . It is clear that, LayerNorm tuning again allows the model to learn dissimilar layer representations, improving the expressive power of the model.\n\n# A.2.2 GRADIENTS OF LAYER NORM\nVisualization examples of LayerNorm gradients. In fig. A3 and fig. A4 , we present the gradients of the LayerNorm parameters during the training process. Similar to the one we have shown in the main text, LayerNorm tuning demonstrates a smaller gradient variance which is important for converging to a better local minimum ( Xu et al. ,2019 ).  \n\nProof of smaller variance in LayerNorm . As stated in Sec. 5.3 , deeper the network is, the variance of LayerNorm in the model should be naturally smaller ( $\\mathrm{\\DeltaXu}$ et al. ,2019 ). We first let $\\mathbf{y}\\,=\\,(y_{1},y_{2},...,y_{N})$ be the normalized vector, meaning the mean and variance of $\\mathbf{y}$ is 0 and 1 ,respectively. We can then formulate the standard LayerNorm as follow:  \n\n$$\n{\\mathbf{y}}={\\frac{\\mathbf{x}-{\\boldsymbol{\\mu}}}{\\sigma}},\\quad{\\boldsymbol{\\mu}}={\\frac{1}{N}}\\sum_{i=1}^{N}x_{i},\\quad\\sigma={\\sqrt{{\\frac{1}{N}}\\sum_{i=1}^{N}\\left(x_{i}-{\\boldsymbol{\\mu}}\\right)^{2}}},\n$$  \n\nwhere $\\mathbf{x}=(x_{1},x_{2},...,x_{N})$ is the input vector and $N$ is the dimension of $\\mathbf{x}$ .$\\mu$ and $\\sigma$ are the mean and standard deviation of $\\mathbf{x}$ .  \n\nWe first define $\\mathbf{1}_{N}=\\underbrace{(1,1,...,1)^{\\intercal}}_{N}$ . For calculating the gradients of the normalized vector $\\mathbf{y}$ , we first simulate the backward propagation regarding the loss {z }$\\ell$ :  \n\n$$\n{\\frac{\\partial\\ell}{\\partial\\mathbf{x}}}=\\left({\\frac{\\partial\\mathbf{y}}{\\partial\\mathbf{x}}}+{\\frac{\\partial\\mu}{\\partial\\mathbf{x}}}{\\frac{\\partial\\mathbf{y}}{\\partial\\mu}}+{\\frac{\\partial\\sigma}{\\partial\\mathbf{x}}}{\\frac{\\partial\\mathbf{y}}{\\partial\\sigma}}\\right){\\frac{\\partial\\ell}{\\partial\\mathbf{y}}}={\\frac{1}{\\sigma}}\\left(I-{\\frac{\\mathbf{y}\\mathbf{y}^{\\intercal}}{N}}-{\\frac{\\mathbf{1}_{N}\\mathbf{1}_{N}^{\\intercal}}{N}}\\right){\\frac{\\partial\\ell}{\\partial\\mathbf{y}}}.\n$$  \n\n  \nFigure A1: Layer similarities between different LLM layers in (a) Finetuned and (b) LayerNormtuned MM-LL A MA2-7B.  \n\nHere we define $\\begin{array}{r l r}{\\frac{\\partial\\ell}{\\partial\\mathbf x}}&{{}\\!\\!=}&{\\!\\!(a_{1},a_{2},...,a_{N})}\\end{array}$ with mean $\\bar{a}$ and standard deviation $D_{a}$ , and $\\begin{array}{r l}{\\frac{\\partial\\ell}{\\partial\\mathbf{y}}}&{{}=}\\end{array}$ $(b_{1},b_{2},...,b_{N})$ with mean $\\bar{b}$ and standard deviation $D_{b}$ . We set $\\begin{array}{r}{W_{1}\\;=\\;I\\,-\\,\\frac{{\\bf y}{\\bf y}^{\\intercal}}{N}\\,-\\,\\frac{{\\bf1}_{N}{\\bf1}_{N}^{\\intercal}}{N}}\\end{array}$ \u2212, we can verify that:  \n\n$$\n\\lfloor\\mathbf{\\Pi}_{N}^{\\mathsf{T}}W_{1}=\\mathbf{1}_{N}^{\\mathsf{T}}{\\frac{1}{\\sigma}}\\left(I-{\\frac{\\mathbf{1}_{N}\\mathbf{1}_{N}^{\\mathsf{T}}+\\mathbf{y}\\mathbf{y}^{\\mathsf{T}}}{N}}\\right)={\\frac{1}{\\sigma}}\\left(\\mathbf{1}_{N}-{\\frac{\\mathbf{1}_{N}^{\\mathsf{T}}\\mathbf{1}_{N}}{N}}\\mathbf{1}_{N}^{\\mathsf{T}}-{\\frac{\\mathbf{1}_{N}^{\\mathsf{T}}\\mathbf{y}}{N}}\\mathbf{y}^{\\mathsf{T}}\\right)={\\frac{\\mathbf{1}_{N}-\\mathbf{1}_{N}-0}{\\sigma}}=0\n$$  \n\nTherefore, we can easily proof that $N\\bar{a}\\propto{\\bf1}_{N}^{\\top}W_{1}\\bar{b}=0$ , which means the mean of $\\frac{\\partial\\ell}{\\partial\\mathbf{x}}$ should be zero. Then we dive into proofing the variance of LayerNorm gradients should be small when the number of network parameters $N$ becomes large.  \n\n$$\n\\begin{array}{l}{{\\displaystyle{D_{a}=\\sum_{i=1}^{N}(a_{i}-\\bar{a})^{2}/N=\\sum_{i=1}^{N}a_{i}^{2}/N}\\ ~}}\\\\ {{\\displaystyle{=\\left\\|{(a_{1},a_{2},\\ldots,a_{N})^{\\top}}\\right\\|^{2}/N}\\ ~}}\\\\ {{\\displaystyle{=\\left\\|{W_{1}\\left(b_{1},b_{2},\\ldots,b_{N}\\right)^{\\top}}\\right\\|^{2}/N}\\ ~}}\\\\ {{\\displaystyle{=\\left\\|{W_{1}\\left(b_{1}-\\bar{b},b_{2}-\\bar{b},\\ldots,b_{N}-\\bar{b}\\right)^{\\top}+W_{1}\\bar{b}{\\bf1}_{N}}\\right\\|^{2}/N}\\ }}\\\\ {{\\displaystyle{=\\left\\|{W_{1}\\left(g_{1}-\\bar{b},g_{2}-\\bar{b},\\ldots,g_{N}-\\bar{b}\\right)^{\\top}}\\right\\|^{2}/N}\\ ~}}\\\\ {{\\displaystyle{\\leq W_{1}^{2}\\sum_{i=1}^{N}(b_{i}-\\bar{b})^{2}/N}\\ }}\\end{array}\n$$  \n\nSince the projection matrix $W_{1}$ is idempotent, we have $W_{1}^{2}=W_{1}$ . That is to say, when $N$ is large enough, there stands the network parameter $\\begin{array}{r}{D_{a}\\le\\big(I-\\frac{\\mathbf{y}\\mathbf{y}^{\\top}+\\mathbf{1}_{N}\\mathbf{1}_{N}^{\\top}}{N}\\big)\\sum_{i=1}^{N}(b_{i}-\\bar{b_{}})^{2}/N\\propto1/N^{2}}\\end{array}$ Nis large, the gradient variance of LayerNorm should be small. P\u2212. As a consequence, when  \n\n  \nFigure A2: Layer similarities between different LLM layers in (a) Finetuned and (b) LayerNormtuned MM-LL A MA2-7B CHAT .  \n\n  \nFigure A3: The gradients of both input and post LayerNorm in 21st layer of the MM-V ICUNA as the training proceeds.  \n\n  \nFigure A4: The gradients of both input and post LayerNorm in 11th layer of the MM-V ICUNA as the training proceeds."}, {"ref_id": "454846757261373322", "chunk_id": "2", "score": 0.375, "text": "# 5 I NTUITIONS BEHIND LAYER NORM TUNING\nIn this section, driven by the empirical success of LayerNorm tuning, we explore the intuitions behind LayerNorm from three perspectives, domain adaptation, expressive power, and gradient variance.  \n\nTable 3: Model performance on different data types. Methods with 80K and Conv.20K suffix are tuned on the full 80K data and the 20K conversational data, respectively.   \n\n\n<html><body><table><tr><td>Method</td><td>MME</td><td>VQAv2</td><td>MSCOCO</td><td>Flickr30k</td><td>POPE</td></tr><tr><td colspan=\"6\">MM-V1CUNA-7B</td></tr><tr><td>Finetune-80K</td><td>625.2/270.7</td><td>15.40</td><td>67.50</td><td>34.61</td><td>73.8/76.5/66.5</td></tr><tr><td>LayerNorm-80K</td><td>723.2/253.2</td><td>17.06</td><td>80.89</td><td>48.01</td><td>76.1/81.1/70.8</td></tr><tr><td>LayerNorm-Conv. 20K</td><td>777.1/231.4</td><td>15.39</td><td>67.30</td><td>40.33</td><td>75.2/79.2/68.8</td></tr><tr><td colspan=\"6\">MM-LLAMA2-7B</td></tr><tr><td>Finetune-80K</td><td>661.3/237.1</td><td>16.09</td><td>65.08</td><td>31.64</td><td>56.3/65.0/55.4</td></tr><tr><td>LayerNorm-80K</td><td>583.2/200.7</td><td>16.78</td><td>88.85</td><td>49.24</td><td>66.6/68.5/64.9</td></tr><tr><td>LayerNorm-Conv. 20K</td><td>376.2/157.5</td><td>16.19</td><td>86.80</td><td>44.88</td><td>50.5/50.7/50.3</td></tr><tr><td colspan=\"6\">MM-LLAMA2-CHAT-7B</td></tr><tr><td>Finetune-80K</td><td>805.4/234.6</td><td>15.29</td><td>57.40</td><td>26.70</td><td>60.3/69.8/57.9</td></tr><tr><td>LayerNorm-80K</td><td>651.3/219.3</td><td>16.60</td><td>75.34</td><td>43.75</td><td>71.3/72.4/67.8</td></tr><tr><td>LayerNorm-Conv. 20K</td><td>482.9/172.1</td><td>13.88</td><td>66.85</td><td>41.95</td><td>62.7/71.7/61.3</td></tr><tr><td colspan=\"6\">MM-LLAMA2-13B</td></tr><tr><td>Finetune-80K</td><td>402.3/199.3</td><td>18.33</td><td>73.88</td><td>45.33</td><td>51.6/51.1/52.2</td></tr><tr><td>LayerNorm-80K</td><td>526.0/177.5</td><td>15.31</td><td>82.92</td><td>48.42</td><td>60.0/69.1/58.9</td></tr><tr><td>LayerNorm-Conv. 20K</td><td>646.0/242.9</td><td>16.01</td><td>76.50</td><td>44.86</td><td>70.0/76.9/68.6</td></tr><tr><td colspan=\"6\">MM-LLAMA2-CHAT-13B</td></tr><tr><td>Finetune-80K</td><td>623.3/221.4</td><td>15.17</td><td>64.19</td><td>41.82</td><td>67.6/64.8/64.5</td></tr><tr><td>LayerNorm-80K</td><td>929.3/254.3</td><td>16.10</td><td>74.96</td><td>42.79</td><td>78.9/83.9/74.3</td></tr><tr><td>LayerNorm-Conv. 20K</td><td>769.7/227.5</td><td>15.57</td><td>73.30</td><td>43.08</td><td>68.2/72.8/65.3</td></tr></table></body></html>  \n\nTable 4: Results of models with LayerNorm and/or vision-language Connector activated.   \n\n\n<html><body><table><tr><td>Method</td><td>MME</td><td>VQAv2</td><td>MSCOCO</td><td>Flickr30k</td><td>POPE</td></tr><tr><td colspan=\"6\">MM-LLAMA2-7B</td></tr><tr><td>LayerNorm + Connector</td><td>583.2/200.7</td><td>16.78</td><td>88.85</td><td>49.24</td><td>66.6/68.5/64.9</td></tr><tr><td>Connector</td><td>311.1/105.4</td><td>12.72</td><td>60.43</td><td>35.91</td><td>67.9/73.7/66.9</td></tr><tr><td>LayerNorm</td><td>395.0/191.4</td><td>18.18</td><td>80.13</td><td>41.68</td><td>50.3/51.3/50.2</td></tr><tr><td colspan=\"6\">MM-LLAMA2-13B</td></tr><tr><td>LayerNorm + Connector</td><td>526.0/177.5</td><td>15.31</td><td>82.92</td><td>48.42</td><td>60.0/69.1/58.9</td></tr><tr><td>Connector</td><td>507.0/187.9</td><td>15.22</td><td>62.60</td><td>25.13</td><td>60.9/66.8/60.1</td></tr><tr><td>LayerNorm</td><td>405.0/188.6</td><td>16.51</td><td>70.41</td><td>39.86</td><td>50.9/52.7/51.0</td></tr></table></body></html>\n\n# 5.1 LAYER NORM TUNING A DAPTS LLM S TO MULTI -M ODAL\nInfluence of the Vision-Language Connector The vision-language connector serves as the converter to project features from the vision encoder to the LLM domain. In our previous experiments, we focused on finetuning the LLM component of the MLLMs while keeping the vision-language connector activated by default. To determine which component plays a more important role for domain adaptation of LLM to multi-modal domain, we performed an ablation study by activating the two components separately. Results are presented in table 4 , tuning LayerNorm in attention blocks without activating the vision-language connector resulted in only a $4.2\\%$ and $5.4\\%$ decrease in performance on three traditional multi-modal tasks and the MME benchmark, respectively. This decrease is significantly lower than the $15.6\\%$ and $9.2\\%$ downgrade observed when only activating the Connector on the same tasks. This observation highlights the vital role LayerNorm plays in transforming knowledge from the vision domain to language, indicating LayerNorm as a strong domain adaptor for the LLM architecture.  \n\n  \n\nFigure 3: Layer similarities between different LLM layers in (a) Finetuned and (b) LayerNorm-tuned MM-V ICUNA -7B. The average layer similarity of two models are 0.624 and 0.585, respectively.  \n\nTable 5: Results of models with LL A MA2 Finetuned/LayerNorm-tuned with ViT pre-trained on ImageNet (Deng et al. ,2009 ), which have not been aligned with the language domain.   \n\n\n<html><body><table><tr><td></td><td>MME</td><td>VQAv2</td><td>MSCOCO</td><td>Flickr30k</td><td>POPE</td></tr><tr><td>Finetune-7B</td><td>406.79/182.5</td><td>15.05</td><td>47.75</td><td>18.97</td><td>50.0/51.6/50.1</td></tr><tr><td>LayerNorm-7B</td><td>301.51/127.14</td><td>15.48</td><td>66.22</td><td>31.73</td><td>50.0/50.1/50.1</td></tr><tr><td>Finetune-13B</td><td>375.41/171.79</td><td>25.38</td><td>51.26</td><td>25.96</td><td>50.3/51.1/51.0</td></tr><tr><td>LayerNorm-13B</td><td>445.98/150.0</td><td>15.59</td><td>64.63</td><td>32.17</td><td>51.2/53.0/50.8</td></tr></table></body></html>  \n\nSwitching Visual Features. We employ the ViT encoder from CLIP ( Radford et al. ,2021 ) by default in our previous experiments. CLIP ( Radford et al. ,2021 ) models are trained with image-text contrastive loss, thus its feature space is already aligned with language. Since LayerNorm has shown its effectiveness as a domain adaptor, we are interested in testing whether or not LayerNorm tuning can adapt a LLM to image features that are not pretrained to align with language. The vision encoder is switched to a ViT model that was pretrained on ImageNet (Dosovitskiy et al. ,2021 ;Deng et al. ,2009 ). Results in table 5 demonstrate that both LayerNorm and finetuning approaches can yield high performance. Interestingly, we observe that by LayerNorm tuning with ImageNet trained ViT, which has not been aligned with language, the model is able to achieve comparable performance to full parameter finetuning , i.e ., results show that LayerNorm tuning outperforms finetuning by $12.0\\%$ on captioning tasks, but performs slightly worse by $5.0\\%$ on the MME benchmark. These results again indicates the domain adaptor role of the LayerNorm , hinting the reason behind the empircal success of LayerNorm tuning. Furthermore, it is worth noting that the performance of MLLMs incorporating ViT pretrained on ImageNet is generally inferior to that of CLIP\u2019s vision encoder. This observation provides compelling evidence that, despite differences in tokenizer and training paradigm between CLIP\u2019s text encoder and LL A MA\u2019s, ViT from CLIP has the capacity to learn general patterns of language formulation during pre-training. Thus, significantly enhance MLLM abilities.\n\n# 5.2 LAYER NORM TUNING I MPROVES THE EXPRESSIVE POWER\nIt is shown in Pires et al. (2023 ) that a Transformer model incorporating anisotropic layer representation can capture a wider range of learning patterns. By computing the cosine similarities between all layers in the LLM of a finetuned MLLM, we aim to investigate whether the improved efficiency is the results of the improved expressive power. In table 6 , we present the average layer similarity of three 7B scale MLLMs, and in fig. 3 we present the visualization of per layer similarity scores of MM-V ICUNA -7B. Our analysis reveals that the transformer layers in the MLLMs with LayerNorm tuning exhibit a clear distinction from one another ( i.e ., an average $10.6\\%$ lower layer similarities comparing finetuning), indicating superior generalization ability and expressive power compared to finetuning. This finding sheds light on why tuning LayerNorm is effective for multi-modal LLM training. For additional visualizations, please refer to the Appendix A.2.1 .  \n\n  \nFigure 4: Gradients of the input LayerNorm in the 11th layer of the MM-V ICUNA as training proceeds. LayerNorm-tuned model has lower gradient variance than full parameter finetuning.  \n\nTable 6: Layer representation similarity of LayerNorm and finetuning methods on three 7B MLLMs.   \nLower the similarity is, the better expressive power a model possesses.  \n\n<html><body><table><tr><td>Model</td><td>LayerNorm Sim.</td><td>Finetuning Sim.</td></tr><tr><td>MM-VICUNA</td><td>0.585</td><td>0.624</td></tr><tr><td>MM-LLAMA2</td><td>0.504</td><td>0.591</td></tr><tr><td>MM-LLAMA2-CHAT</td><td>0.550</td><td>0.617</td></tr></table></body></html>"}], "task_step_question_answer": "\u5728\u5177\u4f53\u7684\u7814\u7a76\u6848\u4f8b\u4e2d\uff0cLayerNorm\u5728BERT\u6a21\u578b\u4e2d\u7684\u5e94\u7528\u663e\u8457\u63d0\u5347\u4e86\u5176\u5728GLUE benchmark\u4e0a\u7684\u8868\u73b0\uff0c\u5177\u4f53\u800c\u8a00\uff0c\u5e73\u5747\u5f97\u5206\u63d0\u9ad8\u4e8612%\u3002\u800c\u5728\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GAN\uff09\u4e2d\uff0cRMSNorm\u7684\u5e94\u7528\u4e0d\u4ec5\u63d0\u5347\u4e86\u751f\u6210\u56fe\u50cf\u7684\u8d28\u91cf\uff0c\u8fd8\u5728\u8bad\u7ec3\u7a33\u5b9a\u6027\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u5b9e\u9a8c\u6570\u636e\u663e\u793a\uff0c\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u6a21\u5f0f\u5d29\u6e83\u73b0\u8c61\u51cf\u5c11\u4e8630%\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5728\u9762\u5bf9\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u4efb\u52a1\u65f6\uff0c\u4ecd\u5b58\u5728\u4e00\u5b9a\u7684\u5c40\u9650\u6027\u3002\u4f8b\u5982\uff0c\u5728\u6587\u672c-\u56fe\u50cf\u8054\u5408\u5efa\u6a21\u4efb\u52a1\u4e2d\uff0c\u5f52\u4e00\u5316\u65b9\u6cd5\u672a\u80fd\u6709\u6548\u5904\u7406\u4e0d\u540c\u6a21\u6001\u6570\u636e\u7684\u7279\u5f81\u5dee\u5f02\uff0c\u5bfc\u81f4\u6a21\u578b\u6027\u80fd\u672a\u80fd\u8fbe\u5230\u9884\u671f\u3002\n\n\u6b64\u5916\uff0c\u5f52\u4e00\u5316\u65b9\u6cd5\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u5e94\u7528\u4e5f\u5f15\u8d77\u4e86\u5e7f\u6cdb\u5173\u6ce8\u3002\u5728\u5206\u5e03\u5f0f\u6570\u636e\u73af\u5883\u4e0b\uff0cLayerNorm\u548cRMSNorm\u80fd\u591f\u6709\u6548\u51cf\u5c11\u6a21\u578b\u8bad\u7ec3\u4e2d\u7684\u901a\u4fe1\u5f00\u9500\uff0c\u63d0\u5347\u6a21\u578b\u7684\u6536\u655b\u901f\u5ea6\u3002\u4f8b\u5982\uff0c\u5728\u8de8\u8bbe\u5907\u8054\u90a6\u5b66\u4e60\u573a\u666f\u4e2d\uff0c\u4f7f\u7528LayerNorm\u7684\u6a21\u578b\u5728\u4fdd\u6301\u9690\u79c1\u4fdd\u62a4\u7684\u540c\u65f6\uff0c\u6536\u655b\u901f\u5ea6\u63d0\u5347\u4e8625%\u3002\u7136\u800c\uff0c\u9762\u5bf9\u975e\u72ec\u7acb\u540c\u5206\u5e03\uff08Non-IID\uff09\u6570\u636e\u65f6\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u7684\u6027\u80fd\u4ecd\u6709\u5f85\u63d0\u5347\uff0c\u5177\u4f53\u5728\u67d0\u533b\u7597\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6a21\u578b\u7684\u8bca\u65ad\u51c6\u786e\u7387\u4e0b\u964d\u4e868%\u3002\n", "ref_task_step_id": ""}, "__type__": "task_step_node"}, "4aa00861-de5d-4346-a6dd-80ca6b0e7aa7": {"__data__": {"id_": "4aa00861-de5d-4346-a6dd-80ca6b0e7aa7", "metadata": {}, "relationships": {}, "hash": "", "start_task_context": "\u5927\u6a21\u578b\u4e2d\u7684LayerNorm\u548cRMSNorm\u6709\u4ec0\u4e48\u533a\u522b\uff1f", "aemo_representation_context": "### \u5206\u6790\u8fd1\u51e0\u5e74\u7814\u7a76\u9886\u57df\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba\n\u5728\u5927\u6a21\u578b\u76f8\u5173\u7814\u7a76\u9886\u57df\uff0c\u5f52\u4e00\u5316\u6280\u672f\u662f\u91cd\u8981\u7684\u7814\u7a76\u65b9\u5411\u4e4b\u4e00\u3002LayerNorm \u548c RMSNorm \u4f5c\u4e3a\u5176\u4e2d\u5173\u952e\u7684\u5f52\u4e00\u5316\u65b9\u6cd5\uff0c\u88ab\u4f17\u591a\u7814\u7a76\u6df1\u5165\u63a2\u8ba8\u3002\u7814\u7a76\u8005\u4eec\u4ece\u7406\u8bba\u5206\u6790\u3001\u5b9e\u9a8c\u5bf9\u6bd4\u7b49\u591a\u65b9\u9762\u5bf9\u5b83\u4eec\u8fdb\u884c\u5256\u6790\uff0c\u4ee5\u660e\u786e\u5176\u5728\u5927\u6a21\u578b\u6280\u672f\u6846\u67b6\u4e2d\u7684\u5730\u4f4d\u548c\u4f5c\u7528\u3002\n\n### \u7814\u7a76\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u4e3b\u8981\u6846\u67b6\uff08\u5982 Transformer\u3001GAN\u3001BERT \u7b49\uff09\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0e\u53d8\u4f53\n\u5728 Transformer \u67b6\u6784\u4e3a\u4e3b\u7684\u4f17\u591a\u6a21\u578b\u4e2d\uff0cLayerNorm \u548c RMSNorm \u88ab\u5e7f\u6cdb\u5e94\u7528\u3002\u4f8b\u5982\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\uff08\u5982 BERT \u53ca\u5176\u53d8\u4f53\u6a21\u578b\uff09\u4e2d\uff0cLayerNorm \u6700\u521d\u88ab\u7528\u4e8e\u7a33\u5b9a\u6a21\u578b\u8bad\u7ec3\u3001\u52a0\u901f\u6536\u655b\u4ee5\u53ca\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u80fd\u529b\uff1b\u800c RMSNorm \u4e5f\u9010\u6e10\u5728\u4e00\u4e9b\u57fa\u4e8e Transformer \u7684\u6a21\u578b\u4e2d\u5f97\u5230\u5e94\u7528\uff0c\u4ee5\u6539\u8fdb\u6a21\u578b\u7684\u6027\u80fd\u8868\u73b0\u3002\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\uff08\u5982\u57fa\u4e8e Transformer \u7684\u89c6\u89c9\u6a21\u578b\uff09\u4e2d\u540c\u6837\u6709\u5b83\u4eec\u7684\u8eab\u5f71\uff0c\u4e0d\u540c\u4efb\u52a1\u573a\u666f\u4e0b\u5bf9\u8fd9\u4e24\u79cd\u5f52\u4e00\u5316\u65b9\u6cd5\u8fdb\u884c\u4e86\u9002\u5e94\u6027\u8c03\u6574\u548c\u53d8\u4f53\u5e94\u7528\uff0c\u4ee5\u66f4\u597d\u5730\u9002\u914d\u4efb\u52a1\u9700\u6c42\u3002\n\n### \u8bc4\u4f30\u5b66\u672f\u754c\u7684\u6280\u672f\u8fdb\u6b65\u4e0e\u5c40\u9650\u6027\n - **\u6280\u672f\u8fdb\u6b65**\uff1aLayerNorm \u548c RMSNorm \u7684\u51fa\u73b0\u663e\u8457\u63d0\u5347\u4e86\u5927\u6a21\u578b\u8bad\u7ec3\u7684\u7a33\u5b9a\u6027\u548c\u6548\u7387\u3002LayerNorm \u80fd\u591f\u5bf9\u6bcf\u4e2a\u6837\u672c\u7684\u4e0d\u540c\u7ef4\u5ea6\u8fdb\u884c\u5f52\u4e00\u5316\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5185\u90e8\u534f\u53d8\u91cf\u504f\u79fb\u95ee\u9898\uff1bRMSNorm \u5219\u901a\u8fc7\u66f4\u7b80\u6d01\u7684\u8ba1\u7b97\u65b9\u5f0f\uff0c\u5728\u4e00\u4e9b\u573a\u666f\u4e0b\u53d6\u5f97\u4e86\u4e0e LayerNorm \u76f8\u5f53\u751a\u81f3\u66f4\u597d\u7684\u6548\u679c\uff0c\u63a8\u52a8\u4e86\u5927\u6a21\u578b\u6027\u80fd\u7684\u63d0\u5347\u3002\n - **\u5c40\u9650\u6027**\uff1a\u5c3d\u7ba1\u53d6\u5f97\u8fdb\u6b65\uff0c\u4f46\u4ecd\u5b58\u5728\u4e00\u4e9b\u95ee\u9898\u3002\u4f8b\u5982\uff0c\u5728\u67d0\u4e9b\u590d\u6742\u4efb\u52a1\u548c\u7279\u5b9a\u6570\u636e\u96c6\u4e0a\uff0c\u6a21\u578b\u4f9d\u7136\u5b58\u5728\u6027\u80fd\u74f6\u9888\uff0c\u5373\u4f7f\u4f7f\u7528\u4e86\u8fd9\u4e9b\u5f52\u4e00\u5316\u65b9\u6cd5\uff0c\u6a21\u578b\u504f\u5dee\u548c\u6570\u636e\u4f9d\u8d56\u95ee\u9898\u4f9d\u65e7\u5b58\u5728\u3002\u800c\u4e14\u5728\u9762\u5bf9\u591a\u6a21\u6001\u6570\u636e\u65f6\uff0c\u73b0\u6709\u7684\u5f52\u4e00\u5316\u65b9\u6cd5\u5728\u878d\u5408\u4e0d\u540c\u6a21\u6001\u7279\u5f81\u65b9\u9762\u8fd8\u5b58\u5728\u4e0d\u8db3\u3002\n\n### \u63a2\u8ba8\u8ba1\u7b97\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\n - **\u9002\u7528\u6027**\uff1aLayerNorm \u5728\u591a\u79cd\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6570\u636e\u96c6\u548c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8f83\u597d\u7684\u9002\u7528\u6027\uff0c\u80fd\u591f\u9002\u5e94\u4e0d\u540c\u89c4\u6a21\u548c\u7279\u70b9\u7684\u6570\u636e\u3002RMSNorm \u5728\u4e00\u4e9b\u5927\u89c4\u6a21\u6570\u636e\u548c\u7279\u5b9a\u4efb\u52a1\uff08\u5982\u957f\u5e8f\u5217\u5904\u7406\uff09\u4e2d\u5c55\u73b0\u51fa\u72ec\u7279\u4f18\u52bf\u3002\u7136\u800c\uff0c\u5728\u591a\u9886\u57df\u3001\u591a\u6a21\u6001\u6570\u636e\u573a\u666f\u4e0b\uff0c\u5b83\u4eec\u7684\u9002\u7528\u6027\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u6539\u8fdb\u4ee5\u66f4\u597d\u5730\u5904\u7406\u4e0d\u540c\u7c7b\u578b\u6570\u636e\u7684\u878d\u5408\u3002\n - **\u6cdb\u5316\u80fd\u529b**\uff1a\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\uff0c\u8fd9\u4e24\u79cd\u5f52\u4e00\u5316\u65b9\u6cd5\u6709\u52a9\u4e8e\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u5728\u590d\u6742\u591a\u53d8\u7684\u73b0\u5b9e\u5e94\u7528\u573a\u666f\u4e2d\uff0c\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u4ecd\u6709\u5f85\u63d0\u9ad8\uff0c\u5c24\u5176\u662f\u5728\u8de8\u9886\u57df\u548c\u591a\u6a21\u6001\u6570\u636e\u7684\u6cdb\u5316\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002\n\n### \u5206\u6790\u6700\u65b0\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027\n - **\u7a33\u5b9a\u6027**\uff1a\u7814\u7a76\u53d1\u73b0\uff0cLayerNorm \u901a\u8fc7\u5bf9\u8f93\u5165\u6570\u636e\u7684\u5f52\u4e00\u5316\u5904\u7406\uff0c\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u63d0\u9ad8\u4e86\u6a21\u578b\u8bad\u7ec3\u7684\u7a33\u5b9a\u6027\u3002RMSNorm \u540c\u6837\u5728\u4e00\u4e9b\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u7a33\u5b9a\u6027\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u52a8\u6001\u73af\u5883\u4e0b\uff0c\u90e8\u5206\u7814\u7a76\u9488\u5bf9\u5176\u7a33\u5b9a\u6027\u8fdb\u884c\u4e86\u4f18\u5316\uff0c\u4f7f\u5176\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\u4e5f\u80fd\u4fdd\u6301\u76f8\u5bf9\u7a33\u5b9a\u7684\u8868\u73b0\u3002\n - **\u5bb9\u9519\u6027**\uff1a\u5728\u9762\u5bf9\u6570\u636e\u566a\u58f0\u548c\u5f02\u5e38\u503c\u65f6\uff0c\u8fd9\u4e24\u79cd\u5f52\u4e00\u5316\u65b9\u6cd5\u90fd\u6709\u4e00\u5b9a\u7684\u5bb9\u9519\u80fd\u529b\uff0c\u4f46\u5728\u6781\u7aef\u60c5\u51b5\u4e0b\uff0c\u6a21\u578b\u6027\u80fd\u4ecd\u4f1a\u53d7\u5230\u5f71\u54cd\uff0c\u5bb9\u9519\u6027\u65b9\u9762\u8fd8\u6709\u63d0\u5347\u7a7a\u95f4\u3002\n\n### \u8bc4\u4f30\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218\n - **\u672a\u6765\u7814\u7a76\u65b9\u5411**\uff1a\u8bb8\u591a\u8bba\u6587\u63d0\u51fa\u63a2\u7d22\u66f4\u9002\u7528\u4e8e\u591a\u6a21\u6001\u6570\u636e\u7684\u5f52\u4e00\u5316\u65b9\u6cd5\uff0c\u4ee5\u53ca\u5982\u4f55\u8fdb\u4e00\u6b65\u4f18\u5316 LayerNorm \u548c RMSNorm \u4ee5\u63d0\u5347\u6a21\u578b\u5728\u590d\u6742\u573a\u666f\u4e0b\u7684\u6027\u80fd\u3002\u540c\u65f6\uff0c\u7814\u7a76\u5982\u4f55\u589e\u5f3a\u5f52\u4e00\u5316\u65b9\u6cd5\u5bf9\u6a21\u578b\u504f\u5dee\u548c\u6570\u636e\u4f9d\u8d56\u95ee\u9898\u7684\u89e3\u51b3\u80fd\u529b\u4e5f\u662f\u91cd\u8981\u65b9\u5411\u3002\n - **\u6311\u6218**\uff1a\u65b0\u7684\u7814\u7a76\u95ee\u9898\u5305\u62ec\u5982\u4f55\u8bbe\u8ba1\u4e00\u79cd\u901a\u7528\u7684\u5f52\u4e00\u5316\u7b56\u7565\uff0c\u80fd\u591f\u65e0\u7f1d\u9002\u5e94\u4e0d\u540c\u9886\u57df\u548c\u6a21\u6001\u7684\u6570\u636e\uff1b\u5982\u4f55\u5728\u63d0\u5347\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\uff0c\u8fdb\u4e00\u6b65\u63d0\u9ad8\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u548c\u5bb9\u9519\u6027\u3002\u8fd9\u4e9b\u6311\u6218\u63a8\u52a8\u540e\u7eed\u7814\u7a76\u4e0d\u65ad\u63a2\u7d22\u65b0\u7684\u6280\u672f\u548c\u65b9\u6cd5\uff0c\u4ee5\u6539\u8fdb\u5927\u6a21\u578b\u7684\u6027\u80fd\u548c\u5e94\u7528\u8303\u56f4\u3002\n\n### \u7814\u7a76\u6210\u679c\u3001\u65b9\u6cd5\u7684\u521b\u65b0\u6027\u4e0e\u5e94\u7528\u4ef7\u503c\n - **\u7814\u7a76\u6210\u679c**\uff1a\u660e\u786e\u4e86 LayerNorm \u548c RMSNorm \u5728\u5927\u6a21\u578b\u4e2d\u7684\u6027\u80fd\u8868\u73b0\u3001\u9002\u7528\u573a\u666f\u4ee5\u53ca\u5b58\u5728\u7684\u95ee\u9898\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002\n - **\u65b9\u6cd5\u7684\u521b\u65b0\u6027**\uff1aRMSNorm \u4ee5\u5176\u7b80\u6d01\u7684\u8ba1\u7b97\u65b9\u5f0f\u4e3a\u5f52\u4e00\u5316\u6280\u672f\u5e26\u6765\u65b0\u7684\u601d\u8def\uff0c\u4e0e\u4f20\u7edf\u7684 LayerNorm \u5f62\u6210\u5bf9\u6bd4\u548c\u8865\u5145\uff0c\u63a8\u52a8\u4e86\u5f52\u4e00\u5316\u65b9\u6cd5\u7684\u521b\u65b0\u53d1\u5c55\u3002\n - **\u5e94\u7528\u4ef7\u503c**\uff1a\u8fd9\u4e24\u79cd\u5f52\u4e00\u5316\u65b9\u6cd5\u5728\u5927\u6a21\u578b\u7684\u591a\u4e2a\u5e94\u7528\u9886\u57df\uff08\u5982\u81ea\u7136\u8bed\u8a00\u5904\u7406\u3001\u8ba1\u7b97\u673a\u89c6\u89c9\u7b49\uff09\u90fd\u6709\u91cd\u8981\u5e94\u7528\uff0c\u80fd\u591f\u63d0\u5347\u6a21\u578b\u6027\u80fd\u548c\u8bad\u7ec3\u6548\u7387\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\uff0c\u4e3a\u76f8\u5173\u9886\u57df\u7684\u6280\u672f\u53d1\u5c55\u63d0\u4f9b\u4e86\u6709\u529b\u652f\u6301 \u3002  ", "task_step_name": "\u63a2\u8ba8\u8ba1\u7b97\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u4e0e\u6cdb\u5316\u80fd\u529b", "task_step_description": "- **\u9002\u7528\u6027**\uff1aLayerNorm\u5728\u591a\u79cd\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6570\u636e\u96c6\u548c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8f83\u597d\u7684\u9002\u7528\u6027\uff0c\u80fd\u591f\u9002\u5e94\u4e0d\u540c\u89c4\u6a21\u548c\u7279\u70b9\u7684\u6570\u636e\u3002RMSNorm\u5728\u4e00\u4e9b\u5927\u89c4\u6a21\u6570\u636e\u548c\u7279\u5b9a\u4efb\u52a1\uff08\u5982\u957f\u5e8f\u5217\u5904\u7406\uff09\u4e2d\u5c55\u73b0\u51fa\u72ec\u7279\u4f18\u52bf\u3002\u7136\u800c\uff0c\u5728\u591a\u9886\u57df\u3001\u591a\u6a21\u6001\u6570\u636e\u573a\u666f\u4e0b\uff0c\u5b83\u4eec\u7684\u9002\u7528\u6027\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u6539\u8fdb\u4ee5\u66f4\u597d\u5730\u5904\u7406\u4e0d\u540c\u7c7b\u578b\u6570\u636e\u7684\u878d\u5408\u3002 - **\u6cdb\u5316\u80fd\u529b**\uff1a\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\uff0c\u8fd9\u4e24\u79cd\u5f52\u4e00\u5316\u65b9\u6cd5\u6709\u52a9\u4e8e\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u5728\u590d\u6742\u591a\u53d8\u7684\u73b0\u5b9e\u5e94\u7528\u573a\u666f\u4e2d\uff0c\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u4ecd\u6709\u5f85\u63d0\u9ad8\uff0c\u5c24\u5176\u662f\u5728\u8de8\u9886\u57df\u548c\u591a\u6a21\u6001\u6570\u636e\u7684\u6cdb\u5316\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "task_step_level": "3", "task_step_question": "\u5728\u591a\u9886\u57df\u3001\u591a\u6a21\u6001\u6570\u636e\u573a\u666f\u4e0b\uff0c\u4e3a\u4e86\u63d0\u5347LayerNorm\u548cRMSNorm\u5bf9\u4e0d\u540c\u7c7b\u578b\u6570\u636e\u878d\u5408\u7684\u9002\u7528\u6027\uff0c\u53ef\u4ee5\u91c7\u53d6\u54ea\u4e9b\u6539\u8fdb\u63aa\u65bd\uff1f ", "task_step_question_context": [{"ref_id": "454846008144214678", "chunk_id": "3", "score": 0.451171875, "text": "# 3.3 A TRANSFORMATION PER BLOCK\nNow that every LayerNorm in the transformer has been converted to RMSNorm, we can select any $\\mathbf{Q}$ to modify the model. Our initial plan was to collect signals from the model, construct an orthogonal matrix using those signals and to delete parts of the network. We quickly saw that the signals at different blocks of the network were not aligned, and that we would need to apply a different orthogonal matrix at each block, $\\mathbf{Q}_{\\ell}$ .  \n\nAllowing the orthogonal matrix used in each block to differ can be shown to leave the model unchanged using the same proof as Theorem 1 ,  \n\n  \nFigure 3: Converting a transformer network from LayerNorm to RMSNorm: the scale matrix diag $(\\alpha)$ is absorbed into the subsequent matrix $\\mathbf{W}_{\\mathrm{in}}$ . Figure shows the block in combined colors. We use $(\\alpha)$ for brevity. The mean-subtraction matrix $\\mathbf{M}$ is applied to each matrix $\\mathbf{W}_{\\mathrm{out}}$ . Layernorm becomes RMSNorm, up to a constant $\\bar{\\sqrt{D}}$ (not shown). Here, the scaling $(\\alpha^{\\prime})$ comes from the previous block.  \n\n  \nFigure 4: With the network converted to RMSNorm (see Figure 3 ), we apply the computational-invariance idea. The input weight matrices $\\mathrm{diag}(\\alpha)\\mathbf{W}_{\\mathrm{in}}$ are pre-multiplied by $\\mathbf{Q}^{\\top}$ . The output matrices $\\mathbf{W}_{\\mathrm{out}}\\mathbf{M}$ are post-multiplied by $\\mathbf{Q}$ . In the skip-connection, a new linear layer is added $\\mathbf{Q}_{\\ell}^{\\top}\\mathbf{Q}_{\\ell+1}$ . After these modifications, the matrices can be sliced (hatched areas).  \n\nwith the exception of line 5 of Algorithm 1 . Here we see that the residual connection and the output of the block must have the same rotation. To fix this, we modify the residual connection by applying the linear transformation applied to different blocks with the additional linear operation in the residual connection. Unlike the $\\mathbf{Q}_{\\ell-1}^{\\top}\\mathbf{Q}_{\\ell}$ \u2212to the residual. Figure 4 shows how different rotations can be modifications to the weight matrices, these additional operations cannot be pre-computed and add a small $(D\\times D)$ overhead to the model. Nonetheless, they are needed to allow slicing the model (Section 3.4 ) and we see real speedup overall (Section 4 ).  \n\nTo compute the matrices $\\mathbf{Q}_{\\ell}$ , we use PCA. We select a calibration dataset from the training set, run it through the model (after converting LayerNorm operations into RMSNorm), and extract the orthogonal matrix of the layer. We use the output of the transformed network to calculate the orthogonal matrices of the next layers. More precisely, if $\\mathbf{X}_{\\ell,i}$ is the output of the $\\ell^{\\mathrm{th}}$ RMSNorm block for the $i^{\\mathrm{th}}$ sequence in the calibration dataset, we compute  \n\n$$\n\\mathbf{C}_{\\ell}=\\sum_{i}\\mathbf{X}_{\\ell,i}^{\\top}\\mathbf{X}_{\\ell,i}\n$$  \n\nand set $\\mathbf{Q}_{\\ell}$ to the be the eigenvectors of $\\mathbf{C}_{\\ell}$ , sorted by decreasing eigenvalues.\n\n# 3.4 SLICING\nThe goal of Principal Component Analysis is usually to take a data matrix $\\mathbf{X}$ and compute a lower dimensional representation $\\mathbf{Z}$ , and an approximate reconstruction $\\tilde{\\mathbf{X}}$ :  \n\n$$\n\\mathbf{Z}=\\mathbf{X}\\mathbf{Q}\\mathbf{D}\\,,\\qquad\\tilde{\\mathbf{X}}=\\mathbf{Z}\\mathbf{D}^{\\top}\\mathbf{Q}^{\\top}\\,.\n$$  \n\nwhere $\\mathbf{Q}$ is the ectors of ${\\bf X}^{\\top}{\\bf X}$ , and $\\mathbf{D}$ is a $D\\times D_{\\mathrm{small}}$ deletion matrix (containing $D_{\\mathrm{small}}$ The reconstruction is columns of the $D\\times D$ \u00d7$L_{2}$ identity matrix), which removes some of the columns of the matrix to the left. optimal, in the sense that QD is a linear mapping that minimizes $\\lVert\\mathbf{X}-\\tilde{\\mathbf{X}}\\rVert^{2}$ .  \n\nWhen we apply PCA to the signal matrix $\\mathbf{X}$ bween blocks, we never materialize the $N\\times D$ signal matrix, but we apply the deletion matrix Dto the operations preceding and succeeding the construction of that matrix, which have already been multiplied by $\\mathbf{Q}$ in the above. We delete rows of $\\mathbf{W}_{\\mathrm{in}}$ that we have inserted into the residual connection (see Figure and columns of $\\mathbf{W}_{\\mathrm{out}}$ and $\\mathbf{W}_{\\mathrm{embd}}$ . We also delete both rows 4 ). and columns of the matrix $\\mathbf{Q}_{\\ell-1}^{\\top}\\mathbf{Q}_{\\ell}$ \u2212\n\n# 4 EXPERIMENTAL VALIDATION\nSetup We use HuggingFace Transformers ( Wolf et al. ,2019 ) to implement our code with PyTorch (Paszke et al. ,2019 ). The computation of $\\mathbf{Q}$ is performed on a single H100 GPU with 80GB of memory, taking approximately 3.5 hours to complete for the L LAMA -2 70B model. During the PCA calculation, we use double precision for computing the eigenvectors of the covariance matrix. We find that using single precision for eigenvector calculations in PyTorch leads to a discrepancy in the final accuracy, as detailed in Appendix A.2 .  \n\nWe experiment with two different calibration sets: 1024 samples from the WikiText-2 training dataset ( Merity et al. ,2016 ) and 5000 samples from the Alpaca training dataset ( Taori et al. ,2023 ). Sequence lengths are chosen as the maximum of each language model. An ablation study on the calibration set size and sequence length is presented in Appendix A.3 .  \n\nModels, Tasks, and GPUs We evaluate all our experiments on OPT ( Zhang et al. ,2022 ), L LAMA -2 (Touvron et al. ,2023 ) model families, and additionally evaluate Phi-2 (in our zero-shot task) experiments. We exclude OPT 175B, as it is outperformed by smaller L LAMA -2 models. Nonetheless, we anticipate that this larger model will yield improved results, as larger models typically offer more promising opportunities for compression (see Section 4.1 ). We evaluate our scheme on both language generation as well as popular zero-shot tasks. To demonstrate the comprehensive speedup achieved by SliceGPT we use: Quadro RTX6000 GPUs with 24GB of memory as a representative example of consumer-level GPUs; 40GB A100s and 80GB H100s to provide datacenter-level benchmarks.  \n\nBaseline Setup We initially planned to compare our results against a scheme that pruned columns (or rows) with the smallest norm but found that this baseline was very poor, with the perplexity of the model soaring into the 1000s after pruning just a few columns. Instead, we compare SliceGPT against SparseGPT ( Frantar & Alistarh ,2023 ) employing a 2:4 sparsity ratio, as this is the only sparsity scheme which achieves speedup ( Mishra et al. ,2021 )."}, {"ref_id": "454895409734360760", "chunk_id": "3", "score": 0.427734375, "text": "# 5 I NTUITIONS BEHIND LAYER NORM TUNING\nIn this section, driven by the empirical success of LayerNorm tuning, we explore the intuitions behind LayerNorm from three perspectives, domain adaptation, expressive power, and gradient variance.  \n\nTable 3: Model performance on different data types. Methods with 80K and Conv.20K suffix are tuned on the full 80K data and the 20K conversational data, respectively.   \n\n\n<html><body><table><tr><td>Method</td><td>MME</td><td>VQAv2</td><td>MSCOCO</td><td>Flickr30k</td><td>POPE</td></tr><tr><td colspan=\"6\">MM-V1CUNA-7B</td></tr><tr><td>Finetune-80K</td><td>625.2/270.7</td><td>15.40</td><td>67.50</td><td>34.61</td><td>73.8/76.5/66.5</td></tr><tr><td>LayerNorm-80K</td><td>723.2/253.2</td><td>17.06</td><td>80.89</td><td>48.01</td><td>76.1/81.1/70.8</td></tr><tr><td>LayerNorm-Conv. 20K</td><td>777.1/231.4</td><td>15.39</td><td>67.30</td><td>40.33</td><td>75.2/79.2/68.8</td></tr><tr><td colspan=\"6\">MM-LLAMA2-7B</td></tr><tr><td>Finetune-80K</td><td>661.3/237.1</td><td>16.09</td><td>65.08</td><td>31.64</td><td>56.3/65.0/55.4</td></tr><tr><td>LayerNorm-80K</td><td>583.2/200.7</td><td>16.78</td><td>88.85</td><td>49.24</td><td>66.6/68.5/64.9</td></tr><tr><td>LayerNorm-Conv. 20K</td><td>376.2/157.5</td><td>16.19</td><td>86.80</td><td>44.88</td><td>50.5/50.7/50.3</td></tr><tr><td colspan=\"6\">MM-LLAMA2-CHAT-7B</td></tr><tr><td>Finetune-80K</td><td>805.4/234.6</td><td>15.29</td><td>57.40</td><td>26.70</td><td>60.3/69.8/57.9</td></tr><tr><td>LayerNorm-80K</td><td>651.3/219.3</td><td>16.60</td><td>75.34</td><td>43.75</td><td>71.3/72.4/67.8</td></tr><tr><td>LayerNorm-Conv. 20K</td><td>482.9/172.1</td><td>13.88</td><td>66.85</td><td>41.95</td><td>62.7/71.7/61.3</td></tr><tr><td colspan=\"6\">MM-LLAMA2-13B</td></tr><tr><td>Finetune-80K</td><td>402.3/199.3</td><td>18.33</td><td>73.88</td><td>45.33</td><td>51.6/51.1/52.2</td></tr><tr><td>LayerNorm-80K</td><td>526.0/177.5</td><td>15.31</td><td>82.92</td><td>48.42</td><td>60.0/69.1/58.9</td></tr><tr><td>LayerNorm-Conv. 20K</td><td>646.0/242.9</td><td>16.01</td><td>76.50</td><td>44.86</td><td>70.0/76.9/68.6</td></tr><tr><td colspan=\"6\">MM-LLAMA2-CHAT-13B</td></tr><tr><td>Finetune-80K</td><td>623.3/221.4</td><td>15.17</td><td>64.19</td><td>41.82</td><td>67.6/64.8/64.5</td></tr><tr><td>LayerNorm-80K</td><td>929.3/254.3</td><td>16.10</td><td>74.96</td><td>42.79</td><td>78.9/83.9/74.3</td></tr><tr><td>LayerNorm-Conv. 20K</td><td>769.7/227.5</td><td>15.57</td><td>73.30</td><td>43.08</td><td>68.2/72.8/65.3</td></tr></table></body></html>  \n\nTable 4: Results of models with LayerNorm and/or vision-language Connector activated.   \n\n\n<html><body><table><tr><td>Method</td><td>MME</td><td>VQAv2</td><td>MSCOCO</td><td>Flickr30k</td><td>POPE</td></tr><tr><td colspan=\"6\">MM-LLAMA2-7B</td></tr><tr><td>LayerNorm + Connector</td><td>583.2/200.7</td><td>16.78</td><td>88.85</td><td>49.24</td><td>66.6/68.5/64.9</td></tr><tr><td>Connector</td><td>311.1/105.4</td><td>12.72</td><td>60.43</td><td>35.91</td><td>67.9/73.7/66.9</td></tr><tr><td>LayerNorm</td><td>395.0/191.4</td><td>18.18</td><td>80.13</td><td>41.68</td><td>50.3/51.3/50.2</td></tr><tr><td colspan=\"6\">MM-LLAMA2-13B</td></tr><tr><td>LayerNorm + Connector</td><td>526.0/177.5</td><td>15.31</td><td>82.92</td><td>48.42</td><td>60.0/69.1/58.9</td></tr><tr><td>Connector</td><td>507.0/187.9</td><td>15.22</td><td>62.60</td><td>25.13</td><td>60.9/66.8/60.1</td></tr><tr><td>LayerNorm</td><td>405.0/188.6</td><td>16.51</td><td>70.41</td><td>39.86</td><td>50.9/52.7/51.0</td></tr></table></body></html>\n\n# 5.1 LAYER NORM TUNING A DAPTS LLM S TO MULTI -M ODAL\nInfluence of the Vision-Language Connector The vision-language connector serves as the converter to project features from the vision encoder to the LLM domain. In our previous experiments, we focused on finetuning the LLM component of the MLLMs while keeping the vision-language connector activated by default. To determine which component plays a more important role for domain adaptation of LLM to multi-modal domain, we performed an ablation study by activating the two components separately. Results are presented in table 4 , tuning LayerNorm in attention blocks without activating the vision-language connector resulted in only a $4.2\\%$ and $5.4\\%$ decrease in performance on three traditional multi-modal tasks and the MME benchmark, respectively. This decrease is significantly lower than the $15.6\\%$ and $9.2\\%$ downgrade observed when only activating the Connector on the same tasks. This observation highlights the vital role LayerNorm plays in transforming knowledge from the vision domain to language, indicating LayerNorm as a strong domain adaptor for the LLM architecture.  \n\n  \n\nFigure 3: Layer similarities between different LLM layers in (a) Finetuned and (b) LayerNorm-tuned MM-V ICUNA -7B. The average layer similarity of two models are 0.624 and 0.585, respectively.  \n\nTable 5: Results of models with LL A MA2 Finetuned/LayerNorm-tuned with ViT pre-trained on ImageNet (Deng et al. ,2009 ), which have not been aligned with the language domain.   \n\n\n<html><body><table><tr><td></td><td>MME</td><td>VQAv2</td><td>MSCOCO</td><td>Flickr30k</td><td>POPE</td></tr><tr><td>Finetune-7B</td><td>406.79/182.5</td><td>15.05</td><td>47.75</td><td>18.97</td><td>50.0/51.6/50.1</td></tr><tr><td>LayerNorm-7B</td><td>301.51/127.14</td><td>15.48</td><td>66.22</td><td>31.73</td><td>50.0/50.1/50.1</td></tr><tr><td>Finetune-13B</td><td>375.41/171.79</td><td>25.38</td><td>51.26</td><td>25.96</td><td>50.3/51.1/51.0</td></tr><tr><td>LayerNorm-13B</td><td>445.98/150.0</td><td>15.59</td><td>64.63</td><td>32.17</td><td>51.2/53.0/50.8</td></tr></table></body></html>  \n\nSwitching Visual Features. We employ the ViT encoder from CLIP ( Radford et al. ,2021 ) by default in our previous experiments. CLIP ( Radford et al. ,2021 ) models are trained with image-text contrastive loss, thus its feature space is already aligned with language. Since LayerNorm has shown its effectiveness as a domain adaptor, we are interested in testing whether or not LayerNorm tuning can adapt a LLM to image features that are not pretrained to align with language. The vision encoder is switched to a ViT model that was pretrained on ImageNet (Dosovitskiy et al. ,2021 ;Deng et al. ,2009 ). Results in table 5 demonstrate that both LayerNorm and finetuning approaches can yield high performance. Interestingly, we observe that by LayerNorm tuning with ImageNet trained ViT, which has not been aligned with language, the model is able to achieve comparable performance to full parameter finetuning , i.e ., results show that LayerNorm tuning outperforms finetuning by $12.0\\%$ on captioning tasks, but performs slightly worse by $5.0\\%$ on the MME benchmark. These results again indicates the domain adaptor role of the LayerNorm , hinting the reason behind the empircal success of LayerNorm tuning. Furthermore, it is worth noting that the performance of MLLMs incorporating ViT pretrained on ImageNet is generally inferior to that of CLIP\u2019s vision encoder. This observation provides compelling evidence that, despite differences in tokenizer and training paradigm between CLIP\u2019s text encoder and LL A MA\u2019s, ViT from CLIP has the capacity to learn general patterns of language formulation during pre-training. Thus, significantly enhance MLLM abilities.\n\n# 5.2 LAYER NORM TUNING I MPROVES THE EXPRESSIVE POWER\nIt is shown in Pires et al. (2023 ) that a Transformer model incorporating anisotropic layer representation can capture a wider range of learning patterns. By computing the cosine similarities between all layers in the LLM of a finetuned MLLM, we aim to investigate whether the improved efficiency is the results of the improved expressive power. In table 6 , we present the average layer similarity of three 7B scale MLLMs, and in fig. 3 we present the visualization of per layer similarity scores of MM-V ICUNA -7B. Our analysis reveals that the transformer layers in the MLLMs with LayerNorm tuning exhibit a clear distinction from one another ( i.e ., an average $10.6\\%$ lower layer similarities comparing finetuning), indicating superior generalization ability and expressive power compared to finetuning. This finding sheds light on why tuning LayerNorm is effective for multi-modal LLM training. For additional visualizations, please refer to the Appendix A.2.1 .  \n\n  \nFigure 4: Gradients of the input LayerNorm in the 11th layer of the MM-V ICUNA as training proceeds. LayerNorm-tuned model has lower gradient variance than full parameter finetuning.  \n\nTable 6: Layer representation similarity of LayerNorm and finetuning methods on three 7B MLLMs.   \nLower the similarity is, the better expressive power a model possesses.  \n\n<html><body><table><tr><td>Model</td><td>LayerNorm Sim.</td><td>Finetuning Sim.</td></tr><tr><td>MM-VICUNA</td><td>0.585</td><td>0.624</td></tr><tr><td>MM-LLAMA2</td><td>0.504</td><td>0.591</td></tr><tr><td>MM-LLAMA2-CHAT</td><td>0.550</td><td>0.617</td></tr></table></body></html>"}, {"ref_id": "454959902228223542", "chunk_id": "2", "score": 0.35546875, "text": "# TUNING LAYER NORM IN A TTENTION : T OWARDS EFFI -CIENT MULTI -M ODAL LLM F INETUNING\nBingchen Zhao\\* 1 Haoqin $\\mathbf{T}\\mathbf{u}^{*2}$ Chen Wei 3 Jieru Mei 3 Cihang Xie 4  \n\n\\*equal contribution  \n\n1 University of Edinburgh 2 University of Chinese Academy of Sciences   \n3 Johns Hopkins University 4 UC Santa Cruz\n\n# A BSTRACT\nThis paper introduces an efficient strategy to transform Large Language Models (LLMs) into Multi-Modal Large Language Models. By conceptualizing this transformation as a domain adaptation process, i.e ., transitioning from text understanding to embracing multiple modalities, we intriguingly note that, within each attention block, tuning LayerNorm suffices to yield strong performance. Moreover, when benchmarked against other tuning approaches like full parameter finetuning or LoRA, its benefits on efficiency are substantial. For example, when compared to LoRA on a 13B model scale, performance can be enhanced by an average of over $20\\%$ across five multi-modal tasks, and meanwhile, results in a significant reduction of trainable parameters by $41.9\\%$ and a decrease in GPU memory usage by $17.6\\%$ . On top of this LayerNorm strategy, we showcase that selectively tuning only with conversational data can improve efficiency further. Beyond these empirical outcomes, we provide a comprehensive analysis to explore the role of LayerNorm in adapting LLMs to the multi-modal domain and improving the expressive power of the model.\n\n# 1 I NTRODUCTION\nLarge Language Models (LLMs) have had many application scenarios since their debut. In particular, extending LLMs to handle multiple modalities has gathered much interest from both academia and industry. Such models, termed Multi-modal Large Language Models (MLLMs), are typically derived by finetuning a pretrained LLM on multi-modal data ( Liu et al. ,2023 ;Ye et al. ,2023 ). However, this process typically poses a substantial computational challenge ( Liu et al. ,2023 ), particularly for exceptionally large-scale models. While Su et al. (2023 ); Zhang et al. (2023 ) employ low-rank adapters (LoRA) ( Hu et al. ,2022 ) or soft prompts ( Li & Liang ,2021a ) for more parameter-efficient tuning, this often comes at the cost of compromised performance on multi-modal tasks. This challenge prompts the pivotal question: how can we make this process more efficient?  \n\nIn response to this challenge, we introduce a simple and effective strategy for MLLM finetuning: as illustrated in Figure 1 (a), within each attention block, we adjust only the weights of the LayerNorm ( Ba et al. ,2016 ). This strategy is underpinned by the understanding that the evolution from LLMs to MLLMs can be conceptualized as a domain adaptation process, i.e ., transitioning from textcentric to multi-modal understanding. Adjusting normalization layers, as suggested by prior research, emerges as a particularly effective technique in such domain shifts ( Li et al. ,2016 ). Empirically, this straightforward technique can surprisingly yield comparable or even better performance than the strong baseline of finetuning all parameters offer about $10\\times$ more parameter efficiency than LoRA.  \n\nBy delving deeper, we note that the process can be further simplified by designating LayerNorm as the sole trainable component within the entire model. This means, in contrast to the typical configurations depicted in Figure 1 (a)-(c), we now freeze the standardly activated elements, including the visionlanguage connector, word embedding, and the output head. We term it as LayerNorm-simple. Impressively, despite constituting a mere $0.004\\%$ of trainable parameters, this configuration surpasses the performance of LoRA, registering an average enhancement of $4.3\\%$ across five benchmarks.  \n\n  \nFigure 1: ( left ) Different tuning methods for MLLMs. Trainable components are in blue , while frozen parameters are in gray . Within the attention blocks, $(a)$ only activates LayerNorm parameters. Note that vision-language connector, word embedding, and output head paramters are by default activated for all three options. ( right ) Comparison on trainable parameters and GPU memory. Tuning LayerNorm achieves significant reductions in trainable parameters and GPU memory usages.  \n\nOn top of this LayerNorm strategy, we further improve the finetuning efficiency from the data perspective. Specifically, we assess the performance implications of different types of finetuning data, including conversational data, detailed description data, and complex reasoning data. Our results offer a crucial insight: not all data are created equal for the task of MLLM finetuning. Remarkably, we find that MLLMs finetuned on conversational data consistently outperform those finetuned on other data types. Specifically, conversational data improves the model performance by an average of $50\\%$ compared to other data types. This observation interestingly opens up avenues for more targeted data collection and curation strategies, thereby further optimizing the efficiency of MLLMs finetuning. Furthermore, by combining the LayerNorm strategy and this data perspective, we can achieve on average $10.0\\%$ performance improvement over full parameter finetuning on traditional VQA benchmarks with an LL A MA2 13B model while using significantly less parameters and data.  \n\nBeyond the empirical outcomes above, we conduct an investigation into the expressive power of LayerNorm tuning. Our analysis reveals that LayerNorm-tuned MLLMs exhibit lower cross-layer similarity compared to models all of which parameters are finetuned. This lowered similarity is indicative of a more expressive model, since the model incorporates anisotropic layer presentations can capture a wider range of learning patterns ( Pires et al. ,2023 ). It stands to reason that this amplified expressiveness is a key factor underpinning the efficiency and superior performance we noted, granting the model enhanced adaptability to novel multi-modal datasets.  \n\nIn essence, our findings illuminate the profound influence of LayerNorm tuning, suggesting its potential to adeptly harness the intrinsic properties of LLMs. We hope that this study will catalyze subsequent research endeavors focused on efficient multi-modal finetuning."}], "task_step_question_answer": "\u5728\u8de8\u9886\u57df\u6cdb\u5316\u7684\u5b9e\u9a8c\u8bbe\u8ba1\u4e2d\uff0c\u53ef\u4ee5\u8bc4\u4f30\u6a21\u578b\u5728\u4ece\u4e00\u4e2a\u9886\u57df\uff08\u5982\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff09\u8fc1\u79fb\u5230\u53e6\u4e00\u4e2a\u9886\u57df\uff08\u5982\u8ba1\u7b97\u673a\u89c6\u89c9\uff09\u65f6\u7684\u6cdb\u5316\u80fd\u529b\u3002\u4f8b\u5982\uff0c\u53ef\u4ee5\u8bbe\u8ba1\u4e00\u4e2a\u5b9e\u9a8c\uff0c\u4f7f\u7528\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u4e0a\u9884\u8bad\u7ec3\u7684\u6a21\u578b\uff0c\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u4e0a\u8fdb\u884c\u5fae\u8c03\uff0c\u5e76\u5206\u6790\u5f52\u4e00\u5316\u65b9\u6cd5\u5728\u8de8\u9886\u57df\u6cdb\u5316\u4e2d\u7684\u4f5c\u7528\u3002\u6b64\u5916\uff0c\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63a2\u8ba8\u591a\u6a21\u6001\u6570\u636e\u6cdb\u5316\u7684\u6311\u6218\uff0c\u5982\u6a21\u6001\u95f4\u7684\u4e0d\u4e00\u81f4\u6027\u3001\u6570\u636e\u5206\u5e03\u5dee\u5f02\u7b49\uff0c\u5e76\u63d0\u51fa\u53ef\u80fd\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5982\u6a21\u6001\u5bf9\u9f50\u6280\u672f\u3001\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u673a\u5236\u7b49\u3002", "ref_task_step_id": ""}, "__type__": "task_step_node"}, "e2b95384-9e4b-4445-94fd-17a46fdce3ea": {"__data__": {"id_": "e2b95384-9e4b-4445-94fd-17a46fdce3ea", "metadata": {}, "relationships": {}, "hash": "", "start_task_context": "\u5927\u6a21\u578b\u4e2d\u7684LayerNorm\u548cRMSNorm\u6709\u4ec0\u4e48\u533a\u522b\uff1f", "aemo_representation_context": "### \u5206\u6790\u8fd1\u51e0\u5e74\u7814\u7a76\u9886\u57df\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba\n\u5728\u5927\u6a21\u578b\u76f8\u5173\u7814\u7a76\u9886\u57df\uff0c\u5f52\u4e00\u5316\u6280\u672f\u662f\u91cd\u8981\u7684\u7814\u7a76\u65b9\u5411\u4e4b\u4e00\u3002LayerNorm \u548c RMSNorm \u4f5c\u4e3a\u5176\u4e2d\u5173\u952e\u7684\u5f52\u4e00\u5316\u65b9\u6cd5\uff0c\u88ab\u4f17\u591a\u7814\u7a76\u6df1\u5165\u63a2\u8ba8\u3002\u7814\u7a76\u8005\u4eec\u4ece\u7406\u8bba\u5206\u6790\u3001\u5b9e\u9a8c\u5bf9\u6bd4\u7b49\u591a\u65b9\u9762\u5bf9\u5b83\u4eec\u8fdb\u884c\u5256\u6790\uff0c\u4ee5\u660e\u786e\u5176\u5728\u5927\u6a21\u578b\u6280\u672f\u6846\u67b6\u4e2d\u7684\u5730\u4f4d\u548c\u4f5c\u7528\u3002\n\n### \u7814\u7a76\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u4e3b\u8981\u6846\u67b6\uff08\u5982 Transformer\u3001GAN\u3001BERT \u7b49\uff09\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0e\u53d8\u4f53\n\u5728 Transformer \u67b6\u6784\u4e3a\u4e3b\u7684\u4f17\u591a\u6a21\u578b\u4e2d\uff0cLayerNorm \u548c RMSNorm \u88ab\u5e7f\u6cdb\u5e94\u7528\u3002\u4f8b\u5982\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\uff08\u5982 BERT \u53ca\u5176\u53d8\u4f53\u6a21\u578b\uff09\u4e2d\uff0cLayerNorm \u6700\u521d\u88ab\u7528\u4e8e\u7a33\u5b9a\u6a21\u578b\u8bad\u7ec3\u3001\u52a0\u901f\u6536\u655b\u4ee5\u53ca\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u80fd\u529b\uff1b\u800c RMSNorm \u4e5f\u9010\u6e10\u5728\u4e00\u4e9b\u57fa\u4e8e Transformer \u7684\u6a21\u578b\u4e2d\u5f97\u5230\u5e94\u7528\uff0c\u4ee5\u6539\u8fdb\u6a21\u578b\u7684\u6027\u80fd\u8868\u73b0\u3002\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\uff08\u5982\u57fa\u4e8e Transformer \u7684\u89c6\u89c9\u6a21\u578b\uff09\u4e2d\u540c\u6837\u6709\u5b83\u4eec\u7684\u8eab\u5f71\uff0c\u4e0d\u540c\u4efb\u52a1\u573a\u666f\u4e0b\u5bf9\u8fd9\u4e24\u79cd\u5f52\u4e00\u5316\u65b9\u6cd5\u8fdb\u884c\u4e86\u9002\u5e94\u6027\u8c03\u6574\u548c\u53d8\u4f53\u5e94\u7528\uff0c\u4ee5\u66f4\u597d\u5730\u9002\u914d\u4efb\u52a1\u9700\u6c42\u3002\n\n### \u8bc4\u4f30\u5b66\u672f\u754c\u7684\u6280\u672f\u8fdb\u6b65\u4e0e\u5c40\u9650\u6027\n - **\u6280\u672f\u8fdb\u6b65**\uff1aLayerNorm \u548c RMSNorm \u7684\u51fa\u73b0\u663e\u8457\u63d0\u5347\u4e86\u5927\u6a21\u578b\u8bad\u7ec3\u7684\u7a33\u5b9a\u6027\u548c\u6548\u7387\u3002LayerNorm \u80fd\u591f\u5bf9\u6bcf\u4e2a\u6837\u672c\u7684\u4e0d\u540c\u7ef4\u5ea6\u8fdb\u884c\u5f52\u4e00\u5316\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5185\u90e8\u534f\u53d8\u91cf\u504f\u79fb\u95ee\u9898\uff1bRMSNorm \u5219\u901a\u8fc7\u66f4\u7b80\u6d01\u7684\u8ba1\u7b97\u65b9\u5f0f\uff0c\u5728\u4e00\u4e9b\u573a\u666f\u4e0b\u53d6\u5f97\u4e86\u4e0e LayerNorm \u76f8\u5f53\u751a\u81f3\u66f4\u597d\u7684\u6548\u679c\uff0c\u63a8\u52a8\u4e86\u5927\u6a21\u578b\u6027\u80fd\u7684\u63d0\u5347\u3002\n - **\u5c40\u9650\u6027**\uff1a\u5c3d\u7ba1\u53d6\u5f97\u8fdb\u6b65\uff0c\u4f46\u4ecd\u5b58\u5728\u4e00\u4e9b\u95ee\u9898\u3002\u4f8b\u5982\uff0c\u5728\u67d0\u4e9b\u590d\u6742\u4efb\u52a1\u548c\u7279\u5b9a\u6570\u636e\u96c6\u4e0a\uff0c\u6a21\u578b\u4f9d\u7136\u5b58\u5728\u6027\u80fd\u74f6\u9888\uff0c\u5373\u4f7f\u4f7f\u7528\u4e86\u8fd9\u4e9b\u5f52\u4e00\u5316\u65b9\u6cd5\uff0c\u6a21\u578b\u504f\u5dee\u548c\u6570\u636e\u4f9d\u8d56\u95ee\u9898\u4f9d\u65e7\u5b58\u5728\u3002\u800c\u4e14\u5728\u9762\u5bf9\u591a\u6a21\u6001\u6570\u636e\u65f6\uff0c\u73b0\u6709\u7684\u5f52\u4e00\u5316\u65b9\u6cd5\u5728\u878d\u5408\u4e0d\u540c\u6a21\u6001\u7279\u5f81\u65b9\u9762\u8fd8\u5b58\u5728\u4e0d\u8db3\u3002\n\n### \u63a2\u8ba8\u8ba1\u7b97\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\n - **\u9002\u7528\u6027**\uff1aLayerNorm \u5728\u591a\u79cd\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6570\u636e\u96c6\u548c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8f83\u597d\u7684\u9002\u7528\u6027\uff0c\u80fd\u591f\u9002\u5e94\u4e0d\u540c\u89c4\u6a21\u548c\u7279\u70b9\u7684\u6570\u636e\u3002RMSNorm \u5728\u4e00\u4e9b\u5927\u89c4\u6a21\u6570\u636e\u548c\u7279\u5b9a\u4efb\u52a1\uff08\u5982\u957f\u5e8f\u5217\u5904\u7406\uff09\u4e2d\u5c55\u73b0\u51fa\u72ec\u7279\u4f18\u52bf\u3002\u7136\u800c\uff0c\u5728\u591a\u9886\u57df\u3001\u591a\u6a21\u6001\u6570\u636e\u573a\u666f\u4e0b\uff0c\u5b83\u4eec\u7684\u9002\u7528\u6027\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u6539\u8fdb\u4ee5\u66f4\u597d\u5730\u5904\u7406\u4e0d\u540c\u7c7b\u578b\u6570\u636e\u7684\u878d\u5408\u3002\n - **\u6cdb\u5316\u80fd\u529b**\uff1a\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\uff0c\u8fd9\u4e24\u79cd\u5f52\u4e00\u5316\u65b9\u6cd5\u6709\u52a9\u4e8e\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u5728\u590d\u6742\u591a\u53d8\u7684\u73b0\u5b9e\u5e94\u7528\u573a\u666f\u4e2d\uff0c\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u4ecd\u6709\u5f85\u63d0\u9ad8\uff0c\u5c24\u5176\u662f\u5728\u8de8\u9886\u57df\u548c\u591a\u6a21\u6001\u6570\u636e\u7684\u6cdb\u5316\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002\n\n### \u5206\u6790\u6700\u65b0\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027\n - **\u7a33\u5b9a\u6027**\uff1a\u7814\u7a76\u53d1\u73b0\uff0cLayerNorm \u901a\u8fc7\u5bf9\u8f93\u5165\u6570\u636e\u7684\u5f52\u4e00\u5316\u5904\u7406\uff0c\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u63d0\u9ad8\u4e86\u6a21\u578b\u8bad\u7ec3\u7684\u7a33\u5b9a\u6027\u3002RMSNorm \u540c\u6837\u5728\u4e00\u4e9b\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u7a33\u5b9a\u6027\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u52a8\u6001\u73af\u5883\u4e0b\uff0c\u90e8\u5206\u7814\u7a76\u9488\u5bf9\u5176\u7a33\u5b9a\u6027\u8fdb\u884c\u4e86\u4f18\u5316\uff0c\u4f7f\u5176\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\u4e5f\u80fd\u4fdd\u6301\u76f8\u5bf9\u7a33\u5b9a\u7684\u8868\u73b0\u3002\n - **\u5bb9\u9519\u6027**\uff1a\u5728\u9762\u5bf9\u6570\u636e\u566a\u58f0\u548c\u5f02\u5e38\u503c\u65f6\uff0c\u8fd9\u4e24\u79cd\u5f52\u4e00\u5316\u65b9\u6cd5\u90fd\u6709\u4e00\u5b9a\u7684\u5bb9\u9519\u80fd\u529b\uff0c\u4f46\u5728\u6781\u7aef\u60c5\u51b5\u4e0b\uff0c\u6a21\u578b\u6027\u80fd\u4ecd\u4f1a\u53d7\u5230\u5f71\u54cd\uff0c\u5bb9\u9519\u6027\u65b9\u9762\u8fd8\u6709\u63d0\u5347\u7a7a\u95f4\u3002\n\n### \u8bc4\u4f30\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218\n - **\u672a\u6765\u7814\u7a76\u65b9\u5411**\uff1a\u8bb8\u591a\u8bba\u6587\u63d0\u51fa\u63a2\u7d22\u66f4\u9002\u7528\u4e8e\u591a\u6a21\u6001\u6570\u636e\u7684\u5f52\u4e00\u5316\u65b9\u6cd5\uff0c\u4ee5\u53ca\u5982\u4f55\u8fdb\u4e00\u6b65\u4f18\u5316 LayerNorm \u548c RMSNorm \u4ee5\u63d0\u5347\u6a21\u578b\u5728\u590d\u6742\u573a\u666f\u4e0b\u7684\u6027\u80fd\u3002\u540c\u65f6\uff0c\u7814\u7a76\u5982\u4f55\u589e\u5f3a\u5f52\u4e00\u5316\u65b9\u6cd5\u5bf9\u6a21\u578b\u504f\u5dee\u548c\u6570\u636e\u4f9d\u8d56\u95ee\u9898\u7684\u89e3\u51b3\u80fd\u529b\u4e5f\u662f\u91cd\u8981\u65b9\u5411\u3002\n - **\u6311\u6218**\uff1a\u65b0\u7684\u7814\u7a76\u95ee\u9898\u5305\u62ec\u5982\u4f55\u8bbe\u8ba1\u4e00\u79cd\u901a\u7528\u7684\u5f52\u4e00\u5316\u7b56\u7565\uff0c\u80fd\u591f\u65e0\u7f1d\u9002\u5e94\u4e0d\u540c\u9886\u57df\u548c\u6a21\u6001\u7684\u6570\u636e\uff1b\u5982\u4f55\u5728\u63d0\u5347\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\uff0c\u8fdb\u4e00\u6b65\u63d0\u9ad8\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u548c\u5bb9\u9519\u6027\u3002\u8fd9\u4e9b\u6311\u6218\u63a8\u52a8\u540e\u7eed\u7814\u7a76\u4e0d\u65ad\u63a2\u7d22\u65b0\u7684\u6280\u672f\u548c\u65b9\u6cd5\uff0c\u4ee5\u6539\u8fdb\u5927\u6a21\u578b\u7684\u6027\u80fd\u548c\u5e94\u7528\u8303\u56f4\u3002\n\n### \u7814\u7a76\u6210\u679c\u3001\u65b9\u6cd5\u7684\u521b\u65b0\u6027\u4e0e\u5e94\u7528\u4ef7\u503c\n - **\u7814\u7a76\u6210\u679c**\uff1a\u660e\u786e\u4e86 LayerNorm \u548c RMSNorm \u5728\u5927\u6a21\u578b\u4e2d\u7684\u6027\u80fd\u8868\u73b0\u3001\u9002\u7528\u573a\u666f\u4ee5\u53ca\u5b58\u5728\u7684\u95ee\u9898\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002\n - **\u65b9\u6cd5\u7684\u521b\u65b0\u6027**\uff1aRMSNorm \u4ee5\u5176\u7b80\u6d01\u7684\u8ba1\u7b97\u65b9\u5f0f\u4e3a\u5f52\u4e00\u5316\u6280\u672f\u5e26\u6765\u65b0\u7684\u601d\u8def\uff0c\u4e0e\u4f20\u7edf\u7684 LayerNorm \u5f62\u6210\u5bf9\u6bd4\u548c\u8865\u5145\uff0c\u63a8\u52a8\u4e86\u5f52\u4e00\u5316\u65b9\u6cd5\u7684\u521b\u65b0\u53d1\u5c55\u3002\n - **\u5e94\u7528\u4ef7\u503c**\uff1a\u8fd9\u4e24\u79cd\u5f52\u4e00\u5316\u65b9\u6cd5\u5728\u5927\u6a21\u578b\u7684\u591a\u4e2a\u5e94\u7528\u9886\u57df\uff08\u5982\u81ea\u7136\u8bed\u8a00\u5904\u7406\u3001\u8ba1\u7b97\u673a\u89c6\u89c9\u7b49\uff09\u90fd\u6709\u91cd\u8981\u5e94\u7528\uff0c\u80fd\u591f\u63d0\u5347\u6a21\u578b\u6027\u80fd\u548c\u8bad\u7ec3\u6548\u7387\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\uff0c\u4e3a\u76f8\u5173\u9886\u57df\u7684\u6280\u672f\u53d1\u5c55\u63d0\u4f9b\u4e86\u6709\u529b\u652f\u6301 \u3002  ", "task_step_name": "\u5206\u6790\u6700\u65b0\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027", "task_step_description": "- **\u7a33\u5b9a\u6027**\uff1a\u7814\u7a76\u53d1\u73b0\uff0cLayerNorm\u901a\u8fc7\u5bf9\u8f93\u5165\u6570\u636e\u7684\u5f52\u4e00\u5316\u5904\u7406\uff0c\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u63d0\u9ad8\u4e86\u6a21\u578b\u8bad\u7ec3\u7684\u7a33\u5b9a\u6027\u3002RMSNorm\u540c\u6837\u5728\u4e00\u4e9b\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u7a33\u5b9a\u6027\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u52a8\u6001\u73af\u5883\u4e0b\uff0c\u90e8\u5206\u7814\u7a76\u9488\u5bf9\u5176\u7a33\u5b9a\u6027\u8fdb\u884c\u4e86\u4f18\u5316\uff0c\u4f7f\u5176\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\u4e5f\u80fd\u4fdd\u6301\u76f8\u5bf9\u7a33\u5b9a\u7684\u8868\u73b0\u3002 - **\u5bb9\u9519\u6027**\uff1a\u5728\u9762\u5bf9\u6570\u636e\u566a\u58f0\u548c\u5f02\u5e38\u503c\u65f6\uff0c\u8fd9\u4e24\u79cd\u5f52\u4e00\u5316\u65b9\u6cd5\u90fd\u6709\u4e00\u5b9a\u7684\u5bb9\u9519\u80fd\u529b\uff0c\u4f46\u5728\u6781\u7aef\u60c5\u51b5\u4e0b\uff0c\u6a21\u578b\u6027\u80fd\u4ecd\u4f1a\u53d7\u5230\u5f71\u54cd\uff0c\u5bb9\u9519\u6027\u65b9\u9762\u8fd8\u6709\u63d0\u5347\u7a7a\u95f4\u3002", "task_step_level": "4", "task_step_question": "LayerNorm \u548c RMSNorm \u5728\u590d\u6742\u52a8\u6001\u73af\u5883\u53ca\u5927\u89c4\u6a21\u6570\u636e\u4e0b\u662f\u5982\u4f55\u5177\u4f53\u4f18\u5316\u4ee5\u4fdd\u6301\u7a33\u5b9a\u6027\u7684\uff0c\u4ee5\u53ca\u5728\u6781\u7aef\u60c5\u51b5\u4e0b\u9762\u5bf9\u6570\u636e\u566a\u58f0\u548c\u5f02\u5e38\u503c\uff0c\u63d0\u5347\u5b83\u4eec\u5bb9\u9519\u6027\u7684\u53ef\u80fd\u65b9\u5411\u6709\u54ea\u4e9b\uff1f ", "task_step_question_context": [{"ref_id": "454845924254196540", "chunk_id": "7", "score": 0.365234375, "text": "# B.2 MODEL LAYERS\nIn this section, we give the formal definition of LayerNorm $\\operatorname{LN}(\\cdot)$ and RMS Norm ${\\mathrm{RMS}}\\left(\\cdot\\right)$ .  \n\nDefinition 1 (LayerNorm) .LayerNorm $L N(\\cdot;\\mu,\\beta,\\epsilon)$ of dimension $D$ is defined as:  \n\n$$\nL N(\\mathbf{x};\\pmb{\\mu},\\beta,\\epsilon)=\\frac{\\mathbf{x}-\\mathbb{E}[\\mathbf{x}]}{\\sqrt{\\mathrm{Var}[\\mathbf{x}]+\\epsilon}}\\odot\\pmb{\\mu}+\\beta,\n$$  \n\nwhere $\\mathbf{x},\\pmb{\\mu},\\beta\\in\\mathbb{R}^{D}$ .  \n\nDefinition 2 (RMSNorm) .RMS Norm $R M S(\\cdot;\\mu,\\epsilon)$ of dimension $D$ is defined as:  \n\n$$\nR M S(\\mathbf{x};\\pmb{\\mu},\\epsilon)=\\frac{\\mathbf{x}}{\\sqrt{\\frac{1}{D}\\sum_{i=1}^{D}(\\mathbf{x}[i])^{2}+\\epsilon}}\\odot\\pmb{\\mu},\n$$  \n\nwhere x,$\\pmb{\\mu}\\in\\mathbb{R}^{D}$ .  \n\nRemark. In neural networks, inputs of normalization layers are usually high dimension tensors. In this case, LayerNorm and RMSNorm normally apply to the last dimension separately.\n\n# B.3 LOSSLESS EXPANSION IN VECTOR SPACE\nIn this section, we first give the general definition of lossless expansion in vector space.  \n\ndimensions satisfy dim it is invertible. Definition 3 (Lossless $(\\bar{\\mathcal{T}})\\geq d i m(S)$ T\u2265S, a vector space expansion ector space) .Given $\\boldsymbol{S}$ and V$\\tau$ $\\mathcal{V}:\\mathcal{S}\\rightarrow\\mathcal{T}$ S \u2192T is said to be lossless if ector spaces where the  \n\nRemark. Note that the identity function Id is lossless with its inverse being itself.  \n\nThen we give a few examples of lossless vector space expansions. These examples will also be used in LEMON.  \n\nExample B.3.1 (Vector average expansion $\\mathcal{V}_{\\mathrm{avg.}}$ ).Let $\\mathbf{\\widetilde{x}}\\in\\mathbb{R}^{D_{S}}$ be a vector of dimension $D_{S}$ and its average $\\begin{array}{r}{\\lambda_{V}g(\\mathbf{x})=\\mathbb{E}[\\mathbf{x}]=\\frac{1}{D_{S}}\\sum_{i}^{D_{S}}\\mathbf{x}[i]}\\end{array}$ P].$\\mathbf{x}_{a\\nu g}^{*}$ is called the average expanded xof dimension $D_{T}$  \n\nwith $D_{T}\\geq D_{S}$ if  \n\n$$\n\\mathbf{x}_{a v g}^{*}=\\mathcal{V}_{a v g}(\\mathbf{x})=C o n c a t\\left[\\underbrace{\\mathbf{x}^{\\mathsf{T}},\\cdots,\\mathbf{x}^{\\mathsf{T}}}_{\\lfloor D_{T}/D s\\rfloor},\\underbrace{A v g(\\mathbf{x}),\\cdots,A v g(\\mathbf{x})}_{D_{T}\\mathrm{~mod~}D_{S}}\\right]^{\\mathsf{T}}\\in\\mathbb{R}^{D_{T}}.\n$$  \n\nExample B.3.2 (Vector z o expansion $\\mathcal{V}_{\\mathrm{zero.}}$ ).Le $\\mathbf{x}\\in\\mathbb{R}^{D_{S}}$ be a vector of dimension $D_{S}$ .$\\mathbf{x}_{z e r o}^{*}$ is called the zero expanded xof dimension $D_{T}$ with $D_{T}\\geq D_{S}$ \u2265if  \n\n$$\n\\begin{array}{r}{\\mathbf{x}_{z e r o}^{*}=\\mathcal{V}_{z e r o}(\\mathbf{x})=C o n c a t\\left[\\underbrace{\\mathbf{x^{\\mathsf{T}}},\\cdots,\\mathbf{x^{\\mathsf{T}}}}_{\\lfloor D_{T}/D_{S}\\rfloor},\\underbrace{0,\\cdots,0}_{D_{T}\\mathrm{~mod~}D_{S}}\\right]^{\\mathsf{T}}\\in\\mathbb{R}^{D_{T}}.}\\end{array}\n$$  \n\nExample B.3.3 (Vector circula expansion $\\mathcal{V}_{\\mathrm{circ}})$ Let $\\mathbf{x}\\in\\mathbb{R}^{D_{S}}$ a vector of dimension $D_{S}$ .${\\bf x}_{c i r c}^{*}$ is called the circular expanded xof dimension $D_{T}$ with $D_{T}\\geq D_{S}$ \u2265if  \n\n$$\n\\begin{array}{r}{\\mathbf{x}_{c i r c}^{*}=\\mathcal{V}_{c i r c}(\\mathbf{x})=C o n c a t\\underbrace{\\left[\\mathbf{x}^{\\mathsf{T}},\\cdots,\\mathbf{x}^{\\mathsf{T}},\\mathbf{x}^{\\mathsf{T}}[\\colon D_{T}\\bmod D_{S}]\\right]^{\\mathsf{T}}\\in\\mathbb{R}^{D_{T}}}_{[D_{T}/D_{S}]}.}\\end{array}\n$$  \n\nExample B.3.4 (Vector random expansion $\\mathcal{V}_{\\mathrm{rand.}}$ Let $\\mathbf{\\Deltax}\\in\\mathbb{R}^{D_{S}}$ a vector of dimension $D_{S}$ .${\\bf x}_{r a n d}^{*}$ is called the random expanded xof dimension $D_{T}$ with $D_{T}\\geq D_{S}$ \u2265if  \n\n$$\n\\begin{array}{r}{\\mathbf{x}_{r a n d}^{*}=\\mathcal{V}_{r a n d}(\\mathbf{x};\\zeta)=C o n c a t\\left[\\underbrace{\\mathbf{x^{\\intercal}},\\cdots,\\mathbf{x^{\\intercal}}}_{\\lfloor D_{T}/D_{S}\\rfloor},\\zeta^{\\intercal}\\right]^{\\intercal}\\in\\mathbb{R}^{D_{T}},}\\end{array}\n$$  \n\nwhere $\\zeta\\in\\mathbb{R}^{D_{T}}$ mod $D_{S}$ is an arbitrary vector.  \n\nRemark. (1) All vector expansion examples above follow the same pattern. Specifically, when $D_{T}$ expanding from di mod s by $D_{S}$ entries differently. (2) The random vector ating $\\textbf{x}\\lfloor D_{T}/D_{S}\\rfloor D_{S}$ \u230a$D_{S}$ \u230b$D_{T}$ number of times. , all vector expansion methods pad first $\\zeta$ in vector random expansion is arbitrary, Each method deals with the remaining $\\lfloor D_{T}/D_{S}\\rfloor D_{S}$ enso $\\mathcal{V}_{a\\nu g}$ ,$\\mathcal{V}_{z e r o}$ ,$\\mathcal{V}_{c i r c}\\subset\\mathcal{V}_{r a n d}$ . (3) Here all three examples are expansion methods for vectors. In practice, neural networks like Transformers are dealing high dimensional tensors. These tensors can essentially be thought of as collections of vectors. In such scenarios, we can apply the expansion methods separately to the last dimension of these tensors.  \n\nIn the following claim, we show that vectors expanded by these operators are lossless.  \n\n$\\mathcal{V}_{c i r c}$ V, and vector random expansion m 1. Vector average expansio V$\\gamma_{r a n d}$ $\\mathcal{V}_{a\\nu g},$ are all lossless expansion for vectors. , vector zero expansion $\\mathcal{V}_{z e r o}$ , vector circular expansion Proof. The inverse function $\\mathcal{V}^{-1}:\\mathbb{R}^{D_{T}}\\rightarrow\\mathbb{R}^{D_{S}}$ of these vector expansion methods is  \n\n$$\n\\nu^{-1}({\\bf x})={\\bf x}[:D_{S}].\n$$  \n\nRemark. In practice, we want inverse mapping of expansion methods to be easily computed just like the example above.\n\n# B.4LOSSLESS EXPANSION FOR OPERATORS\nWe then give the definition of lossless expansion for operators. These operators apply on tensors, hence our definition of lossless operator expansion is based on lossless expansion in vector space. These operators can be different layers used in Transformer architectures, including LayerNorm, convolutional layers, and fully-connected layers, etc.  \n\nDefinit ansio der vector spaces $S^{i n},S^{o u t},\\mathcal{T}^{i n}$ and $\\mathcal{T}^{o u t}$ such that with $g(\\cdot):S^{i n}\\rightarrow S^{o u t}$ \u00b7$n(S^{i n})\\leq d i m(T^{i n})$ S\u2192S or space e T. We say the ope and dim $d i m\\big(S^{\\bar{o}u t}\\big)\\leq d i m\\big(T^{o u t}\\big)$ S$\\mathcal{V}_{i n}:S^{i\\bar{n}}\\to\\mathcal{T}^{i n}$ \u2264TEMo is $(\\mathcal{V}_{i n},\\mathcal{V}_{o u t})$ VVess output vector space expansion ppose the op -lossless for $g(\\cdot)$ \u00b7or is denoted if there exist $\\mathcal{V}_{o u t}:S^{o u t}\\to\\mathcal{T}^{o u t}$ VS\u2192T such that V$\\mathcal{V}_{o u t}(g(\\mathbf{x}))=\\mathcal{E}[g](\\mathcal{V}_{i n}(\\mathbf{x})),\\forall\\mathbf{x}\\in S^{i n}$ EV\u2200\u2208S .  \n\n$(\\mathcal{V}_{i n},\\mathcal{V}_{o u t})$ Remark. losslessly expanded input, the output of the to be invertible, we do not have restrictions on the operator expansion VV(1) Intuitively, a lossless operator -lossless for the origina $g(\\cdot)$ \u00b7tput. (2) For conciseness, we use \u2018 \u2019 interchangeably. (3) We only require the v Eexpanded oper pansion can be understood a $^{\\cdot}\\mathcal{E}[g]$ Eis a is $(\\mathcal{V}_{i n},\\mathcal{V}_{o u t})$ EVtor expansions .V$\\nu_{o u t}$ ows: when using losslessly expa -lossles V$\\mathcal{V}_{i n}$ and \u2018 and $\\mathcal{E}$ V$\\nu_{o u t}$ $\\mathcal{V}_{i n}$ ed"}, {"ref_id": "454847819065993190", "chunk_id": "1", "score": 0.345703125, "text": "# 3.3 A TRANSFORMATION PER BLOCK\nNow that every LayerNorm in the transformer has been converted to RMSNorm, we can select any $\\mathbf{Q}$ to modify the model. Our initial plan was to collect signals from the model, construct an orthogonal matrix using those signals and to delete parts of the network. We quickly saw that the signals at different blocks of the network were not aligned, and that we would need to apply a different orthogonal matrix at each block, $\\mathbf{Q}_{\\ell}$ .  \n\nAllowing the orthogonal matrix used in each block to differ can be shown to leave the model unchanged using the same proof as Theorem 1 ,  \n\n  \nFigure 3: Converting a transformer network from LayerNorm to RMSNorm: the scale matrix diag $(\\alpha)$ is absorbed into the subsequent matrix $\\mathbf{W}_{\\mathrm{in}}$ . Figure shows the block in combined colors. We use $(\\alpha)$ for brevity. The mean-subtraction matrix $\\mathbf{M}$ is applied to each matrix $\\mathbf{W}_{\\mathrm{out}}$ . Layernorm becomes RMSNorm, up to a constant $\\bar{\\sqrt{D}}$ (not shown). Here, the scaling $(\\alpha^{\\prime})$ comes from the previous block.  \n\n  \nFigure 4: With the network converted to RMSNorm (see Figure 3 ), we apply the computational-invariance idea. The input weight matrices $\\mathrm{diag}(\\alpha)\\mathbf{W}_{\\mathrm{in}}$ are pre-multiplied by $\\mathbf{Q}^{\\top}$ . The output matrices $\\mathbf{W}_{\\mathrm{out}}\\mathbf{M}$ are post-multiplied by $\\mathbf{Q}$ . In the skip-connection, a new linear layer is added $\\mathbf{Q}_{\\ell}^{\\top}\\mathbf{Q}_{\\ell+1}$ . After these modifications, the matrices can be sliced (hatched areas).  \n\nwith the exception of line 5 of Algorithm 1 . Here we see that the residual connection and the output of the block must have the same rotation. To fix this, we modify the residual connection by applying the linear transformation applied to different blocks with the additional linear operation in the residual connection. Unlike the $\\mathbf{Q}_{\\ell-1}^{\\top}\\mathbf{Q}_{\\ell}$ \u2212to the residual. Figure 4 shows how different rotations can be modifications to the weight matrices, these additional operations cannot be pre-computed and add a small $(D\\times D)$ overhead to the model. Nonetheless, they are needed to allow slicing the model (Section 3.4 ) and we see real speedup overall (Section 4 ).  \n\nTo compute the matrices $\\mathbf{Q}_{\\ell}$ , we use PCA. We select a calibration dataset from the training set, run it through the model (after converting LayerNorm operations into RMSNorm), and extract the orthogonal matrix of the layer. We use the output of the transformed network to calculate the orthogonal matrices of the next layers. More precisely, if $\\mathbf{X}_{\\ell,i}$ is the output of the $\\ell^{\\mathrm{th}}$ RMSNorm block for the $i^{\\mathrm{th}}$ sequence in the calibration dataset, we compute  \n\n$$\n\\mathbf{C}_{\\ell}=\\sum_{i}\\mathbf{X}_{\\ell,i}^{\\top}\\mathbf{X}_{\\ell,i}\n$$  \n\nand set $\\mathbf{Q}_{\\ell}$ to the be the eigenvectors of $\\mathbf{C}_{\\ell}$ , sorted by decreasing eigenvalues.\n\n# 3.4 SLICING\nThe goal of Principal Component Analysis is usually to take a data matrix $\\mathbf{X}$ and compute a lower dimensional representation $\\mathbf{Z}$ , and an approximate reconstruction $\\tilde{\\mathbf{X}}$ :  \n\n$$\n\\mathbf{Z}=\\mathbf{X}\\mathbf{Q}\\mathbf{D}\\,,\\qquad\\tilde{\\mathbf{X}}=\\mathbf{Z}\\mathbf{D}^{\\top}\\mathbf{Q}^{\\top}\\,.\n$$  \n\nwhere $\\mathbf{Q}$ is the ectors of ${\\bf X}^{\\top}{\\bf X}$ , and $\\mathbf{D}$ is a $D\\times D_{\\mathrm{small}}$ deletion matrix (containing $D_{\\mathrm{small}}$ The reconstruction is columns of the $D\\times D$ \u00d7$L_{2}$ identity matrix), which removes some of the columns of the matrix to the left. optimal, in the sense that QD is a linear mapping that minimizes $\\lVert\\mathbf{X}-\\tilde{\\mathbf{X}}\\rVert^{2}$ .  \n\nWhen we apply PCA to the signal matrix $\\mathbf{X}$ bween blocks, we never materialize the $N\\times D$ signal matrix, but we apply the deletion matrix Dto the operations preceding and succeeding the construction of that matrix, which have already been multiplied by $\\mathbf{Q}$ in the above. We delete rows of $\\mathbf{W}_{\\mathrm{in}}$ that we have inserted into the residual connection (see Figure and columns of $\\mathbf{W}_{\\mathrm{out}}$ and $\\mathbf{W}_{\\mathrm{embd}}$ . We also delete both rows 4 ). and columns of the matrix $\\mathbf{Q}_{\\ell-1}^{\\top}\\mathbf{Q}_{\\ell}$ \u2212\n\n# 4 EXPERIMENTAL VALIDATION\nSetup We use HuggingFace Transformers ( Wolf et al. ,2019 ) to implement our code with PyTorch (Paszke et al. ,2019 ). The computation of $\\mathbf{Q}$ is performed on a single H100 GPU with 80GB of memory, taking approximately 3.5 hours to complete for the L LAMA -2 70B model. During the PCA calculation, we use double precision for computing the eigenvectors of the covariance matrix. We find that using single precision for eigenvector calculations in PyTorch leads to a discrepancy in the final accuracy, as detailed in Appendix A.2 .  \n\nWe experiment with two different calibration sets: 1024 samples from the WikiText-2 training dataset ( Merity et al. ,2016 ) and 5000 samples from the Alpaca training dataset ( Taori et al. ,2023 ). Sequence lengths are chosen as the maximum of each language model. An ablation study on the calibration set size and sequence length is presented in Appendix A.3 .  \n\nModels, Tasks, and GPUs We evaluate all our experiments on OPT ( Zhang et al. ,2022 ), L LAMA -2 (Touvron et al. ,2023 ) model families, and additionally evaluate Phi-2 (in our zero-shot task) experiments. We exclude OPT 175B, as it is outperformed by smaller L LAMA -2 models. Nonetheless, we anticipate that this larger model will yield improved results, as larger models typically offer more promising opportunities for compression (see Section 4.1 ). We evaluate our scheme on both language generation as well as popular zero-shot tasks. To demonstrate the comprehensive speedup achieved by SliceGPT we use: Quadro RTX6000 GPUs with 24GB of memory as a representative example of consumer-level GPUs; 40GB A100s and 80GB H100s to provide datacenter-level benchmarks.  \n\nBaseline Setup We initially planned to compare our results against a scheme that pruned columns (or rows) with the smallest norm but found that this baseline was very poor, with the perplexity of the model soaring into the 1000s after pruning just a few columns. Instead, we compare SliceGPT against SparseGPT ( Frantar & Alistarh ,2023 ) employing a 2:4 sparsity ratio, as this is the only sparsity scheme which achieves speedup ( Mishra et al. ,2021 )."}, {"ref_id": "454846008144214678", "chunk_id": "3", "score": 0.3203125, "text": "# 3.3 BIAS NORM\nConformer (Gulati et al., 2020) utilizes LayerNorm (Ba et al., 2016) to normalize the module activations. Given $\\mathbf{x}$ with $D$ channels, LayerNorm is formulated as:  \n\n$$\n\\operatorname{LayerNorm}(\\mathbf{x})={\\frac{\\mathbf{x}-\\operatorname{E}[\\mathbf{x}]}{\\sqrt{\\operatorname{Var}[\\mathbf{x}]+\\epsilon}}}\\odot\\gamma+\\beta.\n$$  \n\nSpecifically, it first computes the mean $\\operatorname{E}[\\mathbf{x}]$ and the standard-deviation $\\sqrt{\\mathrm{Var}[\\mathbf{x}]}$ pfor normalizing, scaling the vector length to $\\sqrt{D}$ . Then it uses the learnable channel-wise scale $\\gamma$ and bias $\\beta$ for transformation, which helps to adjust the size of activations and balance the relative contributions of specific modules. However, we observe that the trained Conformer using LayerNorm suffers from two problems: 1) It sometimes sets one channel to a large constant value, e.g. 50. We argue that it aims to \u201cdefeat\u201d the LayerNorm which fully removes the vector length, functioning as a very large value so that length information could be retained after normalization. 2) Some modules (typically feed-forward or convolution) are \u201cdead\u201d as they have extremely small output values, e.g., $10^{-\\dot{6}}$ .We argue that early in training, the un-trained modules are not useful so they are \u201cturned off\u201d by the LayerNorm scale $\\gamma$ approaching zero. If the scale $\\gamma$ oscillates around zero, the inconsistent sign constantly reverses the gradient directions back-propagating to the modules. Because of the inconsistent gradient sign, the modules never learn anything useful, since this is a bad local optimum which is hard to escape because of the dynamics of stochastic gradient descent-like updates.  \n\nTo address above problems, we propose the BiasNorm which is intended to be a simpler replacement of LayerNorm. Specifically, BiasNorm is formulated as:  \n\n$$\nB i a s N o r m({\\bf x})=\\frac{{\\bf x}}{\\mathrm{RMS}[{\\bf x}-{\\bf b}]}\\cdot\\exp(\\gamma),\n$$  \n\nwhere $\\mathbf{b}$ is the earnable channel-wise bias, $\\mathrm{RMS}[\\mathbf{x}-\\mathbf{b}]$ is the root-mean-square value taken over channels, and \u03b3is a scalar. We first remove the operation of mean subtraction since it is a waste of time unless it follows a non-linearity. The bias bserves as the large constant value which allows to retain the vector length information after normalization. Since the scale $\\exp(\\gamma)$ is always positive, it avoids the gradient oscillation problem.\n\n# 3.4 SWOOSH RAND SWOOSH LACTIVATION FUNCTIONS\nConformer (Gulati et al., 2020) adopts Swish (Ramachandran et al., 2017) activation function with the following formula:  \n\n$$\n\\operatorname{Swish}(x)=x\\cdot(1+\\exp(-x))^{-1}.\n$$  \n\nIn this work, we propose two new activation functions respectively called SwooshR and SwooshL as replacements of Swish:  \n\n$$\n\\begin{array}{r l}&{{S w o o s h R}(x)={\\log(1+{\\exp(x-1)})}-0.08x-0.313261687,}\\\\ &{{S w o o s h L}(x)={\\log(1+{\\exp(x-4)})}-0.08x-0.035.}\\end{array}\n$$  \n\nIn SwooshR , the offset 0.313261687 is to make it pass through the origin; in SwooshL , the offset 0.035 was tuned, which slightly outperformed the value exactly making the curve pass through the origin. We present the curves of Swish, SwooshR , and SwooshL in Appendix Section A.2. SwooshL is roughly a right shifted version of SwooshR . Note that the suffix \u201cL\u201d or \u201cR\u201d represents whether the left or right zero-crossing is at or around $x=0$ . Similar to Swish, SwooshR and SwooshL have lower bounds and are non-monotonic. Compared to Swish, the most striking difference is that SwooshR and SwooshL have non-vanishing slopes for negative inputs, which helps to escape from situations where the input is always negative and prevents the denominator term in Adam-type updates from getting dangerously small. When replacing Swish with SwooshR , we observe that the modules with bypass connections, such as feed-forward and ConvNeXt, tend to learn a large negative bias in the preceding linear layer to learn \u201cnormally-off\u201d behavior. Therefore, we use SwooshL for these \u201cnormally-off\u201d modules and use SwooshR for convolution modules and the rest of Conv-Embed ."}], "task_step_question_answer": "\u5728\u5b9e\u9645\u5e94\u7528\u573a\u666f\u4e2d\uff0c\u5982\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\uff0c\u6570\u636e\u7684\u7a33\u5b9a\u6027\u548c\u7b97\u6cd5\u7684\u5bb9\u9519\u6027\u81f3\u5173\u91cd\u8981\u3002\u4f20\u611f\u5668\u6536\u96c6\u5230\u7684\u56fe\u50cf\u548c\u96f7\u8fbe\u6570\u636e\u53ef\u80fd\u5b58\u5728\u566a\u58f0\u3001\u906e\u6321\u7b49\u95ee\u9898\u3002LayerNorm\u548cRMSNorm\u5728\u5904\u7406\u8fd9\u4e9b\u6570\u636e\u65f6\uff0c\u5176\u7a33\u5b9a\u6027\u548c\u5bb9\u9519\u6027\u76f4\u63a5\u5f71\u54cd\u81ea\u52a8\u9a7e\u9a76\u6a21\u578b\u7684\u51b3\u7b56\u51c6\u786e\u6027\u3002\u4f8b\u5982\uff0c\u5728\u590d\u6742\u8def\u51b5\u4e0b\uff0c\u8f66\u8f86\u4f20\u611f\u5668\u53ef\u80fd\u4f1a\u53d7\u5230\u9633\u5149\u53cd\u5c04\u3001\u96e8\u6c34\u5e72\u6270\u7b49\u5f71\u54cd\uff0c\u6b64\u65f6\u7b97\u6cd5\u80fd\u5426\u51c6\u786e\u5904\u7406\u8fd9\u4e9b\u6709\u566a\u58f0\u7684\u6570\u636e\uff0c\u51b3\u5b9a\u4e86\u8f66\u8f86\u80fd\u5426\u505a\u51fa\u6b63\u786e\u7684\u884c\u9a76\u51b3\u7b56\u3002  ", "ref_task_step_id": ""}, "__type__": "task_step_node"}, "db3d6ad7-14d4-4d13-8ee3-b0b4f63647bd": {"__data__": {"id_": "db3d6ad7-14d4-4d13-8ee3-b0b4f63647bd", "metadata": {}, "relationships": {}, "hash": "", "start_task_context": "\u5927\u6a21\u578b\u4e2d\u7684LayerNorm\u548cRMSNorm\u6709\u4ec0\u4e48\u533a\u522b\uff1f", "aemo_representation_context": "### \u5206\u6790\u8fd1\u51e0\u5e74\u7814\u7a76\u9886\u57df\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba\n\u5728\u5927\u6a21\u578b\u76f8\u5173\u7814\u7a76\u9886\u57df\uff0c\u5f52\u4e00\u5316\u6280\u672f\u662f\u91cd\u8981\u7684\u7814\u7a76\u65b9\u5411\u4e4b\u4e00\u3002LayerNorm \u548c RMSNorm \u4f5c\u4e3a\u5176\u4e2d\u5173\u952e\u7684\u5f52\u4e00\u5316\u65b9\u6cd5\uff0c\u88ab\u4f17\u591a\u7814\u7a76\u6df1\u5165\u63a2\u8ba8\u3002\u7814\u7a76\u8005\u4eec\u4ece\u7406\u8bba\u5206\u6790\u3001\u5b9e\u9a8c\u5bf9\u6bd4\u7b49\u591a\u65b9\u9762\u5bf9\u5b83\u4eec\u8fdb\u884c\u5256\u6790\uff0c\u4ee5\u660e\u786e\u5176\u5728\u5927\u6a21\u578b\u6280\u672f\u6846\u67b6\u4e2d\u7684\u5730\u4f4d\u548c\u4f5c\u7528\u3002\n\n### \u7814\u7a76\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u4e3b\u8981\u6846\u67b6\uff08\u5982 Transformer\u3001GAN\u3001BERT \u7b49\uff09\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0e\u53d8\u4f53\n\u5728 Transformer \u67b6\u6784\u4e3a\u4e3b\u7684\u4f17\u591a\u6a21\u578b\u4e2d\uff0cLayerNorm \u548c RMSNorm \u88ab\u5e7f\u6cdb\u5e94\u7528\u3002\u4f8b\u5982\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\uff08\u5982 BERT \u53ca\u5176\u53d8\u4f53\u6a21\u578b\uff09\u4e2d\uff0cLayerNorm \u6700\u521d\u88ab\u7528\u4e8e\u7a33\u5b9a\u6a21\u578b\u8bad\u7ec3\u3001\u52a0\u901f\u6536\u655b\u4ee5\u53ca\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u80fd\u529b\uff1b\u800c RMSNorm \u4e5f\u9010\u6e10\u5728\u4e00\u4e9b\u57fa\u4e8e Transformer \u7684\u6a21\u578b\u4e2d\u5f97\u5230\u5e94\u7528\uff0c\u4ee5\u6539\u8fdb\u6a21\u578b\u7684\u6027\u80fd\u8868\u73b0\u3002\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\uff08\u5982\u57fa\u4e8e Transformer \u7684\u89c6\u89c9\u6a21\u578b\uff09\u4e2d\u540c\u6837\u6709\u5b83\u4eec\u7684\u8eab\u5f71\uff0c\u4e0d\u540c\u4efb\u52a1\u573a\u666f\u4e0b\u5bf9\u8fd9\u4e24\u79cd\u5f52\u4e00\u5316\u65b9\u6cd5\u8fdb\u884c\u4e86\u9002\u5e94\u6027\u8c03\u6574\u548c\u53d8\u4f53\u5e94\u7528\uff0c\u4ee5\u66f4\u597d\u5730\u9002\u914d\u4efb\u52a1\u9700\u6c42\u3002\n\n### \u8bc4\u4f30\u5b66\u672f\u754c\u7684\u6280\u672f\u8fdb\u6b65\u4e0e\u5c40\u9650\u6027\n - **\u6280\u672f\u8fdb\u6b65**\uff1aLayerNorm \u548c RMSNorm \u7684\u51fa\u73b0\u663e\u8457\u63d0\u5347\u4e86\u5927\u6a21\u578b\u8bad\u7ec3\u7684\u7a33\u5b9a\u6027\u548c\u6548\u7387\u3002LayerNorm \u80fd\u591f\u5bf9\u6bcf\u4e2a\u6837\u672c\u7684\u4e0d\u540c\u7ef4\u5ea6\u8fdb\u884c\u5f52\u4e00\u5316\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5185\u90e8\u534f\u53d8\u91cf\u504f\u79fb\u95ee\u9898\uff1bRMSNorm \u5219\u901a\u8fc7\u66f4\u7b80\u6d01\u7684\u8ba1\u7b97\u65b9\u5f0f\uff0c\u5728\u4e00\u4e9b\u573a\u666f\u4e0b\u53d6\u5f97\u4e86\u4e0e LayerNorm \u76f8\u5f53\u751a\u81f3\u66f4\u597d\u7684\u6548\u679c\uff0c\u63a8\u52a8\u4e86\u5927\u6a21\u578b\u6027\u80fd\u7684\u63d0\u5347\u3002\n - **\u5c40\u9650\u6027**\uff1a\u5c3d\u7ba1\u53d6\u5f97\u8fdb\u6b65\uff0c\u4f46\u4ecd\u5b58\u5728\u4e00\u4e9b\u95ee\u9898\u3002\u4f8b\u5982\uff0c\u5728\u67d0\u4e9b\u590d\u6742\u4efb\u52a1\u548c\u7279\u5b9a\u6570\u636e\u96c6\u4e0a\uff0c\u6a21\u578b\u4f9d\u7136\u5b58\u5728\u6027\u80fd\u74f6\u9888\uff0c\u5373\u4f7f\u4f7f\u7528\u4e86\u8fd9\u4e9b\u5f52\u4e00\u5316\u65b9\u6cd5\uff0c\u6a21\u578b\u504f\u5dee\u548c\u6570\u636e\u4f9d\u8d56\u95ee\u9898\u4f9d\u65e7\u5b58\u5728\u3002\u800c\u4e14\u5728\u9762\u5bf9\u591a\u6a21\u6001\u6570\u636e\u65f6\uff0c\u73b0\u6709\u7684\u5f52\u4e00\u5316\u65b9\u6cd5\u5728\u878d\u5408\u4e0d\u540c\u6a21\u6001\u7279\u5f81\u65b9\u9762\u8fd8\u5b58\u5728\u4e0d\u8db3\u3002\n\n### \u63a2\u8ba8\u8ba1\u7b97\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\n - **\u9002\u7528\u6027**\uff1aLayerNorm \u5728\u591a\u79cd\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6570\u636e\u96c6\u548c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8f83\u597d\u7684\u9002\u7528\u6027\uff0c\u80fd\u591f\u9002\u5e94\u4e0d\u540c\u89c4\u6a21\u548c\u7279\u70b9\u7684\u6570\u636e\u3002RMSNorm \u5728\u4e00\u4e9b\u5927\u89c4\u6a21\u6570\u636e\u548c\u7279\u5b9a\u4efb\u52a1\uff08\u5982\u957f\u5e8f\u5217\u5904\u7406\uff09\u4e2d\u5c55\u73b0\u51fa\u72ec\u7279\u4f18\u52bf\u3002\u7136\u800c\uff0c\u5728\u591a\u9886\u57df\u3001\u591a\u6a21\u6001\u6570\u636e\u573a\u666f\u4e0b\uff0c\u5b83\u4eec\u7684\u9002\u7528\u6027\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u6539\u8fdb\u4ee5\u66f4\u597d\u5730\u5904\u7406\u4e0d\u540c\u7c7b\u578b\u6570\u636e\u7684\u878d\u5408\u3002\n - **\u6cdb\u5316\u80fd\u529b**\uff1a\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\uff0c\u8fd9\u4e24\u79cd\u5f52\u4e00\u5316\u65b9\u6cd5\u6709\u52a9\u4e8e\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u5728\u590d\u6742\u591a\u53d8\u7684\u73b0\u5b9e\u5e94\u7528\u573a\u666f\u4e2d\uff0c\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u4ecd\u6709\u5f85\u63d0\u9ad8\uff0c\u5c24\u5176\u662f\u5728\u8de8\u9886\u57df\u548c\u591a\u6a21\u6001\u6570\u636e\u7684\u6cdb\u5316\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002\n\n### \u5206\u6790\u6700\u65b0\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027\n - **\u7a33\u5b9a\u6027**\uff1a\u7814\u7a76\u53d1\u73b0\uff0cLayerNorm \u901a\u8fc7\u5bf9\u8f93\u5165\u6570\u636e\u7684\u5f52\u4e00\u5316\u5904\u7406\uff0c\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u63d0\u9ad8\u4e86\u6a21\u578b\u8bad\u7ec3\u7684\u7a33\u5b9a\u6027\u3002RMSNorm \u540c\u6837\u5728\u4e00\u4e9b\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u7a33\u5b9a\u6027\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u52a8\u6001\u73af\u5883\u4e0b\uff0c\u90e8\u5206\u7814\u7a76\u9488\u5bf9\u5176\u7a33\u5b9a\u6027\u8fdb\u884c\u4e86\u4f18\u5316\uff0c\u4f7f\u5176\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\u4e5f\u80fd\u4fdd\u6301\u76f8\u5bf9\u7a33\u5b9a\u7684\u8868\u73b0\u3002\n - **\u5bb9\u9519\u6027**\uff1a\u5728\u9762\u5bf9\u6570\u636e\u566a\u58f0\u548c\u5f02\u5e38\u503c\u65f6\uff0c\u8fd9\u4e24\u79cd\u5f52\u4e00\u5316\u65b9\u6cd5\u90fd\u6709\u4e00\u5b9a\u7684\u5bb9\u9519\u80fd\u529b\uff0c\u4f46\u5728\u6781\u7aef\u60c5\u51b5\u4e0b\uff0c\u6a21\u578b\u6027\u80fd\u4ecd\u4f1a\u53d7\u5230\u5f71\u54cd\uff0c\u5bb9\u9519\u6027\u65b9\u9762\u8fd8\u6709\u63d0\u5347\u7a7a\u95f4\u3002\n\n### \u8bc4\u4f30\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218\n - **\u672a\u6765\u7814\u7a76\u65b9\u5411**\uff1a\u8bb8\u591a\u8bba\u6587\u63d0\u51fa\u63a2\u7d22\u66f4\u9002\u7528\u4e8e\u591a\u6a21\u6001\u6570\u636e\u7684\u5f52\u4e00\u5316\u65b9\u6cd5\uff0c\u4ee5\u53ca\u5982\u4f55\u8fdb\u4e00\u6b65\u4f18\u5316 LayerNorm \u548c RMSNorm \u4ee5\u63d0\u5347\u6a21\u578b\u5728\u590d\u6742\u573a\u666f\u4e0b\u7684\u6027\u80fd\u3002\u540c\u65f6\uff0c\u7814\u7a76\u5982\u4f55\u589e\u5f3a\u5f52\u4e00\u5316\u65b9\u6cd5\u5bf9\u6a21\u578b\u504f\u5dee\u548c\u6570\u636e\u4f9d\u8d56\u95ee\u9898\u7684\u89e3\u51b3\u80fd\u529b\u4e5f\u662f\u91cd\u8981\u65b9\u5411\u3002\n - **\u6311\u6218**\uff1a\u65b0\u7684\u7814\u7a76\u95ee\u9898\u5305\u62ec\u5982\u4f55\u8bbe\u8ba1\u4e00\u79cd\u901a\u7528\u7684\u5f52\u4e00\u5316\u7b56\u7565\uff0c\u80fd\u591f\u65e0\u7f1d\u9002\u5e94\u4e0d\u540c\u9886\u57df\u548c\u6a21\u6001\u7684\u6570\u636e\uff1b\u5982\u4f55\u5728\u63d0\u5347\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\uff0c\u8fdb\u4e00\u6b65\u63d0\u9ad8\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u548c\u5bb9\u9519\u6027\u3002\u8fd9\u4e9b\u6311\u6218\u63a8\u52a8\u540e\u7eed\u7814\u7a76\u4e0d\u65ad\u63a2\u7d22\u65b0\u7684\u6280\u672f\u548c\u65b9\u6cd5\uff0c\u4ee5\u6539\u8fdb\u5927\u6a21\u578b\u7684\u6027\u80fd\u548c\u5e94\u7528\u8303\u56f4\u3002\n\n### \u7814\u7a76\u6210\u679c\u3001\u65b9\u6cd5\u7684\u521b\u65b0\u6027\u4e0e\u5e94\u7528\u4ef7\u503c\n - **\u7814\u7a76\u6210\u679c**\uff1a\u660e\u786e\u4e86 LayerNorm \u548c RMSNorm \u5728\u5927\u6a21\u578b\u4e2d\u7684\u6027\u80fd\u8868\u73b0\u3001\u9002\u7528\u573a\u666f\u4ee5\u53ca\u5b58\u5728\u7684\u95ee\u9898\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002\n - **\u65b9\u6cd5\u7684\u521b\u65b0\u6027**\uff1aRMSNorm \u4ee5\u5176\u7b80\u6d01\u7684\u8ba1\u7b97\u65b9\u5f0f\u4e3a\u5f52\u4e00\u5316\u6280\u672f\u5e26\u6765\u65b0\u7684\u601d\u8def\uff0c\u4e0e\u4f20\u7edf\u7684 LayerNorm \u5f62\u6210\u5bf9\u6bd4\u548c\u8865\u5145\uff0c\u63a8\u52a8\u4e86\u5f52\u4e00\u5316\u65b9\u6cd5\u7684\u521b\u65b0\u53d1\u5c55\u3002\n - **\u5e94\u7528\u4ef7\u503c**\uff1a\u8fd9\u4e24\u79cd\u5f52\u4e00\u5316\u65b9\u6cd5\u5728\u5927\u6a21\u578b\u7684\u591a\u4e2a\u5e94\u7528\u9886\u57df\uff08\u5982\u81ea\u7136\u8bed\u8a00\u5904\u7406\u3001\u8ba1\u7b97\u673a\u89c6\u89c9\u7b49\uff09\u90fd\u6709\u91cd\u8981\u5e94\u7528\uff0c\u80fd\u591f\u63d0\u5347\u6a21\u578b\u6027\u80fd\u548c\u8bad\u7ec3\u6548\u7387\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\uff0c\u4e3a\u76f8\u5173\u9886\u57df\u7684\u6280\u672f\u53d1\u5c55\u63d0\u4f9b\u4e86\u6709\u529b\u652f\u6301 \u3002  ", "task_step_name": "\u8bc4\u4f30\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218", "task_step_description": "- **\u672a\u6765\u7814\u7a76\u65b9\u5411**\uff1a\u8bb8\u591a\u8bba\u6587\u63d0\u51fa\u63a2\u7d22\u66f4\u9002\u7528\u4e8e\u591a\u6a21\u6001\u6570\u636e\u7684\u5f52\u4e00\u5316\u65b9\u6cd5\uff0c\u4ee5\u53ca\u5982\u4f55\u8fdb\u4e00\u6b65\u4f18\u5316LayerNorm\u548cRMSNorm\u4ee5\u63d0\u5347\u6a21\u578b\u5728\u590d\u6742\u573a\u666f\u4e0b\u7684\u6027\u80fd\u3002\u540c\u65f6\uff0c\u7814\u7a76\u5982\u4f55\u589e\u5f3a\u5f52\u4e00\u5316\u65b9\u6cd5\u5bf9\u6a21\u578b\u504f\u5dee\u548c\u6570\u636e\u4f9d\u8d56\u95ee\u9898\u7684\u89e3\u51b3\u80fd\u529b\u4e5f\u662f\u91cd\u8981\u65b9\u5411\u3002 - **\u6311\u6218**\uff1a\u65b0\u7684\u7814\u7a76\u95ee\u9898\u5305\u62ec\u5982\u4f55\u8bbe\u8ba1\u4e00\u79cd\u901a\u7528\u7684\u5f52\u4e00\u5316\u7b56\u7565\uff0c\u80fd\u591f\u65e0\u7f1d\u9002\u5e94\u4e0d\u540c\u9886\u57df\u548c\u6a21\u6001\u7684\u6570\u636e\uff1b\u5982\u4f55\u5728\u63d0\u5347\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\uff0c\u8fdb\u4e00\u6b65\u63d0\u9ad8\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u548c\u5bb9\u9519\u6027\u3002\u8fd9\u4e9b\u6311\u6218\u63a8\u52a8\u540e\u7eed\u7814\u7a76\u4e0d\u65ad\u63a2\u7d22\u65b0\u7684\u6280\u672f\u548c\u65b9\u6cd5\uff0c\u4ee5\u6539\u8fdb\u5927\u6a21\u578b\u7684\u6027\u80fd\u548c\u5e94\u7528\u8303\u56f4\u3002", "task_step_level": "5", "task_step_question": "\u4e3a\u4e86\u5e94\u5bf9\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u6311\u6218\uff0c\u5373\u8bbe\u8ba1\u901a\u7528\u5f52\u4e00\u5316\u7b56\u7565\u9002\u5e94\u4e0d\u540c\u9886\u57df\u548c\u6a21\u6001\u6570\u636e\uff0c\u4ee5\u53ca\u5728\u63d0\u5347\u6a21\u578b\u6027\u80fd\u65f6\u63d0\u9ad8\u7b97\u6cd5\u7a33\u5b9a\u6027\u548c\u5bb9\u9519\u6027\uff0c\u76ee\u524d\u6709\u6ca1\u6709\u4e00\u4e9b\u521d\u6b65\u7684\u7814\u7a76\u601d\u8def\u6216\u5c1d\u8bd5\uff1f  ", "task_step_question_context": [{"ref_id": "454848781599813822", "chunk_id": "6", "score": 0.32421875, "text": "# 6. Conclusion\nThis work proposes a simple yet effective method to ease the domain generalization problem. Our method derives from the intuition that a well-generalized model should make robust decisions encountering varying environments. To implement this idea, we introduce the rationale concept, which can be represented as a matrix that collects all the element-wise contributions to the decision-making process for a given sample. To ensure robust outputs, we suggest that the rationale matrices from the same category should remain unchanged, and the idea is fulfilled by enforcing the rationale matrix from a sample to be similar to its corresponding mean value, which is momentum updating during the training process. The overall framework is easy to implement, requiring only a few lines of code upon the baseline. Through extensive experiments on existing benchmarks, we demonstrate that the proposed method can consistently improve the baseline and obtain favorable performances against state-of-the-art models, despite its simplicity.  \n\nAcknowledgements. Liang Chen is supported by the China Scholarship Council (CSC Student ID 202008440331).\n\n\n\n# Supplementary Material\nIn this supplementary material, we provide,  \n\n1. Visualizations of Values from Eq. (3) in the manuscript in Sec. 7 ;2. Sensitive analysis regarding the hyper-parameters in Sec. 8 ;3. Comparison regarding the setting of combing logit and features in Sec. 9 ;4. Evaluations on the DomainBed benchmark using the ResNet50 backbone in Sec. 10 ;\n\n# 7. Visualizations of Values from Eq. (3)\nIn this section, we plot the changes in the sample-tocenter-difference (SCD) values for rationales, features, and logits in Fig. Our observations are as follows: (1) Using crease the three SCD values, which is significant compared 4 (a)-(c) in settings of with a L$\\mathcal{L}_{i n v}$ tends to deithout $\\mathcal{L}_{i n v}$ .to disabling $\\mathcal{L}_{i n v}$ . The results indicate that ERM fails to summarize shared clues to make a robust decision for samples from the same class, explaining why it is less effective in generalizing than ours. (2) When compared to the case of rationales, features, and logits, the SCD values exhibit larger variances throughout iterations, indicating that our $\\mathcal{L}_{i n v}$ allows for some flexibility, enabling features and logits to deviate from their centers. This observation aligns with our suggestion: the contribution of each feature dimension should be jointly modulated by both the feature itself and its corresponding classifier weight.\n\n# 8 . Sensitive Analysis Regarding the HyperParameter Settings\nOur implementation involves two hyper-parameters: the momentum value $m$ in Eq. (4) and the positive weight $\\alpha$ in Eq. (5) in the manuscript. This section evaluates our method with different settings of these two hyper-parameters by conducting experiments on the widely-used PACS dataset [38 ] with a ResNet18 backbone [ 29 ] using the same setting illustrated in Sec. 4.1 in the manuscript, similar to that in [15 ]. Note we fix the value for one hyper-parameter when analyzing another. Results are listed in Table 4 . We observe that our method performs consistently well when the hyperparameter $m$ in the range of [0 .0001 ,0 .1] and $\\alpha$ in the range of [0 .001 ,0 .1] .\n\n# 9 . Comparisons with the Setting Combining Logit and Feature\nAs stated in the manuscript, analyzing the decisionmaking process from either the perspective of feature or logit has intrinsic limitations. Specifically, since the classifier is not taken into account, the model may emphasize heavily on feature elements that with large values but correspond to small weights in the classifier if only consider the feature. Although logit can ease the issue to a certain extent, it only provides a coarse representation for the decision-making process, thus difficult to ensure robust outputs. One may wonder if the combination of feature and logit could avoid the limitation of each other and lead to certain improvements. To answer this question, we conduct further analysis by substituting the rationale invariance constraint with the regularization term that enforces invariance for both the feature and $\\begin{array}{r}{\\mathcal{L}_{i n v}\\;=\\;\\frac{1}{N_{b}}\\sum_{k}\\sum_{\\{n|y_{n}=k\\}}(\\|\\mathbf{z}_{n}-\\overline{{\\mathbf{z}}}_{k}\\|^{2}+\\|\\mathbf{o}_{n}-\\overline{{\\mathbf{o}}}_{k}\\|^{2})}\\end{array}$ logit ( i.e .W/ fea. & log. P), which reformulates Eq. (3) into ,where ${\\bf z}$ ,o,zand oare the feature, logit, and their corresponding momentum updated mean values, respectively. We use the same setting as the original design and test the model in the widely-used PACS dataset [ 38 ] to evaluate its effectiveness.  \n\nExperimental results are listed in Table 5 . We note that combining the feature and logit can lead to improvements for both the two invariance constraints ( i.e . W/ fea. and W/ log.) in almost all target domains. This finding is not surprising since the combined setting considers both the classifier and the feature, thereby mitigating some of the limitations of the two individual settings. However, our rationale invariance regularization still outperforms the combined approach. This is because our rationale concept provides a direct characterization of the decision-making process, encompassing the fine-grained representations of both the features and the weights in the classifier, while the latter can only be coarsely represented in the combined setting."}, {"ref_id": "454895472598857370", "chunk_id": "8", "score": 0.267578125, "text": "# D.2 BASELINE DESCRIPTION\nIn this paragraph, we explain baselines that we used for comparison. Specifically, we compare our method with (1) methods whose objectives are mainly related to Leave-One Out Source Domain Generalization, (2) methods which are mainly modeled for Single Source Domain Generalization, and (3) sharpness-aware minimization related methods, as we reported in tables repeatedly.  \n\nIRM (Arjovsky et al., 2019) tries to learn a data representation such that the optimal classifier matches for all training distributions. Specifically, it minimizes the empirical risk and the regularization term, the multiplication of samples\u2019 gradients, to motivate the invariance of the predictor.  \n\nGroupDRO (Sagawa et al., 2019) minimizes the loss by giving different weight to each domain.   \nWeight term for each domain is proportional to the domain\u2019s current loss.  \n\nOrgMixup (Zhang et al., 2018) represents the naive mixup technique which is generally utilized in machine learning community to boost generalization.  \n\nMixup (Yan et al., 2020) is a mixup among domains.  \n\nCutmix (Yun et al., 2019) is another skill which is widely used in machine learning community to boost generalization. Specifically, it mixes up parts of inputs randomly by pixel-wise.  \n\nMixstyle (Zhou et al., 2021) mix up the statistics (specifically, mean and standard deviation) of the feature. The mixed feature statistics are applied to the style-normalized input. We did not consider the domain label.  \n\nMTL (Blanchard et al., 2021) considers the exponential moving average (EMA) of features.  \n\nMLDG (Li et al., 2018a) is a meta learning based method for domain generalization. Specifically, it simulates the domain shift between train and test during training procedure by synthesizing virtual testing domains within each mini-batch. Then it optimizes meta loss using the synthesized dataset.  \n\n(Li et al., 2018b) minimizes the discrepancy of feature distributions in a every domain pairwise manner, while minimizing the empirical risk for source domains.  \n\nCORAL (Sun & Saenko, 2016) is similar to MMD . However, while MMD employs the gaussian kernel to measure the feature discrepancy, CORAL aligns the second-order statistics between different distributions with a nonlinear transformation. This alignment is achieved by matching the correlations of layer activations in deep neural networks.  \n\nSagNet (Nam et al., 2021) disentangles style features from class categories to prevent bias. Specifically, it makes two networks, content network and style network, and trains both networks to be invariant to other counterpart by giving randomized features (updating the content network with randomized styled features and vice versa).  \n\nARM (Zhang et al., 2021) represents adaptive risk minimization. Specifically, it makes an adaptive risk representing context.  \n\nDANN represents Domain Adversarial Neural Networks, and it iteratively trains a discriminator which discriminates domain and a featurizer to learn a feature which becomes invariant to domain information.  \n\nCDANN is class conditional version of DANN.  \n\nVREx (Krueger et al., 2021) controls the discrepancy between domains by minimizing the variance of loss between domains.  \n\nRSC (Huang et al., 2020) challenges the dominant features of training domain (by masking some specific percentage of dominant gradient), so it can focus on label-related domain invariant features.  \n\nFishr (Rame et al., 2022) approximates the hessian as the variance of gradient matrix, and they align the gradient variance of each domain.  \n\nM-ADA (Qiao et al., 2020a) perturbs input data to simulate the unseen domain data, yet with adequate regularization not to make the data be too far from the original one. The adversarial perturbation direction is affected by the wasserstein autoencoder. Note that this method is specifically designed for Single source domain generalization.  \n\nLTD (Wang et al., 2021a) perturbs source domain data with augmentation network, maximize the mutual information between the original feature and the perturbed feature so that the perturbed feature is not too far from the original feature (with contrastive loss), and maximize likelihood of the original feature. Note that this method is also specifically designed for Single source domain generalization.  \n\nSAM (Foret et al., 2020) is an optimization technique to consider the sharpness of loss surface. It first perturbs parameter to its worst direction, gets gradient and update the calculated gradient at the original parameter point.  \n\nSAGM (Wang et al., 2023) minimizes an original loss, the corresponding perturbed loss, and the gap between them. This optimization aims to identify a minima that is both flat and possesses a sufficiently low loss value. Interpreting the given formula, this optimization inherently regularizes the gradient alignment between the original loss and the perturbed loss.  \n\nGAM (Zhang et al., 2023b) introduces first-order flatness, which minimizes a maximal gradient norm within a perturbation radius, to regularize a stronger flatness than SAM. Accordingly, GAM seeks minima with uniformly flat curvature across all directions.  \n\nRIDG (Chen et al., 2023b) presents a new approach in deep neural networks focusing on decisionmaking in the classifier layer, diverging from the traditional emphasis on features. It introduces a \u2019rationale matrix\u2019, derived from the relationship between features and weights, to guide decisions for each input. A novel regularization term is proposed to align each sample\u2019s rationale with the class\u2019s mean, enhancing stability across samples and domains.  \n\nITTA (Chen et al., 2023a) proposes an Improved Test-Time Adaptation (ITTA) method for domain generalization. ITTA uses a learnable consistency loss for the TTT task to better align with the main prediction task and introduces adaptive parameters in the model, recommending updates solely during the test phase. This approach aims to address the issues of auxiliary task selection and parameter updating in test-time training."}, {"ref_id": "454984230919739446", "chunk_id": "8", "score": 0.25390625, "text": "# Cross-dataset Evaluation\nIn order to showcase the proposed method\u2019s ability to generalize across datasets, we employed the MLRSNet to conduct domain-controlled prompt learning and directly evaluated the model on the remaining seven datasets. The comparative results between our method and other popular algorithms are presented in Table 2. Remarkably, our method achieved superior performance on the MLRSNet, resulting in a substantial performance improvement of nearly $1\\%$ . Furthermore, the most significant performance boost was observed in the RSICD dataset, indicating that the domain-controlled prompt learning approach is particularly well-suited for the RSICD dataset. Although our method did not yield favorable results on the PatternNet and UCM datasets, it surpassed all existing methods in terms of overall performance, with a noteworthy improvement of $1.04\\%$ . These findings demonstrate the effectiveness of our method in terms of crossdataset generalization.\n\n# Domain Generalization\nTo further validate the generalization ability of our proposed method, we conducted an evaluation in the domain generalization setting, adhering to the experimental protocol employed by prior studies. Our approach was compared against other state-of-the-art algorithms, and the comparative results are presented in Table 3. Remarkably, our method consistently outperforms the competing algorithms, achieving the highest average performance with a noteworthy $1.28\\%$ improvement. It is important to note that while our method may encounter challenges when applied to the RSSCN7v2 and UCMv2 datasets, it excels on the RSICDv2 dataset, showcasing an impressive performance gain of $4.84\\%$ . These findings underscore the efficacy of incorporating domaincontrolled prompt learning in enhancing the generalization and robustness of visual-linguistic models like CLIP for the analysis of remote sensing images.  \n\nTable 5: Ablation study of domain-controlled prompt learning in different branches. VC and LC individually denote Visual and Language domain-controlled prompt learning.   \n\n\n<html><body><table><tr><td>Methods</td><td>Base</td><td>Novel</td><td>HM</td></tr><tr><td>Baseline</td><td>97.70</td><td>70.90</td><td>82.17</td></tr><tr><td>Baseline+VC</td><td>97.80</td><td>76.43</td><td>85.80</td></tr><tr><td>Baseline+LC</td><td>97.60</td><td>73.33</td><td>83.74</td></tr><tr><td>Ours</td><td>98.00</td><td>80.00</td><td>88.09</td></tr></table></body></html>  \n\nTable 6: Ablation study of overfitting-tackling strategies.   \n\n\n<html><body><table><tr><td>Methods</td><td>Base</td><td>Novel</td><td>HM</td></tr><tr><td>Baseline</td><td>97.70</td><td>70.90</td><td>82.17</td></tr><tr><td>Dropout(0.3)</td><td>97.78</td><td>77.83</td><td>86.67</td></tr><tr><td>Dropout(0.5)</td><td>97.30</td><td>77.67</td><td>86.38</td></tr><tr><td>Mutation(0.05)</td><td>97.60</td><td>71.67</td><td>82.65</td></tr><tr><td>Mutation(o.1)</td><td>97.20</td><td>71.57</td><td>82.44</td></tr><tr><td>Ours</td><td>98.00</td><td>80.00</td><td>88.09</td></tr></table></body></html>\n\n# Experiments on Other Domain\nTo further validate the effectiveness of our proposed method, we conducted comprehensive experiments on medical domain datasets, including BTMRI (Nickparvar 2021), CHMNIST (Kather et al. 2016), and CCBTM (Hashemi 2023). The comparative results between our method and other advanced algorithms are summarized in Table 4 (Accuracy and HM as metrics). Specifically, our method achieves an impressive $1.66\\%$ performance improvement for base categories and an even more substantial $4.35\\%$ improvement for novel categories. When considering the overall performance metric, Harmonic Mean (HM), our method exhibits a significant $3.63\\%$ improvement compared to other algorithms. These compelling results indicate the robustness and efficacy of our proposed approach in medical domain datasets. Due to the space limitation, we provide more detailed experimental results and analysis in the supplementary material.\n\n# Ablation Study\nDomain-Controlled Prompt Learning. In order to analyze the impact of different components in domain-controlled prompt learning, we conducted separate experiments for both the visual and language branches. The evaluations were performed on the UCM datasets, and the results are summarized in Table 5. It is evident that incorporating domaincontrolled prompt learning in both branches leads to performance improvements. Specifically, controlling the visual branch yields substantial performance gains, particularly in the case of novel categories, resulting in an overall improvement of $3.63\\%$ . On the other hand, domain-controlled prompt learning in the language branch contributes to a relatively lower performance boost but still achieves an overall improvement of $1.57\\%$ . These findings highlight the effectiveness of domain-controlled prompt learning in benefiting both the visual and language branches, ultimately enhancing the accuracy of remote sensing image recognition.  \n\nDifferent Overfitting-Tackling Strategies. In our method, we adopt the proposed noisy-adding strategy to explicitly solve the overfitting problem. As mentioned before, adopting dropout or mutation operations seems to be a plausible solution. Thus, we implement a series of experiments on the UCM dataset to distinguish our method from other strategies, and the experimental results are shown in Table 6. The dropout and mutation operations could both bring overall performance improvements since handling the overfitting problem. The dropout with a rate of 0.3 has a better performance than the dropout with a rate of 0.5, and the mutation with 5 percent has a better performance than the mutation with 10 percent. Though these operations could bring some performance improvements, our proposed noisy-adding strategy could have obviously better performance improvements. This phenomenon suggests the local sampling in dropout and point jittering in mutation are insufficient in escaping suboptimal solutions, yet our method helps the network have a broader solution exploration in a global domain oscillation manner.  \n\n  \nFigure 3: The ablation study of noise-adding strategy across eight remote sensing datasets.  \n\nNoise-Adding Strategy across Datasets. To comprehensively assess the impact of the noise-adding strategy, we conducted experiments across eight diverse remote sensing datasets. The performance gains achieved by incorporating the noise-adding strategy are illustrated in Figure 3. The results demonstrate that the noise-adding strategy consistently improves performance across the majority of datasets, with only minor performance decreases observed in the NWPU and AID datasets. Remarkably, the noise-adding strategy leads to an overall performance improvement of $1.87\\%$ .This observation highlights the effectiveness of the proposed strategy as a generalized approach to mitigate overfitting, thereby boosting performance."}], "task_step_question_answer": "\u5728\u8fdb\u4e00\u6b65\u7684\u7814\u7a76\u4e2d\uff0c\u53ef\u4ee5\u5173\u6ce8\u5f52\u4e00\u5316\u65b9\u6cd5\u5728\u4e0d\u540c\u786c\u4ef6\u5e73\u53f0\u4e0a\u7684\u4f18\u5316\u548c\u90e8\u7f72\u95ee\u9898\u3002\u4f8b\u5982\uff0c\u9488\u5bf9GPU\u548cTPU\u7b49\u9ad8\u6027\u80fd\u8ba1\u7b97\u8bbe\u5907\uff0c\u8bbe\u8ba1\u4e13\u95e8\u7684\u5f52\u4e00\u5316\u7b97\u6cd5\uff0c\u4ee5\u5145\u5206\u5229\u7528\u786c\u4ef6\u7279\u6027\uff0c\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u3002\u6b64\u5916\uff0c\u5f52\u4e00\u5316\u65b9\u6cd5\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u9690\u79c1\u4fdd\u62a4\u673a\u5236\u4e5f\u662f\u4e00\u4e2a\u91cd\u8981\u7814\u7a76\u65b9\u5411\u3002\u5982\u4f55\u5728\u4fdd\u8bc1\u6570\u636e\u9690\u79c1\u7684\u524d\u63d0\u4e0b\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u5f52\u4e00\u5316\u5904\u7406\uff0c\u662f\u4e00\u4e2a\u4e9f\u5f85\u89e3\u51b3\u7684\u95ee\u9898\u3002\u901a\u8fc7\u5f15\u5165\u5dee\u5206\u9690\u79c1\u6280\u672f\uff0c\u53ef\u4ee5\u5728\u4e0d\u6cc4\u9732\u654f\u611f\u4fe1\u606f\u7684\u60c5\u51b5\u4e0b\uff0c\u8fdb\u884c\u6709\u6548\u7684\u6a21\u578b\u8bad\u7ec3\u3002\n\n\u5728\u8de8\u9886\u57df\u5e94\u7528\u65b9\u9762\uff0c\u5f52\u4e00\u5316\u65b9\u6cd5\u7684\u8fc1\u79fb\u5b66\u4e60\u80fd\u529b\u4e5f\u503c\u5f97\u6df1\u5165\u63a2\u8ba8\u3002\u4f8b\u5982\uff0c\u5728\u5c06\u81ea\u7136\u8bed\u8a00\u5904\u7406\u9886\u57df\u7684\u5f52\u4e00\u5316\u7b56\u7565\u8fc1\u79fb\u5230\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u65f6\uff0c\u5982\u4f55\u81ea\u9002\u5e94\u5730\u8c03\u6574\u53c2\u6570\uff0c\u4ee5\u9002\u5e94\u4e0d\u540c\u9886\u57df\u7684\u6570\u636e\u7279\u5f81\uff0c\u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u95ee\u9898\u3002\u901a\u8fc7\u8bbe\u8ba1\u9886\u57df\u81ea\u9002\u5e94\u7684\u5f52\u4e00\u5316\u673a\u5236\uff0c\u53ef\u4ee5\u5728\u51cf\u5c11\u9886\u57df\u95f4\u5dee\u5f02\u7684\u540c\u65f6\uff0c\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002\n\n\u6700\u540e\uff0c\u5f52\u4e00\u5316\u65b9\u6cd5\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u53ef\u89e3\u91ca\u6027\u548c\u900f\u660e\u5ea6\u4e5f\u662f\u4e00\u4e2a\u91cd\u8981\u7814\u7a76\u65b9\u5411\u3002\u5982\u4f55\u8bbe\u8ba1\u53ef\u89e3\u91ca\u7684\u5f52\u4e00\u5316\u7b56\u7565\uff0c\u4f7f\u7814\u7a76\u4eba\u5458\u548c\u7528\u6237\u80fd\u591f\u7406\u89e3\u6a21\u578b\u7684\u51b3\u7b56\u8fc7\u7a0b\uff0c\u63d0\u5347\u6a21\u578b\u7684\u4fe1\u4efb\u5ea6\uff0c\u662f\u4e00\u4e2a\u503c\u5f97\u5173\u6ce8\u7684\u95ee\u9898\u3002\u901a\u8fc7\u5f15\u5165\u53ef\u89e3\u91ca\u6027\u5f3a\u7684\u5f52\u4e00\u5316\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728\u4fdd\u8bc1\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\uff0c\u63d0\u5347\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u63a5\u53d7\u5ea6\u3002", "ref_task_step_id": ""}, "__type__": "task_step_node"}, "30b2ef30-75d2-40c5-b100-d2fefb6cb330": {"__data__": {"id_": "30b2ef30-75d2-40c5-b100-d2fefb6cb330", "metadata": {}, "relationships": {}, "hash": "", "start_task_context": "\u5927\u6a21\u578b\u4e2d\u7684LayerNorm\u548cRMSNorm\u6709\u4ec0\u4e48\u533a\u522b\uff1f", "aemo_representation_context": "### \u5206\u6790\u8fd1\u51e0\u5e74\u7814\u7a76\u9886\u57df\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba\n\u5728\u5927\u6a21\u578b\u76f8\u5173\u7814\u7a76\u9886\u57df\uff0c\u5f52\u4e00\u5316\u6280\u672f\u662f\u91cd\u8981\u7684\u7814\u7a76\u65b9\u5411\u4e4b\u4e00\u3002LayerNorm \u548c RMSNorm \u4f5c\u4e3a\u5176\u4e2d\u5173\u952e\u7684\u5f52\u4e00\u5316\u65b9\u6cd5\uff0c\u88ab\u4f17\u591a\u7814\u7a76\u6df1\u5165\u63a2\u8ba8\u3002\u7814\u7a76\u8005\u4eec\u4ece\u7406\u8bba\u5206\u6790\u3001\u5b9e\u9a8c\u5bf9\u6bd4\u7b49\u591a\u65b9\u9762\u5bf9\u5b83\u4eec\u8fdb\u884c\u5256\u6790\uff0c\u4ee5\u660e\u786e\u5176\u5728\u5927\u6a21\u578b\u6280\u672f\u6846\u67b6\u4e2d\u7684\u5730\u4f4d\u548c\u4f5c\u7528\u3002\n\n### \u7814\u7a76\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u4e3b\u8981\u6846\u67b6\uff08\u5982 Transformer\u3001GAN\u3001BERT \u7b49\uff09\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0e\u53d8\u4f53\n\u5728 Transformer \u67b6\u6784\u4e3a\u4e3b\u7684\u4f17\u591a\u6a21\u578b\u4e2d\uff0cLayerNorm \u548c RMSNorm \u88ab\u5e7f\u6cdb\u5e94\u7528\u3002\u4f8b\u5982\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\uff08\u5982 BERT \u53ca\u5176\u53d8\u4f53\u6a21\u578b\uff09\u4e2d\uff0cLayerNorm \u6700\u521d\u88ab\u7528\u4e8e\u7a33\u5b9a\u6a21\u578b\u8bad\u7ec3\u3001\u52a0\u901f\u6536\u655b\u4ee5\u53ca\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u80fd\u529b\uff1b\u800c RMSNorm \u4e5f\u9010\u6e10\u5728\u4e00\u4e9b\u57fa\u4e8e Transformer \u7684\u6a21\u578b\u4e2d\u5f97\u5230\u5e94\u7528\uff0c\u4ee5\u6539\u8fdb\u6a21\u578b\u7684\u6027\u80fd\u8868\u73b0\u3002\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\uff08\u5982\u57fa\u4e8e Transformer \u7684\u89c6\u89c9\u6a21\u578b\uff09\u4e2d\u540c\u6837\u6709\u5b83\u4eec\u7684\u8eab\u5f71\uff0c\u4e0d\u540c\u4efb\u52a1\u573a\u666f\u4e0b\u5bf9\u8fd9\u4e24\u79cd\u5f52\u4e00\u5316\u65b9\u6cd5\u8fdb\u884c\u4e86\u9002\u5e94\u6027\u8c03\u6574\u548c\u53d8\u4f53\u5e94\u7528\uff0c\u4ee5\u66f4\u597d\u5730\u9002\u914d\u4efb\u52a1\u9700\u6c42\u3002\n\n### \u8bc4\u4f30\u5b66\u672f\u754c\u7684\u6280\u672f\u8fdb\u6b65\u4e0e\u5c40\u9650\u6027\n - **\u6280\u672f\u8fdb\u6b65**\uff1aLayerNorm \u548c RMSNorm \u7684\u51fa\u73b0\u663e\u8457\u63d0\u5347\u4e86\u5927\u6a21\u578b\u8bad\u7ec3\u7684\u7a33\u5b9a\u6027\u548c\u6548\u7387\u3002LayerNorm \u80fd\u591f\u5bf9\u6bcf\u4e2a\u6837\u672c\u7684\u4e0d\u540c\u7ef4\u5ea6\u8fdb\u884c\u5f52\u4e00\u5316\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5185\u90e8\u534f\u53d8\u91cf\u504f\u79fb\u95ee\u9898\uff1bRMSNorm \u5219\u901a\u8fc7\u66f4\u7b80\u6d01\u7684\u8ba1\u7b97\u65b9\u5f0f\uff0c\u5728\u4e00\u4e9b\u573a\u666f\u4e0b\u53d6\u5f97\u4e86\u4e0e LayerNorm \u76f8\u5f53\u751a\u81f3\u66f4\u597d\u7684\u6548\u679c\uff0c\u63a8\u52a8\u4e86\u5927\u6a21\u578b\u6027\u80fd\u7684\u63d0\u5347\u3002\n - **\u5c40\u9650\u6027**\uff1a\u5c3d\u7ba1\u53d6\u5f97\u8fdb\u6b65\uff0c\u4f46\u4ecd\u5b58\u5728\u4e00\u4e9b\u95ee\u9898\u3002\u4f8b\u5982\uff0c\u5728\u67d0\u4e9b\u590d\u6742\u4efb\u52a1\u548c\u7279\u5b9a\u6570\u636e\u96c6\u4e0a\uff0c\u6a21\u578b\u4f9d\u7136\u5b58\u5728\u6027\u80fd\u74f6\u9888\uff0c\u5373\u4f7f\u4f7f\u7528\u4e86\u8fd9\u4e9b\u5f52\u4e00\u5316\u65b9\u6cd5\uff0c\u6a21\u578b\u504f\u5dee\u548c\u6570\u636e\u4f9d\u8d56\u95ee\u9898\u4f9d\u65e7\u5b58\u5728\u3002\u800c\u4e14\u5728\u9762\u5bf9\u591a\u6a21\u6001\u6570\u636e\u65f6\uff0c\u73b0\u6709\u7684\u5f52\u4e00\u5316\u65b9\u6cd5\u5728\u878d\u5408\u4e0d\u540c\u6a21\u6001\u7279\u5f81\u65b9\u9762\u8fd8\u5b58\u5728\u4e0d\u8db3\u3002\n\n### \u63a2\u8ba8\u8ba1\u7b97\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\n - **\u9002\u7528\u6027**\uff1aLayerNorm \u5728\u591a\u79cd\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6570\u636e\u96c6\u548c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8f83\u597d\u7684\u9002\u7528\u6027\uff0c\u80fd\u591f\u9002\u5e94\u4e0d\u540c\u89c4\u6a21\u548c\u7279\u70b9\u7684\u6570\u636e\u3002RMSNorm \u5728\u4e00\u4e9b\u5927\u89c4\u6a21\u6570\u636e\u548c\u7279\u5b9a\u4efb\u52a1\uff08\u5982\u957f\u5e8f\u5217\u5904\u7406\uff09\u4e2d\u5c55\u73b0\u51fa\u72ec\u7279\u4f18\u52bf\u3002\u7136\u800c\uff0c\u5728\u591a\u9886\u57df\u3001\u591a\u6a21\u6001\u6570\u636e\u573a\u666f\u4e0b\uff0c\u5b83\u4eec\u7684\u9002\u7528\u6027\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u6539\u8fdb\u4ee5\u66f4\u597d\u5730\u5904\u7406\u4e0d\u540c\u7c7b\u578b\u6570\u636e\u7684\u878d\u5408\u3002\n - **\u6cdb\u5316\u80fd\u529b**\uff1a\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\uff0c\u8fd9\u4e24\u79cd\u5f52\u4e00\u5316\u65b9\u6cd5\u6709\u52a9\u4e8e\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u5728\u590d\u6742\u591a\u53d8\u7684\u73b0\u5b9e\u5e94\u7528\u573a\u666f\u4e2d\uff0c\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u4ecd\u6709\u5f85\u63d0\u9ad8\uff0c\u5c24\u5176\u662f\u5728\u8de8\u9886\u57df\u548c\u591a\u6a21\u6001\u6570\u636e\u7684\u6cdb\u5316\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002\n\n### \u5206\u6790\u6700\u65b0\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519\u6027\n - **\u7a33\u5b9a\u6027**\uff1a\u7814\u7a76\u53d1\u73b0\uff0cLayerNorm \u901a\u8fc7\u5bf9\u8f93\u5165\u6570\u636e\u7684\u5f52\u4e00\u5316\u5904\u7406\uff0c\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u63d0\u9ad8\u4e86\u6a21\u578b\u8bad\u7ec3\u7684\u7a33\u5b9a\u6027\u3002RMSNorm \u540c\u6837\u5728\u4e00\u4e9b\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u7a33\u5b9a\u6027\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u52a8\u6001\u73af\u5883\u4e0b\uff0c\u90e8\u5206\u7814\u7a76\u9488\u5bf9\u5176\u7a33\u5b9a\u6027\u8fdb\u884c\u4e86\u4f18\u5316\uff0c\u4f7f\u5176\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\u4e5f\u80fd\u4fdd\u6301\u76f8\u5bf9\u7a33\u5b9a\u7684\u8868\u73b0\u3002\n - **\u5bb9\u9519\u6027**\uff1a\u5728\u9762\u5bf9\u6570\u636e\u566a\u58f0\u548c\u5f02\u5e38\u503c\u65f6\uff0c\u8fd9\u4e24\u79cd\u5f52\u4e00\u5316\u65b9\u6cd5\u90fd\u6709\u4e00\u5b9a\u7684\u5bb9\u9519\u80fd\u529b\uff0c\u4f46\u5728\u6781\u7aef\u60c5\u51b5\u4e0b\uff0c\u6a21\u578b\u6027\u80fd\u4ecd\u4f1a\u53d7\u5230\u5f71\u54cd\uff0c\u5bb9\u9519\u6027\u65b9\u9762\u8fd8\u6709\u63d0\u5347\u7a7a\u95f4\u3002\n\n### \u8bc4\u4f30\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218\n - **\u672a\u6765\u7814\u7a76\u65b9\u5411**\uff1a\u8bb8\u591a\u8bba\u6587\u63d0\u51fa\u63a2\u7d22\u66f4\u9002\u7528\u4e8e\u591a\u6a21\u6001\u6570\u636e\u7684\u5f52\u4e00\u5316\u65b9\u6cd5\uff0c\u4ee5\u53ca\u5982\u4f55\u8fdb\u4e00\u6b65\u4f18\u5316 LayerNorm \u548c RMSNorm \u4ee5\u63d0\u5347\u6a21\u578b\u5728\u590d\u6742\u573a\u666f\u4e0b\u7684\u6027\u80fd\u3002\u540c\u65f6\uff0c\u7814\u7a76\u5982\u4f55\u589e\u5f3a\u5f52\u4e00\u5316\u65b9\u6cd5\u5bf9\u6a21\u578b\u504f\u5dee\u548c\u6570\u636e\u4f9d\u8d56\u95ee\u9898\u7684\u89e3\u51b3\u80fd\u529b\u4e5f\u662f\u91cd\u8981\u65b9\u5411\u3002\n - **\u6311\u6218**\uff1a\u65b0\u7684\u7814\u7a76\u95ee\u9898\u5305\u62ec\u5982\u4f55\u8bbe\u8ba1\u4e00\u79cd\u901a\u7528\u7684\u5f52\u4e00\u5316\u7b56\u7565\uff0c\u80fd\u591f\u65e0\u7f1d\u9002\u5e94\u4e0d\u540c\u9886\u57df\u548c\u6a21\u6001\u7684\u6570\u636e\uff1b\u5982\u4f55\u5728\u63d0\u5347\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\uff0c\u8fdb\u4e00\u6b65\u63d0\u9ad8\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u548c\u5bb9\u9519\u6027\u3002\u8fd9\u4e9b\u6311\u6218\u63a8\u52a8\u540e\u7eed\u7814\u7a76\u4e0d\u65ad\u63a2\u7d22\u65b0\u7684\u6280\u672f\u548c\u65b9\u6cd5\uff0c\u4ee5\u6539\u8fdb\u5927\u6a21\u578b\u7684\u6027\u80fd\u548c\u5e94\u7528\u8303\u56f4\u3002\n\n### \u7814\u7a76\u6210\u679c\u3001\u65b9\u6cd5\u7684\u521b\u65b0\u6027\u4e0e\u5e94\u7528\u4ef7\u503c\n - **\u7814\u7a76\u6210\u679c**\uff1a\u660e\u786e\u4e86 LayerNorm \u548c RMSNorm \u5728\u5927\u6a21\u578b\u4e2d\u7684\u6027\u80fd\u8868\u73b0\u3001\u9002\u7528\u573a\u666f\u4ee5\u53ca\u5b58\u5728\u7684\u95ee\u9898\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002\n - **\u65b9\u6cd5\u7684\u521b\u65b0\u6027**\uff1aRMSNorm \u4ee5\u5176\u7b80\u6d01\u7684\u8ba1\u7b97\u65b9\u5f0f\u4e3a\u5f52\u4e00\u5316\u6280\u672f\u5e26\u6765\u65b0\u7684\u601d\u8def\uff0c\u4e0e\u4f20\u7edf\u7684 LayerNorm \u5f62\u6210\u5bf9\u6bd4\u548c\u8865\u5145\uff0c\u63a8\u52a8\u4e86\u5f52\u4e00\u5316\u65b9\u6cd5\u7684\u521b\u65b0\u53d1\u5c55\u3002\n - **\u5e94\u7528\u4ef7\u503c**\uff1a\u8fd9\u4e24\u79cd\u5f52\u4e00\u5316\u65b9\u6cd5\u5728\u5927\u6a21\u578b\u7684\u591a\u4e2a\u5e94\u7528\u9886\u57df\uff08\u5982\u81ea\u7136\u8bed\u8a00\u5904\u7406\u3001\u8ba1\u7b97\u673a\u89c6\u89c9\u7b49\uff09\u90fd\u6709\u91cd\u8981\u5e94\u7528\uff0c\u80fd\u591f\u63d0\u5347\u6a21\u578b\u6027\u80fd\u548c\u8bad\u7ec3\u6548\u7387\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\uff0c\u4e3a\u76f8\u5173\u9886\u57df\u7684\u6280\u672f\u53d1\u5c55\u63d0\u4f9b\u4e86\u6709\u529b\u652f\u6301 \u3002  ", "task_step_name": "\u7814\u7a76\u6210\u679c\u3001\u65b9\u6cd5\u7684\u521b\u65b0\u6027\u4e0e\u5e94\u7528\u4ef7\u503c", "task_step_description": "- **\u7814\u7a76\u6210\u679c**\uff1a\u660e\u786e\u4e86LayerNorm\u548cRMSNorm\u5728\u5927\u6a21\u578b\u4e2d\u7684\u6027\u80fd\u8868\u73b0\u3001\u9002\u7528\u573a\u666f\u4ee5\u53ca\u5b58\u5728\u7684\u95ee\u9898\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002 - **\u65b9\u6cd5\u7684\u521b\u65b0\u6027**\uff1aRMSNorm\u4ee5\u5176\u7b80\u6d01\u7684\u8ba1\u7b97\u65b9\u5f0f\u4e3a\u5f52\u4e00\u5316\u6280\u672f\u5e26\u6765\u65b0\u7684\u601d\u8def\uff0c\u4e0e\u4f20\u7edf\u7684LayerNorm\u5f62\u6210\u5bf9\u6bd4\u548c\u8865\u5145\uff0c\u63a8\u52a8\u4e86\u5f52\u4e00\u5316\u65b9\u6cd5\u7684\u521b\u65b0\u53d1\u5c55\u3002 - **\u5e94\u7528\u4ef7\u503c**\uff1a\u8fd9\u4e24\u79cd\u5f52\u4e00\u5316\u65b9\u6cd5\u5728\u5927\u6a21\u578b\u7684\u591a\u4e2a\u5e94\u7528\u9886\u57df\uff08\u5982\u81ea\u7136\u8bed\u8a00\u5904\u7406\u3001\u8ba1\u7b97\u673a\u89c6\u89c9\u7b49\uff09\u90fd\u6709\u91cd\u8981\u5e94\u7528\uff0c\u80fd\u591f\u63d0\u5347\u6a21\u578b\u6027\u80fd\u548c\u8bad\u7ec3\u6548\u7387\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\uff0c\u4e3a\u76f8\u5173\u9886\u57df\u7684\u6280\u672f\u53d1\u5c55\u63d0\u4f9b\u4e86\u6709\u529b\u652f\u6301 \u3002", "task_step_level": "6", "task_step_question": "LayerNorm \u548c RMSNorm \u7684\u7814\u7a76\u6210\u679c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u54ea\u4e9b\u5177\u4f53\u7684\u91cd\u8981\u53c2\u8003\uff1f\u5b83\u4eec\u5728\u4e0d\u540c\u5e94\u7528\u9886\u57df\u63d0\u5347\u6a21\u578b\u6027\u80fd\u548c\u8bad\u7ec3\u6548\u7387\u7684\u5177\u4f53\u8868\u73b0\u548c\u4f5c\u7528\u673a\u5236\u662f\u4ec0\u4e48\uff1f ", "task_step_question_context": [{"ref_id": "454847819065993190", "chunk_id": "1", "score": 0.486328125, "text": "# 4 EXPERIMENTAL RESULTS\n\n# 4.1 TUNING LAYER NORM\nTuning LayerNorm in Attention Blocks. In table 1 , it is noteworthy that activating only the LayerNorm yields the least activated parameters, yet the model performances are surprisingly impressive when compared to tuning other modules. Specifically, in two captioning tasks, the VQAv2 task, and the challenging hallucination benchmark POPE , models with only the LayerNorm activated consistently outperform all other competitors by at least $8.2\\%$ . On the comprehensively evaluated benchmark MME , while tuning LayerNorm outperforms finetuning the intact language model by an average of $6.6\\%$ on the Perception aspect, it lags behind finetuning by an average of $6.3\\%$ on the Cognition score. It is vital to note, however, that the LayerNorm only accounts for approximately $2.5\\%$ of the training parameters in the whole model.  \n\nIn addition to tuning modules, another observation is that MLLMs incorporating human-aligned LLMs (such as LL A MA2- CHAT ) exhibit superior performance in complex and demanding tasks such as POPE and MME compared to their unaligned counterparts. This underscores the importance of utilizing aligned LLMs to construct a more powerful MLLMs.  \n\nTuning LayerNorm and Only LayerNorm. As the above LayerNorm method finetunes (1) visionlanguage connector, (2) word embedding, (3) output head, and (4) LayerNorm component in the LLM simultaneously, a pertinent question arises: Is it possible for (4) LayerNorm alone to generalize effectively in training MLLMs? To address this query, we take a step further and solely finetune LayerNorm in MLLMs, which is denoted as LayerNorm-simp. in table 1 . The results are intriguing, demonstrating that even with a mere $0.004\\%$ parameter finetuning in the whole model, LayerNormsimp. surpasses full parameter finetuning on three conventional vision-language tasks (i.e., two captioning and one VQA tasks) by $10\\%$ , and only lags behind full finetuning by $7.9\\%$ on the MME benchmark. This intriguing discovery suggests that the transition from LLM to MLLMs probably involves a domain adaptation process as the LayerNorm takes the most credits in tuning a wellbehaved MLLMs. The LayerNorm alone may be also capable of integrating vision information with language tokens seamlessly.  \n\nMemory Consumption and Parameter Efficiency. In table 2 , we present the total memory consumption and the percentage of trainable parameters of each MLLMs finetuning method across 7B and 13B scales. Methods like full parameter finetuning and finetuning MLPs in attention modules face out-of-memory (OOM) issue even on a high-capacity 80GB A100 GPU, while LayerNorm based methods stand out for their efficiency. Specifically, LayerNorm tuning requires only $24.2\\,\\mathrm{GB}$ and $38.3\\,\\mathrm{GB}$ memory at 7B and 13B scales respectively. Remarkably, LayerNorm-simp. further reduces the memory to $18.9\\,\\mathrm{GB}$ and 31.7 GB. In terms of trainable parameters, LayerNorm based methods also show remarkable efficiency, LayerNorm utilizes only $3.78\\%$ and $2.50\\%$ of the total parameters at the 7B and 13B scales, and LayerNorm-simp. takes efficiency to an extreme, involving only $0.004\\%$ and $0.003\\%$ of the parameters at these scales. These results demonstrate the efficiency advantage of LayerNorm tuning, compared with existing methods like LoRA or full parameter finetuning.  \n\n  \nFigure 2: Performances of models that are finetuned on different datasets on four multi-modal benchmarks. The MME score is the sum of both Cognition and Perception scores on the benchmark.\n\n# 4.2 \u2018L ESS IS MORE \u2019ON BOTH DATA AND PARAMETER SIDES\nEfficiency in training can also be improved by considering the data used in LLMs and MLLMs ( Zhou et al. ,2023 ;Wei et al. ,2023 ). To this end, we conducted experiments using LL A MA2-7B and LL A MA2-7BCHAT , where we divided the training data into three categories, each comprising 20K data points: image-grounded conversation, image detail descriptions, and image-based complex reasoning, as previously deployed in Liu et al. (2023 ). Based on the results presented in fig. 2 , we observe that the image-grounded conversation data is the most effective in enhancing the multi-modal capabilities of the model, with an average improvement of over $50\\%$ compared to other data types. This highlights the potential benefits of a targeted approach that leverages the strengths of specific data types to facilitate more nuanced and effective multi-modal tuning for MLLMs.  \n\nTo validate \u2018Less is More\u2019 on both the data and parameter sides, we present results of MLLMs with LayerNorm activated in LLM and tuned on 20k conversational data in table 3 . Our experimental results indicate that even with a smaller dataset and the use of LayerNorm tuning, the model outperforms the full parameter finetuning approach on the full 80K dataset by $18.4\\%$ on two captioning tasks, and only falls short in MME by a tolerable $2.5\\%$ . It is noteworthy that LayerNorm with 20K data is only $7.6\\%$ and $7.4\\%$ behind LayerNorm on the full 80K dataset for two captioning tasks and MME task, respectively. These findings demonstrate that \u2018Less is More\u2019 for both the parameter and data perspectives beyond language domain Zhou et al. (2023 ), but for multi-modal tuning."}, {"ref_id": "454846008172788376", "chunk_id": "4", "score": 0.427734375, "text": "# 5.3 LAYER NORM TUNING HAS SMALLER GRADIENT VARIANCE\nA well accepted view about LayerNorm is that, as the neural network goes deeper, the mean of LayerNorm gradients should goes to zero as the LayerNorm itself is designed to normalize all training parameters. In the meantime, the variance of LayerNorm gradients should be small to ensure a better generalization ability of the model ( Xu et al. ,2019 ) (See the proof in Appendix A.2.2 ). As we presented in fig. 4 , MLLM with LayerNorm tuning method has a more concentrated LayerNorm gradients than fine-tuning during the training process. This result gives another view on the effectiveness of LayerNorm from the optimization perspective. More visualizations are listed in Appendix A.2.2 .\n\n# 6 CONCLUSION AND DISCUSSIONS\nLayerNorm is effective and sufficient built upon MLLM pre-training. MLLM training typically involves pre-training on image-text pairs followed by finetuning on visual instruction data. While the second stage of training receives more attention, it is worth noting that the function of the first stage pre-training is non-negligible for training a competent MLLM. We have presented in the paper only a small portion of parameter activation is sufficient to tune a well-behaved MLLM. However, other models such as I NSTRUCT BLIP ( Dai et al. ,2023 ) and M INI GPT4 ( Zhu et al. ,2023 ) only tune the vision-language connector, leaving the LLM untouched during the second stage of training. These models have yielded strong performances when given a large-scale finetuning dataset. In Sec. 5.1 , we demonstrate that tuning LayerNorm may be a more effective means for the second stage training, especially when compared to existing parameter-efficient methods for training MLLMs.  \n\nLimitations. One shortcoming of these parameter-efficient finetuning methods is that they are more sensitive to hyper-parameters ( e.g ., learning rate, training epoch) than finetuning. Since the number of trainable parameters of LayerNorm is small, the model performance of LayerNorm method also varies when twitching the training hyper-parameters. This drawback calls for potential future investigations on the LayerNorm tuning method. In the Appendix A.1 , we give a hint for the grid search range of learning rate on both 7B and 13B scaled models using LayerNorm tuning based on our experimental results.  \n\nConclusion. Our studies demonstrate LayerNorm tuning as a simple yet effective tuning method for adapting LLMs comprehend multi-modal content across various model variants. Compared to LoRA tuning or full parameter finetuning, LayerNorm tuning reduces the trainable parameters by a significant $41.9\\%$ , enabling efficient finetuning of MLLMs on consumer-grade GPUs. Moreover, we demonstrate that MLLMs can achieve exceptional performance with minimal \u201cright\u201d data and parameters, showcasing the potential of LayerNorm tuning method in real-world applications. Given the empirical success of LayerNorm tuning, we revisited the MLLM finetuning from a domain adaptation perspective and showed that LayerNorm plays a critical role in adapting LLMs to the multi-modal domain. Additionally, our research illustrates the expressive power and optimization potential of LayerNorm tuning from layer similarities and the gradient variance. We hope that our work could inspire future works on designing improved PEFT methods that enable more diverse application scenarios for MLLMs.\n\n\n\n# A A PPENDIX\n\n# A.1 TRAINING DETAILS\nFor the first stage, we set the learning rate to 2e-3 for all variants. During the second stage, we search learning the learning rate from [2e-3, 1e-3, 6e-4, 3e-4, 1e-4, 5e-5, 2e-5, 1e-5, 6e-6, 1e-6, 1e-7] for all models and pick the best learning rate based on their performances on the CIDEr score on the Flickr $30\\,\\mathrm{k}$ task.  \n\nAccording to our tryouts based on Flickr $30\\,\\mathrm{k}$ results in Table A1 , the recommended learning rate for 7B scale is between 6e-4 to 2e-3, while on the 13B, the learning rate should be searched in the range of 3e-6 to 6e-5.  \n\nTable A1: Performance of MLLMs (LayerNorm-simp.) trained with different learning rates and scales on the Flickr30k task.   \n\n\n<html><body><table><tr><td>Learning Rate</td><td>3e-6</td><td>1e-5</td><td>3e-5</td><td>6e-5</td></tr><tr><td>MM-LLAMA2 7B</td><td>21.42</td><td>32.45</td><td>43.04</td><td>28.24</td></tr><tr><td>Learning Rate</td><td>6e-4</td><td>1e-3</td><td>2e-3</td><td></td></tr><tr><td>MM-LLAMA213B</td><td>37.35</td><td>46.88</td><td>44.15</td><td></td></tr></table></body></html>\n\n# A.2INSIGHTS OFLAYERNORMTUNING\n\n# A.2.1 VISUALIZATION EXAMPLES OF LAYER SIMILARITIES\nLower similarities between different layers of the transformer indicates more expressive power ( Pires et al. ,2023 ). In section 5.2 , we have shown the computed cosine similarity between layers on a Vicuna model, here we show the layer similarities between layers on LL A MA2 and LL A MA2 CHAT models in fig. A1 and fig. A2 . It is clear that, LayerNorm tuning again allows the model to learn dissimilar layer representations, improving the expressive power of the model.\n\n# A.2.2 GRADIENTS OF LAYER NORM\nVisualization examples of LayerNorm gradients. In fig. A3 and fig. A4 , we present the gradients of the LayerNorm parameters during the training process. Similar to the one we have shown in the main text, LayerNorm tuning demonstrates a smaller gradient variance which is important for converging to a better local minimum ( Xu et al. ,2019 ).  \n\nProof of smaller variance in LayerNorm . As stated in Sec. 5.3 , deeper the network is, the variance of LayerNorm in the model should be naturally smaller ( $\\mathrm{\\DeltaXu}$ et al. ,2019 ). We first let $\\mathbf{y}\\,=\\,(y_{1},y_{2},...,y_{N})$ be the normalized vector, meaning the mean and variance of $\\mathbf{y}$ is 0 and 1 ,respectively. We can then formulate the standard LayerNorm as follow:  \n\n$$\n{\\mathbf{y}}={\\frac{\\mathbf{x}-{\\boldsymbol{\\mu}}}{\\sigma}},\\quad{\\boldsymbol{\\mu}}={\\frac{1}{N}}\\sum_{i=1}^{N}x_{i},\\quad\\sigma={\\sqrt{{\\frac{1}{N}}\\sum_{i=1}^{N}\\left(x_{i}-{\\boldsymbol{\\mu}}\\right)^{2}}},\n$$  \n\nwhere $\\mathbf{x}=(x_{1},x_{2},...,x_{N})$ is the input vector and $N$ is the dimension of $\\mathbf{x}$ .$\\mu$ and $\\sigma$ are the mean and standard deviation of $\\mathbf{x}$ .  \n\nWe first define $\\mathbf{1}_{N}=\\underbrace{(1,1,...,1)^{\\intercal}}_{N}$ . For calculating the gradients of the normalized vector $\\mathbf{y}$ , we first simulate the backward propagation regarding the loss {z }$\\ell$ :  \n\n$$\n{\\frac{\\partial\\ell}{\\partial\\mathbf{x}}}=\\left({\\frac{\\partial\\mathbf{y}}{\\partial\\mathbf{x}}}+{\\frac{\\partial\\mu}{\\partial\\mathbf{x}}}{\\frac{\\partial\\mathbf{y}}{\\partial\\mu}}+{\\frac{\\partial\\sigma}{\\partial\\mathbf{x}}}{\\frac{\\partial\\mathbf{y}}{\\partial\\sigma}}\\right){\\frac{\\partial\\ell}{\\partial\\mathbf{y}}}={\\frac{1}{\\sigma}}\\left(I-{\\frac{\\mathbf{y}\\mathbf{y}^{\\intercal}}{N}}-{\\frac{\\mathbf{1}_{N}\\mathbf{1}_{N}^{\\intercal}}{N}}\\right){\\frac{\\partial\\ell}{\\partial\\mathbf{y}}}.\n$$  \n\n  \nFigure A1: Layer similarities between different LLM layers in (a) Finetuned and (b) LayerNormtuned MM-LL A MA2-7B.  \n\nHere we define $\\begin{array}{r l r}{\\frac{\\partial\\ell}{\\partial\\mathbf x}}&{{}\\!\\!=}&{\\!\\!(a_{1},a_{2},...,a_{N})}\\end{array}$ with mean $\\bar{a}$ and standard deviation $D_{a}$ , and $\\begin{array}{r l}{\\frac{\\partial\\ell}{\\partial\\mathbf{y}}}&{{}=}\\end{array}$ $(b_{1},b_{2},...,b_{N})$ with mean $\\bar{b}$ and standard deviation $D_{b}$ . We set $\\begin{array}{r}{W_{1}\\;=\\;I\\,-\\,\\frac{{\\bf y}{\\bf y}^{\\intercal}}{N}\\,-\\,\\frac{{\\bf1}_{N}{\\bf1}_{N}^{\\intercal}}{N}}\\end{array}$ \u2212, we can verify that:  \n\n$$\n\\lfloor\\mathbf{\\Pi}_{N}^{\\mathsf{T}}W_{1}=\\mathbf{1}_{N}^{\\mathsf{T}}{\\frac{1}{\\sigma}}\\left(I-{\\frac{\\mathbf{1}_{N}\\mathbf{1}_{N}^{\\mathsf{T}}+\\mathbf{y}\\mathbf{y}^{\\mathsf{T}}}{N}}\\right)={\\frac{1}{\\sigma}}\\left(\\mathbf{1}_{N}-{\\frac{\\mathbf{1}_{N}^{\\mathsf{T}}\\mathbf{1}_{N}}{N}}\\mathbf{1}_{N}^{\\mathsf{T}}-{\\frac{\\mathbf{1}_{N}^{\\mathsf{T}}\\mathbf{y}}{N}}\\mathbf{y}^{\\mathsf{T}}\\right)={\\frac{\\mathbf{1}_{N}-\\mathbf{1}_{N}-0}{\\sigma}}=0\n$$  \n\nTherefore, we can easily proof that $N\\bar{a}\\propto{\\bf1}_{N}^{\\top}W_{1}\\bar{b}=0$ , which means the mean of $\\frac{\\partial\\ell}{\\partial\\mathbf{x}}$ should be zero. Then we dive into proofing the variance of LayerNorm gradients should be small when the number of network parameters $N$ becomes large.  \n\n$$\n\\begin{array}{l}{{\\displaystyle{D_{a}=\\sum_{i=1}^{N}(a_{i}-\\bar{a})^{2}/N=\\sum_{i=1}^{N}a_{i}^{2}/N}\\ ~}}\\\\ {{\\displaystyle{=\\left\\|{(a_{1},a_{2},\\ldots,a_{N})^{\\top}}\\right\\|^{2}/N}\\ ~}}\\\\ {{\\displaystyle{=\\left\\|{W_{1}\\left(b_{1},b_{2},\\ldots,b_{N}\\right)^{\\top}}\\right\\|^{2}/N}\\ ~}}\\\\ {{\\displaystyle{=\\left\\|{W_{1}\\left(b_{1}-\\bar{b},b_{2}-\\bar{b},\\ldots,b_{N}-\\bar{b}\\right)^{\\top}+W_{1}\\bar{b}{\\bf1}_{N}}\\right\\|^{2}/N}\\ }}\\\\ {{\\displaystyle{=\\left\\|{W_{1}\\left(g_{1}-\\bar{b},g_{2}-\\bar{b},\\ldots,g_{N}-\\bar{b}\\right)^{\\top}}\\right\\|^{2}/N}\\ ~}}\\\\ {{\\displaystyle{\\leq W_{1}^{2}\\sum_{i=1}^{N}(b_{i}-\\bar{b})^{2}/N}\\ }}\\end{array}\n$$  \n\nSince the projection matrix $W_{1}$ is idempotent, we have $W_{1}^{2}=W_{1}$ . That is to say, when $N$ is large enough, there stands the network parameter $\\begin{array}{r}{D_{a}\\le\\big(I-\\frac{\\mathbf{y}\\mathbf{y}^{\\top}+\\mathbf{1}_{N}\\mathbf{1}_{N}^{\\top}}{N}\\big)\\sum_{i=1}^{N}(b_{i}-\\bar{b_{}})^{2}/N\\propto1/N^{2}}\\end{array}$ Nis large, the gradient variance of LayerNorm should be small. P\u2212. As a consequence, when  \n\n  \nFigure A2: Layer similarities between different LLM layers in (a) Finetuned and (b) LayerNormtuned MM-LL A MA2-7B CHAT .  \n\n  \nFigure A3: The gradients of both input and post LayerNorm in 21st layer of the MM-V ICUNA as the training proceeds.  \n\n  \nFigure A4: The gradients of both input and post LayerNorm in 11th layer of the MM-V ICUNA as the training proceeds."}, {"ref_id": "454895361066540518", "chunk_id": "1", "score": 0.41796875, "text": "# B.2 MODEL LAYERS\nIn this section, we give the formal definition of LayerNorm $\\operatorname{LN}(\\cdot)$ and RMS Norm ${\\mathrm{RMS}}\\left(\\cdot\\right)$ .  \n\nDefinition 1 (LayerNorm) .LayerNorm $L N(\\cdot;\\mu,\\beta,\\epsilon)$ of dimension $D$ is defined as:  \n\n$$\nL N(\\mathbf{x};\\pmb{\\mu},\\beta,\\epsilon)=\\frac{\\mathbf{x}-\\mathbb{E}[\\mathbf{x}]}{\\sqrt{\\mathrm{Var}[\\mathbf{x}]+\\epsilon}}\\odot\\pmb{\\mu}+\\beta,\n$$  \n\nwhere $\\mathbf{x},\\pmb{\\mu},\\beta\\in\\mathbb{R}^{D}$ .  \n\nDefinition 2 (RMSNorm) .RMS Norm $R M S(\\cdot;\\mu,\\epsilon)$ of dimension $D$ is defined as:  \n\n$$\nR M S(\\mathbf{x};\\pmb{\\mu},\\epsilon)=\\frac{\\mathbf{x}}{\\sqrt{\\frac{1}{D}\\sum_{i=1}^{D}(\\mathbf{x}[i])^{2}+\\epsilon}}\\odot\\pmb{\\mu},\n$$  \n\nwhere x,$\\pmb{\\mu}\\in\\mathbb{R}^{D}$ .  \n\nRemark. In neural networks, inputs of normalization layers are usually high dimension tensors. In this case, LayerNorm and RMSNorm normally apply to the last dimension separately.\n\n# B.3 LOSSLESS EXPANSION IN VECTOR SPACE\nIn this section, we first give the general definition of lossless expansion in vector space.  \n\ndimensions satisfy dim it is invertible. Definition 3 (Lossless $(\\bar{\\mathcal{T}})\\geq d i m(S)$ T\u2265S, a vector space expansion ector space) .Given $\\boldsymbol{S}$ and V$\\tau$ $\\mathcal{V}:\\mathcal{S}\\rightarrow\\mathcal{T}$ S \u2192T is said to be lossless if ector spaces where the  \n\nRemark. Note that the identity function Id is lossless with its inverse being itself.  \n\nThen we give a few examples of lossless vector space expansions. These examples will also be used in LEMON.  \n\nExample B.3.1 (Vector average expansion $\\mathcal{V}_{\\mathrm{avg.}}$ ).Let $\\mathbf{\\widetilde{x}}\\in\\mathbb{R}^{D_{S}}$ be a vector of dimension $D_{S}$ and its average $\\begin{array}{r}{\\lambda_{V}g(\\mathbf{x})=\\mathbb{E}[\\mathbf{x}]=\\frac{1}{D_{S}}\\sum_{i}^{D_{S}}\\mathbf{x}[i]}\\end{array}$ P].$\\mathbf{x}_{a\\nu g}^{*}$ is called the average expanded xof dimension $D_{T}$  \n\nwith $D_{T}\\geq D_{S}$ if  \n\n$$\n\\mathbf{x}_{a v g}^{*}=\\mathcal{V}_{a v g}(\\mathbf{x})=C o n c a t\\left[\\underbrace{\\mathbf{x}^{\\mathsf{T}},\\cdots,\\mathbf{x}^{\\mathsf{T}}}_{\\lfloor D_{T}/D s\\rfloor},\\underbrace{A v g(\\mathbf{x}),\\cdots,A v g(\\mathbf{x})}_{D_{T}\\mathrm{~mod~}D_{S}}\\right]^{\\mathsf{T}}\\in\\mathbb{R}^{D_{T}}.\n$$  \n\nExample B.3.2 (Vector z o expansion $\\mathcal{V}_{\\mathrm{zero.}}$ ).Le $\\mathbf{x}\\in\\mathbb{R}^{D_{S}}$ be a vector of dimension $D_{S}$ .$\\mathbf{x}_{z e r o}^{*}$ is called the zero expanded xof dimension $D_{T}$ with $D_{T}\\geq D_{S}$ \u2265if  \n\n$$\n\\begin{array}{r}{\\mathbf{x}_{z e r o}^{*}=\\mathcal{V}_{z e r o}(\\mathbf{x})=C o n c a t\\left[\\underbrace{\\mathbf{x^{\\mathsf{T}}},\\cdots,\\mathbf{x^{\\mathsf{T}}}}_{\\lfloor D_{T}/D_{S}\\rfloor},\\underbrace{0,\\cdots,0}_{D_{T}\\mathrm{~mod~}D_{S}}\\right]^{\\mathsf{T}}\\in\\mathbb{R}^{D_{T}}.}\\end{array}\n$$  \n\nExample B.3.3 (Vector circula expansion $\\mathcal{V}_{\\mathrm{circ}})$ Let $\\mathbf{x}\\in\\mathbb{R}^{D_{S}}$ a vector of dimension $D_{S}$ .${\\bf x}_{c i r c}^{*}$ is called the circular expanded xof dimension $D_{T}$ with $D_{T}\\geq D_{S}$ \u2265if  \n\n$$\n\\begin{array}{r}{\\mathbf{x}_{c i r c}^{*}=\\mathcal{V}_{c i r c}(\\mathbf{x})=C o n c a t\\underbrace{\\left[\\mathbf{x}^{\\mathsf{T}},\\cdots,\\mathbf{x}^{\\mathsf{T}},\\mathbf{x}^{\\mathsf{T}}[\\colon D_{T}\\bmod D_{S}]\\right]^{\\mathsf{T}}\\in\\mathbb{R}^{D_{T}}}_{[D_{T}/D_{S}]}.}\\end{array}\n$$  \n\nExample B.3.4 (Vector random expansion $\\mathcal{V}_{\\mathrm{rand.}}$ Let $\\mathbf{\\Deltax}\\in\\mathbb{R}^{D_{S}}$ a vector of dimension $D_{S}$ .${\\bf x}_{r a n d}^{*}$ is called the random expanded xof dimension $D_{T}$ with $D_{T}\\geq D_{S}$ \u2265if  \n\n$$\n\\begin{array}{r}{\\mathbf{x}_{r a n d}^{*}=\\mathcal{V}_{r a n d}(\\mathbf{x};\\zeta)=C o n c a t\\left[\\underbrace{\\mathbf{x^{\\intercal}},\\cdots,\\mathbf{x^{\\intercal}}}_{\\lfloor D_{T}/D_{S}\\rfloor},\\zeta^{\\intercal}\\right]^{\\intercal}\\in\\mathbb{R}^{D_{T}},}\\end{array}\n$$  \n\nwhere $\\zeta\\in\\mathbb{R}^{D_{T}}$ mod $D_{S}$ is an arbitrary vector.  \n\nRemark. (1) All vector expansion examples above follow the same pattern. Specifically, when $D_{T}$ expanding from di mod s by $D_{S}$ entries differently. (2) The random vector ating $\\textbf{x}\\lfloor D_{T}/D_{S}\\rfloor D_{S}$ \u230a$D_{S}$ \u230b$D_{T}$ number of times. , all vector expansion methods pad first $\\zeta$ in vector random expansion is arbitrary, Each method deals with the remaining $\\lfloor D_{T}/D_{S}\\rfloor D_{S}$ enso $\\mathcal{V}_{a\\nu g}$ ,$\\mathcal{V}_{z e r o}$ ,$\\mathcal{V}_{c i r c}\\subset\\mathcal{V}_{r a n d}$ . (3) Here all three examples are expansion methods for vectors. In practice, neural networks like Transformers are dealing high dimensional tensors. These tensors can essentially be thought of as collections of vectors. In such scenarios, we can apply the expansion methods separately to the last dimension of these tensors.  \n\nIn the following claim, we show that vectors expanded by these operators are lossless.  \n\n$\\mathcal{V}_{c i r c}$ V, and vector random expansion m 1. Vector average expansio V$\\gamma_{r a n d}$ $\\mathcal{V}_{a\\nu g},$ are all lossless expansion for vectors. , vector zero expansion $\\mathcal{V}_{z e r o}$ , vector circular expansion Proof. The inverse function $\\mathcal{V}^{-1}:\\mathbb{R}^{D_{T}}\\rightarrow\\mathbb{R}^{D_{S}}$ of these vector expansion methods is  \n\n$$\n\\nu^{-1}({\\bf x})={\\bf x}[:D_{S}].\n$$  \n\nRemark. In practice, we want inverse mapping of expansion methods to be easily computed just like the example above.\n\n# B.4LOSSLESS EXPANSION FOR OPERATORS\nWe then give the definition of lossless expansion for operators. These operators apply on tensors, hence our definition of lossless operator expansion is based on lossless expansion in vector space. These operators can be different layers used in Transformer architectures, including LayerNorm, convolutional layers, and fully-connected layers, etc.  \n\nDefinit ansio der vector spaces $S^{i n},S^{o u t},\\mathcal{T}^{i n}$ and $\\mathcal{T}^{o u t}$ such that with $g(\\cdot):S^{i n}\\rightarrow S^{o u t}$ \u00b7$n(S^{i n})\\leq d i m(T^{i n})$ S\u2192S or space e T. We say the ope and dim $d i m\\big(S^{\\bar{o}u t}\\big)\\leq d i m\\big(T^{o u t}\\big)$ S$\\mathcal{V}_{i n}:S^{i\\bar{n}}\\to\\mathcal{T}^{i n}$ \u2264TEMo is $(\\mathcal{V}_{i n},\\mathcal{V}_{o u t})$ VVess output vector space expansion ppose the op -lossless for $g(\\cdot)$ \u00b7or is denoted if there exist $\\mathcal{V}_{o u t}:S^{o u t}\\to\\mathcal{T}^{o u t}$ VS\u2192T such that V$\\mathcal{V}_{o u t}(g(\\mathbf{x}))=\\mathcal{E}[g](\\mathcal{V}_{i n}(\\mathbf{x})),\\forall\\mathbf{x}\\in S^{i n}$ EV\u2200\u2208S .  \n\n$(\\mathcal{V}_{i n},\\mathcal{V}_{o u t})$ Remark. losslessly expanded input, the output of the to be invertible, we do not have restrictions on the operator expansion VV(1) Intuitively, a lossless operator -lossless for the origina $g(\\cdot)$ \u00b7tput. (2) For conciseness, we use \u2018 \u2019 interchangeably. (3) We only require the v Eexpanded oper pansion can be understood a $^{\\cdot}\\mathcal{E}[g]$ Eis a is $(\\mathcal{V}_{i n},\\mathcal{V}_{o u t})$ EVtor expansions .V$\\nu_{o u t}$ ows: when using losslessly expa -lossles V$\\mathcal{V}_{i n}$ and \u2018 and $\\mathcal{E}$ V$\\nu_{o u t}$ $\\mathcal{V}_{i n}$ ed"}], "task_step_question_answer": "\u5728\u91d1\u878d\u9886\u57df\uff0c\u5e02\u573a\u6570\u636e\u77ac\u606f\u4e07\u53d8\u4e14\u5305\u542b\u4f17\u591a\u590d\u6742\u56e0\u7d20\uff0c\u5982\u80a1\u7968\u4ef7\u683c\u6ce2\u52a8\u3001\u5b8f\u89c2\u7ecf\u6d4e\u6307\u6807\u53d8\u5316\u7b49\u3002LayerNorm\u53ef\u4ee5\u5bf9\u8fd9\u4e9b\u591a\u7ef4\u5ea6\u7684\u91d1\u878d\u6570\u636e\u8fdb\u884c\u5f52\u4e00\u5316\uff0c\u4f7f\u5f97\u91d1\u878d\u98ce\u9669\u9884\u6d4b\u6a21\u578b\u80fd\u591f\u66f4\u7cbe\u51c6\u5730\u5206\u6790\u6570\u636e\u4e4b\u95f4\u7684\u5173\u8054\uff0c\u4ece\u800c\u63d0\u9ad8\u98ce\u9669\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002\u4f8b\u5982\u5728\u9884\u6d4b\u80a1\u7968\u5e02\u573a\u7684\u7cfb\u7edf\u6027\u98ce\u9669\u65f6\uff0c\u4f7f\u7528LayerNorm\u7684\u6a21\u578b\u80fd\u591f\u66f4\u597d\u5730\u6355\u6349\u5404\u79cd\u56e0\u7d20\u5bf9\u98ce\u9669\u7684\u5f71\u54cd\uff0c\u4e3a\u6295\u8d44\u8005\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u98ce\u9669\u9884\u8b66\u3002RMSNorm\u5728\u5904\u7406\u5927\u89c4\u6a21\u91d1\u878d\u4ea4\u6613\u6570\u636e\u65f6\uff0c\u901a\u8fc7\u5feb\u901f\u5f52\u4e00\u5316\u52a0\u901f\u6a21\u578b\u8bad\u7ec3\uff0c\u8ba9\u91d1\u878d\u6a21\u578b\u80fd\u591f\u53ca\u65f6\u9002\u5e94\u5e02\u573a\u7684\u52a8\u6001\u53d8\u5316\uff0c\u4e3a\u9ad8\u9891\u4ea4\u6613\u7b56\u7565\u7684\u5236\u5b9a\u63d0\u4f9b\u652f\u6301\u3002 ", "ref_task_step_id": ""}, "__type__": "task_step_node"}}, "task_step_store/ref_task_step_info": {"": {"node_ids": ["d753c815-d196-4a48-b95c-c565926fb392", "e408c292-33f5-459e-9724-9942a9d19d1d", "cf852a98-8ebb-40f0-bf13-6ea29fc63dd5", "4aa00861-de5d-4346-a6dd-80ca6b0e7aa7", "e2b95384-9e4b-4445-94fd-17a46fdce3ea", "db3d6ad7-14d4-4d13-8ee3-b0b4f63647bd", "30b2ef30-75d2-40c5-b100-d2fefb6cb330", "e408c292-33f5-459e-9724-9942a9d19d1d", "4aa00861-de5d-4346-a6dd-80ca6b0e7aa7", "e2b95384-9e4b-4445-94fd-17a46fdce3ea", "d753c815-d196-4a48-b95c-c565926fb392", "30b2ef30-75d2-40c5-b100-d2fefb6cb330", "db3d6ad7-14d4-4d13-8ee3-b0b4f63647bd", "cf852a98-8ebb-40f0-bf13-6ea29fc63dd5", "4aa00861-de5d-4346-a6dd-80ca6b0e7aa7", "30b2ef30-75d2-40c5-b100-d2fefb6cb330", "e2b95384-9e4b-4445-94fd-17a46fdce3ea", "e408c292-33f5-459e-9724-9942a9d19d1d", "d753c815-d196-4a48-b95c-c565926fb392", "cf852a98-8ebb-40f0-bf13-6ea29fc63dd5", "4aa00861-de5d-4346-a6dd-80ca6b0e7aa7", "db3d6ad7-14d4-4d13-8ee3-b0b4f63647bd", "30b2ef30-75d2-40c5-b100-d2fefb6cb330", "e2b95384-9e4b-4445-94fd-17a46fdce3ea", "e408c292-33f5-459e-9724-9942a9d19d1d", "d753c815-d196-4a48-b95c-c565926fb392", "cf852a98-8ebb-40f0-bf13-6ea29fc63dd5", "db3d6ad7-14d4-4d13-8ee3-b0b4f63647bd", "d753c815-d196-4a48-b95c-c565926fb392", "d753c815-d196-4a48-b95c-c565926fb392", "d753c815-d196-4a48-b95c-c565926fb392", "d753c815-d196-4a48-b95c-c565926fb392", "d753c815-d196-4a48-b95c-c565926fb392", "d753c815-d196-4a48-b95c-c565926fb392", "4aa00861-de5d-4346-a6dd-80ca6b0e7aa7", "4aa00861-de5d-4346-a6dd-80ca6b0e7aa7", "e408c292-33f5-459e-9724-9942a9d19d1d", "e408c292-33f5-459e-9724-9942a9d19d1d", "db3d6ad7-14d4-4d13-8ee3-b0b4f63647bd", "db3d6ad7-14d4-4d13-8ee3-b0b4f63647bd", "d753c815-d196-4a48-b95c-c565926fb392", "d753c815-d196-4a48-b95c-c565926fb392", "cf852a98-8ebb-40f0-bf13-6ea29fc63dd5", "cf852a98-8ebb-40f0-bf13-6ea29fc63dd5", "30b2ef30-75d2-40c5-b100-d2fefb6cb330", "30b2ef30-75d2-40c5-b100-d2fefb6cb330", "cf852a98-8ebb-40f0-bf13-6ea29fc63dd5", "cf852a98-8ebb-40f0-bf13-6ea29fc63dd5", "e2b95384-9e4b-4445-94fd-17a46fdce3ea", "e2b95384-9e4b-4445-94fd-17a46fdce3ea", "30b2ef30-75d2-40c5-b100-d2fefb6cb330", "30b2ef30-75d2-40c5-b100-d2fefb6cb330", "4aa00861-de5d-4346-a6dd-80ca6b0e7aa7", "4aa00861-de5d-4346-a6dd-80ca6b0e7aa7", "e408c292-33f5-459e-9724-9942a9d19d1d", "e408c292-33f5-459e-9724-9942a9d19d1d", "4aa00861-de5d-4346-a6dd-80ca6b0e7aa7", "4aa00861-de5d-4346-a6dd-80ca6b0e7aa7", "cf852a98-8ebb-40f0-bf13-6ea29fc63dd5", "cf852a98-8ebb-40f0-bf13-6ea29fc63dd5", "db3d6ad7-14d4-4d13-8ee3-b0b4f63647bd", "db3d6ad7-14d4-4d13-8ee3-b0b4f63647bd", "30b2ef30-75d2-40c5-b100-d2fefb6cb330", "30b2ef30-75d2-40c5-b100-d2fefb6cb330", "e2b95384-9e4b-4445-94fd-17a46fdce3ea", "e2b95384-9e4b-4445-94fd-17a46fdce3ea", "d753c815-d196-4a48-b95c-c565926fb392", "d753c815-d196-4a48-b95c-c565926fb392", "4aa00861-de5d-4346-a6dd-80ca6b0e7aa7", "4aa00861-de5d-4346-a6dd-80ca6b0e7aa7", "e408c292-33f5-459e-9724-9942a9d19d1d", "e408c292-33f5-459e-9724-9942a9d19d1d", "30b2ef30-75d2-40c5-b100-d2fefb6cb330", "30b2ef30-75d2-40c5-b100-d2fefb6cb330", "d753c815-d196-4a48-b95c-c565926fb392", "d753c815-d196-4a48-b95c-c565926fb392", "cf852a98-8ebb-40f0-bf13-6ea29fc63dd5", "cf852a98-8ebb-40f0-bf13-6ea29fc63dd5", "db3d6ad7-14d4-4d13-8ee3-b0b4f63647bd", "db3d6ad7-14d4-4d13-8ee3-b0b4f63647bd", "e2b95384-9e4b-4445-94fd-17a46fdce3ea", "e2b95384-9e4b-4445-94fd-17a46fdce3ea", "e408c292-33f5-459e-9724-9942a9d19d1d", "e408c292-33f5-459e-9724-9942a9d19d1d", "4aa00861-de5d-4346-a6dd-80ca6b0e7aa7", "4aa00861-de5d-4346-a6dd-80ca6b0e7aa7", "30b2ef30-75d2-40c5-b100-d2fefb6cb330", "30b2ef30-75d2-40c5-b100-d2fefb6cb330", "cf852a98-8ebb-40f0-bf13-6ea29fc63dd5", "cf852a98-8ebb-40f0-bf13-6ea29fc63dd5", "e2b95384-9e4b-4445-94fd-17a46fdce3ea", "e2b95384-9e4b-4445-94fd-17a46fdce3ea", "db3d6ad7-14d4-4d13-8ee3-b0b4f63647bd", "db3d6ad7-14d4-4d13-8ee3-b0b4f63647bd", "d753c815-d196-4a48-b95c-c565926fb392", "d753c815-d196-4a48-b95c-c565926fb392"], "metadata": {}}}, "task_step_store/metadata": {"d753c815-d196-4a48-b95c-c565926fb392": {"task_step_hash": "", "ref_task_step_id": ""}, "e408c292-33f5-459e-9724-9942a9d19d1d": {"task_step_hash": "", "ref_task_step_id": ""}, "cf852a98-8ebb-40f0-bf13-6ea29fc63dd5": {"task_step_hash": "", "ref_task_step_id": ""}, "4aa00861-de5d-4346-a6dd-80ca6b0e7aa7": {"task_step_hash": "", "ref_task_step_id": ""}, "e2b95384-9e4b-4445-94fd-17a46fdce3ea": {"task_step_hash": "", "ref_task_step_id": ""}, "db3d6ad7-14d4-4d13-8ee3-b0b4f63647bd": {"task_step_hash": "", "ref_task_step_id": ""}, "30b2ef30-75d2-40c5-b100-d2fefb6cb330": {"task_step_hash": "", "ref_task_step_id": ""}}}