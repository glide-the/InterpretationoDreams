角色,内容,分镜
e408c292-33f5-459e-9724-9942a9d19d1d,研究论文中采用的主要框架（如Transformer、GAN、BERT等）在不同任务中的应用与变体,1
e408c292-33f5-459e-9724-9942a9d19d1d,在基于Transformer的计算机视觉模型中，LayerNorm和RMSNorm具体进行了哪些适应性调整和变体应用来适配视觉任务需求？ ,1
e408c292-33f5-459e-9724-9942a9d19d1d,"ref_ids: 454895409734360760, chunk_ids: 3, Score: 0.3711, Text: # 3.3 A TRANSFORMATION PER BLOCK
Now that every LayerNorm in the transformer has been converted to RMSNorm, we can select any $\\mathbf{Q}$ to modify the model. Our initial plan was to collect signals from the model, construct an orthogonal matrix using those signals and to delete parts of the network. We quickly saw that the signals at different blocks of the network were not aligned, and that we would need to apply a different orthogonal matrix at each block, $\\mathbf{Q}_{\\ell}$ .  

Allowing the orthogonal matrix used in each block to differ can be shown to leave the model unchanged using the same proof as Theorem 1 ,  

  
Figure 3: Converting a transformer network from LayerNorm to RMSNorm: the scale matrix diag $(\\alpha)$ is absorbed into the subsequent matrix $\\mathbf{W}_{\\mathrm{in}}$ . Figure shows the block in combined colors. We use $(\\alpha)$ for brevity. The mean-subtraction matrix $\\mathbf{M}$ is applied to each matrix $\\mathbf{W}_{\\mathrm{out}}$ . Layernorm becomes RMSNorm, up to a constant $\\bar{\\sqrt{D}}$ (not shown). Here, the scaling $(\\alpha^{\\prime})$ comes from the previous block.  

  
Figure 4: With the network converted to RMSNorm (see Figure 3 ), we apply the computational-invariance idea. The input weight matrices $\\mathrm{diag}(\\alpha)\\mathbf{W}_{\\mathrm{in}}$ are pre-multiplied by $\\mathbf{Q}^{\\top}$ . The output matrices $\\mathbf{W}_{\\mathrm{out}}\\mathbf{M}$ are post-multiplied by $\\mathbf{Q}$ . In the skip-connection, a new linear layer is added $\\mathbf{Q}_{\\ell}^{\\top}\\mathbf{Q}_{\\ell+1}$ . After these modifications, the matrices can be sliced (hatched areas).  

with the exception of line 5 of Algorithm 1 . Here we see that the residual connection and the output of the block must have the same rotation. To fix this, we modify the residual connection by applying the linear transformation applied to different blocks with the additional linear operation in the residual connection. Unlike the $\\mathbf{Q}_{\\ell-1}^{\\top}\\mathbf{Q}_{\\ell}$ −to the residual. Figure 4 shows how different rotations can be modifications to the weight matrices, these additional operations cannot be pre-computed and add a small $(D\\times D)$ overhead to the model. Nonetheless, they are needed to allow slicing the model (Section 3.4 ) and we see real speedup overall (Section 4 ).  

To compute the matrices $\\mathbf{Q}_{\\ell}$ , we use PCA. We select a calibration dataset from the training set, run it through the model (after converting LayerNorm operations into RMSNorm), and extract the orthogonal matrix of the layer. We use the output of the transformed network to calculate the orthogonal matrices of the next layers. More precisely, if $\\mathbf{X}_{\\ell,i}$ is the output of the $\\ell^{\\mathrm{th}}$ RMSNorm block for the $i^{\\mathrm{th}}$ sequence in the calibration dataset, we compute  

$$
\\mathbf{C}_{\\ell}=\\sum_{i}\\mathbf{X}_{\\ell,i}^{\\top}\\mathbf{X}_{\\ell,i}
$$  

and set $\\mathbf{Q}_{\\ell}$ to the be the eigenvectors of $\\mathbf{C}_{\\ell}$ , sorted by decreasing eigenvalues.

# 3.4 SLICING
The goal of Principal Component Analysis is usually to take a data matrix $\\mathbf{X}$ and compute a lower dimensional representation $\\mathbf{Z}$ , and an approximate reconstruction $\\tilde{\\mathbf{X}}$ :  

$$
\\mathbf{Z}=\\mathbf{X}\\mathbf{Q}\\mathbf{D}\\,,\\qquad\\tilde{\\mathbf{X}}=\\mathbf{Z}\\mathbf{D}^{\\top}\\mathbf{Q}^{\\top}\\,.
$$  

where $\\mathbf{Q}$ is the ectors of ${\\bf X}^{\\top}{\\bf X}$ , and $\\mathbf{D}$ is a $D\\times D_{\\mathrm{small}}$ deletion matrix (containing $D_{\\mathrm{small}}$ The reconstruction is columns of the $D\\times D$ ×$L_{2}$ identity matrix), which removes some of the columns of the matrix to the left. optimal, in the sense that QD is a linear mapping that minimizes $\\lVert\\mathbf{X}-\\tilde{\\mathbf{X}}\\rVert^{2}$ .  

When we apply PCA to the signal matrix $\\mathbf{X}$ bween blocks, we never materialize the $N\\times D$ signal matrix, but we apply the deletion matrix Dto the operations preceding and succeeding the construction of that matrix, which have already been multiplied by $\\mathbf{Q}$ in the above. We delete rows of $\\mathbf{W}_{\\mathrm{in}}$ that we have inserted into the residual connection (see Figure and columns of $\\mathbf{W}_{\\mathrm{out}}$ and $\\mathbf{W}_{\\mathrm{embd}}$ . We also delete both rows 4 ). and columns of the matrix $\\mathbf{Q}_{\\ell-1}^{\\top}\\mathbf{Q}_{\\ell}$ −

# 4 EXPERIMENTAL VALIDATION
Setup We use HuggingFace Transformers ( Wolf et al. ,2019 ) to implement our code with PyTorch (Paszke et al. ,2019 ). The computation of $\\mathbf{Q}$ is performed on a single H100 GPU with 80GB of memory, taking approximately 3.5 hours to complete for the L LAMA -2 70B model. During the PCA calculation, we use double precision for computing the eigenvectors of the covariance matrix. We find that using single precision for eigenvector calculations in PyTorch leads to a discrepancy in the final accuracy, as detailed in Appendix A.2 .  

We experiment with two different calibration sets: 1024 samples from the WikiText-2 training dataset ( Merity et al. ,2016 ) and 5000 samples from the Alpaca training dataset ( Taori et al. ,2023 ). Sequence lengths are chosen as the maximum of each language model. An ablation study on the calibration set size and sequence length is presented in Appendix A.3 .  

Models, Tasks, and GPUs We evaluate all our experiments on OPT ( Zhang et al. ,2022 ), L LAMA -2 (Touvron et al. ,2023 ) model families, and additionally evaluate Phi-2 (in our zero-shot task) experiments. We exclude OPT 175B, as it is outperformed by smaller L LAMA -2 models. Nonetheless, we anticipate that this larger model will yield improved results, as larger models typically offer more promising opportunities for compression (see Section 4.1 ). We evaluate our scheme on both language generation as well as popular zero-shot tasks. To demonstrate the comprehensive speedup achieved by SliceGPT we use: Quadro RTX6000 GPUs with 24GB of memory as a representative example of consumer-level GPUs; 40GB A100s and 80GB H100s to provide datacenter-level benchmarks.  

Baseline Setup We initially planned to compare our results against a scheme that pruned columns (or rows) with the smallest norm but found that this baseline was very poor, with the perplexity of the model soaring into the 1000s after pruning just a few columns. Instead, we compare SliceGPT against SparseGPT ( Frantar & Alistarh ,2023 ) employing a 2:4 sparsity ratio, as this is the only sparsity scheme which achieves speedup ( Mishra et al. ,2021 ).",1
e408c292-33f5-459e-9724-9942a9d19d1d,"ref_ids: 454847819065993190, chunk_ids: 1, Score: 0.3203, Text: # 4.2 HE-Friendly Normalization
To enhance training stability, transformers rely on LayerNorm , which is formulated as follows:  

$$
\\operatorname{LayerNorm}(x)={\\frac{x-\\mu}{\\sqrt{\\sigma^{2}}}}\\cdot\\gamma+\\beta
$$  

where $x$ is the input vector, $\\mu$ is the mean of $x,\\sigma^{2}$ is the variance, and $\\gamma$ and $\\beta$ are learnable parameters. Computing LayerNorm over HE requires calculating the inverse square root, which is not a polynomial operation. A common practice in designing neural networks for secure inference over HE is to replace LayerNorm with BatchNorm , as it can be implemented by a straightforward constant affine transformation at inference time. Therefore, we attempted to train transformers using $\\sigma$ -attention and BatchNorm . We observed that these models were highly unstable, performing poorly on vision tasks and failing to converge in NLP tasks. Consequently, we adopt two distinct approaches for vision and NLP tasks.  

Normalization for Vision Transformers For vision transformers, to improve performance and mitigate training instability, we add two components: (i) Additional BatchNorm in the MLP of ViT, which is proposed in ( Yao et al. ,2021 ) as a stabilizer for ViT training, and (ii) additional BatchNorm within the $\\sigma$ -attention, which normalizes values across different attention heads, since we observe that those are the sources of instability. The resulting $\\sigma$ -attention variant is:  

$$
{\\frac{1}{\\mathrm{S}(L)}}\\sigma\\left({\\mathrm{BatchNorm~}}2\\mathrm{D}\\left({\\frac{Q K^{T}}{\\sqrt{d_{k}}}}\\right)\\right)V
$$  

Normalization for NLP Transformers For NLP, models with $\\sigma$ -attention and BatchNorm completely failed, even when augmented by stabilizing factors from the literature, such as ( Wang et al. ,2022 ). Consequently, we had to confront the challenge of approximating LayerNorm by polynomials, which entails approximating the inverse square root function. Empirically, we found that the values of the variance in trained transformers (with $\\sigma$ -attention) ranged between 1 and $10^{9}$ , causing approximation challenges due to the extremely large domain. To solve this problem, we first focus on narrowing the domain of the variance, which then makes it easier to approximate the inverse square root over this restricted domain. The method is similar to ( Baruch et al. ,2023 ), which introduces an additional loss function that encourages the model to minimize the range of the input to activation layers. We apply this technique on the variance at each layer via the following objective:  

$$
\\mathbb{L}_{\\mathrm{Variance\\;Minimization}}:=\\Sigma_{m=1}^{L_{N}}\\operatorname*{max}_{c\\in C,x_{i}\\in X}\\left(\\mathrm{var}_{m,c}^{i}\\right)
$$  

where we denote the number of layers by $N_{L}$ , the number of channels by $C$ , and the train dataset by $X:=[x_{1},x_{2},..]$ . Furthermore, we denote the variance at layer $m$ and channel $c$ , when the model processes the $x_{i}$ example by $\\mathrm{var}_{m,c}^{i}$ . For reasons of efficiency, we compute the loss over each batch rather than the whole training set $X$ . By extending this method to operate on layer normalization instead of activations, we succeed in reducing the variance range to a smaller domain. This reduction makes it feasible to use well-known approximations, such as the technique described in ( Baruch et al. ,2023 ), for the inverse square root.

# 4.3 A Recipe for a Polynomial Transformer
Fig. 2 illustrates the entire method, which comprises three stages: (i) First we modify the architecture from the original transformer architecture (first column) to a HE-friendly architecture (second column), namely, an architecture that can eventually be converted into a polynomial form. Then we train the modified model from scratch with the same hyperparameters. (ii) In the second stage, we perform a supplementary training procedure to obtain a model with HE-friendly weights ,which means that each non-polynomial component will only operate on specific and restricted domains. To do so, we add a loss function that minimizes the range of inputs to non-polynomial layers. For the activations (standard activations and attention-activations), we directly apply the method from ( Baruch et al. ,2023 ), which defines the range loss for activations. For the LayerNorm layers, we use the loss defined in Eq. 11 . The whole training objective $\\mathbb{L}$ is defined by:

# αLRange Minimization +βLVariance Minimization +Loriginal
where $\\alpha$ and $\\beta$ are hyperparameters. (iii) Finally, each non-polynomial layer is directly replaced with its polynomial approximation, resulting in a polynomial model . Appendix A contains details on the approximation we used. Those approximations are accurate for the HE-friendly architecture & weights obtained from earlier stages.

# 5 Experiments
We evaluate the polynomial models generated by our method in Section 5.1 , focusing on language modeling with the Wikitext-103 dataset and image classification using standard benchmarks, including Tiny-ImageNet and CIFAR-10. Section 5.2 justifies our methodological choices, specifically the use of scaled $\\sigma$ -attention and an additional training phase designed to manipulate the input values of non-polynomial layers. Furthermore, that section contains several ablation studies to assess the impact of each method component on the overall performance degradation. Section 5.3 discusses the accuracy and latency implications of applying our models over FHE. The experimental setup is detailed in Appendix B.

# 5.1 Polynomial Models
Polynomial Language Modeling We evaluated our BERT-like transformer model for language modeling as our NLP task. Specifically, we trained on Wikitext-103 with a self-supervised scheme for Next Token Prediction (NTP). The results in Table 1 show that after architectural and training modifications, we achieved a fully polynomial model with competitive perplexity scores. In particular, the perplexity increased by 0.91 compared to a vanilla transformer of the same size, from 18.98 to 19.89 for a 6-layer transformer (53.3M parameters), and by 2.02 from 16.89 to 18.91 for a 12- layer model (95.8 Mparameters). Considering that at least $80\\%$ of the gap between the vanilla transformer and our corresponding polynomial model is caused in the last stage where polynomial approximations are used (0.74 for 6 layers model and 1.76 for 12 layers), we hypothesize that more accurate polynomials can mitigate most of the performance gap.  

<html><body><table><tr><td>Depth</td><td>Original</td><td>P</td><td>P+MR</td><td>Poly</td></tr><tr><td>6</td><td>18.98</td><td>19.07</td><td>19.15</td><td>19.89</td></tr><tr><td>12</td><td>16.89</td><td>16.98</td><td>17.15</td><td>18.91</td></tr></table></body></html>  

Table 1: NLP Results: Perplexity results of a polynomial BERT-like transformer on the Wikitext-103 benchmark. ‘Depth’ indicates the number of transformer layers. ‘Original’ denotes the perplexity of the vanilla Softmax-based transformer of equivalent size. ‘P’ represents models utilizing scaled $\\sigma$ -attention, while $\\mathbf{\\dot{P}+M R}$ ’shows perplexity at the end of the range minimization training. ‘Poly’ details the final performance after substituting LayerNorm and activation functions with polynomial approximations.  

Polynomial Image Classification We evaluated our vision models on two image classification benchmarks: Tiny-ImageNet and CIFAR-100. The results, presented in Table 2 , indicate that our vision models, which are converted to polynomial form by our methods, remain competitive. Specifically, for ViT on CIFAR-100, the original ViT (denoted as $\\mathbf{\\omega}^{\\bullet}\\mathbf{O}^{\\bullet}$ ) achieved a score of $73.4\\%$ , whereas our HE-friendly alternative $(\\mathrm{P+BN+QK+A})$ ), achieved a score of $71.1\\%$ . The HE-friendly alternative employs BatchNorm as the normalization layer, includes additional stabilizers described in 4.2 and is based on scaled$\\sigma$ attention. After applying our range-aware training procedure, the accuracy of our model decreased marginally by $0.1\\%$ to $71.0\\%$ , and it further decreased to $70.8\\%$ after approximating nonpolynomial components. For the Swin Transformer on Tiny-ImageNet, the original model achieved $59.4\\%$ , and transitioning to the HE-friendly architecture resulted in a performance decrease of $0.3\\%$ to $59.1\\%$ . After employing range-aware training to obtain HE-friendly weights, the performance further degraded by $0.2\\%$ to $58.9\\%$ , while the final performance of the polynomial model remained the same. In conclusion, the performance gap between the polynomial models and the original architectures is less than $4\\%$ , demonstrating the practicality of our methods in this domain.  

<html><body><table><tr><td colspan=""3"">Model Dataset 0 P+BN+QK+A MR Poly</td></tr><tr><td>ViT CIFAR-100</td><td>73.4 71.1</td><td>71.0 70.8</td></tr><tr><td>Swin</td><td>Tiny-ImgNet 59.4</td><td>59.1 58.9 58.9</td></tr></table></body></html>  

Table 2: Vision Results: Test accuracy results of a polynomial ViT. ‘O’ represents the original vanilla model, $\\mathrm{{^{\\circ}\\mathrm{{P}+\\mathrm{{BN}+\\mathrm{{QK}+\\mathrm{{A}^{\\circ}}}}}}}$ represents scaled$\\boldsymbol{\\sigma}$ attention-based ViT trained with BatchNorm as the normalization function instead of LayerNorm , and contains the additional stabilizers described in 4.2 . ‘MR’ refers to the accuracy at the end of the range minimization training, and ‘Poly’ details the final performance after substituting polynomial approximations.",1
e408c292-33f5-459e-9724-9942a9d19d1d,"ref_ids: 454895289196085862, chunk_ids: 3, Score: 0.2715, Text: # 3 Experiments and Results
We evaluate the performance of PreNorm and PostNorm for ZST on various datasets and language pairs. We then analyze the off-target rates and structural discrepancies between PreNorm and PostNorm to understand performance differences.  

$$
\\mathrm{LayerNorm}(\\mathbf{x})=\\frac{\\mathbf{x}-\\mathbf{E}(\\mathbf{x})}{\\sqrt{\\mathbf{V}(\\mathbf{x})}}\\cdot\\mathbf{g}+\\mathbf{b},
$$  

where $\\mathbf{g}$ and $\\mathbf{b}$ are trainable gain and bias. $\\mathbf{E}$ and $\\mathbf{V}$ indicate expectation and variance. LayerNorm is commonly used in two positions in the Transformer, as shown in Fig. 1 . PostNorm, which is the originally proposed setting of the Transformer ( Vaswani et al. ,2017 ), involves applying LayerNorm after each sub-module (i.e., selfattention or feed-forward network) and residual connections. PreNorm ( Baevski and Auli ,2019 ), on the other hand, involves applying LayerNorm directly before each sub-module and is known to stabilize Transformer training. While variants of Transformer LayerNorm like RMSNorm ( Zhang and Sennrich ,2019 ) have been proposed, the vanilla PreNorm and PostNorm are still the most widely adopted settings in current multilingual

# 3.1 Experimental Settings
Datasets We perform ZST experiments on three datasets: OPUS ( Zhang et al. ,2020 ), IWSLT ( Cettolo et al. ,2017 ), and Europarl ( Koehn ,2005 ). The statistics of the datasets are summarized in Table 1 .We include 7 ,4 , and 5 languages for each dataset. The training data consists of only English-centric sentence pairs, resulting in 30 ,6 , and 12 ZST directions for each dataset. The total number of parallel sentences for each dataset is 12 .00 M, 1 .38 M, and 15 .78 M, respectively. We apply BPE ( Sennrich et al. ,2016 ) with merge operations of 50 k, 40 k, and $50\\mathbf{k}$ to create a joint vocabulary for each dataset.  

Training We employ Transformer-base model for OPUS and IWSLT, and Transformer-big for Europarl, in accordance with the distinct sizes of training data. We consider the following settings: (1) PreNorm or PostNorm : PreNorm involves LayerNorm directly before each sub-module (i.e., self-attention or feed-forward network), while PostNorm applies LayerNorm after each sub-module and residual connections, as shown in Fig. 1 .(2) S-ENC-T-DEC or T-ENC : Source language tag on the encoder-side and target language tag on the decoder-side; or only target language tag on the encoder-side. Wu et al. (2021 ) showed that this setting impacts ZST for Transformer with PreNorm. (3) w/ or w/o Res. : With the residual connection for self-attention in the middle $(4^{t h})$ encoder layer or not. Liu et al. (2021 ) revealed that “w/o Res.” improves ZST for the model trained with PreNorm. We experiment this with different LayerNorm settings as this may reduce the potential of overfitting on supervised directions, then further impacts ZST, which aligns with our hypothesis.  

Table 2: BLEU scores and off-target rates (shown in brackets) . We report the average score of three seeds; refer to Appendix Gfor BLEU score of each translation direction and seed. “Res.” indicates the residual connection of self-attention in the $4^{t h}$ encoder layer. We mark lower off-target rates and significantly higher BLEU scores ( Koehn ,2004 ) between PreNorm and PostNorm in bold for ZST.   


<html><body><table><tr><td>#</td><td>Layer Norm</td><td>Language Tag</td><td>Res.</td><td></td><td>Zero-shot</td><td></td><td></td><td>Supervised</td><td></td></tr><tr><td>0</td><td></td><td>Pivot</td><td></td><td>OPUS 21.8</td><td>IWSLT 20.0</td><td>Europarl 29.5</td><td>OPUS</td><td>IWSLT</td><td>Europarl</td></tr><tr><td>1</td><td>PreNorm</td><td>S-ENC-T-DEC</td><td>w/</td><td>10.1 (42.19%)</td><td>4.9 (64.84%)</td><td>24.9 ( 7.73%)</td><td>33.7</td><td>31.5</td><td>34.3</td></tr><tr><td>2</td><td>PostNorm</td><td>S-ENC-T-DEC</td><td>w/</td><td>16.8 ( 8.59%)</td><td>12.4 (10.61%)</td><td>29.2( 0.34%)</td><td>33.9</td><td>31.5</td><td>34.5</td></tr><tr><td>3</td><td>PreNorm</td><td>T-ENC</td><td>w/</td><td>13.3 (22.99%)</td><td>13.7 ( 3.98%)</td><td>29.5( 0.23%)</td><td>33.7</td><td>31.6</td><td>34.4</td></tr><tr><td>4</td><td>PostNorm</td><td>T-ENC</td><td>w/</td><td>14.0 (22.86%)</td><td>15.5 ( 4.59%)</td><td>30.8 ( 0.11%)</td><td>34.1</td><td>31.5</td><td>34.5</td></tr><tr><td>5</td><td>PreNorm</td><td>S-ENC-T-DEC</td><td>w/o</td><td>14.3 (20.67%)</td><td>8.0 (50.16%)</td><td>16.7 (41.87%)</td><td>33.6</td><td>30.9</td><td>34.3</td></tr><tr><td>6</td><td>PostNorm</td><td>S-ENC-T-DEC</td><td>w/o</td><td>16.0 (15.27%)</td><td>17.4 (1.83%)</td><td>29.0 ( 0.41%)</td><td>33.8</td><td>30.7</td><td>34.4</td></tr><tr><td>7</td><td>PreNorm</td><td>T-ENC</td><td>w/o</td><td>13.4 (27.15%)</td><td>16.2 ( 1.54%)</td><td>29.9 ( 2.15%)</td><td>33.5</td><td>30.9</td><td>34.3</td></tr><tr><td>8</td><td>PostNorm</td><td>T-ENC</td><td>w/o</td><td>13.9 (26.68%)</td><td>17.8 (1.50%)</td><td>30.8 ( 0.13%)</td><td>33.9</td><td>30.6</td><td>34.4</td></tr></table></body></html>  

The settings above lead to eight different combinations, shown in Table 2 (#1 - #8). Additional training details are in Appendix A .

# 3.2 Main Results
We evaluate ZST systems using SacreBLEU ( Post ,2018 ) and off-target rates. We report in Table 2 BLEU scores for both zero-shot and supervised directions. For ZST, we also present pivot-based translation results as a reference. Implementation details of evaluation can be found in Appendix B.Our findings are as follows:  

PreNorm vs. PostNorm :We find that PostNorm consistently yields better BLEU scores than PreNorm for ZST across various language tag and residual connection settings, while their performance is comparable for supervised directions.  

Impact of Language Tag and Residual Connection: We observe that using the “T-ENC” language tag and “w/ Res.” improves ZST performance for IWSLT, which aligns with the findings of $\\mathrm{Wu}$ et al. (2021 ) and Liu et al. (2021 ). Nevertheless, the best performance is achieved using “w/ Res.” for PostNorm with “S-ENC-T-DEC” and “T-ENC” tags for OPUS and Europarl, respectively (#2 and #4). Given that Wu et al. (2021 ) and Liu et al. (2021 )used PreNorm as the default setting (#2, #4, #6 and #8 are unreported results in their work), our results emphasize the need to consider PostNorm as the default setting for ZST, while the language tag and residual connection settings have less impact.  

Off-target Rates : Off-target rates help understand the different BLEU score gaps between PreNorm and PostNorm, which ranges from 0 .5 to 12 .3 BLEU points. For PreNorm and PostNorm with the “T-ENC” language tag (#3, #4, #7, and #8), they have similar off-target rates, with a discrepancy ranging from $-0.61\\%$ to $2.02\\%$ , which results in narrow BLEU score gaps, ranging from 0 .5 to 1 .8 points. However, for PreNorm and PostNorm with the “S-ENC-T-DEC” language tag (#1, #2, #5, and #6), the off-target rates show a more considerable discrepancy, ranging from $5.40\\%$ to $54.23\\%$ , resulting in BLEU score gaps from 1 .7 to 12 .3 points. Further analysis of the nature of Transformer hidden states in the next section explores the reason for these different off-target rates in translations.",1
