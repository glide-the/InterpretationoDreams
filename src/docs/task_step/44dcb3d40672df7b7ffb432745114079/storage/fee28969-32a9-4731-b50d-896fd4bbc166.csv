角色,内容,分镜
fee28969-32a9-4731-b50d-896fd4bbc166,评估论文中提出的未来研究方向与挑战,5
fee28969-32a9-4731-b50d-896fd4bbc166,为了克服当前模型蒸馏的局限性并提升模型性能和泛化能力，在设计更智能的知识转移机制、探索新的模型结构和训练策略方面，可能会面临哪些具体困难？  ,5
fee28969-32a9-4731-b50d-896fd4bbc166,"ref_ids: 454846429944424370, chunk_ids: 5, Score: 0.5039, Text: # B. Challenges
(1) The Balance Between Model Compression and Performance  

KDisaneffectivemodelcompressiontechnique,andeffective knowledge transfer helps with the construction of a lightweight network model; such lightweight models are more suitable for practical applications with particular model efficiency requirements and device performance limitations [4] ,[61] ,[62] .Complex networks have large-scale network parameters and a long running time, making them unsuitable for such practical applications [63] . However, it is also challenging for lightweight networks to learn effective visual features from large-scale complex datasets [4] . A balance must therefore be struck between model compression and model performance improvement.  

(2) The Imbalance Between Multi-Modal Data Features  

At present, most KD-based OD models focus on transferring knowledge within the image domain; only a few works have attempted to extract additional features from other modal data (infrared images, depth images, text, index data, etc.) [19] ,[35] ,[55] . The introduction of multi-modal features is beneficial to KD. However, a more challenging problem is that of how to deal with the imbalance between multi-modal data features. Here, “imbalance” means that significant differences exist between the feature dimensions and semantic information of different modalities. For example, the dimensions of features extracted from visual data and index data were significantly different. In addition, the semantic feature gaps between the textual and visualfeaturesofRGBimagesalsomakeitdifficulttousetextinformation to guide the visual feature learning of student models. Therefore, another challenge is that of combining imbalanced multi-modal features to guide student models for feature learning [19] ,[64] ; new feature fusion mechanisms or multi-modal information-guided mechanisms need to be designed for student model learning.  

(3) Designing or Selecting Superior Teacher and Student Models  

KDtechnologyisutilizedtotransfertheknowledgelearnedby complex teacher models to lightweight student models. An optimal teacher model or model combination has a very favourable influence on guiding the feature learning of a student model. Therefore, the selection of teacher and student models makes a very important contribution to the performance of the final student models [53] ,[65] . However, as there are many complex and lightweight models for OD, it is difficult to choose the appropriate teacher and student models for specific OD problems.",5
fee28969-32a9-4731-b50d-896fd4bbc166,"ref_ids: 454967402972578392, chunk_ids: 6, Score: 0.3535, Text: # Discussion & Conclusion
Our experiments demonstrate our approach can distill knowledge between two or more models regardless of architecture, algorithm, feature overlap, and under small or large data settings. Since our method targets specific weaknesses of each model, we can distill knowledge between any combination of high and/or low-performance models, compared to traditional knowledge distillation techniques which tend to only distill knowledge from a single high-performance model to a low-performance model (Hinton 2015)(Gou et al. 2021b). Though our method performed well on real-world data sets it does have some assumptions. It assumes there is some overlap between the features of the data sets and most importantly, our method works best when the distribution of the datasets used to train models are significantly different from each other. Further, our method is fundamentally limited by the strength of counterfactual generation. Counterfactual explanations are easy to compute on tabular data but their performance on more complex data, such as images is more challenging. However, more recent approaches have found success in more basic image networks (Goyal et al. 2019) (Sauer and Geiger 2021), so as research progresses, we believe this limitation will be removed.  

We show in Figure 5 the number of instances each model teaches to the others. Interestingly, this quantity is asymmetrical which will motivate future work to better understand the mechanisms of how each model teaches the others.  

Conclusion We present a novel form of knowledge distillation that can be used between multiple models, in multiple directions and is focused. Each model simultaneously acts as teacher and student, distilling knowledge to the other by encoding learned information into virtual counterfactual instances and passing them into the training sets of other models. Unlike other knowledge distillation algorithms, which always distill knowledge from the teacher to student, we use a targeting mechanism to ensure that teachers only distill correct knowledge tailored to a student’s deficiencies.  

In our four main experiments, our method beats the competitors studied, including state-of-the-art knowledge distillation algorithms. In a stress test to determine if knowledge could be distilled between many (10) models, our model surpasses all but one competitor and remains competitive. We find our method particularly useful in the setting where models can be freely shared, but raw data cannot, and the data sets share some features. This is common in medical imaging or finance communities where data is confidential. Given our method’s strong performance on experiments simulating the aforementioned setting, we believe this to be a viable approach to knowledge distillation under such circumstances.
#
Ahn, S.; Hu, S. X.; Damianou, A.; Lawrence, N. D.; and Dai, Z. 2019. Variational Information Distillation for Knowledge Transfer. arXiv:1904.05835.   
Allison, P. D. 2001. Missing Data . Sage Publications. Alsenani, D. 2020. US Cars Dataset: Online Car Auction in North American. Retrieved from https://www.kaggle.com/doaaalsenani/usa-cers-dataset. Ashrapov, I. 2020. Tabular GANs for uneven distribution. arXiv preprint arXiv:2010.00638 .  
Chen, D.; Mei, J.-P.; Wang, C.; Feng, Y.; and Chen, C. 2020. Online Knowledge Distillation With Diverse Peers. In Proceedings of the AAAI Conference on Artificial Intelligence ,volume 34, 3430–3437.   
Chen, D.; Mei, J.-P.; Zhang, Y.; Wang, C.; Wang, Z.; Feng, Y.; and Chen, C. 2021. Cross-layer distillation with semantic calibration. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 35.   
Dua, D.; and Graff, C. 2017. UCI Machine Learning Repository. Http://archive.ics.uci.edu/ml.   
Gou, J.; Yu, B.; Maybank, S. J.; and Tao, D. 2021a. Knowledge distillation: A survey. International Journal of Computer Vision , 129(6): 1789–1819.   
Gou, J.; Yu, B.; Maybank, S. J.; and Tao, D. 2021b. Knowledge Distillation: A Survey. International Journal of Computer Vision , 129(6).   
Goyal, Y.; Wu, Z.; Ernst, J.; Batra, D.; Parikh, D.; and Lee, S. 2019. Counterfactual visual explanations. In International Conference on Machine Learning , 2376–2384. PMLR. Gromski, P. S.; Xu, Y.; Kotze, H. L.; Correa, E.; Ellis, D. I.; Armitage, E. G.; Turner, M. L.; and Goodacre, R. 2014. Influence of Missing Values Substitutes on Multivariate Analysis of Metabolomics Data. Metabolites , 4(2): 433–452. Heo, B.; Lee, M.; Yun, S.; and Choi, J. Y. 2019. Knowledge Transfer via Distillation of Activation Boundaries Formed by Hidden Neurons. Proceedings of the AAAI Conference on Artificial Intelligence , 33(01).   
Hinton, G. e. A. 2015. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531 .  
Janosi, A. e. a. 1988. Heart Disease Data Sets.   
Keane, M. T.; and Smyth, B. 2020. Good counterfactuals and where to find them: A case-based technique for generating counterfactuals for explainable ai (xai). In International Conference on Case-Based Reasoning . Springer.   
Kennedy, J.; and Eberhart, R. 1995. Particle swarm optimization. In Proceedings of ICNN’95-international conference on neural networks , volume 4, 1942–1948. IEEE. Kim, J.; and Park, S. 2020. Paraphrasing complex network: Network compression via factor transfer. arXiv preprint arXiv:1802.04977 .  
Liu, Y.; Cao, J.; Li, B.; Yuan, C.; Hu, W.; Li, Y.; and Duan, Y. 2019. Knowledge Distillation via Instance Relationship Graph. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) .Mirzadeh, S. I.; Farajtabar, M.; Li, A.; Levine, N.; Matsukawa, A.; and Ghasemzadeh, H. 2020. Improved knowledge distillation via teacher assistant. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 34, 5191–5198.   
Mital, A. 2020. US Used cars dataset. Https://www.kaggle.com/ananaymital/us-used-cars-dataset. Molnar, C. 2019. Interpretable Machine Learning . Independently published.   
Pan, S. J.; and Yang, Q. 2010. A Survey on Transfer Learning. IEEE Transactions on Knowledge and Data Engineering , 22(10): 1345–1359.   
Radford, A.; Metz, L.; and Chintala, S. 2016. Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434 .Reese, A. 2021. Used Cars Dataset Vehicles listings from Craigslist.org. Https://www.kaggle.com/austinreese/craigslist-carstrucksdata.   
Romero, A.; Ballas, N.; Kahou, S. E.; Chassang, A.; Gatta, C.; and Bengio, Y. 2014. Fitnets: Hints for thin deep nets. arXiv preprint arXiv:1412.6550 .  
Sauer, A.; and Geiger, A. 2021. Counterfactual generative networks. arXiv preprint arXiv:2101.06046 .  
Tung, F.; and Mori, G. 2019. Similarity-preserving knowledge distillation. In Proceedings of the IEEE/CVF International Conference on Computer Vision , 1365–1374.   
Xiao, H.; Rasul, K.; and Vollgraf, R. 2017. Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms. arXiv:1708.07747.   
Xie, Q.; Luong, M.-T.; Hovy, E.; and Le, Q. V. 2020. Selftraining with noisy student improves imagenet classification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 10687–10698.   
Yim, J.; Joo, D.; Bae, J.; and Kim, J. 2017. A Gift From Knowledge Distillation: Fast Optimization, Network Minimization and Transfer Learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) .  
Zhang, H.; Cisse, M.; Dauphin, Y. N.; and Lopez-Paz, D. 2017. mixup: Beyond Empirical Risk Minimization.   
Zhang, H.; Hu, Z.; Qin, W.; Xu, M.; and Wang, M. 2021. Adversarial co-distillation learning for image recognition. Pattern Recognition , 111: 107659.   
Zhang, Y.; Xiang, T.; Hospedales, T. M.; and Lu, H. 2018. Deep mutual learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , 4320– 4328.   
Zhang, Z.; and Sabuncu, M. R. 2020. Self-distillation as instance-specific label smoothing. arXiv preprint arXiv:2006.05065 .  
Zhuang, F.; Qi, Z.; Duan, K.; Xi, D.; Zhu, Y.; Zhu, H.; Xiong, H.; and He, Q. 2020. A comprehensive survey on transfer learning. Proceedings of the IEEE , 109(1): 43–76.",5
fee28969-32a9-4731-b50d-896fd4bbc166,"ref_ids: 454845941042685952, chunk_ids: 2, Score: 0.3242, Text: # 4 Method
Figure 2a outlines the KiOP framework, which operates over two main phases: the Synthesize Period and the Knowledge Storing Period.

# 4.1. Synthesize Period.
Initially, the system takes in two distinct models a splits the $\\mathcal{V}\\mathcal{P}$ designated for $(\\mathcal{P}\\mathcal{P})$ training into t . Model A o components: the Prompt Core ( assumes the role of a secondary transfer agent, serving as a PC ) and the Prompt Periphery backbone of $\\mathcal{V}\\mathcal{P}$ for the knowledge transfer process. In addition, Model $\\boldsymbol{\\beta}$ is cast as the primary service receiver of the knowledge being transferred to $\\mathcal{V}\\mathcal{P}$ . Within our system, both Model $\\mathcal{A}$ and Model $\\boldsymbol{\\beta}$ are set frozen, meaning bot of them do not going through further training during the whole pipeline. Model a dual Model Fusion operation to yield two derivative models—Student of Model A undergoes $\\mathcal{A}\\ (S\\mathcal{M}A)$ and udent of Model $\\boldsymbol{\\beta}$ $(S\\mathcal M B)$ .$S\\mathcal{M}A$ emerges from the fusion to Mo integration of these elements, a pre-determined label mapping strategy is applied PC . Conversely, A with PC a SMB is a composite of Model instrumental in imparting A h,PC nowled , and PP of Model . Upon the A to $_{S M B}$ ’s output to align it with the source dataset associated with Model $\\boldsymbol{\\beta}$ .  

$\\bullet$ del n and Input Passing. Figure 2b details the components of SMA and SMB and the procedure for dividing utilizing $\\mathcal{V P}$ . The left side of Figure 2b depicts the division of a full $\\mathcal{V P}$ into PC and $\\mathcal{P P}$ , intended for later use. This split is strategic, with the goal of appropriately allocating parameter capacity to handle knowledge transfer tasks of varying complexity. For $S\\mathcal{M}A$ , the challenge is akin to transferring knowledge within the same domain. Although Model $\\mathcal{A}$ hich we refer to as the core model, has been pre-equipped with a $\\mathcal{P}\\mathcal{C}$ layer, model was trained, SMA retains a fundamental grasp of the source d its task simpler than that of $S\\mathcal{M B}.\\ S\\mathcal{M B}$ nthe core , sharing the core model with as the core model is unfamiliar with Mo SMA , faces a full-fled $\\boldsymbol{\\beta}$ ’s source data. As shown in Figure d cross-domain learning challenge, $\\mathcal{P}\\mathcal{C}$ is utilized by both $S\\mathcal{M}A$ and SMB , while $\\mathcal{P P}$ is used exclusively by SMB . In our experiments, we assess the impact of various $\\mathcal{P}\\mathcal{C}$ -$\\mathcal{P P}$ configurations on overall performance. We discover that different source model pairs respond diversely to the size of $\\mathcal{P}\\mathcal{C}{-}\\mathcal{P}\\mathcal{P}$ setups.  

•data-free scenario, we adopted ideas from previous DFKD works as earlier noted Synthesize System Design. Considering a more realistic and challenging in Sec.3 and introduced a Synthesize System. This system is designed to extract synthetic data from two source models to facilitate the learning process. As illustrated in Figure 2c, the system takes a model pair and a random Gaussian noise vector $z$ as inputs and uses an initialized generator to create synthetic data. For the design of the Data-Free Distillation Block, we draw inspiration from [16] as previously stated, which incorporates contrastive learning to enhance the diversity of the synthetic samples, thus, we firstly pass the synthesized data through an augmenter for random augmentation, then feed it together with the data previously stored in the data bank to the model pair and get the judgement for its quality, thereby ensuring the diversity of the synthetic data.

# 4.2. Knowledge Storing Period.
In each training iteration, the two model pairs are passed through the Synthesize System a total of $T$ times. Note that with each iteration, we start with a freshly initialized generator. As shown in Figure 2c, during the $T$ times, once we get a better loss value, we will update the current optimal data. Upon the completion of $T$ times, the optimal data is then preserved in the data bank to assist with the training of the prompt in subsequent steps. As the data bank grows with each addition of the best data from these iterations, the prompt is exposed to an increasingly rich array of knowledge. This process ensures a progressively more effective knowledge transfer and storage, as the prompt harnesses a broader spectrum of information with each iteration.  

In the Knowledge Storing Period in Figure 2a, we train the $\\mathcal{P}\\mathcal{C}$ and $\\mathcal{P P}$ with the synthetic data that has been curated in the data bank. It’s important to emphasize that before this period, all the models in the framework—the two source models and one prompt—are set frozen. Focus g o Model Pair $\\mathcal{A}$ aim is to facilitate the knowledge transfer from Model $\\mathcal{A}$ to PC within the MA model. To do so, we feed the data from Data Bank A into both Model $\\mathcal{A}$ and $S\\mathcal{M}A$ . The outputs fro h—termed $\\mathcal{P}_{\\mathcal{M A}}$ for the output of Model A , and ${\\mathcal{P}}_{S{\\mathcal{M A}}}$ for the output of SMA —are then ed to calculate the similarity so that $S\\mathcal{M}A$ can imitate the behavior of Model A :  

$$
\\mathcal{L}_{k}^{A}=\\sum_{\\hat{x}_{A}\\in\\mathcal{B}_{k}^{A}}\\mathrm{Sim}\\left(f_{A}(\\hat{x}_{A}),f_{A}(\\gamma\\mathcal{P}c,\\varphi(\\hat{x}_{A}))\\right)
$$  

where $\\operatorname{Sim}(\\cdot,\\cdot)$ is the pre-defined similar y measu ement (i.e., KL-Divergance), $B_{k}^{A}$ re esents the state of data bank A in the k-th iteration; $f_{A}(\\cdot)$ denotes model A ;$\\hat{x}_{A}$ stands for the data sel ted from $B_{k}^{A}$ ;$\\gamma_{\\mathcal{P C},\\varphi}(\\cdot)$ is the Prompt Core arameterized by $\\varphi$ . For M del P $\\boldsymbol{\\beta}$ we use the synthetic data from data bank the pre-defined label mapping method, the output dimension of that of Model Bas input for both model $\\boldsymbol{\\beta}$ thus we can measure their similarity as follows: Band SMB , performing identical ope SMB s. Due to matches  

$$
\\mathcal{L}_{k}^{\\mathcal{B}}=\\sum_{\\hat{x}_{B}\\in\\mathcal{B}_{k}^{\\mathcal{B}}}\\mathrm{Sim}\\left(f_{B}(\\hat{x}_{B}),\\mathcal{M}\\left(f_{A}\\left(\\gamma_{\\mathcal{P P},\\eta}\\big(\\gamma_{\\mathcal{P C},\\varphi}\\big(\\hat{x}_{B}\\big)\\big)\\right)\\right)\\right)
$$  

where $B_{k}^{B}$ represents the state of data bank $\\boldsymbol{\\beta}$ in the $k$ -th iteration; $f_{B}(\\cdot)$ denotes model B;${\\hat{x}}{B}$ stands f he data selected from $B_{k}^{B};\\gamma_{P\\mathcal{P}}(\\cdot)$ PP ·is the Prompt Periphery parameterized by $\\eta$ ;Mis the predefined label mapping method.  

For $\\hat{x}_{A}$ and ${\\hat{x}}{B}$ , they are all generated by the Synthesize System and stored in the corresponding data bank. For the $k$ -th iteration of a Model Pair $j$ , this process can be formally described as:  

$$
\\mathcal{B}_{k}^{j}=S\\left(z,(f_{\\mathrm{tea}}^{j}(\\cdot),f_{\\mathrm{stu}}^{j}(\\cdot)),k,T\\right)
$$  

where $\\boldsymbol{S}$ represents the Synthesize System as described in Eq.4, $(f_{\\mathrm{tea}}^{j}(\\cdot),f_{\\mathrm{stu}}^{j}(\\cdot))$ ··represents the currently used Model Pair, in which $f_{\\mathrm{tea}}^{j}(\\cdot)$ ·is the knowledge pro der (teacher) of synthetic data, and $\\tau$ represents predefined round number for S. The final objective function can be defined as:  

$$
\\begin{array}{r}{\\begin{array}{r l}{\\mathcal{L}_{\\mathrm{KiOP}}=\\alpha\\cdot\\mathcal{L}_{A}(\\mathcal{B}_{A},f_{A},\\gamma_{P C,\\varphi},S)}&{{}}\\\\ {+\\;\\beta\\cdot\\mathcal{L}_{B}(\\mathcal{B}_{B},f_{A},\\gamma_{P C,\\varphi},\\gamma_{P P,\\eta},S,\\mathcal{M})}&{{}}\\end{array}}\\end{array}
$$  

where $\\alpha$ and $\\beta$ represent the coefficients that control the weight of the two components. In our experiment, unless stated differently, we set them both to 1.",5
