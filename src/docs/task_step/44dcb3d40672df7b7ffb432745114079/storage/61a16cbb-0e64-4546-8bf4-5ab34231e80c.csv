角色,内容,分镜
61a16cbb-0e64-4546-8bf4-5ab34231e80c,未来研究方向,6
61a16cbb-0e64-4546-8bf4-5ab34231e80c,"### 问题：
在模型蒸馏的未来研究方向中，如何设计一种更高效的蒸馏方法，以确保学生模型在保持计算效率的同时，能够更有效地从教师模型中迁移知识，并提升其在未见数据上的泛化能力？此外，多模态蒸馏的拓展如何在不同模态（如图像和文本）之间实现更有效的知识迁移，以应对复杂多模态任务的需求？",6
61a16cbb-0e64-4546-8bf4-5ab34231e80c,"ref_ids: 454965221526340508, chunk_ids: 0, Score: 0.6289, Text: # 6 CONCLUSION
In this paper, we proposed a new method for model compression, which simultaneously considers model accuracy and model size. In particular, we first revisited model compression and analyzed the factors that influence the performance of model compression. Specifically, we found that there are two attributes for model compression: distillability and sparsablity. Distillability indicates how much useful knowledge can be extracted from a teacher network. Sparsablity indicates the extent to which the model can be pruned. By exploring distillability and sparsablity, a guide for model compression during training can be obtained. Inspired by our observations, we proposed a novel dynamically distillability-and-sparsablity learning framework (DDSL), comprising a teacher, a student and a dean. The teacher guides the student using the distilled knowledge, while the student learns to become more accurate and more compact. In addition, the dean controls the whole learning process. In this manner, the supervisions are dynamically adjusted and a good balance is achieved. In order to optimize the proposed framework, an ADMM-based knowledge distillation-with-pruning (KDP) joint optimization algorithm was presented for updating the model. The joint optimization improves the accuracy and the pruned ratio of our method. Finally, experimental results showed that our method outperforms 24 state-of-the-art methods in terms of accuracy and pruned ratio on both small-scale and large-scale databases.  

  
Fig. 15. The test Top-1 accuracy curves and pruned ratio PR FLOPS curves at different outer loop learning rates $\\zeta_{\\mathrm{out}}$ and batch sizes. Note that the inter loop learning rate $\\zeta_{\\mathrm{in}}$ is commonly set to 0.1. (a) The curves of pruning ResNet101 on ImageNet. (b) The curves of pruning MobileNetV2 on ImageNet.  

We foresee three directions for future research in this area. First, it would be promising to extend our method to datafree situation. Second, the acceleration and simplification of the training procedure is another future work, for making it more convenient and efficient. Third, in addition to the convolutional neural network, it is interesting to extend our method to other architectures, such as Transformer [ 70 ].

# A CKNOWLEDGMENTS
This work was supported by the National Key Research and Development Program of China (Grant No. 2020AAA0106800), the Natural Science Foundation of China (Grant No.61902401, No. 62192785, No. 61972071, No. U1936204, No. 62122086, No. 62036011, No. 62192782 and No. 61721004), the Beijing Natural Science Foundation No. M22005, the CAS Key Research Program of Frontier Sciences (Grant No. QYZDJ-SSW-JSC040). The work of Bing Li was also supported by the Youth Innovation Promotion Association, CAS.",6
61a16cbb-0e64-4546-8bf4-5ab34231e80c,"ref_ids: 454846429944424370, chunk_ids: 5, Score: 0.5938, Text: # 2. Related Work
Knowledge distillation (KD) was initially introduced as a model compression technique [ 7 ], where the goal is to train a smaller student model from the output of a teacher model [ 23 ]. While early work focused on predicting the final outputs of a classification model, the idea was rapidly extended to other forms of distillation, such as distilling intermediate representations [ 1 ,21 ,22 ,49 ,73 ,75 ,79 ]. These methods perform well but require careful layer selection and loss balancing [ 21 ]. In our work, instead of matching layer-wise representations between the student and teacher architectures, we add shortcut connections from intermediate layers of the student to the loss of each teacher.  

Multi-teacher knowledge distillation. KD can naturally be extended to an ensemble of teachers so that student can benefit from their potential complementarity. While the final outputs of teachers trained for the same task can simply be averaged [ 3 ,15 ,23 ,75 ], multi-teacher distillation with teachers trained for different tasks is more challenging. UDON [ 76 ] first trains domain-specialist teachers which are subsequently distilled in a student model using adaptive data sampling for balancing the different domains. In [ 60 ], contrastive learning is used for ensemble distillation while [ 56 ] proposes a framework tailored for teachers trained with masked image modeling and contrastive learning. But such approaches are not straightforward to extend to teachers learned differently. Similarly, [ 71 ] combines self-supervised teachers from arbitrary heterogeneous pretext tasks. [13 ,16 ,51 ] focus on jointly utilizing pseudo- and true labels for multi-teacher distillation. Roth et al .[51 ] formulate multi-teacher distillation as continual learning and further propose a novel method for data partitioning based on confidence. Here we develop a more generic method for combining teachers, that is not limited to certain types of teachers or losses, and, unlike [ 30 ,51 ], does not require labeled data, nor classifiers associated with each teacher for obtaining pseudo-labels.  

Loss balancing is shown to be crucial in multi-task learning [ 11 ,24 ,26 ,78 ]. Similar strategies to automatically balance losses have also been proposed for multi-teacher distillation [ 15 ,32 ]. In [ 24 ], adaptive loss weights inversely proportional to the average of each loss are introduced, while [ 32 ]learns instance-level teacher importance weights using ground-truth labels. In [ 15 ], the random selection of one teacher per mini-batch is shown to help. Our experiments show that our proposed generalized teacher dropping strategy leads to better models compared to [ 15 ,24 ].  

Distilling from a “foundation model” like CLIP [43 ] or DINOv2 [ 39 ] is an effective approach for tasks with limited training data [ 36 ,42 ,67 ]. Distilling from multiple foundation models allows for more versatile students. Recent works like AMRADIO [ 46 ], SAM-CLIP [ 65 ], and Open Vocabulary SAM [ 77 ] combine the semantics captured by CLIP with the localization capabilities of models like DINOv2 [ 39 ] or SAM [ 27 ]. AM-RADIO [ 46 ] builds on the same base setup as our study, but employs no loss balancing. Another difference comes from the fact that their student encoder is only a part of the final model: AM-RADIO requires the teacher-specific projectors learned during distillation to also be used at test time, effectively increasing the parameters of the encoder with task-specific ones. Instead, our method performs well on multiple classification tasks out-ofthe-box , without any additional parameters.  

Combining models beyond distillation. Other ways to combine pretrained models have been proposed. Works like [ 37 ,44 ,45 ,59 ,68 ] explore different weight averaging strategies. They typically only combine models that differ by their hyperparameter configuration. Aiming at generalization, [72 ] merges multiple ViTs, each specialized to a classification task, into a single encoder that solves all classification tasks jointly, via a gating network. Instead, our students are distilled from scratch, have a simple ViT architecture, and tackle diverse tasks with simple linear probing.  

Expendable projectors are extra modules that act as buffers between the final encoder output and the space where the loss is computed. They have been successfully used for both self-supervised [ 9 ,10 ] and supervised learning [ 55 ,66 ]. We extend this idea and add projectors during training to intermediate layers as well. Roth et al . [ 50 ] use several such projectors of varied dimensionality for metric learning, but do not use features from intermediate layers. Moreover, we use a specific set of projectors per teacher, similar to [ 3 ,46 ]. This way, projectors become loss-specific ,i.e . they contribute to the loss for only one of the teachers.

# 3. Improving multi-teacher distillation
In this section we first present the multi-teacher distillation setup we use as a basis for our analysis (Sec. 3.1 ) and a summary of our evaluation protocol (Sec. 3.2 ). We then delve into challenges around multi-teacher distillation of ViT encoders (Sec. 3.3 ), and offer improvements to the basic setup to overcome them, like enhanced expendable teacherspecific projectors heads (Sec. 3.4 ) and strategies to more equally learn from all teachers (Sec. 3.5 ).",6
61a16cbb-0e64-4546-8bf4-5ab34231e80c,"ref_ids: 454845510461164774, chunk_ids: 2, Score: 0.5508, Text: # 2 Related Work
Language Model Compression. Pre-trained language models ( Devlin et al. ,2019 ;Clark et al. ,2020 ;Brown et al. ,2020 ) perform remarkably well on various applications but at the cost of high computation and memory usage. To deploy these powerful models into resource-scarce scenarios, various attempts have been made to compress the language models into small ones. Quantization methods ( Zafrir et al. ,2019 ;Shen et al. ,2020 ;Zhang et al. ,2020 ;Bai et al. ,2021 ) convert the model parameters to lower precision. Pruning approaches identify then remove unimportant individual weights or structures ( Michel et al. ,2019 ;Fan et al. ,2019 ;Gordon et al. ,2020 ;Hou et al. ,2020 ). Weight sharing techniques ( Dehghani et al. ,2018 ;Lan et al. ,2019 ) allow the model to reuse the transformer layer multiple times to reduce parameters.  

Knowledge Distillation. Knowledge distillation ( Hinton et al. ,2015 ) is another major line of research to do model compression, which is the main concentration in this paper. Hinton et al. (2015 )first proposed to minimize the KL-divergence between the predicted distributions of the teacher and the student. Sanh et al. (2019 ); Sun et al. (2019 ); Liang et al. (2020 ) adopted this objective to teach the student on masked language modeling or text classification tasks. Romero et al. (2014 ) proposed to directly match the feature activations of the teacher and the student. Jiao et al. (2020 ) followed the idea and took the intermediate representations in each transformer layer of the teacher as one of the knowledge to be transferred. Tian et al. (2019 ) proposed a contrastive distillation framework where the teacher’s representations were treated as positives to the corresponding student’s representations. Sun et al. (2020 ); Fu et al. (2021 ) customized this idea to language model compression and proved its effectiveness. Researchers also attempted to use the mutual relations of representations as the knowledge to transfer. In the literature of image classification, Peng et al. (2019 ); Tung and Mori (2019 ); Park et al. (2019 ) pointed out that the relations of the image representations of the teacher should be preserved in the student’s feature space, and adopted a series of geometric measurements to model the sample relations. For distilling transformer models, Park et al. (2021 ) enforced the relations across tokens and layers between the teacher and the student to be consistent. Jiao et al. (2020 ); Wang et al. (2020 ,2021 ) used the attention dependencies between tokens to teach the student. In this paper, we propose to transfer the multi-granularity knowledge to the student. Different from previous works that only considered a single granularity of representations, we jointly transfer the token-level, span-level and sample-level structural knowledge. And compared with Shao and Chen (2021 ) which considered the multi-granularity visual features in an image as the knowledge, our method works in a different modality, presents a different definition of granularity, and prepares the multi-granularity knowledge as the structural relations among representations.  

  
Figure 1: The overall framework of MGSKD.

# 3 Method
We propose MultiGranularity Structural Knowledge Distillation, a novel framework to distill the knowledge from a large transformer language model to a small one. Different from previous works that transferred the knowledge derived from either token-level or sample-level outputs, we prepare the knowledge in three semantic granularities: token-level, span-level and sample-level. Given some granularity of representations of the teacher model, we form the knowledge as the structural relations, i.e., the pair-wise interactions and the triplet-wise geometric angles, between the representations. We then distill the well-organized structural knowledge to the student hierarchically across layers, where the token-level and the span-level knowledge are transferred to the bottom layers to provide more syntactic guidance while the sample-level knowledge is transferred to the upper layers to offer more help of semantic understanding. The framework of MGSKD is illustrated in Figure 1 .

# 3.1 Multi-granularity Representation
Natural languages have multiple granularities of conceptual units. In the context of pre-trained transformers ( Devlin et al. ,2019 ), the basic unit is the tokens produced by sub-word tokenizers ( Wu et al. ,2016 ;Radford et al. ,2019 ). Several consecutive tokens become a text span, and the sample is comprised of all the tokens it contains. Existing knowledge distillation approaches ( Jiao et al. ,2020 ;Wang et al. ,2020 ;Sun et al. ,2020 ;Fu et al. ,2021 ) focused on one granularity of representation, neglecting that texts are built upon language units from multiple granularities. Intuitively, incorporating multi-granularity representations in knowledge distillation may provide more guidance since the student can be taught how to compose the semantic concepts from small granularities to larger ones. Therefore, we propose to gather multi-granularity representations for knowledge distillation. We construct three granularities of representations: tokens, spans that hold complete meanings, and samples.  

Token Representation. The first granularity is the sub-word token, which is the foundation of high-level granularity. Given an input text, a tokenizer such as WordPiece ( Wu et al. ,2016 ) splits it into $n$ tokens $x\\,=\\,[t_{1},t_{2},.\\ldots,t_{n}]$ . The tokens are converted to a sequence of continuous representations $\\pmb{E}=[e_{1},e_{2},\\allowbreak...,e_{n}]\\in\\mathbb{R}^{n\\times d}$ through the embedding layer. For the sake of clarity, we treat the embedding layer as the 0 -th layer and set $H^{0}\\;=\\;E$ . Then the token embeddings $H^{0}$ are passed to $L$ stacked transformer layers. The $l$ -th layer takes the output representations $H^{l-1}$ of the previous layer as its input, and returns the updated representations $H^{l}$ using multi-head attention (MHA) and position-wise feed-forward network (FFN). Herein, we obtain $L{+1}$ layers of token representations $\\{H^{l}\\}_{l=0}^{L}$ where $\\pmb{H}^{l}\\in\\mathbb{R}^{n\\times d}$ .  

Span Representation. The second granularity is the span, which is comprised of several consecutive tokens. Different from SpanBERT ( Joshi et al. ,2020 ) that randomly selects token spans whose start positions and lengths are sampled from some distributions for masked language modeling, we propose to extract spans that have complete meanings. Widely adopted sub-word tokenizers in pre-trained transformers split some of the English words into several sub-word tokens. We consider these whole words consisting of multiple sub-word tokens, and phrases, as meaningful spans. Sub-word tokens for whole words are easy to obtain using WordPiece tokenizer ( Wu et al. ,2016 ). While for phrase identification, we train a classifier-based English chunker on CoNLL-2000 corpus ( Tjong Kim Sang and Buchholz ,2000 ) following the instructions 1 . We then use the trained chunker to extract noun phrases (NP), verb phrases (VP), and prepositional phrases (PP). These identified phrases are tokenized by WordPiece tokenizer to obtain tokens. Herein, we can obtain $n_{s}$ token spans $x_{\\mathrm{span}}=[s_{1},s_{2},...\\,,s_{n_{s}}]$ ,where $s_{i}=[t_{j},t_{j+1},\\ldots,t_{j+n_{s_{i}}-1}]$ denotes the $i$ -th span that starts at the $j$ -th token and contains $n_{s_{i}}$ tokens. We then build span representations based on token representations using mean pooling:  

$$
\\begin{array}{r}{\\hat{h}_{i}^{l}=\\operatorname{Pool}(H_{j:j+n_{s_{i}}}^{l}),}\\end{array}
$$  

various relations to other elements, we propose that the knowledge is better specified as the structural relations of the representations in a semantic space, instead of the individual representations themselves. Therefore, instead of directly matching each hidden representation between the teacher and the student, we propose to extract structural relations from multi-granularity representations as the knowledge to teach the student. We first project the representations into multiple sub-spaces, then we extract two types of structural knowledge: pairwise interactions and triplet-wise geometric angles.  

Sample Representation. The third granularity is the input text sample itself. Based on token representations again, we use mean-pooling to aggregate all the token representations in a text sample to form sample representation:  

where $\\hat{h}_{i}^{l}\\,\\in\\,\\mathbb{R}^{d}$ ∈is the repr ion of the $i$ -th span in layer l. We obtain $L+1$ layers of span representations as $\\{\\hat{H}^{l}\\}_{l=0}^{L}$ }where $\\hat{H}^{l}\\in\\mathbb{R}^{n_{s}\\times{d}}$ ∈.  

Herein, we get $L+1$ layers of sample representations as $\\{\\tilde{h}^{l}\\}_{l=0}^{L}$ where $\\bar{\\tilde{h}}^{l}\\in\\mathbb{R}^{d}$ .  

$$
\\tilde{h}^{l}=\\mathrm{Pool}({\\cal H}^{l}),
$$",6
