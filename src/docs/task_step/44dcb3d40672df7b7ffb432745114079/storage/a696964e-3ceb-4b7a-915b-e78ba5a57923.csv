角色,内容,分镜
a696964e-3ceb-4b7a-915b-e78ba5a57923,探讨计算模型在不同数据集与应用场景下的适用性与泛化能力,3
a696964e-3ceb-4b7a-915b-e78ba5a57923,在多领域、多模态数据场景下，为了让模型蒸馏能有效融合不同模态知识，可能需要设计哪些复杂机制？  ,3
a696964e-3ceb-4b7a-915b-e78ba5a57923,"ref_ids: 454848340878587076, chunk_ids: 0, Score: 0.5117, Text: # MULTIMODAL FEDERATED LEARNING VIA CON -TRASTIVE REPRESENTATION ENSEMBLE
Qiying $\\mathbf{Y_{u}}^{1,4}$ , Yang ${\\bf L i u^{1,4*}}$ ∗, Yimu Wang 2 , Ke $\\mathbf{X}\\mathbf{u}^{3}$ , Jingjing ${\\mathrm{Li}}{\\bf u}^{1*}$  

1 Institute for AI Industry Research, Tsinghua University   
2 University of Waterloo 3 Carnegie Mellon University   
4 Shanghai Artificial Intelligence Laboratory   
, {liuy03,jjliu }@air.tsinghua.edu.cn

# A BSTRACT
With the increasing amount of multimedia data on modern mobile systems and IoT infrastructures, harnessing these rich multimodal data without breaching user privacy becomes a critical issue. Federated learning (FL) serves as a privacyconscious alternative to centralized machine learning. However, existing FL methods extended to multimodal data all rely on model aggregation on single modality level, which restrains the server and clients to have identical model architecture for each modality. This limits the global model in terms of both model complexity and data capacity, let alone task diversity. In this work, we propose Contrastive Representation Ensemble and Aggregation for Multimodal FL (CreamFL) , a multimodal federated learning framework that enables training larger server models from clients with heterogeneous model architectures and data modalities, while only communicating knowledge on public dataset. To achieve better multimodal representation fusion, we design a global-local cross-modal ensemble strategy to aggregate client representations. To mitigate local model drift caused by two unprecedented heterogeneous factors stemming from multimodal discrepancy ( modality $g a p$ and task gap ), we further propose inter-modal and intra-modal contrasts to regularize local training, which complements information of the absent modality for uni-modal clients and regularizes local clients to head towards global consensus. Thorough evaluations and ablation studies on image-text retrieval and VQA tasks showcase the superiority of CreamFL over state-of-the-art FL methods.

# 1 I NTRODUCTION
Federated Learning (FL) (Yang et al., 2019; Li et al., 2020; Kairouz et al., 2021; Zhao et al., 2018), a decentralized training paradigm that allows multiple parties to collaboratively train models without compromising privacy, has emerged as an alternative to centralized machine learning. Most existing FL methods only consider scenarios where the private data from clients belong to the same modality (e.g. , image or text). However, with the fast development of mobile technology and IoT infrastructures (Brunete et al., 2021) that harness data from different modalities ( e.g. sensory, visual, audio) with privacy constraints, there is an increasing need for advanced FL algorithms to allow the training of larger and capable model that can absorb heterogeneous private data (across modalities) at edge and simultaneously handle diverse multimodal tasks (Gan et al., 2022; Chen et al., 2020b).  

In the past, there has been some early attempts at applying FL to multimodal tasks (Xiong et al., 2022; Zhao et al., 2022; Liu et al., 2020), which all adopt the FedAvg (McMahan et al., 2017) framework by using homogeneous models for each modality. In practice, however, edge devices may have limited computational and memory resources, restraining the capacity of the global model to smaller and lighter scales. Moreover, naive aggregation of modality-dependent models is inadequate in addressing the model drift (Karimireddy et al., 2020) problem between clients.  

Recently, a few algorithms (Cho et al., 2022; Cheng et al., 2021) have been proposed to enable larger server model training. For example, FedET (Cho et al., 2022) proposes an ensemble Knowledge Distillation (KD) based framework to enable a large model at server and relatively small yet deployable models on edge devices. However, they transfer and ensemble knowledge from a bag of client teachers through logit , which is difficult to extend to multimodal setting. Most multimodal tasks (e.g., image/video captioning (Vinyals et al., 2015)) typically operate on fused cross-modality representation level, whereas existing strategies for aggregating logits are no longer applicable.  

  
Figure 1: Illustration of multimodal FL. A large model at server supports multimodal tasks with public data, and heterogeneous clients at edge handle uni- and multi-modal tasks with private data.  

In this paper, we design a novel KD-based multimodal federated learning framework, CreamFL (Contrastive Representation Ensemble and Aggregation for Multimodal $F L$ ), which simultaneously leverages uni- and multi-modal data across heterogeneous clients to learn a larger global model, through representation-level ensemble knowledge transfer. The global model learns from clients via communicating private knowledge on public dataset from diverse client networks without revealing private models and data. CreamFL transmits low-dimensional representations of public data between server and clients, which are usually contextual and applicable to more complex tasks than logits. To effectively aggregate representations transmitted from heterogeneous clients, we propose a global-local cross-modal contrastive aggregation strategy, to 1) filter out drifting outliers by contrasting local representations to global ones; 2) pick out outstanding candidates that better match their paired partners, by contrasting to representations from another modality.  

Moreover, FL with multimodal data brings about two new types of model gap: modality gap and task gap . Uni-modal clients trained under a single modality ( e.g. image) have never seen data from other modalities ( e.g. , text) in the training procedure, therefore lacking the capability of recognizing another modality. We call this mutual incompatibility between clients the ‘modality gap’. Task gap refers to the fact that different clients may be trained for diverse tasks, e.g. , uni-modal clients for image classification task and multimodal clients for image-text retrieval task. Both gaps cause unprecedented model drift (Karimireddy et al., 2020) problems. To tackle this, we introduce two novel contrastive objectives to regularize local training. An inter-modal contrastive objective is designed to mitigate the modality gap, by performing cross-modality contrasts using public data in the local training phase, which complements for the information of the absent modality in unimodal clients. To bridge the task gap, an intra-modal contrastive objective is proposed to contrast local representations to their corresponding global ones in each modality, regularizing models to head towards the global consensus (Li & Wang, 2019).  

In summary, 1) CreamFL is the first KD-based multimodal FL framework to support heterogeneous modality and model architectures between server and clients, while only communicating private knowledge on public dataset without revealing private models and data. Experiments show that CreamFL outperforms other FL systems in multimodal setting in terms of both model performance and communication cost; 2) CreamFL ensembles representations instead of logits for knowledge transfer between clients and server, with a novel global-local cross-modal aggregation strategy for better representation learning and inter/intra-modal contrastive objectives to address model drift; 3) Our framework enables larger model training at server that absorbs modality-diverse knowledge from resource-constrained clients, which is required for complex cross-modal tasks.",3
a696964e-3ceb-4b7a-915b-e78ba5a57923,"ref_ids: 454845793061599624, chunk_ids: 15, Score: 0.4414, Text: # 2 RELATED WORK
Model Fusion The integration of capabilities from diverse models has been a long-standing objective, with existing approaches mainly falling into two categories. Firstly, the traditional technique of model ensemble combines the outputs of multiple models to enhance overall system performance (Littlestone & Warmuth, 1994; Sagi & Rokach, 2018). Note that this technique doesn’t involve the explicit merging of multiple models into a new one. Common methods for model ensemble typically employ weighted averaging (Littlestone & Warmuth, 1994) or majority voting (Monteith et al., 2011) to consolidate predictions from various models. Recently, Jiang et al. (2023) introduced an ensemble framework designed to leverage the diverse strengths of multiple open-source LLMs. This framework first employs a pairwise comparison method to detect subtle distinctions among candidate outputs. Then, it combines the top-ranked candidates to produce an enhanced output, capitalizing on their strengths while mitigating their weaknesses.  

Secondly, weight merging presents another approach that facilitates model fusion at the parameter level. Gupta et al. (2020) and Wortsman et al. (2022) merged weights from models with identical structures, obtained through different strategies or configurations, to achieve improved overall performance. Similarly, Cha et al. (2021), Rame et al. (2022), and Arpit et al. (2022) explored weighted averaging of models derived from different configurations to enhance out-of-distribution generalization. Furthermore, Jin et al. (2022) merged models designed for specific domains or tasks to create a generalist capable of addressing all domains or tasks. Going beyond parameter merging of entire models, Wang et al. (2022b), Huang et al. (2023), and Zhang et al. (2023) applied linear mathematical operations to adapter parameters to achieve superior generalization performance.  

In a nutshell, while model ensemble requires the parallel deployment of multiple models, weight merging is generally limited to models with identical architectures. In contrast, the approach proposed in this paper supports the fusion of multiple LLMs with diverse architectures by explicitly transferring their knowledge and capabilities to a target LLM.  

Knowledge Distillation Knowledge distillation (Hinton et al., 2015), initially proposed for model compression, involves training a student model under the guidance of one or more teacher models. In the NLP community, knowledge distillation has been widely applied to text classification tasks. These applications include training the student model to replicate the teacher’s output distribution (Sanh et al., 2019; Turc et al., 2019), as well as features (Sun et al., 2019; Jiao et al., 2020) and relations (Wang et al., 2020) derived from intermediate layers of the teacher model. In the realm of text generation, the conventional approach focuses on minimizing the KL divergence between the student and teacher generation distributions. This is achieved by using the teacher’s probability distributions at each time step as supervision (Khanuja et al., 2021; Gu et al., 2023; Agarwal et al., 2023) or by directly training on the teacher’s generated texts (Peng et al., 2023; Xu et al., 2023).  

While our method shares a framework similar to multi-teacher knowledge distillation, there are two significant distinctions. First, in traditional knowledge distillation, the student models are typically constrained to be smaller in size than the teachers. In our scenario, however, there are no limitations on the size of the target model. Second, traditional knowledge distillation often results in the student models lagging behind the teachers in performance after distillation. In contrast, we anticipate that after the fusion, the target model will surpass any of the source models in performance.

# 3 KNOWLEDGE FUSION OF LLM S
The primary objective of LLMs fusion is to externalize the collective knowledge embedded within multiple source LLMs and integrate their capabilities into a target LLM. Given $K$ source LLMs $\\{\\mathcal{M}_{j}^{s}\\}_{j=1}^{K}$ with varying architectures, each having undergone individual pre-training or fine-tuning on distinct datasets, the key idea behind our approach is to initially stimulate LLMs to manifest their inherent knowledge by challenging them to predict the next token. The probabilistic distributions of these predictions are thoroughly assessed, and the most accurate predictions are utilized to continually train the target LLM the following sections, we start with a brief introduction to the preliminaries, followed by a detailed ${\\mathcal{M}}^{t}$ on a corpus $\\mathcal{C}$ using the causal language modeling objective. In explanation of our LLMs fusion framework. Finally, we delve into the implementation details.

# 3.1 PRELIMINARIES
Let $t$ denote a text sequence of len th $N$ sampled from the corpus $\\mathcal{C}$ and $t_{<i}\\,=\\,(t_{1},t_{2},.\\,.\\,.\\,,t_{i-1})$ denote the sequence preceding the i th token. The causal language modeling (CLM) objective for training a language model parameterized by $\\theta$ is defined as minimizing the negative log-likelihood:  

$$
\\mathcal{L}_{\\mathrm{CLM}}=-\\mathbb{E}_{t\\sim\\mathcal{C}}\\left[\\sum_{i}\\log p_{\\theta}(t_{i}|t_{<i})\\right],
$$  

where $p_{\\theta}(t_{i}|t_{<i})$ is the model’s predicted probability for token $t_{i}$ given the preceding tokens.  

The above objective decomposes sequence likelihood into token-level cross-entropy losses, comparing each token’s predicted distribution to its one-hot representation. To provide a more generalized perspective, we reframe this token-level view into a sequential distribution format. Specifically, for the text sequence $t$ , we aggregate token-level predictions and create a probabilistic distribution matrix, $\\mathbf{P}_{t}^{\\theta}\\,\\in\\,\\mathbb{R}^{N\\times V}$ ∈, where the $i$ -th ro represents the distribution predicted by the model for the i th token over the vocabulary of size V. The CLM objective can then be interpreted as reducing the discrepancy between $\\mathbf{P}_{t}^{\\theta}$ and the one-hot label matrix, $\\mathbf{O}_{t}\\,\\in\\,\\{0,1\\}^{N\\times V}$ , where each row is a one-hot representation of the corresponding gold token. Formally, the CLM objective is transformed into the following representation:  

$$
\\mathcal{L}_{\\mathrm{CLM}}=-\\mathbb{E}_{t\\sim\\mathcal{C}}\\left[\\mathbb{D}(\\mathbf{P}_{t}^{\\theta},\\mathbf{O}_{t})\\right],
$$  

where $\\mathbb{D}(\\cdot,\\cdot)$ represents the discrepancy function between two matrices, and it is equivalent to Eq.   
1 when implemented as the KL divergence.",3
a696964e-3ceb-4b7a-915b-e78ba5a57923,"ref_ids: 455038130636200316, chunk_ids: 0, Score: 0.4121, Text: # 2 Related Work
Our work is closely related to MHPFL with complete model heterogeneity. These methods support flexible model heterogeneity, and can be divided into three categories: 1) knowledge distillation, 2) mutual learning, and 3) model mixup.  

Knowledge Distillation-based MHPFL . Most knowledge distillation-based MHPFL methods rely on a public dataset (e.g., FedMD [Li and Wang, 2019], FedDF [Lin and others, 2020], FCCL [Huang and others, 2022b], DS-FL [Itahara and others, 2023], CFD [Sattler and others, 2022], FedHeNN [Makhija and others, 2022], Cronus [Chang and others, 2021], FSFL [Huang and others, 2022a], FedAUX [Sattler and others, 2021], FedKT [Li and others, 2021], Fed-ET [Cho and others, 2022], FedKEMF [Yu and others, 2022], FedGEMS [Cheng and others, 2021], KT-pFL [Zhang and others, 2021]) and allow the server to aggregate the logits or representations of heterogeneous local models on a public dataset to integrate knowledge from different clients. However, the public dataset is not always available. In addition, models only perform well when the public dataset follows a similar distribution to local data. Knowledge distillation-based MHPFL methods not requiring a public dataset are starting to emerge. FedGen [Zhu and others, 2021b] and FedZKT [Zhang and others, 2022] introduce zero-shot knowledge distillation into FL to train a generator for generating local representations or public shared datasets. However, time-consuming iterative training for the generator incurs high computational costs. In FD [Jeong and others, 2018], HFD [Ahn and others, 2019; Ahn and others, 2020], FedProto [Tan and others, 2022b], FedGKT [He and others, 2020], each client uploads the (class-average) logits or representations of local data samples to the server for aggregation, the aggregated global logits or representations for each class are sent to clients. Each client calculates the distillation loss between the global logits/representation and each local data sample logits/representation belonging to the same class for training the local model. This incurs high computational costs for clients. Besides, uploading the logits/representations might compromise privacy.  

Mutual Learning-based MHPFL .FML [Shen and others, 2020] and FedKD [Wu and others, 2022] enable each client to train a large heterogeneous model and a small homogeneous model in a mutual learning manner. The large model is always trained locally and the small model is uploaded to the server for aggregation. Although they implement information interaction through the small homogeneous models, training the homogeneous model increases local computational costs for clients, and transmitting the homogeneous models incurs high communication costs.  

Model Mixup-based MHPFL . These methods split each local model into a feature extractor and a classifier. In FedRep [Collins and others, 2021], FedPer [Arivazhagan and others, 2019], FedMatch [Chen and others, 2021], FedBABU [Oh and others, 2022] and FedAlt/FedSim [Pillutla and others, 2022], the feature extractor is homogeneous and used for aggregation by the FL server to enhance generalization. The classifier can be heterogeneous. Since the feature extractor has more parameters than the classifier, these methods can only support model heterogeneity to a low degree. In contrast, LG-FedAvg [Liang and others, 2020], FedClassAvg [Jang and others, 2022] and CHFL [Liu and others, 2022] use heterogeneous feature extractors and homogeneous classifiers (i.e., executing the same classification task). The local classifiers are uploaded to the FL server for aggregation to generate the global classifier. To acquire global knowledge, LG-FedAvg directly replaces each client’s local classifier with the global classifier. Each client in $\\mathrm{CHFL}$ or FedClassAvg calculates the regularization term or distillation loss between the local and global classifiers. Although these methods support a higher degree of model heterogeneity, they ignore the semantic similarity of classifier parameters belonging to the same class, thus achieving limited performance improvement. Our FedSSA sets out to address the aforementioned limitations.

# 3 Preliminaries

# 3.1 Notations and Objective of Typical FL
A typical FL system involves a central FL server and $N$ decentralized FL clients. In each round, the server selects a $C$ fraction of clients at random. The selected client set is denoted as $\\mathcal{K}$ ,$|{\\mathcal{K}}|\\,=\\,C\\cdot N\\,=\\,K$ . Th server then broadcasts the global model $f(\\omega)~(f(\\cdot)$ ·and ωdenote the model structure and the model parameters) to the selected clients. A client $k$ trains the received $f(\\omega)$ on its local dataset $D_{k}$ to produce a local model $f(\\omega_{k})$ through gradient descent $\\omega_{k}\\leftarrow\\omega\\!-\\!\\eta\\nabla\\ell(f(\\mathbf{x}_{i};\\omega),y_{i})$ .$\\ell(f(\\pmb{x}_{i};\\omega),y_{i})$ eglobal model $f(\\omega)$ on the sample ($(x_{i},y_{i})\\,\\in\\,D_{k}$ ∈.$D_{k}\\sim P_{k}$ ∼indicates that $D_{k}$ obeys distribution $P_{k}$ (i.e., local data from different clients are non-IID). Then, client $k$ uploads its local model parameters $\\omega_{k}$ to the server. The server aggregates the received local models to update the global model, $\\begin{array}{r}{\\bar{\\omega}=\\sum_{k=0}^{K-1}\\frac{n_{k}}{n}\\omega_{k}}\\end{array}$ . That is, the objective of typical $\\mathrm{FL}$ is to minimize the average loss of the global model $f(\\omega)$ on data from all clients:  

$$
\\operatorname*{min}_{\\omega\\in\\mathbb{R}^{d}}\\sum_{k=0}^{K-1}\\frac{n_{k}}{n}\\mathcal{L}_{k}(D_{k};f(\\omega)),
$$  

where $\\omega$ are $d$ -dimensional real numbers. $n_{k}\\,=\\,|D_{k}|$ is the $k$ $\\begin{array}{r}{n=\\sum_{k=0}^{N-1}n_{k}}\\end{array}$ .  
$\\begin{array}{r}{\\mathcal{L}_{k}(D_{k};f(\\omega))=\\frac{1}{|D_{k}|}\\sum_{(\\pmb{x}_{i},y_{i})\\in D_{k}}\\ell(f(\\pmb{x}_{i};\\omega),y_{i})}\\end{array}$ Pis the loss ||∈  
of the global model $f(\\omega)$ on $D_{k}$ .

# 3.2 Problem Definition
The problem we aim to solve in this work belongs to the category of MHPFL for supervised classification tasks. Each FL client $k$ owns local models $f_{k}(\\omega_{k})$ with a model structure $f_{k}(\\cdot)$ and parameters $\\omega_{k}$ . They can be heterogeneous for different clients. We assume all clients execute the same classification task and each client’s local model $f_{k}(\\omega_{k})$ consists of a heterogeneous feature extractor $\\mathcal{F}_{k}(\\varphi_{k})$ and a homogeneous class cation header $\\mathcal{\\mathrm{H}}(\\theta_{k})$ , i.e., $f_{k}(\\omega_{k})=\\mathcal{F}_{k}(\\varphi_{k})\\circ\\mathcal{H}(\\theta_{k})$ and parameters of the feature extractor. It maps data with inputs as ◦de $\\mathcal{F}_{k}(D_{k};\\varphi_{k})$ el splicin , where $\\bar{\\mathcal{F}}_{k}(\\cdot),\\varphi_{k}$ ·denote the ature extractor takes $D_{k}$ from cture the input space to the feature space. The classification header takes input as $\\mathcal{H}(\\mathcal{F}_{k}(D_{k};\\varphi_{k});\\theta_{k})$ , where $\\mathcal{\\mathrm{H}}(\\cdot),\\theta_{k}$ denote the structure and parameters of the classification header. It involves the last two linear layers of the model which maps the features $\\mathcal{F}_{k}(D_{k};\\varphi_{k})$ extracted by $\\mathcal{F}_{k}(\\cdot)$ to the output space.  

  
Figure 1: The FedSSA framework.  

Our objective is to minimize the sum of losses of all clients’ heterogeneous local models:  

$$
\\operatorname*{min}_{\\omega_{0},\\ldots,\\omega_{K-1}}\\sum_{k=0}^{K-1}\\mathcal{L}_{k}(D_{k};f_{k}(\\omega_{k}))=\\sum_{k=0}^{K-1}\\mathcal{L}_{k}(D_{k};\\mathcal{F}_{k}(\\varphi_{k})\\circ\\mathcal{H}(\\theta_{k})),
$$  

where local models $\\omega_{0},\\ldots,\\omega_{K-1}\\in\\mathbb{R}^{d_{0},\\ldots,d_{K-1}}$ .",3
