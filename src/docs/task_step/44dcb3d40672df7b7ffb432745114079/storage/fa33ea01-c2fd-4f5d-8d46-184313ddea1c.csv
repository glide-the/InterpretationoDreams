角色,内容,分镜
fa33ea01-c2fd-4f5d-8d46-184313ddea1c,基本概念,0
fa33ea01-c2fd-4f5d-8d46-184313ddea1c,"### 问题

在模型蒸馏的基本概念中，教师模型和学生模型的主要区别是什么？请简要描述它们各自的特点和作用。",0
fa33ea01-c2fd-4f5d-8d46-184313ddea1c,"ref_ids: 454846524788653356, chunk_ids: 3, Score: 0.4941, Text: # (3) Students Guiding Each Other
As is evident, students in real-world scenarios can also learn from each other. Accordingly, the works in [36] ,[65] apply this approach to KD to improve the performance of student models in OD. Kang et al. [36] proposed an oracle knowledge extraction framework based on a neural structure search to solve the capacity and complexity problems of the integration model. Moreover, teacher-student KD [65] involves not only a teacher model guiding multiple students, but also mutual guidance between student models. For knowledge sharing between students, the final objective function is as follows:  

$$
\\begin{array}{r l}&{L_{s_{1}}^{k D}=L_{s_{1}}^{k T}+\\sigma\\left(s_{1}\\right)L^{k s}\\left(s_{1}\\|s_{2}\\right)\\!,}\\\\ &{L_{S_{2}}^{K D}=L_{S_{2}}^{K T}+\\beta\\cdot\\sigma\\left(S_{2}\\right)L^{K S}\\left(S_{2}\\|S_{1}\\right)\\!,}\\end{array}
$$  

where $S_{1}$ and $S_{2}$ are a slow and a smart student model respectively. The smarter the student model is, the better it will learn the knowledge.  

5) Self-Feature Distillation: Traditional KD methods usually require a pre-trained teacher model to train a student model. To reduce the dependence of the student model on the teacher model, many scholars have designed so-called teacherfree KD models. The existing teacher-free KD methods include self-distillation, cooperative learning, and label regularization, among others. In addition, there is another type of KD, namely feature distillation, which is dissimilar to logits methods (object distillation). The student model in logits methods only learns the teacher model’s logits as the resulting knowledge, rather than the middle-layer features. Many scholars have applied feature distillation to self-feature distillation, enabling a KD scheme without teachers to be devised.  

Self-feature distillation does not require teacher networks, which reduces training cost. Moreover, these methods carry out one-to-one feature transformation learning between layers. For example, LGD [57] is a self-distillation model used for OD. The knowledge learned by networks can be transmitted between different layers through top-down distillation [38] ,and attention maps are used to capture more effective feature information and thereby promote the development of OD. An auxiliary classifier is added to the intermediate feature layers to strengthen self-supervision and enable student models to learn more effective feature representations [56] . Progressive self-knowledge distillation (PS-KD) [39] carries out feature learning by gradually extracting the knowledge from the features the model obtains. Notably, since all the lower layers imitate the attention maps of upper layers, the attention information of the lower layers may be lost. Accordingly, Bi-SAD [18] is proposed to solve this problem. Moreover, in YOLOv6 [148] ,a simple self-distillation technique using a pre-trained student model as the teacher model is introduced to minimize the data distribution difference between the predictions of the teacher and student models.  

Inordertosolvetheproblemofmodelseasilyfallingintooverfitting due to limited sample availability during few-shot OD, Li et al. [139] utilized the self-feature KD strategy to improve the generalization ability of the student model by designing an attention loss, which includes classification, regression, and class-specific features in a small number of examples. The core concept is that the student model can learn the feature mapping functions to approximate the original model through location and category feature transformation. Moreover, to improve the generalization ability of OD models, Wu et al. realized crossdomain OD based on Single-Domain through self-distillation technology [58] . Cyclic-disentangled self-distillation is proposed to continuously strip the scene information of the objects during model training, as well as to extract the shared feature expression suitable for OD in different domains. Specifically, the fine-grained location and classification information contained in multiple convolution layers of the teacher model is used to guide the backbone network (student model) to learn crossdomain features, thereby improving the generalization ability of OD models and achieving cross-domain OD. Similarly, selfdistillation technology was used by He et al. for cross-domain OD [149] . These authors designed two self-distilling branches to learn the shared proposal features from the source and target domain. In addition, there are also some distillation frameworks that use self-distillation as part of their models, such as the methods in [129] and [118] , which have successfully integrated self-feature distillation patterns into their models to achieve performance improvements.  

6) Specific Information Guidance for OD: General OD models based on KD make full use of a heavyweight teacher model to transfer the complex knowledge learned from a specific dataset to a lightweight student model. In this section, we will introduce several models in which some specific knowledge information extracted by the teacher models is transferred to the student models. Among these models are the mask-guided OD models that focus on the local features of images, along with the textual guided models that introduce textual prior knowledge information for improving the student models’ ability to learn the visual features; there are also some other models that pay attention to the semantic information of different object relationships in images through semantic guidance, etc.

# (1) Mask-Guided KD-Based OD
The mask-guided network [32] is designed based on the two-stage OD model structure. Here, the mask information is used to guide the student model to pay attention to the global and local features, and the loss function of KD is given by combining global and local loss. Similarly, the method proposed by Wang et al. [17] uses the ground truth in the initial generation model as a mask to guide student models in learning the features of the objects of interest and the adjacent objects. For fine-feature imitation, a combination of the adaptation layer and the generated mask enables the student model to simulate the teacher model’sattentiontolocalfeaturesandnearbyobjects.Limitation loss combined with testing loss is proposed for training the student model. In current KD-based OD models, student models largely depend on the outputs of teacher models, or transitional trust teacher models. However, in real-world scenarios, teacher models may not be able to provide very reliable output features or prediction results. Therefore, student models should learn the knowledge from teacher models selectively. Several methods have been developed to rank the outputs (including feature maps, proposals or predictions) of the teacher models based on their own quality measurements, and select a few key predictive regions or reliable predictions to guide the student OD models [128] ,[143] ,[150] .",0
fa33ea01-c2fd-4f5d-8d46-184313ddea1c,"ref_ids: 455037545741550556, chunk_ids: 6, Score: 0.4414, Text: # (3) Interactive Distillation Between Multi-Teacher and MultiStudent Models for OD
Distillation learning is similar to the pattern of human learning. Thus, as in real-world scenarios, a student model can be guided by multiple teacher models for feature learning. At the same time, a task can also be completed by multiple student models and student models can learn from each other. In existing methods, a teacher model can guide multiple student models to conduct feature learning, then select the student model with superiorperformance [65] .Multi-teachermodelscanalsojointly train student models in two stages [37] ; alternatively, multiple teacher models can be combined to train student models by means of weight fusion [53] . These methods make full use of the advantages of multiple teacher models to train a superior student model and achieve good results. However, it is worth conducting further research into how knowledge might be seamlessly transferred from multiple teacher models to student models. In addition, it is important to investigate how the teacher model and student model interact with each other through certain mechanisms or technologies during joint model training, which may also be an effective way to improve the performance of both teacher and student models. Finally, as multiple teacher models can train multiple different types of student models, the question of how to select the optimal student model or combine multiple student models for OD should receive attention in the future.  

(4) New Knowledge and Multiple Modal Features Distillation for OD  

The introduction of new knowledge is a very effective way to improve OD models’ performance, and the ability to extract new knowledge from other modes is a major advantage of KD. Existing methods of this kind take a variety of multimodal data (RGB images [35] , thermal images [55] , depth images [14] ,[35] ,textual information [19] ,[64] , etc.) as the input of the teacher model, then use the teacher model to extract relevant features from these multimodal data for guiding the student model to learn the visual features from 2D RGB images. However, a question that merits future exploration is that of how to minimize the gaps between the different types of features learned by the teacher model from multimodal data and the visual features for OD. In addition, using the features of other modal data as the prior knowledge of weakly supervised OD could also be consideredwhendesigningweaklysupervisedODmodelsbased on KD.

# (5) Model Compression for 3D OD
3D OD is mostly used in automatic driving and other fields. The OD models dealing with 3D image data are more complex than those of 2D data. It is therefore of great practical significance to use KD technology to compress the 3D OD models, and there are many innovative works on this topic worth exploring. First, as there are large-scale network parameters in 3D OD models, model compression based on KD is an important research field that needs to be considered. For example, we could use a complex deep neural network as the teacher model to guide a finely designed lightweight student model. However, another key question is that of how to improve the accuracy of OD models. Future research into KD-based 3D OD could focus on the image data and the models. For 3D image data, the models using the point cloud data as input can adopt self-distillation to improve the accuracy of 3D OD. For the models using multiple forms of data as input, the initial detection results can first be obtained from simple 2D images, after which they could be used as weakly supervised tags to optimize the 3D OD models with complex 3D data. In addition, we can use more sophisticated complex models to optimize 3D OD models, such as the KD method that combines the segmentation model and detection model. It would also be beneficial to design a suitable KD framework that is specifically tailored for the unique context of 3D OD.  

Finally, KD has great advantages in model compression and model performance improvement, and has been widely used in multiple computer vision tasks. In recent years, the use of KD technology for OD-related tasks has attracted increasing research attention; at the same time, KD-based OD has also encountered many challenges. In the future, KD should be extended to a wide range of visual detection tasks, such as 3D OD, weakly supervised/unsupervised OD, visual relationship detection, social relationship detection, and so on. In addition, KD-based OD can be further applied to other specific types of data, such as multi-source remote sensing images, multi-modal images, textual data, audio data, etc.

# VI. C ONCLUSION
This survey reviews KD-based OD models. First, we detailed the basic principles for designing OD models based on KD. We thensummarizedandanalyzedthepreviousworksintermsofthe KD-based OD tasks, the KD strategies employed in OD models, the related problems to be solved, and the datasets associated with model application. Finally, we discussed the promising possible research directions to be further explored in the future. As shown by the above comprehensive analysis of current KD-based OD models, KD brings great potential to traditional OD models in terms of model compression and performance improvement. Therefore, there are many novel ideas and techniques in this research field that merit further exploration.",0
fa33ea01-c2fd-4f5d-8d46-184313ddea1c,"ref_ids: 454845516158078890, chunk_ids: 13, Score: 0.4297, Text: # 2 RELATED WORK

# 2.1 Knowledge distillation
The concept of knowledge distillation is introduced by Hinton et al. [5 ] based on a teacher-student framework. This method transfers knowledge from the trained teacher to the student network. Recently, it has been applied mainly to two areas: model compression [ 13 ] and knowledge transfer [ 14 ]. For model compression, a compact small student model is trained to mimic the pre-trained cumbersome teacher model.  

Most knowledge distillation methods explore distilled knowledge in order to guide the student network, including instance feature, instance feature relationship and feature space transformation, etc. For instance feature, the related methods [ 5 ], [ 15 ] in the early time distill logits at the end of the network. The logits reflect the class distribution and contain more information than one-hot label. In this manner, the student network can be improved by learning more information. After that, features containing richer spatial information from intermediate layers [ 16 ], [ 17 ], [ 18 ] are extracted as the distilled knowledge. For example, FitNet [ 16 ]extracts the feature maps of the intermediate layers as well as the final output to teach the student network. Zagoruyko et al. [17 ]define Attention Transfer (AT) based on attention maps to improve the performance of the student network. More recently, structural knowledge [ 19 ], [ 20 ], [ 21 ], e.g., instance feature relationship and feature space transformation, has been presented, which represents more comprehensive information. For example, Liu et al. [19 ] propose the Instance Relationship Graph (IRG) to represent instance feature relationship and feature space transformation. It considers the geometry of the feature spaces and allows for dimensionagnostic transfer of knowledge. Yim et al. [21 ] present the Flow of Solution Procedure (FSP) to transfer the inference procedure of the teacher, which can be seen as a feature space transformation rather than the intermediate layer results.  

Though the above methods have reached a milestone in knowledge distillation, all of them follow a classic single-teachersingle-student framework. Recently, some works have explored new frameworks for knowledge distillation. For instance, [ 22 ]and [ 23 ] propose a mutual learning framework where multiple peer networks learn from each other. The papers [ 24 ] and [ 25 ]present self-distillation frameworks that enable the network to distill from itself. Meta learning methods are adopted to design new frameworks. Jang et al. [26 ] make use of meta learning to determine which information should be transferred during knowledge transfer. Liu et al. [27 ] directly learn soft targets via a meta network for self-distillation. However, nearly all of the previous works perform optimization with a fixed student network. A better resource-performance trade-off can be achieved, if the architecture design is considered during training.

# 2.2 Structured sparsity pruning
In model compression, structured sparsity pruning directly removes redundant neurons and channels rather than irregular weights. Thus, it is hardware-friendly and has been widely applied in recent years. Some works [ 28 ], [ 29 ], [ 30 ], [ 31 ], [ 32 ] aim to exploit a criterion of the filter importance and prune the unimportant filters, while some other works [ 33 ], [ 34 ], [ 35 ], [ 36 ]devote to training the network with additional sparse constraints and removing the sparse part of the network. For example, Li et al. [28 ] consider that the parameters with small $L_{1}$ -norm are less important. He et al. [29 ] calculate the geometric median of the filters within the same layer and prune the filters near the geometric median. Afterwards, HRank [ 30 ] uses rank to assess the filter importance and pruned filters with low-rank feature maps. He et al. [31 ], [ 32 ] exploit a measure of the filter importance. The unimportant filters are pruned in a soft manner. In particular, the unimportant filters are just set to be zero but they may still be updated in the next training epoch. In contrast, some works [ 33 ], [34 ] impose sparse regularization to learn the importance of each channel. Huang et al. [35 ] present a scaling factor to scale the outputs of specific structures and add sparsity constraints on these factors, so that the structure corresponding to a zero-value scaling factor can be removed. ThiNet [ 36 ] regards filter pruning as an optimization problem, and prune each filter layer using statistical information from their next layer.  

More recently, some works [ 37 ], [ 38 ], [ 39 ], [ 40 ], [ 41 ], [ 42 ]learn the sparse allocation of pruning, to meet budget constraints. For example, Gordon et al. [37 ] propose a general technique, i.e. , MorphNet, for resource-constrained optimization of DNN architecture. But the width multiplier that uniformly expands all layer sizes does not consider the difference among layers so that the resource allocation may not be optimal. ECC [ 38 ] introduces an energy consumption model to optimize the DNN compression problem and update the pruned ratio, under an energy constraint. ADMM is leveraged to solve the gradient-based learning problem. Besides, some works [ 39 ], [ 40 ], [ 41 ], [ 42 ] automatically learn the pruned ratio of each DNN layer. For instance, AMC [ 39 ] uses reinforcement learning to find a proper sparsity ratio for each layer. MetaPruning [ 41 ] constructs a meta network to directly generate the weights of the compressed model, given the sparse allocated ratios. Ning et al. [42 ] present a differentiable pruning process to learn the sparse allocation. ADMM is also used for the budgeted pruning problem. Though these previous works use complex optimization processes to meet the compression budget, no extra operation is adopted to enhance the model performance.  

Recent works [ 10 ], [ 11 ], [ 12 ] combine knowledge distillation and model compression to obtain a compact model with high accuracy. Li et al. [11 ] first compress a teacher network to obtain a student network, and then add a $1\\times1$ convolution layer at the end of each block to make the student mimic the teacher. After that, they merged the $1\\times1$ convolution layer into the previous layer. Bai et al. [12 ] combine cross distillation and network pruning by adding regularization to a loss function. However, these methods either treat knowledge distillation and model compression as two independent stages or simply combine the loss functions. Without a framework-level re-design, it is difficult to achieve an optimal trade-off between performance and model complexity.",0
