角色,内容,分镜
4b640109-4354-4389-8737-291b0be13b95,研究论文中采用的主要框架在不同任务中的应用与变体,1
4b640109-4354-4389-8737-291b0be13b95,在模型蒸馏里，除了改进损失函数设计外，GAN结构在生成任务中还有哪些变体形式来适应任务需求？ ,1
4b640109-4354-4389-8737-291b0be13b95,"ref_ids: 454846336184946896, chunk_ids: 2, Score: 0.2578, Text: # 2. Background
Improving GANs. Generative Adversarial Networks [ 23 ], effective in image generation [ 8 ,40 ,57 ], image-to-image translation [ 45 ,88 ,89 ,125 ], video synthesis [ 86 ,103 ,110 ], 3D generation [ 92 ,114 ,124 ] and text-to-image generation [35 ,82 ,94 ], suffer from unstable training [ 44 ,96 ], mode collapse [ 63 ,77 ], and discriminator overfitting [ 39 ,120 ]. Improving GANs includes architecture modifications [ 8 ,38 ,40 ,41 ,53 ,112 ], loss function design [ 3 ,71 ,119 ,123 ]and regularization design [ 25 ,44 ,59 ,65 ,97 ]. BigGAN [8 ] scales up GANs for large-scale datasets with increased batch sizes. StyleGANs [ 38 ,40 ,41 ] revolutionize generator architecture by style integration. OmniGAN [ 123 ] modifies the projection loss [ 64 ] into a multi-label softmax loss. WGAN-GP [ 25 ], SNGAN [ 65 ] and SRGAN [ 59 ] regularize discriminator using a gradient penalty or spectral norm constraints for stable training. Our novel normalization effectively enhances GANs under limited data scenarios, applicable across various architectures and loss functions.  

Image generation under limited data. To address discriminator overfitting in limited data scenarios, where data is scarce or privacy-sensitive, previous methods have employed data augmentation techniques such as DA [ 120 ], ADA [ 39 ], MaskedGAN [ 29 ], FakeCLR [ 55 ] and InsGen [108 ] to expand the data diversity. Approaches [ 48 ,122 ], KDDLGAN [ 15 ], and TransferGAN [ 102 ], leverage knowledge from models trained on extensive datasets to enhance performance. However, these approaches may risk leaking augmentation artifacts [ 39 ,72 ,113 ] or misusing pre-trained knowledge [ 22 ,50 ,106 ]. Alternatives such as LeCam loss [97 ], GenCo [ 14 ] and the gradient norm reduction of DigGAN [ 20 ] aim to balance real and fake distributions. Our approach uniquely combines generalization benefits from BN with improved stability in GAN training, offering an effective and distinct solution to regularizing discriminator.  

GAN Generalization. Deviating from conventional methods that link the generalization of GANs [ 32 ,115 ] with the Rademacher complexity [ 6 ] of neural networks [ 116 ], we introduce a new error bound that highlights the need for reducing discrepancies between seen and unseen data for enhanced generalization. This bound is further refined using the so-called non-vacuous PAC-Bayesian theory [ 10 ], focusing on discriminator weight gradients for a practical GAN generalization improvement.  

Normalization. Batch Normalization (BN) [ 31 ] and its variants such as Group Normalization (GN) [ 104 ], Layer Normalization (LN) [ 5 ], Instance Normalization (IN) [ 98 ]have been pivotal in normalizing latent features to improve training. BN, in particular, is renowned for its role in improving generalization across various tasks [ 7 ,36 ,62 ,80 ]. However, its application in discriminator design, especially under limited data scenarios where generalization is crucial, remains underexplored. Several BN modifications, such as RMSNorm [ 111 ], GraphNorm [ 9 ], PowerNorm [ 85 ], MBN [107 ] and EvoNorm [ 58 ] have been proposed to address issues such as the gradient explosion in transformers [ 99 ] or information loss in graph learning, often by altering or removing the centering step. Our work stands out in GAN discriminator design by linking centering, scaling, and gradient issues in GAN training. Our innovative solution not only mitigates the gradient explosion but also retains the benefits of BN, offering a robust solution for GAN training.

# 3. Method
We begin by linking GAN generalization with the gradient of discriminator weights, motivating the use of BN for generalization and identifying gradient issues in BN. We then introduce CHAIN, a design that tackles these gradient issues while retaining benefits of BN. Lastly, we present a theoretical justification for CHAIN, underscoring its efficacy in improving generalization and training stability.

# 3.1. Generalization Error of GAN
The goal of GAN is to train a generator capable of deceiving a discriminator by minimizing the integral probability metric (IPM) [ 67 ], typically with the assumption of infinite real and fake distributions $(\\mu,\\nu)$ . However, in real-world scenarios, we are usually confined to working with a finite real dataset $\\textstyle{\\hat{\\mu}}_{n}$ of size $n$ . This limitation restricts the optimization of GAN to the empirical loss as discussed in [ 115 ]:  

$$
\\operatorname*{inf}_{\\nu\\in\\mathcal{G}}\\{d_{\\mathcal{H}}(\\hat{\\mu}_{n},\\nu):=\\operatorname*{sup}_{h\\in\\mathcal{H}}\\{\\mathbb{E}_{\\pmb{x}\\sim\\hat{\\mu}_{n}}[h(\\pmb{x})]-\\mathbb{E}_{\\tilde{\\pmb{x}}\\sim\\nu}[h(\\tilde{\\pmb{x}})]\\}\\},
$$  

where $\\textbf{\\em x}$ and $\\tilde{{\\boldsymbol{x}}}$ are real and fake samples. Function sets of teriz discriminator and generator, $\\mathcal{H}$ es and $\\mathcal{H}_{\\mathrm{nn}}:=\\{h(\\cdot;\\pmb{\\theta}_{d}):\\pmb{\\theta}_{d}\\in\\pmb{\\Theta}_{d}\\}$ $\\mathcal{G}$ , are typically parameand [84 ,G115 $\\mathcal{G}_{\\mathrm{nn}}:=\\{g(\\cdot;\\pmb{\\theta}_{g}):\\pmb{\\theta}_{g}\\in\\pmb{\\Theta}_{g}\\}$ ] encompassed by the IPM and the variability of Pu. Given the varied divergence discriminator loss function $\\phi(\\cdot)$ across different tas and architectures, we integrate it with the discriminator Dfor simplified analysis [ 3 ,4 ,115 ], yielding $h(\\cdot):=\\phi(D(\\cdot))$ .This integration streamlines the alternating optimization process between the discriminator and the generator:  

$$
\\left\\{\\begin{array}{l l}{\\mathcal{L}_{D}:=\\displaystyle\\operatorname*{min}_{\\pmb{\\theta}_{d}}\\mathbb{E}_{\\tilde{\\pmb{x}}\\sim\\nu_{n}}[h(\\pmb{\\tilde{x}};\\pmb{\\theta}_{d})]-\\mathbb{E}_{\\pmb{x}\\sim\\hat{\\mu}_{n}}[h(\\pmb{x};\\pmb{\\theta}_{d})]}\\\\ {\\mathcal{L}_{G}:=\\displaystyle\\operatorname*{min}_{\\pmb{\\theta}_{g}}-\\mathbb{E}_{z\\sim p_{z}}[h(g(z;\\pmb{\\theta}_{g}))],}\\end{array}\\right.
$$  

where $z\\sim p_{z}$ repr nts the noise input to the generator and assumed that $\\nu_{n}$ minimizes $d\\varkappa(\\hat{\\mu}_{n},\\nu)$ to a precision $\\epsilon\\!\\geqslant\\!0$ ě, implying that $d_{\\mathcal{H}}(\\hat{\\mu}_{n},\\nu_{n})\\!\\leqslant\\!\\operatorname*{inf}_{\\nu\\in\\mathcal{G}}d_{\\mathcal{H}}(\\hat{\\mu}_{n},\\nu)\\!+\\!\\epsilon$ .  

To evaluate how closely the generator distribution $\\nu_{n}$ approximates the unknown infinite distribution $\\mu$ , we draw on work of Ji et al . [ 32 ] who extended Theorem 3.1 in [ 115 ] by considering the limited access to both real and fake images.  

Lemma 3.1 (Partial results of Theorem 1 in [ 32 ].) Assume the discriminator set $\\mathcal{H}$ is eve .e ., $h\\in\\mathcal H$ implies $-h\\!\\in\\!\\mathcal{H}$ ,nd $\\|h\\|_{\\infty}\\leqslant\\Delta$ . L t$\\textstyle{\\hat{\\mu}}_{n}$ and $\\hat{\\nu}_{n}$ res of $\\mu$ and $\\nu_{n}$ with size n. Denote $\\nu_{n}^{*}=\\operatorname*{inf}_{\\nu\\in\\mathcal{G}}d_{\\mathcal{H}}(\\hat{\\mu}_{n},\\nu)$ “generalization error of GAN, defined as $\\epsilon_{g a n}\\!:=\\!d_{\\mathcal{H}}(\\mu,\\nu_{n})\\!-$ “Hpq´ $\\operatorname{inf}_{\\nu\\in\\mathcal{G}}d_{\\mathcal{H}}(\\mu,\\nu)$ , is bounded as:  

$$
\\begin{array}{r}{\\begin{array}{c}{\\epsilon_{g a n}\\leqslant2\\big(\\operatorname*{sup}_{h\\in\\mathcal{H}}\\left|\\mathbb{E}_{\\mu}[h]-\\mathbb{E}_{\\hat{\\mu}_{n}}[h]\\right|+\\operatorname*{sup}_{h\\in\\mathcal{H}}\\left|\\mathbb{E}_{\\nu_{n}^{*}}[h]-\\mathbb{E}_{\\hat{\\nu}_{n}}[h]\\right|\\big)}\\\\ {=2d_{\\mathcal{H}}(\\mu,\\hat{\\mu}_{n})+2d_{\\mathcal{H}}(\\nu_{n}^{*},\\hat{\\nu}_{n}).\\qquad\\qquad\\qquad\\qquad(3)}\\end{array}}\\end{array}
$$  

Lemma 3.1 (proof in $\\S B.1\\rangle$ ) indicates that GAN generalization can be improved by reducing the divergence between real training and unseen data, as well as observed and unobserved fake distributions. Given that the ideal $\\nu_{n}^{*}$ aligns with the observed real data $\\textstyle{\\hat{\\mu}}_{n}$ , Lemma 3.1 also emphasizes narrowing the gap between observed fake and real data to lower $d\\varkappa(\\nu_{n}^{*},\\hat{\\nu}_{n})$ q. This explains why prior efforts [12 ,20 ,27 ,33 ,97 ] focusing on diminishing the real-fake distribution divergence help limit overfitting. However, excessive reduction should be avoided, as this makes the discriminator struggle to differentiate real and fake data [ 115 ].  

While reducing $d\\varkappa(\\nu_{n}^{*},\\hat{\\nu}_{n})$ qis achievable, lowering $d\\varkappa(\\mu,\\hat{\\mu}_{n})$ remains challenging due to inaccessibility of infinite $\\mu$ . Fortunately, neural network parameterization of GANs enables adopting PAC Bayesian theory [ 10 ] to further analyze $d\\varkappa(\\mu,\\hat{\\mu}_{n})$ . Integrating the analysis of Theorem 1 in [ 21 ], Lemma 3.1 is further formulated as follows:  

Proposition 3.1 Utilizing notations from Lemma 3.1 , we define $\\epsilon_{g a n}^{n n}$ as the generalization error of GAN parameterized as neural network classes. Let $\\nabla_{\\theta_{d}}$ and ${\\cal H}_{\\theta_{d}}$ present the gradient and Hessian matrix of discriminator hevaluated at $\\theta_{d}$ over real t ng da $\\textstyle{\\hat{\\mu}}_{n}$ $\\tilde{\\nabla}_{\\theta_{d}}$ d$\\widetilde{\\pmb{H}}_{\\pmb{\\theta}_{d}}$ over observed fake data $\\hat{\\nu}_{n}$ . Denoting $\\lambda_{m a x}^{H}$ and $\\lambda_{m a x}^{\\widetilde{H}}$ Ăas the larg envalues of ${{H}_{\\theta_{d}}}$ and $\\widetilde{\\pmb{H}}_{\\pmb{\\theta}_{d}}$ Ă, respectively, and for any $\\omega>0$ ą, the generalization error is bounded as:  

$$
\\begin{array}{r l}&{\\epsilon_{g a n}^{n n}\\leqslant2\\omega\\big(\\|\\nabla_{\\pmb{\\theta}_{d}}\\|_{2}+\\|\\widetilde{\\pmb{\\nabla}}_{\\pmb{\\theta}_{d}}\\|_{2}\\big)+4R\\Big(\\frac{\\|\\pmb{\\theta}_{d}\\|_{2}^{2}}{\\omega^{2}},\\frac{1}{n}\\Big)}\\\\ &{\\quad\\quad\\quad+\\omega^{2}\\big(|\\lambda_{m a x}^{H}|+|\\lambda_{m a x}^{\\widetilde{H}}|\\big),}\\end{array}
$$  

where $\\begin{array}{r}{R\\Big(\\frac{\\|\\pmb{\\theta}_{d}\\|_{2}^{2}}{\\omega^{2}},\\frac{1}{n}\\Big)}\\end{array}$ \\` ˘, a term related to discriminator weights norm, is inversely related to the data size $n$ .  

Prop. 3.1 (proof in $\\S B.2)$ suggests several strategies to lower the generalization error of GANs. These include increasing data size $(n)$ , implementing regularization to decrease weight norm of the discriminator and the largest eigenvalues in Hessian matrices, and crucially, reducing the gradient norm of discriminator weights. Although this proposition is specific to GANs, the concept of regularizing weight gradient norms aligns with findings in other studies [60 ,91 ,93 ,100 ,117 ,121 ], which emphasize that reducing weight gradients can smooth the loss landscape, thereby enhancing generalization of various deep learning tasks.",1
4b640109-4354-4389-8737-291b0be13b95,"ref_ids: 454845772418806494, chunk_ids: 14, Score: 0.1855, Text: # 2 Training GANs with diffusion-based mixture distribution
Below we first briefly review GANs and diffusion models and then present our method and analyze its theoretical properties. GANs aim to model data distribution $p({\\pmb x})$ via a min-max game played between a generator and a discriminator [Goodfellow et al., 2014]. The generator $G$ aims to transform noise $_{z}$ into $G(z)$ that mimics real data, while the discriminator $D$ learns to distinguish the generated samples $G(z)$ from real ones $\\pmb{x}\\sim p(\\pmb{x})$ . The min-max objective of GANs can be expressed as  

$$
\\operatorname*{min}_{G}\\operatorname*{max}_{D}V(G,D)=\\mathbb{E}_{\\mathbf{x}\\sim p(\\mathbf{x})}[\\log(D(\\mathbf{x}))]+\\mathbb{E}_{\\mathbf{z}\\sim p(\\mathbf{z})}[\\log(1-D(G(\\mathbf{z})))].
$$  

While in practice this vanilla objective is often modified in a certain way [Goodfellow et al., 2014, Miyato et al., 2018, Fedus et al., 2018], the use of adversarial learning between $G$ and $D$ in general remains intact.  

2019] assume Diffusion-based generative models [Ho et al., 2020, Sohl-Dickstein et al., 2015, Song and Ermon, $\\begin{array}{r}{p_{\\theta}(\\mathbf{x}_{0}):=\\int p_{\\theta}(\\mathbf{x}_{0:T})d\\mathbf{x}_{1:T}}\\end{array}$ R, where $\\mathbf{\\Delta}x_{1},\\dots,x_{T}$ are latent variables of the same dimensionality as $x_{0}\\sim p(x_{0})$ . There is a forward diffusion chain tha gradually adds ise to the data $x_{0}\\sim q(x_{0})$ ∼in Tsteps with pre-defined variance schedule βand variance $\\sigma^{2}$ :  

$$
\\begin{array}{r}{q(x_{1:T}\\mid x_{0}):=\\prod_{t=1}^{T}q(x_{t}\\mid x_{t-1}),\\quad q(x_{t}\\mid x_{t-1}):=\\mathcal{N}(x_{t};\\sqrt{1-\\beta_{t}}x_{t-1},\\beta_{t}\\sigma^{2}I).}\\end{array}
$$  

A notable property is that $\\mathbf{\\nabla}x_{t}$ at an arbitrary time-step $t$ can be sampled in closed form  

$$
\\begin{array}{r}{q({\\boldsymbol x}_{t}\\mid{\\boldsymbol x}_{0})=\\mathcal{N}({\\boldsymbol x}_{t};\\sqrt{\\bar{\\alpha}_{t}}{\\boldsymbol x}_{0},(1-\\bar{\\alpha}_{t})\\sigma^{2}I),\\quad\\mathrm{where~}\\alpha_{t}:=1-\\beta_{t},\\;\\bar{\\alpha}_{t}:=\\prod_{s=1}^{t}\\alpha_{s}.}\\end{array}
$$  

A reverse diffusion chain, constructed as  

$$
\\begin{array}{r}{p_{\\theta}(\\mathbf{\\boldsymbol{x}}_{0:T}):=\\mathcal{N}(\\mathbf{\\boldsymbol{x}}_{T};\\mathbf{0},\\sigma^{2}I)\\prod_{t=1}^{T}p_{\\theta}(\\mathbf{\\boldsymbol{x}}_{t-1}\\mid\\mathbf{\\boldsymbol{x}}_{t}),}\\end{array}
$$  

is then optimized with the evidence lower bound [Jordan et al., 1999, Blei et al., 2017] as   
$\\begin{array}{r}{\\mathbb{E}_{q}[\\ln\\frac{p_{\\theta}(\\mathbf{x}_{0:T})}{q(\\mathbf{x}_{1:T}\\mid\\mathbf{x}_{0})}]}\\end{array}$ .|

# 2.1 Instance noise injection via a diffusion-based mixture distribution
Let $z\\ \\sim\\ p(z)$ denote the latent variable distribution and $p_{g}(x)$ denote the distribution of $\\pmb{x}_{g}\\,=\\,G(z)$ . We inject instance noise through a forward diffusion chain-based mixture distribution as  

$$
\\begin{array}{r l}&{\\quad x\\sim p(x),\\;y\\sim q(y\\,|\\,x),\\;\\;q(y\\,|\\,x):=\\sum_{t=1}^{T}\\pi_{t}q(y\\,|\\,x,t),}\\\\ &{x_{g}\\sim p_{g}(x),\\;y_{g}\\sim q(y_{g}\\,|\\,x_{g}),\\;\\;q(y_{g}\\,|\\,x_{g}):=\\sum_{t=1}^{T}\\pi_{t}q(y_{g}\\,|\\,x_{g},t),}\\end{array}
$$  

where $q(\\boldsymbol{y}\\mid\\boldsymbol{x})$ is a $T$ -component mixture distribution, the mixture weights $\\pi_{t}$ are non-negative and sum to one, and the mixture components $q(\\pmb{y}\\mid\\pmb{x},t)$ are obtained via diffusion as in (3), expressed as  

$$
q(\\pmb{y}\\,|\\,\\pmb{x},t)=\\mathcal{N}(\\pmb{y};\\sqrt{\\bar{\\alpha}_{t}}\\pmb{x},(1-\\bar{\\alpha}_{t})\\sigma^{2}\\pmb{I}).
$$  

Samples from this mixture can be drawn as $t\\sim p_{\\pi}:=\\mathrm{Discrete}(\\pi_{1},\\ldots,\\pi_{T})$ ,$\\pmb{y}\\sim q(\\pmb{y}\\,|\\,\\pmb{x},t)$ .Under this construction, $q(\\pmb{y}\\mid\\pmb{x},t)$ becomes the marginal distribution of $_{\\textbf{\\em x}}$ after going through the forward iffusion chain $t$ steps, and $q(\\pmb{y}\\mid\\pmb{x})$ becomes a Gaussi n mixture distribution that involves all Tsteps of the diffusion chain. We note the larger the tis, the stronger the noise is injected into $\\pmb{y}\\sim q(\\pmb{y}\\,|\\,\\pmb{x},t)$ and the weaker the data becomes. A simple choice for $p_{\\pi}$ is to let $\\pi_{t}=1/T$ for all $t$ . Below we will discuss a different choice that favors sampling a larger $t$ and hence a larger noise-to-data ratio. Next we introduce Diffusion-GAN that trains its discriminator and generator with the help of the diffusion-induced mixture distribution.  

The Diffusion-GAN’s counterpart to the vanilla GAN min-max objective is defined as  

$$
V(G,D)=\\mathbb{E}_{x\\sim p(x),t\\sim p_{\\pi},y\\sim q(y\\mid x,t)}[\\log(D_{\\phi}(y,t))]+\\mathbb{E}_{z\\sim p(z),t\\sim p_{\\pi},y_{g}\\sim q(y\\mid G_{\\theta}(z),t)}[\\log(1-D_{\\phi}(y_{g},t))].
$$  

The discriminator $D$ learns to distinguish the diffused generated samples ${\\bf{\\nabla}}_{\\!{\\bf{\\nabla}}}\\!{\\bf{\\nabla}}_{\\!{\\bf{\\nabla}}}\\!{\\bf{\\nabla}}_{\\!{\\bf{\\nabla}}}\\!{\\bf{\\nabla}}_{\\!{\\bf{\\nabla}}}\\!{\\bf{\\nabla}}_{\\!{\\bf{\\nabla}}}\\!{\\bf{\\nabla}}_{\\!{\\bf{\\nabla}}}\\!{\\bf{\\nabla}}_{\\!{\\bf{\\nabla}}}\\!{\\bf{\\nabla}}_{\\!{\\bf{\\nabla}}}\\!{\\bf{\\nabla}}_{\\!{\\bf{\\nabla}}}\\!{\\bf{\\cdot}}\\!{\\bf{\\nabla}}_{\\!{\\bf{\\nabla}}}\\!{\\bf{\\cdot}}\\!{\\bf{\\nabla}}_{\\!{\\bf{\\nabla}}}\\!{\\bf{\\cdot}}\\!{\\bf{\\nabla}}_{\\!{\\bf{\\nabla}}}\\!{\\bf{\\cdot}}\\!{\\bf{\\nabla}}_{\\!{\\bf{\\nabla}}}\\!{\\bf{\\cdot}}\\!{\\bf{\\nabla}}_{\\!{\\bf{\\nabla}}}\\!{\\bf{\\cdot}}\\!{\\bf{\\nabla}}_{\\!{\\bf{\\nabla}}}\\!{\\bf{\\cdot}}\\!{\\bf{\\nabla}}\\!{\\bf{\\cdot}}$ from the diffused real observations $\\mathbf{\\deltay}$ for $\\forall t\\in\\{1,\\ldots,T\\}$ , with specific priorities determined by the values of $\\pi_{t}$ . The generator Glearns to map a latent variable $_{z}$ to its output ${\\pmb x}_{g}=G_{\\theta}({\\pmb z})$ , which can fool the discriminator at any step of the diffusion chain. Since reparameterized as Equation (8) can be backpropagated to the generator directly. Similar to Goodfellow et al. $\\pmb{y}_{g}=\\sqrt{\\bar{\\alpha}_{t}}G_{\\theta}(\\pmb{z})+\\sqrt{(1-\\bar{\\alpha}_{t})}\\pmb{\\sigma}\\pmb{\\epsilon},\\pmb{\\epsilon}\\sim\\mathcal{N}(0,\\pmb{I})$ p−∼N , the gradient computed on $\\pmb{y}_{g}\\sim q(\\pmb{y}\\,|\\,G_{\\theta}(\\pmb{z}),t)$ can be [2014], one can show that the adversarial loss in Equation (8) approximately minimizes the Jensen–Shannon (JS) divergence ,  

$$
\\mathcal{D}_{\\mathrm{JS}}(p(\\pmb{y},t)||p_{g}(\\pmb{y},t))=\\mathbb{E}_{t\\sim p_{\\pi}}[\\mathcal{D}_{\\mathrm{JS}}(p(\\pmb{y}|t)||p_{g}(\\pmb{y}|t))].
$$  

The derivation of the equality in Equation (9) is provided in Appendix B. One critical question is whether minimizing minimizing the original $\\mathcal{D}_{\\mathrm{JS}}(p(\\pmb{x})||p_{g}(\\pmb{x}))$ $\\mathcal{D}_{\\mathrm{JS}}(p(\\pmb{y},t)||p_{g}(\\pmb{y},t))$ . We will theoretically show that the answer is yes in induces the same optimal generator as Section 2.3.

# 2.2 Adaptiveness of diffusion
Note that as $t$ increases, $\\alpha_{t}$ decreases towards zero and hence the noise-to-data ratios in both $\\textbf{\\emph{y}}$ and ${\\bf mathit{\\Delta}}_{y_{\\mathrm{\\Delta}}}$ increase, inducing a more and more difficult task for the discriminator $D$ . Since GANs are known to suffer from the discriminator overfitting issue [Karras et al., 2020a, Zhao et al., 2020], we design an adaptive control of the diffusion intensity so as to better train the discriminator. We achieve this by adaptively modifying $T$ .  

Ideally we want the discriminator to start with the original data samples, and as the discriminator becomes more confident, we feed it with harder samples from a larger $t$ . Thus, we design a self-paced schedule for $T$ based on a metric $r_{d}$ , which evaluates the overfitting of the discriminator:  

$$
r_{d}=\\mathbb{E}_{y,t\\sim p(y,t)}[\\mathrm{sign}(D_{\\phi}(\\pmb{y},t)-0.5)],\\quad T=T+\\mathrm{sign}(r_{d}-d_{t a r g e t})*C
$$  

where $r_{d}$ follows the choice of Karras et al. [2020a] and $C$ is a fixed constant. We evaluate $r_{d}$ every four minibatches and update $T$ at the same time. To better resist discriminator overfitting, we define  

$$
\\begin{array}{r}{t\\sim p_{\\pi}:=\\mathrm{Discrete}\\left(\\frac{1}{\\sum_{t=1}^{T}t},\\frac{2}{\\sum_{t=1}^{T}t},\\cdot\\cdot\\cdot,\\frac{T}{\\sum_{t=1}^{T}t}\\right)}\\end{array}
$$  

as an asymmetric discrete distribution, which encourages the discriminator to observe newly added diffusion samples as $T$ increases. This is because when $T$ begins to increase, it implies the discriminator has already been confident about the seen samples so we want it to explore more new samples to counteract discriminator overfitting. Note that as $q(\\pmb{y}\\mid\\pmb{x})$ is a Gaussian mixtu e defined over all steps of th diff n chai whil e use of $\\pi_{t}:=t/\\sum_{s=1}^{T}s$ favors a larger t, there is still a large probability to traverse small $t$ . To smooth the change of $T$ during training, we sample an exploration tlist t$t_{e p l}$ from p$p_{\\pi}$ , fix t$\\pmb{t_{e p l}}$ during the update interval of $T$ ,and then sample $t$ from $\\pmb{t}_{e p l}$ for augmenting data samples. This drives the model to explore every $t$ sufficiently before querying to increment $T$ . We summarize training Diffusion-GAN in Algorithm 1 in Appendix F.",1
4b640109-4354-4389-8737-291b0be13b95,"ref_ids: 454845536381442298, chunk_ids: 8, Score: 0.1855, Text: # 3.2.6 Loss Sensitive GAN (LS-GAN)
Similar to WGAN, LS-GAN [20] also has a Lipschitz constraint. It is assumed in LS-GAN that $p_{d a t a}$ lies in a set of Lipschitz densities with a compact support. In LS-GAN , the loss function $L_{\\theta}\\left(x\\right)$ is parameterized with $\\theta$ and LS-GAN assumes that a generated sample should have larger loss than a real one. The loss function can be trained to satisfy the following constraint:  

$$
L_{\\theta}\\left(x\\right)\\leq L_{\\theta}\\left(G\\left(z\\right)\\right)-\\Delta\\left(x,G\\left(z\\right)\\right)
$$  

where $\\Delta\\left(x,G\\left(z\\right)\\right)$ is the margin measuring the difference between generated sample $\\bar{G}(z)$ and real sample $x$ . The objective function of LS-GAN is  

$$
\\begin{array}{r l}&{\\underset{D}{\\operatorname*{min}}\\,\\mathcal{L}_{D}=E_{x\\sim p_{d a t a}\\left(x\\right)}\\left[L_{\\theta}\\left(x\\right)\\right]}\\\\ &{+\\lambda E_{x\\sim p_{d a t a}\\left(x\\right),\\mathbf{\\Phi}}\\bigl[\\Delta\\left(x,G\\left(z\\right)\\right)+L_{\\theta}\\left(x\\right)-L_{\\theta}\\left(G\\left(z\\right)\\right)\\bigr]_{+},}\\\\ &{\\quad\\quad z\\sim p_{z}\\left(z\\right)}\\end{array}
$$  

$$
\\operatorname*{min}_{G}\\mathcal{L}_{G}=E_{z\\sim p_{z}\\left(z\\right)}\\left[L_{\\theta}\\left(G\\left(z\\right)\\right)\\right],
$$  

where $\\lbrack y\\rbrack^{+}=\\operatorname*{max}(0,y),\\lambda$ is the free tuning-parameter, and $\\theta$ is the paramter of the discriminator $D$ .

# 3.2.7 Summary
There is a website called “The GAN $Z\\mathrm{oo}^{\\prime\\prime}$ (https: //github.com/hindupuravinash/the-gan-zoo) which lists many GANs’ variants. Please refer to this website for more details.

# 3.3 GANs Training
Despite the theoretical existence of unique solutions, GANs training is hard and often unstable for several reasons [29], [32], [179]. One difficulty is from the fact that optimal weights for GANs correspond to saddle points, and not minimizers, of the loss function.  

There are many papers on GANs training. Yadav et al. [180] stabilized GANs with prediction methods. By using independent learning rates, [181] proposed a two timescale update rule (TTUR) for both discriminator and generator to ensure that the model can converge to a stable local Nash equilibrium. Arjovsky [179] made theoretical steps towards fully understanding the training dynamics of GANs; analyzed why GANs was hard to train; studied and proved rigorously the problems including saturation and instability that occurred when training GANs; examined a practical and theoretically grounded direction to mitigate these problems; and introduced new tools to study them. Liang et al. [182] think that GANs training is a continual learning problem [183].  

One method to improve GANs training is to assess the empirical “symptoms” that might occur in training. These symptoms include: the generative model collapsing to produce very similar samples for diverse inputs [29]; the discriminator loss converging quickly to zero [179], providing no gradient updates to the generator; difficulties in making the pair of models converge [32].  

We will introduce GANs training from three perspectives: objective function, skills, and structure.

# 3.3.1 Objective function
As we can see from Subsection 3.1, utilizing the original objective function in equation (1) will have the gradient vanishing problem for training $G$ and utilizing the alternative $G$ loss (12) in non-saturating game will get the mode collapse problem. These problems are caused by the objective function and cannot be solved by changing the structures of GANs. Re-designing the objective function is a natural solution to mitigate these problems. Based on the theoretical flaws of GANs, many objective function based variants have been proposed to change the objective function of GANs based on theoretical analyses such as least squares generative adversarial networks [21], [22].  

3.3.1.1 Least squares generative adversarial networks (LSGANs) : LSGANs [21], [22] are proposed to overcome the vanishing gradient problem in the original GANs. This work shows that the decision boundary for $D$ of original GAN penalizes very small error to update $G$ for those generated samples which are far from the decision boundary. LSGANs adopt the least squares loss rather than the cross-entropy loss in the original GANs. Suppose that the $a{-}b$ coding is used for the LSGANs’ discriminator [21], where $a$ and $b$ are the labels for generated sample and real sample, respectively. The LSGANs’ discriminator loss $V_{L S G A N}\\left(\\bar{D}\\right)$ and generatorloss $V_{L S G A N}\\left(G\\right)$ are defined as:  

$$
\\begin{array}{c}{{\\underset{D}{\\operatorname*{min}}\\ V_{L S G A N}\\left(D\\right)=E_{x\\sim p_{d a t a}\\left(x\\right)}\\left[\\left(D\\left(x\\right)-b\\right)^{2}\\right]}}\\\\ {{+E_{z\\sim p_{z}\\left(z\\right)}\\left[\\left(D\\left(G\\left(z\\right)\\right)-a\\right)^{2}\\right],}}\\end{array}
$$  

$$
\\operatorname*{min}_{G}\\;V_{L S G A N}\\left(G\\right)=E_{z\\sim p_{z}\\left(z\\right)}\\left[\\left(D\\left(G\\left(z\\right)\\right)-c\\right)^{2}\\right],
$$  

where $c$ is the value that $G$ hopes for $D$ to believe for generated samples. The reference [21] shows that there are two advantages of LSGANs in comparison with the original GANs:  

•The new decision boundary produced by $D$ penalizes large error to those generated samples which are far from the decision boundary, which makes those “low quality” generated samples move toward the decision boundary. This is good for generating higher quality samples.   
•Penalizing the generated samples far from the decision boundary can supply more gradient when updating the $G,$ which overcomes the vanishing gradient problems in the original GANs.  

3.3.1.2 Hinge loss based GAN: Hinge loss based GAN is proposed and used in [23]–[25] and its objective function is $V\\left(D,G\\right)$ :  

$$
\\begin{array}{r l}&{V_{D}\\left(\\hat{G},D\\right)=E_{x\\sim p_{d a t a}\\left(x\\right)}\\left[\\operatorname*{min}(0,-1+D\\left(x\\right))\\right]}\\\\ &{\\qquad+E_{z\\sim p_{z}\\left(z\\right)}\\left[\\operatorname*{min}(0,-1-D\\left(\\hat{G}(z)\\right))\\right].}\\end{array}
$$  

$$
V_{D}\\left(G,\\hat{D}\\right)=-E_{z\\sim p_{z}(z)}\\left[\\hat{D}\\left(G(z)\\right)\\right].
$$  

The softmax cross-entropy loss [184] is also used in GANs.  

3.3.1.3 Energy-based generative adversarial network (EBGAN):  

EBGAN’s discriminator is seen as an energy function, giving high energy to the fake (“generated”) samples and lower energy to the real samples. As for the energy function, please refer to [185] for the corresponding tutorial. Given a positive margin $m$ , the loss functions for EBGAN can be defined as follows:  

$$
\\begin{array}{r}{\\mathcal{L}_{D}(x,z)=D(x)+[m-D(G(z))]^{+},}\\end{array}
$$  

$$
{\\mathcal{L}}_{G}(z)=D(G(z)),
$$  

where $\\left[y\\right]^{+}=\\operatorname*{max}(0,y)$ is the rectified linear unit (ReLU) function. Note that in the original GANs, the discriminator $D$ give high score to real samples and low score to the generated (“fake”) samples. However, the discriminator in EBGAN attributes low energy (score) to the real samples and higher energy to the generated ones. EBGAN has more stable behavior than original GANs during training.  

3.3.1.4 Boundary equilibrium generative adversarial networks (BEGAN): Similar to EBGAN [38], dual-agent GAN (DA-GAN) [186], [187], and margin adaptation for GANs (MAGANs) [188], BEGAN also uses an auto-encoder as the discriminator. Using proportional control theory, BEGAN proposes a novel equilibrium method to balance generator and discriminator in training, which is fast, stable, and robust to parameter changes.  

3.3.1.5 Mode regularized generative adversarial networks (MDGAN) : Che et al. [26] argue that GANs’ unstable training and model collapse is due to the very special functional shape of the trained discriminators in high dimensional spaces, which can make training stuck or push probability mass in the wrong direction, towards that of higher concentration than that of the real data distribution. Che et al. [26] introduce several methods of regularizing the objective, which can stabilize the training of GAN models. The key idea of MDGAN is utilizi g an encoder $E\\left(x\\right):x\\rightarrow z$ to produce the latent variable zfor the generator Grather than utilizing noise. This procedure has two advantages:  

•Encoder guarantees the correspondence between $z$ $(E(x))$ and $x,$ which makes $G$ capable of covering diverse modes in the data space. Therefore, it prevents the mode collapse problem.   
•Because the reconstruction of encoder can add more information to the generator $G_{.}$ , it is not easy for the discriminator $D$ to distinguish between real samples and generated ones.  

The loss function for the generator and the encoder of MDGAN is  

$$
\\begin{array}{r l}&{\\mathcal{L}_{G}=-E_{z\\sim p_{z}\\left(z\\right)}\\left[\\log\\left(D\\left(G\\left(z\\right)\\right)\\right)\\right]}\\\\ &{+E_{x\\sim p_{d a t a}\\left(x\\right)}\\left[\\begin{array}{l}{\\lambda_{1}d\\left(x,G\\circ E\\left(x\\right)\\right)}\\\\ {+\\lambda_{2}\\log D\\left(G\\circ E\\left(x\\right)\\right)}\\end{array}\\right],}\\end{array}
$$  

$$
\\mathcal{L}_{E}=E_{x\\sim p_{d a t a}\\left(x\\right)}\\left[\\begin{array}{l}{\\lambda_{1}d\\left(x,G\\circ E\\left(x\\right)\\right)}\\\\ {+\\lambda_{2}\\log D\\left(G\\circ E\\left(x\\right)\\right)}\\end{array}\\right],
$$  

where both $\\lambda_{1}$ and $\\lambda_{2}$ are free tuning parameters, $d$ is the metric such as Euclidean distance, and $G\\circ E\\left(x\\right)=$ $G\\left(E\\left(x\\right)\\right)$ .",1
