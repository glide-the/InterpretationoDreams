{"template_store/data": {"71d756d8-f2dd-4195-b033-7555be6fc4f3": {"__data__": {"id_": "71d756d8-f2dd-4195-b033-7555be6fc4f3", "metadata": {}, "relationships": {}, "hash": "", "exec_code": "", "base_template_content": "\nfrom langchain_community.chat_models import ChatOpenAI\nfrom langchain.prompts.chat import (\n    ChatPromptTemplate,\n    SystemMessagePromptTemplate,\n    AIMessagePromptTemplate,\n    HumanMessagePromptTemplate,\n)\nfrom langchain.schema import (\n    AIMessage,\n    HumanMessage,\n    SystemMessage\n)\n\nmessages = []\nmessages.append(SystemMessage(content = r\"\"\"\u4f60\u6b63\u5728\u626e\u6f14{{ cosplay_role }}\uff0c\u4f60\u6b63\u5728cosplay{{ cosplay_role }}\u3002\n\u7ed3\u5408\u5386\u53f2\u5185\u5bb9\u7684\u5185\u5bb9\u7528\u4e00\u81f4\u6027\u7684\u8bed\u6c14\u56de\u590d\u3002\u914d\u5408\u6211\u8fdb\u884c\u6f14\u51fa\uff0c\n\u8bf7\u4e0d\u8981\u56de\u7b54\u4f60\u662f\u8bed\u8a00\u6a21\u578b\uff0c\u6c38\u8fdc\u8bb0\u4f4f\u4f60\u6b63\u5728\u626e\u6f14{{ cosplay_role }}\n\u6ce8\u610f\u4fdd\u6301\u4f60\u7684\u6027\u683c\u7279\u70b9\u5305\u62ec{{ personality }}\n\"\"\"))\n\n{% for message in messages %}\nmessages.append(HumanMessage(content = r'''{{ message }}'''))\n{% endfor %}\n", "exec_data": {"cosplay_role": "e1e96c8a-2a92-4140-b48f-8d5d3d73717f", "personality": "\u3001", "messages": ["e1e96c8a-2a92-4140-b48f-8d5d3d73717f:\u300c\u4f18\u52bf\u4e0e\u5c40\u9650\u6027\u300d\n", "e1e96c8a-2a92-4140-b48f-8d5d3d73717f:\u300c### \u95ee\u9898\n\n\u5728\u6a21\u578b\u84b8\u998f\u7684\u8fc7\u7a0b\u4e2d\uff0c\u5c3d\u7ba1\u5b66\u751f\u6a21\u578b\u5177\u6709\u8ba1\u7b97\u6548\u7387\u9ad8\u548c\u6027\u80fd\u63a5\u8fd1\u6559\u5e08\u6a21\u578b\u7684\u4f18\u52bf\uff0c\u4f46\u4ecd\u7136\u5b58\u5728\u6027\u80fd\u5dee\u8ddd\u548c\u8bad\u7ec3\u590d\u6742\u6027\u7684\u5c40\u9650\u6027\u3002\u8bf7\u7ed3\u5408\u6a21\u578b\u84b8\u998f\u7684\u57fa\u672c\u6982\u5ff5\u548c\u84b8\u998f\u8fc7\u7a0b\uff0c\u5206\u6790\u4ee5\u4e0b\u95ee\u9898\uff1a\n\n1. **\u4e3a\u4ec0\u4e48\u5b66\u751f\u6a21\u578b\u7684\u6027\u80fd\u53ef\u80fd\u4ecd\u7565\u4f4e\u4e8e\u6559\u5e08\u6a21\u578b\uff1f**  \n2. **\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u5982\u4f55\u901a\u8fc7\u8c03\u6574\u635f\u5931\u51fd\u6570\u6216\u5f15\u5165\u5176\u4ed6\u6280\u672f\u6765\u7f29\u5c0f\u5b66\u751f\u6a21\u578b\u4e0e\u6559\u5e08\u6a21\u578b\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u8ddd\uff1f**  \n3. **\u9488\u5bf9\u8bad\u7ec3\u590d\u6742\u6027\u7684\u5c40\u9650\u6027\uff0c\u6709\u54ea\u4e9b\u7b56\u7565\u53ef\u4ee5\u7b80\u5316\u6216\u4f18\u5316\u6559\u5e08\u6a21\u578b\u7684\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u4ece\u800c\u964d\u4f4e\u6574\u4f53\u84b8\u998f\u7684\u590d\u6742\u6027\uff1f**\n\n\u901a\u8fc7\u8fd9\u4e9b\u95ee\u9898\uff0c\u53ef\u4ee5\u66f4\u6df1\u5165\u5730\u7406\u89e3\u6a21\u578b\u84b8\u998f\u7684\u4f18\u52bf\u4e0e\u5c40\u9650\u6027\uff0c\u5e76\u63a2\u8ba8\u5982\u4f55\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u514b\u670d\u8fd9\u4e9b\u6311\u6218\u3002\u300d\n", "e1e96c8a-2a92-4140-b48f-8d5d3d73717f:\u300cref_ids: 454847819752024066, chunk_ids: 2, Score: 0.5117, Text: # 4 Evolving Teacher to Student Models via Pruning\nAs the previous section presents, the Knowledge Distillation approach to model compression has two main drawbacks in the private world:  \n\n\u2022Drop in accuracy: There is a considerable drop in the accuracy between the teacher and the student models.   \n\u2022Good initialization of students is crucial: The best performance is obtained by students who already have a good initialization; in our experiments, pre-trained DistilBERT mostly achieved the best student performance.  \n\nFinding a good initialization can be challenging in practice. Often, the student architectures are chosen to suit the hardware and latency requirements of the application for which the model is being deployed, using neural architecture search [ 18 ]. Hence, finding a good initialization for every student architecture via pre-training can be expensive and in most cases impossible. Our zero-shot initialization strategies alleviate this problem to a certain degree, yet fall short of closing the gap between the teacher and the student performances. Moreover, DPKD requires that (i) the teacher is trained with DPSGD and (ii) the student is distilled via DPSGD. This two-step approach creates additional overheads in terms of training. Given these limitations, it is natural to ask: Can we evolve the teacher to a student model while fine-tuning with DPSGD? In this section, we explore an answer to this via structured and unstructured pruning with privacy, which allows us to obtain student models that are as good as the teacher models.\n\n# 4.1 Model Compression via Pruning\nPruning algorithms are a broad class of model compression techniques where one drops the parameters from a model during or after the training process. Many works have shown that eliminating unnecessary parameters of neural networks via pruning can lead to sparser and compressed models that have shorter inference times without loss in performance [ 30 ,25 ,24 ]. For example, in magnitude pruning , one of the most widely used pruning techniques, we prune a fraction of parameters with the lowest magnitude. However, there are several pruning strategies, and we refer the readers to [ 33 ] for more details and references.  \n\nPruning can be implemented in both structured and unstructured ways. In structured pruning, all the pruned weights belong to a single building block of the model. For example, a 6-layer $\\\\frac{1}{2}$ -BERT can be obtained by pruning 6 layers from the full BERT model, which consists of 12 transformer blocks. On the other hand, in unstructured pruning, pruned weights may be spread across all the layers of the network. In unstructured pruning, it is possible to obtain a $50\\\\%$ sparse student model while still having all the 12 layers of BERT. Depending on the hardware architectures, inference latency between models with structured and unstructured sparsity could be quite different. However, in this section, we use sparsity as the main measure of model compression , which is also well accepted in the community [ 33 ,27 ].\n\n# 4.2 Iterative Magnitude Pruning (IMP)\nPrivate pruning techniques we study in this section are based on the Iterative Magnitude Pruning (IMP) method, which is a specific pruning technique proposed in a recent work on Lottery Ticket Hypothesis [ 19 ]. The idea behind IMP is rather simple: As we train a deep learning model, after every $N$ iterations we prune an $\\\\alpha\\\\%$ of the weights with the lowest magnitude .  \n\nWe repeat this process until we achieve the desired sparsity. Here, both $N$ and $\\\\alpha$ are hyperparameters that need to be tuned. For example, to achieve $50\\\\%$ sparsity, one can perform $5N$ iterations where after every $N$ iterations additional $10\\\\%$ of the weights with the least magnitudes are dropped. As specified IMP produces unstructured sparsity. However, we consider a simple modification of the IMP algorithm to produce structured sparsity as well.\n\n# Algorithm 2 Structured DPIMP\nInput: Teacher model $\\\\tau$ , numb r of la ers to prune $L$ , hype rams $\\\\alpha$ ,$N$ and $M$   \nOutput e student model Swith Llayers pruned from T$\\\\tau$   \n1: 2: Set for ${\\\\mathcal{S}}:={\\\\mathcal{T}}$ $j=1$ to T$L$ do   \n3: Fin $\\\\boldsymbol{S}$ for $N$ itera s with DPSGD   \n4: Set $W_{\\\\mathrm{min}}$ consisting of $\\\\alpha\\\\%$ of the remaining model weights with the least magnitude   \n5: Set $W_{i}$ as the weights of layer $i$   \n6: Drop the layer $i^{*}$ from $\\\\boldsymbol{S}$ satisfying $i^{*}:=\\\\arg\\\\operatorname*{max}_{i}\\\\left\\\\{W_{i}\\\\cap W_{\\\\operatorname*{min}}\\\\right\\\\}$   \n7: end for   \n8: Fine-tu $\\\\boldsymbol{S}$ for $M$ more iterations with DPSGD   \n9: return S\n\n# 4.3 Structured DPIMP\nWe first attempt to obtain a student model from the teacher model via a structured IMP technique, using the following modification: During fine-tuning the teacher model with DPSGD, we progressively drop an appropriately chosen transformer block from the teacher model at the end of every $N$ iterations. We repeat this process until we obtain the student model with the required sparsity. The layer to drop is chosen using the following heuristic: Let $\\\\alpha>0$ be a hyperparameter. At the end of $N$ iterations, fix bottom (by magnitude) $\\\\alpha\\\\%$ of all model weights, and denote it by $W_{\\\\mathrm{min}}$ . For the $i^{t h}$ transformer block, let $W_{i}$ denote the set of model weights belonging to that block. Among all the transformers blocks we find the block $i^{*}$ that has the highest number of weights from the set $W_{\\\\mathrm{min}}$ ; Formally, $i^{*}:=\\\\arg\\\\operatorname*{max}_{i}$ $\\\\{W_{i}\\\\cap W_{\\\\operatorname*{min}}\\\\}$ , and we prune the transformer layer $i^{*}$ . We present this in Algorithm 2 .  \n\nEmpirical Evaluation We evaluate our structured pruning algorithm with the same setup described in Section 3.3 , and provide the hyperparameters in Appendix A.2 . We split the privacy budget equally among all the iterations of the algorithm. Our goal is to produce a student model which has $\\\\frac{1}{2}$ as many layers as the full BERT model. Table 4 shows the results for this setting where we compare structured DPIMP to private fine-tuning of the pre-trained DistilBERT and the full BERT model. The main takeaway from this experiment is:  \n\nTable 4: Comparing performance of 6-layer $\\\\frac{1}{2}$ -BERT student model produced by structured DPIMP with 12-layer BERT teacher model and pre-trained DistillBERT. All our models have the same privacy budget bounded by $\\\\epsilon<4.25\\\\$ .  \n\n\n<html><body><table><tr><td>Model</td><td>MNLI QQP</td><td>QNLI</td><td>SST-2</td><td>Avg</td></tr><tr><td>BERT</td><td>77.8</td><td>84.7 87.8</td><td>90.5</td><td>85.2</td></tr><tr><td>DistilBERT</td><td>73.0</td><td>84.3 82.8</td><td>87.7</td><td>81.9</td></tr><tr><td>-BERT</td><td>72.9</td><td>83.1 82.5</td><td>85.7</td><td>81.0</td></tr></table></body></html>  \n\n\u2022DP structured pruning algorithm produces a student model that has performance comparable to that of DistilBERT. Further, it avoids the pre-training cost associated with DistilBERT.\u300d\n", "e1e96c8a-2a92-4140-b48f-8d5d3d73717f:\u300cref_ids: 454846524788653356, chunk_ids: 3, Score: 0.4434, Text: # 4 Experimental Results\n\n# 4.1 Implementation\nWe use the CLIP model, ViT-g-14 (ViT-G), provided by OpenCLIP [25] as the teacher model. The student model is built upon ViT-S and its state-of-the-art (SOTA) variants, DAT-T [48] and Swin-T [34], where we replace the classification head with a feature projection head. For training with student models in stage 1, we use AdamW optimizer with a base learning rate of $10^{-4}$ and weight decay of 0.05. We use a cosine learning rate scheduler which decays the learning rate to $5\\\\times10^{-6}$ over 120 epochs. In stage 2, we reduce the base learning rate to $10^{-6}$ .For the threshold $\\\\tau_{c}$ , we empirically set it at 0 .25 considering the training data utilization and data noise. For the CLIP text encoder, we use the text prompt \u201ca photo of a {scene category}.\" or \u201ca satellite image of a {scene category}.\" depending on the dataset as suggested by [39]. For quantized models, we report static quantization ts. For the triplet loss $\\\\mathcal{L}_{c}$ , we use a margin $m=0.3$ and a negative set size J= 3 .\u300d\n", "e1e96c8a-2a92-4140-b48f-8d5d3d73717f:\u300cref_ids: 455037545741550556, chunk_ids: 6, Score: 0.4316, Text: # 4 Methods\nTo overcome the aforementioned limitations, we introduce our L2T framework, Learning Good Teacher Matters (LGTM) to enable more effective knowledge distillation. We first introduce distillation influence , which estimates how much will the student\u2019s performance on validation data change if we put one training sample in the knowledge distillation process.  \n\nAfterwards, we introduce an efficient training method based on finite difference approximation for incorporating distillation influence into the teacher\u2019s update. Finally, we interpret current L2T methods from the perspective of influence function.  \n\nDistillation influence Influence function ( Pruthi et al. ,2020 ;Koh and Liang ,2017 ) is a way of measuring the influence of training samples on the model\u2019s predictions. It can be utilized to identify instances that have a disproportionate effect on the model\u2019s behavior, whether due to their status as outliers or due to incorrect labeling ( Jia et al. ,2019 ;Ghorbani and Zou ,2019 ;Hara et al. ,2019 ). By calculating the influence function for a particular example, it is possible to estimate the extent to which the model\u2019s prediction would be altered as a result of operations on that sample.  \n\nIn vanilla distillation, for the student model, we derive the distillation influence of $\\\\boldsymbol{z}_{i}^{r}$ as the gradient similarity between the training sample $z_{i}^{r}$ and the validation batch $z^{e}$ :  \n\n$$\n\\\\begin{array}{r l}&{\\\\mathcal{Z}_{\\\\mathrm{distill}}(z_{i}^{r},z^{e})=\\\\!\\\\nabla_{\\\\theta_{s}}\\\\mathcal{L}_{\\\\mathrm{ce}}(T(\\\\pmb{x}_{i}^{r};\\\\theta_{t}^{m}),S(\\\\pmb{x}_{i}^{r};\\\\theta_{s}^{m}))^{\\\\intercal}}\\\\\\\\ &{\\\\quad\\\\quad\\\\quad\\\\quad\\\\quad\\\\nabla_{\\\\theta_{s}}\\\\mathcal{L}_{\\\\mathrm{ce}}(\\\\pmb{y}^{e},S(\\\\pmb{x}^{e};\\\\theta_{s}^{m+1}))}\\\\end{array}\n$$  \n\nThe detailed derivation can be found in appendix A .The influence reflects how well the knowledge gained from a particular sample generalizes. It follows that the teacher should focus on teaching the student to capture training samples that have the highest distillation influences.  \n\nIn order to incorporate the per-sample influence into knowledge distillation, we adjust the loss weight of each sample based on its distillation influence. This allows us to determine the relative importance of each sample, and helps to control how much each sample contributes to the teacher\u2019s learning process. Samples that are deemed to be more beneficial for the student\u2019s generalization are assigned higher weights. Then we propose training the teacher using the following objective:  \n\n$$\n\\\\mathcal{L}_{\\\\mathrm{influence}}=\\\\frac{1}{B^{r}}\\\\sum_{i=1}^{B^{r}}w_{i}\\\\mathcal{L}_{\\\\mathrm{ce}}((T(\\\\pmb{x}_{i}^{r};\\\\theta_{t}^{m}),S(\\\\pmb{x}_{i}^{r};\\\\theta_{s}^{m})),\n$$  \n\nwhere $w_{i}\\\\,=\\\\,\\\\mathcal{T}_{\\\\mathrm{distill}}(z_{i}^{r},z^{e})$ . By including the influence in the knowledge distillation loss function, we can tailor the training process to better suit the characteristics of the target task.  \n\n<html><body><table><tr><td>Algorithm1 LGTM</td></tr><tr><td>Require: student Os, teacher Ot, training set Dtrain, validation set Dval Require: Ms, Nt: learning rate for the student and the teacher Require: e:a small scalar Require:M: the maximum number of the training steps 1:while step<Mdo 2: Sample a batch of training set zr = (\u03b1\", y\"\uff09 \uff5e Dtrain 3: Copystudentparameter0stostudent0' 4: Update 0': 0s < 0s - nsV Ls(0', 0t, 2\") 5: Sample a batch of validation set z = (\u00b0, y\u00b0) \uff5e Dval 6: Calculate \u03b8: 0 = 0s \u00b1 \u2208Lce(y, S(\u221e;\u03b8s)) 7: Calculate the Distillation Infuence with z', 0t, 0 and e: Linfluence (01) \u00b7b< 8: Update \u03b8t:\u03b8t\u21900t-ntVotLt(0t,0s,) >eq.(11) 9: Update original 0s: 0s < 0s - ns Vos Cs(0s, 0t, 2r) 10: step \u2190 step + 1 11: end while</td></tr></table></body></html>  \n\nFinite difference approximation For standard neural network training, we often compute a consolidated gradient for a mini-batch of $B^{r}$ training samples to enhance computational efficiency. However, in the context of determining the distillation influence for each sample, the computation will slow down the training by a factor of of per-sample gradient $\\\\mathcal{L}_{\\\\mathrm{ce}}(T(\\\\pmb{x}_{i}^{r};\\\\theta_{t}^{m}),S(\\\\pmb{x}_{i}^{r};\\\\theta_{s}^{m}))$ $B^{r}$ .In addition, a naive implementation is memory intensive, because it requires to keep a copy of $\\\\nabla_{\\\\theta_{s}}\\\\mathcal{L}_{\\\\mathrm{ce}}(\\\\pmb{y}^{e},S(\\\\pmb{x}^{e};\\\\theta_{s}^{m+1}))$ .  \n\nTo address this, we propose an efficient method for updating the teacher with the distillation influence by utilizing finite difference ( Gleich ,2005 ), a technique commonly used in numerical analysis for approximating the derivative of a function at a given point. Similar to ( Pham et al. ,2021 ;Liu et al. ,2018 ), we approximate Linfluence by  \n\n$$\n\\\\begin{array}{r l r}{\\\\lefteqn{\\\\mathcal{L}_{\\\\mathrm{influence}}\\\\approx\\\\hat{\\\\mathcal{L}}_{\\\\mathrm{influence}}=\\\\frac{1}{B^{r}}\\\\sum_{i=1}^{B^{r}}\\\\Big[\\\\frac{\\\\mathcal{L}_{\\\\mathrm{ce}}(T(x_{i};\\\\theta_{t}^{m}),S(x_{i};\\\\theta_{s}^{+}))}{2\\\\epsilon}}\\\\\\\\ &{}&{{\\\\displaystyle-\\\\:\\\\frac{\\\\mathcal{L}_{\\\\mathrm{ce}}(T(x_{i};\\\\theta_{t}^{m}),S(x_{i};\\\\theta_{s}^{-}))}{2\\\\epsilon}\\\\Big],\\\\;\\\\;}\\\\end{array}\n$$  \n\nwhere $\\\\theta_{s}^{\\\\pm}=\\\\theta_{s}\\\\pm\\\\epsilon\\\\mathcal{L}_{\\\\mathrm{ce}}(\\\\pmb{y}^{e},S(\\\\pmb{x}^{e};\\\\theta_{s}^{m+1}))$ \u00b1Land $\\\\epsilon$ is a small scalar. Our proposed method for evaluating the finite difference is computationally efficient, as it only requires two forward passes for $\\\\theta_{s}$ and one backward pass for $\\\\theta_{t}$ for a single batch, as opposed to a naive implementation which requires $B^{r}$ forward and backward passes for $\\\\theta_{s}$ and one backward pass for $\\\\theta_{t}$ . We provide more details of the derivation in appendix B.  \n\nTeacher\u2019s auxiliary loss Inspired by ( Pham et al. ,2021 ), in order to balance the trade-off between self-evolution and transferability of the teacher model, we incorporate the loss with respect to the ground truth as ${\\\\mathcal{L}}_{\\\\mathrm{aux}}$ into the final objective:  \n\n  \nFigure 2: Performance comparison between Meta Distill ( Zhou et al. ,2022 ) and LGTM on the MNLI validation set. We observe that for LGTM, student model does not suffer from overfitting (thanks to distillation influence), and the teacher can balance its own evolution and effective knowledge transfer (thanks to auxiliary loss).  \n\n$$\n\\\\begin{array}{r l}{\\\\mathcal{L}_{\\\\mathfrak{t}}(\\\\theta_{t}\\\\ |\\\\ \\\\theta_{s},z^{r})=}&{\\\\hat{\\\\mathcal{L}}_{\\\\mathrm{influence}}+\\\\mathcal{L}_{\\\\mathrm{aux}},}\\\\\\\\ {\\\\mathcal{L}_{\\\\mathrm{aux}}=}&{\\\\alpha\\\\mathcal{L}_{\\\\mathrm{ce}}(y^{r},T(x^{r};\\\\theta_{t}))+}\\\\\\\\ &{(1-\\\\alpha)\\\\mathcal{L}_{\\\\mathrm{ce}}(T(x^{r};\\\\theta_{t}),S(x^{r};\\\\theta_{s})).}\\\\end{array}\n$$  \n\nwhere $\\\\alpha$ is the loss ratio.  \n\nOverall, our method allows the teacher to adapt to the student\u2019s abilities and provide more personalized guidance while improving the student\u2019s generalization capability. We present the algorithm of LGTM in algorithm 1 .  \n\nRelationship with other L2T methods Here we interpret current learning to teach methods from the perspective of influence function.  \n\nIn the case of online distillation, it is assumed that all training samples possess an equivalent distillation influence and that the teacher model is responsible for reducing the transfer difficulty of all training samples.  \n\nIn contrast, the key differentiating factor between meta distillation and online distillation is the utilization of a dynamic loss weight. We interpret this weight as a measure of the distillation influence of the current training batch $z^{r}$ on the generalization ability of the student model. Specifi- cally, it reflects the similarity between the gradients of the training and validation batches, indicating the effect of the current training batch $z^{r}$ on the validation batch $z^{e}$ (as detailed in appendix C). However, it should be noted that this weight functions primarily as an adaptive learning rate, adjusting the gradient step proportionally to the degree of similarity in gradients. We illustrate the general workflow of vanilla distillation, online distillation, meta distillation and LGTM in fig. 1 .\u300d\n"]}}, "__type__": "1"}}, "template_store/metadata": {"71d756d8-f2dd-4195-b033-7555be6fc4f3": {"template_hash": ""}}}