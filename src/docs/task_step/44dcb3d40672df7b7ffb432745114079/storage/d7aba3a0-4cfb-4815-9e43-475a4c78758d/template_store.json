{"template_store/data": {"ed858b7c-1913-4a71-a2a9-6ab83c0bba61": {"__data__": {"id_": "ed858b7c-1913-4a71-a2a9-6ab83c0bba61", "metadata": {}, "relationships": {}, "hash": "", "exec_code": "", "base_template_content": "\nfrom langchain_community.chat_models import ChatOpenAI\nfrom langchain.prompts.chat import (\n    ChatPromptTemplate,\n    SystemMessagePromptTemplate,\n    AIMessagePromptTemplate,\n    HumanMessagePromptTemplate,\n)\nfrom langchain.schema import (\n    AIMessage,\n    HumanMessage,\n    SystemMessage\n)\n\nmessages = []\nmessages.append(SystemMessage(content = r\"\"\"\u4f60\u6b63\u5728\u626e\u6f14{{ cosplay_role }}\uff0c\u4f60\u6b63\u5728cosplay{{ cosplay_role }}\u3002\n\u7ed3\u5408\u5386\u53f2\u5185\u5bb9\u7684\u5185\u5bb9\u7528\u4e00\u81f4\u6027\u7684\u8bed\u6c14\u56de\u590d\u3002\u914d\u5408\u6211\u8fdb\u884c\u6f14\u51fa\uff0c\n\u8bf7\u4e0d\u8981\u56de\u7b54\u4f60\u662f\u8bed\u8a00\u6a21\u578b\uff0c\u6c38\u8fdc\u8bb0\u4f4f\u4f60\u6b63\u5728\u626e\u6f14{{ cosplay_role }}\n\u6ce8\u610f\u4fdd\u6301\u4f60\u7684\u6027\u683c\u7279\u70b9\u5305\u62ec{{ personality }}\n\"\"\"))\n\n{% for message in messages %}\nmessages.append(HumanMessage(content = r'''{{ message }}'''))\n{% endfor %}\n", "exec_data": {"cosplay_role": "d7aba3a0-4cfb-4815-9e43-475a4c78758d", "personality": "\u4e13\u4e1a\u4e14\u6280\u672f\u7cbe\u6e5b\u3001\u6279\u5224\u4e14\u654f\u9510\u3001\u521b\u65b0\u4e14\u524d\u77bb\u3001\u52a1\u5b9e\u4e14\u6ce8\u91cd\u5e94\u7528\u3001", "messages": ["d7aba3a0-4cfb-4815-9e43-475a4c78758d:\u300c\u6700\u65b0\u8fdb\u5c55\u300d\n", "d7aba3a0-4cfb-4815-9e43-475a4c78758d:\u300c### \u95ee\u9898\uff1a\n\u5728\u6a21\u578b\u84b8\u998f\u7684\u6700\u65b0\u8fdb\u5c55\u4e2d\uff0c\u8de8\u6a21\u6001\u84b8\u998f\uff08Cross-Modal Distillation\uff09\u662f\u5982\u4f55\u5728\u4e0d\u540c\u6a21\u6001\uff08\u5982\u56fe\u50cf\u548c\u6587\u672c\uff09\u4e4b\u95f4\u5b9e\u73b0\u77e5\u8bc6\u8fc1\u79fb\u7684\uff1f\u8fd9\u79cd\u6280\u672f\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u6709\u54ea\u4e9b\u6311\u6218\u548c\u6f5c\u5728\u4f18\u52bf\uff1f\u300d\n", "d7aba3a0-4cfb-4815-9e43-475a4c78758d:\u300cref_ids: 454846907942800944, chunk_ids: 6, Score: 0.5430, Text: # Discussion & Conclusion\nOur experiments demonstrate our approach can distill knowledge between two or more models regardless of architecture, algorithm, feature overlap, and under small or large data settings. Since our method targets specific weaknesses of each model, we can distill knowledge between any combination of high and/or low-performance models, compared to traditional knowledge distillation techniques which tend to only distill knowledge from a single high-performance model to a low-performance model (Hinton 2015)(Gou et al. 2021b). Though our method performed well on real-world data sets it does have some assumptions. It assumes there is some overlap between the features of the data sets and most importantly, our method works best when the distribution of the datasets used to train models are significantly different from each other. Further, our method is fundamentally limited by the strength of counterfactual generation. Counterfactual explanations are easy to compute on tabular data but their performance on more complex data, such as images is more challenging. However, more recent approaches have found success in more basic image networks (Goyal et al. 2019) (Sauer and Geiger 2021), so as research progresses, we believe this limitation will be removed.  \n\nWe show in Figure 5 the number of instances each model teaches to the others. Interestingly, this quantity is asymmetrical which will motivate future work to better understand the mechanisms of how each model teaches the others.  \n\nConclusion We present a novel form of knowledge distillation that can be used between multiple models, in multiple directions and is focused. Each model simultaneously acts as teacher and student, distilling knowledge to the other by encoding learned information into virtual counterfactual instances and passing them into the training sets of other models. Unlike other knowledge distillation algorithms, which always distill knowledge from the teacher to student, we use a targeting mechanism to ensure that teachers only distill correct knowledge tailored to a student\u2019s deficiencies.  \n\nIn our four main experiments, our method beats the competitors studied, including state-of-the-art knowledge distillation algorithms. In a stress test to determine if knowledge could be distilled between many (10) models, our model surpasses all but one competitor and remains competitive. We find our method particularly useful in the setting where models can be freely shared, but raw data cannot, and the data sets share some features. This is common in medical imaging or finance communities where data is confidential. Given our method\u2019s strong performance on experiments simulating the aforementioned setting, we believe this to be a viable approach to knowledge distillation under such circumstances.\n#\nAhn, S.; Hu, S. X.; Damianou, A.; Lawrence, N. D.; and Dai, Z. 2019. Variational Information Distillation for Knowledge Transfer. arXiv:1904.05835.   \nAllison, P. D. 2001. Missing Data . Sage Publications. Alsenani, D. 2020. US Cars Dataset: Online Car Auction in North American. Retrieved from https://www.kaggle.com/doaaalsenani/usa-cers-dataset. Ashrapov, I. 2020. Tabular GANs for uneven distribution. arXiv preprint arXiv:2010.00638 .  \nChen, D.; Mei, J.-P.; Wang, C.; Feng, Y.; and Chen, C. 2020. Online Knowledge Distillation With Diverse Peers. In Proceedings of the AAAI Conference on Artificial Intelligence ,volume 34, 3430\u20133437.   \nChen, D.; Mei, J.-P.; Zhang, Y.; Wang, C.; Wang, Z.; Feng, Y.; and Chen, C. 2021. Cross-layer distillation with semantic calibration. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 35.   \nDua, D.; and Graff, C. 2017. UCI Machine Learning Repository. Http://archive.ics.uci.edu/ml.   \nGou, J.; Yu, B.; Maybank, S. J.; and Tao, D. 2021a. Knowledge distillation: A survey. International Journal of Computer Vision , 129(6): 1789\u20131819.   \nGou, J.; Yu, B.; Maybank, S. J.; and Tao, D. 2021b. Knowledge Distillation: A Survey. International Journal of Computer Vision , 129(6).   \nGoyal, Y.; Wu, Z.; Ernst, J.; Batra, D.; Parikh, D.; and Lee, S. 2019. Counterfactual visual explanations. In International Conference on Machine Learning , 2376\u20132384. PMLR. Gromski, P. S.; Xu, Y.; Kotze, H. L.; Correa, E.; Ellis, D. I.; Armitage, E. G.; Turner, M. L.; and Goodacre, R. 2014. Influence of Missing Values Substitutes on Multivariate Analysis of Metabolomics Data. Metabolites , 4(2): 433\u2013452. Heo, B.; Lee, M.; Yun, S.; and Choi, J. Y. 2019. Knowledge Transfer via Distillation of Activation Boundaries Formed by Hidden Neurons. Proceedings of the AAAI Conference on Artificial Intelligence , 33(01).   \nHinton, G. e. A. 2015. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531 .  \nJanosi, A. e. a. 1988. Heart Disease Data Sets.   \nKeane, M. T.; and Smyth, B. 2020. Good counterfactuals and where to find them: A case-based technique for generating counterfactuals for explainable ai (xai). In International Conference on Case-Based Reasoning . Springer.   \nKennedy, J.; and Eberhart, R. 1995. Particle swarm optimization. In Proceedings of ICNN\u201995-international conference on neural networks , volume 4, 1942\u20131948. IEEE. Kim, J.; and Park, S. 2020. Paraphrasing complex network: Network compression via factor transfer. arXiv preprint arXiv:1802.04977 .  \nLiu, Y.; Cao, J.; Li, B.; Yuan, C.; Hu, W.; Li, Y.; and Duan, Y. 2019. Knowledge Distillation via Instance Relationship Graph. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) .Mirzadeh, S. I.; Farajtabar, M.; Li, A.; Levine, N.; Matsukawa, A.; and Ghasemzadeh, H. 2020. Improved knowledge distillation via teacher assistant. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 34, 5191\u20135198.   \nMital, A. 2020. US Used cars dataset. Https://www.kaggle.com/ananaymital/us-used-cars-dataset. Molnar, C. 2019. Interpretable Machine Learning . Independently published.   \nPan, S. J.; and Yang, Q. 2010. A Survey on Transfer Learning. IEEE Transactions on Knowledge and Data Engineering , 22(10): 1345\u20131359.   \nRadford, A.; Metz, L.; and Chintala, S. 2016. Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434 .Reese, A. 2021. Used Cars Dataset Vehicles listings from Craigslist.org. Https://www.kaggle.com/austinreese/craigslist-carstrucksdata.   \nRomero, A.; Ballas, N.; Kahou, S. E.; Chassang, A.; Gatta, C.; and Bengio, Y. 2014. Fitnets: Hints for thin deep nets. arXiv preprint arXiv:1412.6550 .  \nSauer, A.; and Geiger, A. 2021. Counterfactual generative networks. arXiv preprint arXiv:2101.06046 .  \nTung, F.; and Mori, G. 2019. Similarity-preserving knowledge distillation. In Proceedings of the IEEE/CVF International Conference on Computer Vision , 1365\u20131374.   \nXiao, H.; Rasul, K.; and Vollgraf, R. 2017. Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms. arXiv:1708.07747.   \nXie, Q.; Luong, M.-T.; Hovy, E.; and Le, Q. V. 2020. Selftraining with noisy student improves imagenet classification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 10687\u201310698.   \nYim, J.; Joo, D.; Bae, J.; and Kim, J. 2017. A Gift From Knowledge Distillation: Fast Optimization, Network Minimization and Transfer Learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) .  \nZhang, H.; Cisse, M.; Dauphin, Y. N.; and Lopez-Paz, D. 2017. mixup: Beyond Empirical Risk Minimization.   \nZhang, H.; Hu, Z.; Qin, W.; Xu, M.; and Wang, M. 2021. Adversarial co-distillation learning for image recognition. Pattern Recognition , 111: 107659.   \nZhang, Y.; Xiang, T.; Hospedales, T. M.; and Lu, H. 2018. Deep mutual learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , 4320\u2013 4328.   \nZhang, Z.; and Sabuncu, M. R. 2020. Self-distillation as instance-specific label smoothing. arXiv preprint arXiv:2006.05065 .  \nZhuang, F.; Qi, Z.; Duan, K.; Xi, D.; Zhu, Y.; Zhu, H.; Xiong, H.; and He, Q. 2020. A comprehensive survey on transfer learning. Proceedings of the IEEE , 109(1): 43\u201376.\u300d\n", "d7aba3a0-4cfb-4815-9e43-475a4c78758d:\u300cref_ids: 454898870263482564, chunk_ids: 0, Score: 0.5195, Text: # B. Challenges\n(1) The Balance Between Model Compression and Performance  \n\nKDisaneffectivemodelcompressiontechnique,andeffective knowledge transfer helps with the construction of a lightweight network model; such lightweight models are more suitable for practical applications with particular model efficiency requirements and device performance limitations [4] ,[61] ,[62] .Complex networks have large-scale network parameters and a long running time, making them unsuitable for such practical applications [63] . However, it is also challenging for lightweight networks to learn effective visual features from large-scale complex datasets [4] . A balance must therefore be struck between model compression and model performance improvement.  \n\n(2) The Imbalance Between Multi-Modal Data Features  \n\nAt present, most KD-based OD models focus on transferring knowledge within the image domain; only a few works have attempted to extract additional features from other modal data (infrared images, depth images, text, index data, etc.) [19] ,[35] ,[55] . The introduction of multi-modal features is beneficial to KD. However, a more challenging problem is that of how to deal with the imbalance between multi-modal data features. Here, \u201cimbalance\u201d means that significant differences exist between the feature dimensions and semantic information of different modalities. For example, the dimensions of features extracted from visual data and index data were significantly different. In addition, the semantic feature gaps between the textual and visualfeaturesofRGBimagesalsomakeitdifficulttousetextinformation to guide the visual feature learning of student models. Therefore, another challenge is that of combining imbalanced multi-modal features to guide student models for feature learning [19] ,[64] ; new feature fusion mechanisms or multi-modal information-guided mechanisms need to be designed for student model learning.  \n\n(3) Designing or Selecting Superior Teacher and Student Models  \n\nKDtechnologyisutilizedtotransfertheknowledgelearnedby complex teacher models to lightweight student models. An optimal teacher model or model combination has a very favourable influence on guiding the feature learning of a student model. Therefore, the selection of teacher and student models makes a very important contribution to the performance of the final student models [53] ,[65] . However, as there are many complex and lightweight models for OD, it is difficult to choose the appropriate teacher and student models for specific OD problems.\u300d\n", "d7aba3a0-4cfb-4815-9e43-475a4c78758d:\u300cref_ids: 454845510461164774, chunk_ids: 2, Score: 0.4570, Text: # 2 Related Work\n\n# 2.1 Multi-modal Machine Translation\nAs an intersection of multimedia and neural machine translation (NMT), MMT has drawn great attention in the research community. Technically, existing methods mainly focus on how to better integrate visual information into the framework of NMT. 1) Calixto et al. (2017 ) propose a doublyattentive decoder to incorporate two separate attention over the source words and visual features. 2) Ive et al. (2019 ) propose a translate-and-refine approach to refine draft translations by visual features. 3) Yao and Wan (2020 ) propose the multimodal Transformer to induce the image representations from the text under the guide of image-aware attention. 4) Yin et al. (2020 )employs a unified multimodal graph to capture various semantic interactions between multimodal semantic units.  \n\nHowever, the quantity and quality of the annotated images limit the development of this task, which is scarce and expensive. In this work, we aim to perform the MMT in an image-free manner, which has the ability to break data constraints.\n\n# 2.2 Knowledge Distillation\nKnowledge distillation (KD) ( Buciluco et al. ,2006 ;Hinton et al. ,2015 ) aims to use a knowledgerich teacher network to guide the parameter learning of the student network. In fact, KD has been investigated in a wide range of fields. Romero et al. (2014 ) transfer knowledge through an intermediate hidden layer to extend the KD. Yim et al. (2017 ) define the distilled knowledge to be transferred in terms of flow between layers, which is calculated by the inner product between features from two layers. In the multimedia field, Gupta et al. (2016 ) first introduce the technique that transfers supervision between images from different modalities. Yuan and Peng (2018 )propose the symmetric distillation networks for the text-to-image synthesis task.  \n\nInspired by these pioneering efforts, our IKDMMT framework is intents to take full advantage of KD to generate a multimodal feature to overcome triplet data constraints.\n\n# 3 IKD-MMT Model\nAs illustrated in Figure 2 , the proposed framework consists of two components: an image-free MMT backbone and a multimodal feature generator.  \n\n  \nFigure 2: The framework of our IKD-MMT model. The multimodal feature generator, multimodal student network and visual teacher network are the most critical modules, which help break the dataset constraints of image-must.\n\n# 3.1 Image-Free MMT Backbone\nGiven a source sentence $X{=}\\\\left(x_{1},\\\\ldots,x_{I}\\\\right)$ , each token $x_{i}$ is mapped into a word embedding vector $E_{x_{i}}\\\\,\\\\in\\\\,\\\\mathbb{R}^{d_{w}}$ through the textual embedding w position encoding ( Gehring et al. ,2017 ). $d_{w}$ and $t=(E_{x_{1}},\\\\ldots,E_{x_{I}})$ are the word embedding dimension and the textual feature, respectively.  \n\nThen, we feed the text feature $t$ together with the multimodal feature $m$ (detail in Section 3.2.1) into the multimodal transformer encoder ( Yao and Wan ,2020 ). In the multimodal encoder layer, we cascade the multimodal feature $m$ and the text feature $t$ to reorganize a new multimodal feature $\\\\widetilde{x}$ as the query vector:  \n\n$$\n\\\\widetilde{x}{=}[t;m W^{m}]\\\\in\\\\mathbb{R}^{(I+P)*d},\n$$  \n\nwhere $I$ is the length of source sentence, and e$P$ is the size of multimodal feature. Here, we can understand this modal fusion from the perspective of nodes and graphs. If we treat each source token as a node, each region of the multimodal feature can also be regarded as a pseudo-token and added to the source token graph for modal fusion. The key and value vectors are preserved as the text feature $t$ , and the multimodal encoder layer is calculated as follows:  \n\n$$\n\\\\begin{array}{c}{{c_{k}=\\\\displaystyle\\\\sum_{i=1}^{I}\\\\tilde{\\\\alpha}_{k i}\\\\left(t_{i}W^{V}\\\\right),}}\\\\\\\\ {{\\\\tilde{\\\\alpha}_{k i}=s o f t m a x\\\\left(\\\\frac{\\\\left(\\\\tilde{\\\\alpha}_{k}W^{Q}\\\\right)\\\\left(t_{i}W^{K}\\\\right)^{\\\\top}}{\\\\sqrt{d}}\\\\right).}}\\\\end{array}\n$$  \n\nIn this paper, we directly adopt the Transformer decoder 2 (Vaswani et al. ,2017 ) for translation.  \n\nGiven a target sentence $Y{=}\\\\left(y_{1},\\\\ldots,y_{J}\\\\right)$ , our framework outputs the predicted probability of the target word $y_{j}$ as follow:  \n\n$$\np\\\\left(y_{j}|y_{<j},\\\\mathbf{X},m\\\\right)\\\\propto\\\\exp\\\\left(W^{h}H_{j}^{L}+b^{h}\\\\right),\n$$  \n\nwhere $H_{j}^{L}$ represents the top output of the decoder at $j$ -th decoding time step, $W^{h}$ and $b^{h}$ are learnable multi-layer perceptrons, and $\\\\exp()$ is a Softmax layer.\n\n# 3.2 Multimodal Feature Generation\n\n# 3.2.1 Preliminaries\nIn this part, we introduce the frame, symbol definitions and task goal of multimodal feature generation in advance.  \n\nThe frame is composed of a multimodal feature generator $F$ , a visual teacher model $T$ and a multimodal student model $S$ .The detailed architecture of each module is shown in Table 7 of the appendix. The model parameters of $S$ are denoted as $\\\\theta^{s}$ . When the global text feature $\\\\bar{t}$ is fed into by the $l$ -th layer is denoted as $S$ , the hidden representation produced $\\\\varphi_{l}^{S}\\\\left(\\\\bar{t},\\\\theta_{l}^{s}\\\\right)$ \u300d\n"]}}, "__type__": "1"}}, "template_store/metadata": {"ed858b7c-1913-4a71-a2a9-6ab83c0bba61": {"template_hash": ""}}}