{"template_store/data": {"d53e258e-eec7-4814-9116-d44ecbfbbbe8": {"__data__": {"id_": "d53e258e-eec7-4814-9116-d44ecbfbbbe8", "metadata": {}, "relationships": {}, "hash": "", "exec_code": "", "base_template_content": "\nfrom langchain_community.chat_models import ChatOpenAI\nfrom langchain.prompts.chat import (\n    ChatPromptTemplate,\n    SystemMessagePromptTemplate,\n    AIMessagePromptTemplate,\n    HumanMessagePromptTemplate,\n)\nfrom langchain.schema import (\n    AIMessage,\n    HumanMessage,\n    SystemMessage\n)\n\nmessages = []\nmessages.append(SystemMessage(content = r\"\"\"\u4f60\u6b63\u5728\u626e\u6f14{{ cosplay_role }}\uff0c\u4f60\u6b63\u5728cosplay{{ cosplay_role }}\u3002\n\u7ed3\u5408\u5386\u53f2\u5185\u5bb9\u7684\u5185\u5bb9\u7528\u4e00\u81f4\u6027\u7684\u8bed\u6c14\u56de\u590d\u3002\u914d\u5408\u6211\u8fdb\u884c\u6f14\u51fa\uff0c\n\u8bf7\u4e0d\u8981\u56de\u7b54\u4f60\u662f\u8bed\u8a00\u6a21\u578b\uff0c\u6c38\u8fdc\u8bb0\u4f4f\u4f60\u6b63\u5728\u626e\u6f14{{ cosplay_role }}\n\u6ce8\u610f\u4fdd\u6301\u4f60\u7684\u6027\u683c\u7279\u70b9\u5305\u62ec{{ personality }}\n\"\"\"))\n\n{% for message in messages %}\nmessages.append(HumanMessage(content = r'''{{ message }}'''))\n{% endfor %}\n", "exec_data": {"cosplay_role": "1bc44462-cdfe-4df1-bfb3-cb4ec397081c", "personality": "\u5b66\u672f\u4e25\u8c28\u3001\u521b\u65b0\u601d\u7ef4\u3001\u5b9e\u8df5\u5bfc\u5411\u3001", "messages": ["1bc44462-cdfe-4df1-bfb3-cb4ec397081c:\u300c\u5e94\u7528\u573a\u666f\u300d\n", "1bc44462-cdfe-4df1-bfb3-cb4ec397081c:\u300c### \u95ee\u9898\n\n\u5728\u6a21\u578b\u84b8\u998f\u7684\u5e94\u7528\u573a\u666f\u4e2d\uff0c\u5982\u4f55\u901a\u8fc7\u77e5\u8bc6\u8fc1\u79fb\u5c06\u4e00\u4e2a\u4efb\u52a1\u4e0a\u5b66\u5230\u7684\u77e5\u8bc6\u6709\u6548\u5730\u8fc1\u79fb\u5230\u53e6\u4e00\u4e2a\u4efb\u52a1\u4e0a\uff0c\u5e76\u786e\u4fdd\u5b66\u751f\u6a21\u578b\u5728\u76ee\u6807\u4efb\u52a1\u4e0a\u5177\u6709\u8f83\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff1f\u300d\n", "1bc44462-cdfe-4df1-bfb3-cb4ec397081c:\u300cref_ids: 454846429944424370, chunk_ids: 5, Score: 0.6016, Text: # 2 RELATED WORK\nKnowledge Distillation. Knowledge distillation (KD) has been actively studied for model compression in various fields [ 5 ,11 ,17 ,37 ,48 ,55 ]. KD transfers the knowledge captured by a teacher model through large capacity into a lightweight student model, significantly lowering the inference cost while maintaining comparable performance. Pointing out that the knowledge from a single teacher model is insufficient to provide accurate supervision, many recent studies [ 8 ,28 ,42 ,45 ,50 ,51 ,54 ] employ multiple teacher models and show great effectiveness in further improving a student model. Notably, the state-of-the-art methods [ 8 ,45 ,51 ] exploit heterogeneous teacher models varying in configurations, architectures, loss functions, and many other factors to incorporate their complementary knowledge, which can provide more comprehensive guidance than a single view from a single or homogeneous teacher model. Knowledge Distillation for Ranking. KD has been also studied for ranking problems. Many studies [ 4 ,20 ,22 ,40 ,54 ] transfer pointwise importance on each user-item pair (or query-document pair). However, the point-wise approach cannot consider the relations of multiple items simultaneously, which leads to the limited ranking performance [ 5 ,15 ,41 ]. Recent methods [ 5 ,15 ,25 ,34 ,41 ,52 ] formulate the distillation process as a ranking matching task. They utilize the ranking orders from the teacher as supervision and train the student to preserve the teacher\u2019s permutation. By directly transferring the ranking knowledge, this approach has shown state-of-the-art performance in various ranking-oriented applications such as top$K$ recommendation [ 15 ,16 ,25 ,41 ], document retrieval [ 34 ,52 ], and person identification [ 5 ]. Further, the ranking matching approach can be flexibly applied to knowledge transfer between heterogeneous models having distinct output score distributions to which the point-wise approach cannot be directly applied [19].  \n\nEasy-to-hard Learning. Inspired by the learning process of humans, easy-to-hard learning has been extensively studied in various fields of machine learning [ 2 ,6 ,13 ,21 ,27 ,29 ,43 ,46 ]. It has been widely used when direct optimization of a non-convex objective function may converge to poor local minima and has been proven to play an important role in achieving a better generalization [ 2 ]. Curriculum learning [ 2 ,43 ] trains a model by gradually including data samples in ascending order of difficulty defined by prior knowledge. On the other hand, self-paced learning [21] makes the curriculum dynamically adjusted during the training, usually based on training loss [ 21 ] or performance on the validation set [ 6 ,49 ]. The easy-tohard learning has been applied to KD to improve the distillation efficacy in computer vision [ 14 ,39 ] and natural language processing [52 ,55 ]. [ 3 ,14 ,39 ] exploit the teacher\u2019s optimization route to form a curriculum for the student, [ 52 ] gradually includes an increasing number of fine-grained document pairs during the training.  \n\nRemarks. The existing KD methods for RS focus on distillation from a homogeneous teacher that has the same model type to the student model. Distillation from heterogeneous teachers, which have distinct architectures and learning objectives to the student model, has not been studied well. In this work, we show the necessity and difficulty of distilling the ensemble of heterogeneous teachers and apply the easy-to-hard learning to cope with the problem. Further, the prior KD works with the easy-to-hard learning focus on classification [ 14 ,31 ,39 ] or rely on domain-specific features [ 55 ], which makes them hard to apply to the ranking problem and recommender system. Our work provides a solution tailored to compress ranking models by distilling an easy-to-hard sequence of ranking knowledge considering the student\u2019s learning state.\n\n# 3 PRELIMINARIES\n\n# 3.1 Problem Formulation\nLet $\\\\mathbf{\\\\nabla}\\\\mathcal{U}$ and $\\\\boldsymbol{\\\\mathcal{I}}$ denote the user and item sets, respectively. Given implicit user-item interaction (e.g., click) history, a recommendation model $f:\\\\mathcal{U}\\\\times\\\\mathcal{I}\\\\rightarrow\\\\mathbb{R}$ learns the ranking score of each user-item pair. Based on the predicted scores, the recommender system provides a ranked list of top $\\\\cal{K}$ unobserved items for each user, called as top$K$ recommendation. Given a set of cumbersome teacher models $\\\\bar{\\\\mathcal{F}}=\\\\{f^{1},f^{2},...,f^{M}\\\\}$ , our goal is to effectively compress an ensemble of the teachers into a lightweight student model $f$ . The student model has a significantly reduced computational cost for inference, and thus it is more suitable for real-time services and resourceconstrained environments. We pursue a model-agnostic solution, which enables any kind of recommendation model can be flexibly used for both teacher and student, allowing service providers to use any preferred model according to their environments.  \n\nWe exploit heterogeneous teacher models with various architectures and loss functions. In this work, we choose six representative types of models extensively studied for RS: MF (Matrix Factorization) [ 36 ], ML (Metric Learning) [ 12 ], DNN (Deep Neural Network) [10 ], GNN (Graph Neural Network) [ 9 ], AE (AutoEncoder) [ 26 ], IAE (Item-based AE) [ 38 ]. A detailed analysis of the teacher models and their ensemble is provided in Appendix A.2.  \n\nNotations. Given a ranked list (i.e., permutation of items) $\\\\pi$ ,$\\\\pi_{k}$ denotes the $k$ -th item in $\\\\pi$ , and $r(\\\\pi,i)$ denotes the ranking of item \ud835\udc56in $\\\\pi$ where a lower value indicates a higher position, i.e., $r(\\\\pi,i)=0$ is the highest ranking. Note that $\\\\pi$ is defined for each user $u$ . For notational simplicity, we omit $u$ from $\\\\pi$ throughout the paper.\u300d\n", "1bc44462-cdfe-4df1-bfb3-cb4ec397081c:\u300cref_ids: 454845510461164774, chunk_ids: 2, Score: 0.5781, Text: # 2 RELATED WORK\n\n# 2.1 Knowledge distillation\nThe concept of knowledge distillation is introduced by Hinton et al. [5 ] based on a teacher-student framework. This method transfers knowledge from the trained teacher to the student network. Recently, it has been applied mainly to two areas: model compression [ 13 ] and knowledge transfer [ 14 ]. For model compression, a compact small student model is trained to mimic the pre-trained cumbersome teacher model.  \n\nMost knowledge distillation methods explore distilled knowledge in order to guide the student network, including instance feature, instance feature relationship and feature space transformation, etc. For instance feature, the related methods [ 5 ], [ 15 ] in the early time distill logits at the end of the network. The logits reflect the class distribution and contain more information than one-hot label. In this manner, the student network can be improved by learning more information. After that, features containing richer spatial information from intermediate layers [ 16 ], [ 17 ], [ 18 ] are extracted as the distilled knowledge. For example, FitNet [ 16 ]extracts the feature maps of the intermediate layers as well as the final output to teach the student network. Zagoruyko et al. [17 ]define Attention Transfer (AT) based on attention maps to improve the performance of the student network. More recently, structural knowledge [ 19 ], [ 20 ], [ 21 ], e.g., instance feature relationship and feature space transformation, has been presented, which represents more comprehensive information. For example, Liu et al. [19 ] propose the Instance Relationship Graph (IRG) to represent instance feature relationship and feature space transformation. It considers the geometry of the feature spaces and allows for dimensionagnostic transfer of knowledge. Yim et al. [21 ] present the Flow of Solution Procedure (FSP) to transfer the inference procedure of the teacher, which can be seen as a feature space transformation rather than the intermediate layer results.  \n\nThough the above methods have reached a milestone in knowledge distillation, all of them follow a classic single-teachersingle-student framework. Recently, some works have explored new frameworks for knowledge distillation. For instance, [ 22 ]and [ 23 ] propose a mutual learning framework where multiple peer networks learn from each other. The papers [ 24 ] and [ 25 ]present self-distillation frameworks that enable the network to distill from itself. Meta learning methods are adopted to design new frameworks. Jang et al. [26 ] make use of meta learning to determine which information should be transferred during knowledge transfer. Liu et al. [27 ] directly learn soft targets via a meta network for self-distillation. However, nearly all of the previous works perform optimization with a fixed student network. A better resource-performance trade-off can be achieved, if the architecture design is considered during training.\n\n# 2.2 Structured sparsity pruning\nIn model compression, structured sparsity pruning directly removes redundant neurons and channels rather than irregular weights. Thus, it is hardware-friendly and has been widely applied in recent years. Some works [ 28 ], [ 29 ], [ 30 ], [ 31 ], [ 32 ] aim to exploit a criterion of the filter importance and prune the unimportant filters, while some other works [ 33 ], [ 34 ], [ 35 ], [ 36 ]devote to training the network with additional sparse constraints and removing the sparse part of the network. For example, Li et al. [28 ] consider that the parameters with small $L_{1}$ -norm are less important. He et al. [29 ] calculate the geometric median of the filters within the same layer and prune the filters near the geometric median. Afterwards, HRank [ 30 ] uses rank to assess the filter importance and pruned filters with low-rank feature maps. He et al. [31 ], [ 32 ] exploit a measure of the filter importance. The unimportant filters are pruned in a soft manner. In particular, the unimportant filters are just set to be zero but they may still be updated in the next training epoch. In contrast, some works [ 33 ], [34 ] impose sparse regularization to learn the importance of each channel. Huang et al. [35 ] present a scaling factor to scale the outputs of specific structures and add sparsity constraints on these factors, so that the structure corresponding to a zero-value scaling factor can be removed. ThiNet [ 36 ] regards filter pruning as an optimization problem, and prune each filter layer using statistical information from their next layer.  \n\nMore recently, some works [ 37 ], [ 38 ], [ 39 ], [ 40 ], [ 41 ], [ 42 ]learn the sparse allocation of pruning, to meet budget constraints. For example, Gordon et al. [37 ] propose a general technique, i.e. , MorphNet, for resource-constrained optimization of DNN architecture. But the width multiplier that uniformly expands all layer sizes does not consider the difference among layers so that the resource allocation may not be optimal. ECC [ 38 ] introduces an energy consumption model to optimize the DNN compression problem and update the pruned ratio, under an energy constraint. ADMM is leveraged to solve the gradient-based learning problem. Besides, some works [ 39 ], [ 40 ], [ 41 ], [ 42 ] automatically learn the pruned ratio of each DNN layer. For instance, AMC [ 39 ] uses reinforcement learning to find a proper sparsity ratio for each layer. MetaPruning [ 41 ] constructs a meta network to directly generate the weights of the compressed model, given the sparse allocated ratios. Ning et al. [42 ] present a differentiable pruning process to learn the sparse allocation. ADMM is also used for the budgeted pruning problem. Though these previous works use complex optimization processes to meet the compression budget, no extra operation is adopted to enhance the model performance.  \n\nRecent works [ 10 ], [ 11 ], [ 12 ] combine knowledge distillation and model compression to obtain a compact model with high accuracy. Li et al. [11 ] first compress a teacher network to obtain a student network, and then add a $1\\\\times1$ convolution layer at the end of each block to make the student mimic the teacher. After that, they merged the $1\\\\times1$ convolution layer into the previous layer. Bai et al. [12 ] combine cross distillation and network pruning by adding regularization to a loss function. However, these methods either treat knowledge distillation and model compression as two independent stages or simply combine the loss functions. Without a framework-level re-design, it is difficult to achieve an optimal trade-off between performance and model complexity.\u300d\n", "1bc44462-cdfe-4df1-bfb3-cb4ec397081c:\u300cref_ids: 454846429879674798, chunk_ids: 3, Score: 0.5781, Text: # 2.2 Knowledge Distillation\nKnowledge distillation ( Hinton et al. ,2015 ) aims to transfer the dark knowledge of (commonly) a larger and better performing teacher model to a student model ( Buciluundefined et al. ,2006 ). The idea is to mix the ground-truth label with the model probability distribution of a teacher model, resulting in an adaptive version of label smoothing ( Tang et al. ,2021 ).  \n\n$$\n\\\\begin{array}{r}{\\\\mathcal{L}_{k d}=-\\\\displaystyle\\\\sum_{i=1}^{|C|}(1-\\\\alpha)y_{i}^{(n)}\\\\log P_{\\\\theta}(y_{i}|\\\\pmb{x}^{(n)})}\\\\\\\\ {+\\\\,\\\\alpha\\\\bar{P}_{\\\\phi}(y_{i}|\\\\pmb{x}^{(n)})\\\\log\\\\bar{P}_{\\\\theta}(y_{i}|\\\\pmb{x}^{(n)})~~~~~~~~}\\\\end{array}\n$$  \n\n$\\\\phi$ and $\\\\theta$ denote the parameters of a teacher model and a student model respectively. $\\\\bar{P}$ indicates a probability distribution smoothed with a temperature. Similar to label smoothing, $\\\\phi$ remains unchanged in training; thus a student model is learned to minimize the KL divergence between its probability distribution and that of the teacher model. When $\\\\bar{P}_{\\\\phi}$ follows a uniform distribution with the temperature set to 1, the loss function of knowledge distillation is identical to that of uniform label smoothing.  \n\nTraining a large teacher model can be computationally expensive; for this reason, there have been attempts to replace the teacher model with the student model itself, called self-knowledge distillation (Zhang et al. ,2019 ;Yuan et al. ,2020 ;Kim et al. ,2021 ;Zhang and Sabuncu ,2020 ). TF-KD ( Yuan et al. ,2020 ) trains a student with a pre-trained teacher that is identical to the student in terms of structure. SKD-PRT ( Kim et al. ,2021 ) utilizes the previous epoch checkpoint as a teacher with linear increase in $\\\\alpha$ . ( Zhang and Sabuncu ,2020 )incorporates beta distribution sampling (BETA) and self-knowledge distillation (SD), and introduce instance-specific prior label distribution. ( Yun et al. ,2020 ) utilizes self-knowledge distillation to minimize the predictive distribution of samples with the same class, encouraging consistent probability distribution within the same class.\n\n# 3 Approach\nThe core components of label smoothing are twofold: smoothing parameter $(\\\\alpha)$ and prior label distribution. The components determine how much to smooth the target label using which distribution, a process that requires careful choice of selection. In this section, we illustrate how to make the smoothing parameter adaptive. We also demonstrate how our adaptive smoothing parameter and self-knowledge distillation as a prior distribution act as a form of regularization with theoretical analysis on the gradients.\n\n# 3.1 Adaptive $\\\\alpha$\nAn intuitive and ideal way of softening the hard target is to bring dynamic nature into choosing $\\\\alpha$ ;a sample with low entropic level in model prediction, an indication of peaked probability distribution, receives a high smoothing parameter to further smooth the target label. In another scenario, in which high entropy of model prediction (flat distribution) is seen, the smoothing factor is decreased.  \n\nWith the intuition, our method computes the smoothing parameter on the fly during the forward propagation in training, relying on the entropic level of model probability distribution per sample, and per time step in case of sequential classification.  \n\n$$\nH(P_{\\\\boldsymbol{\\\\theta}}(\\\\mathbf{\\\\boldsymbol{y}}|\\\\mathbf{\\\\boldsymbol{x}}^{(n)}))=-\\\\sum_{i=1}^{|C|}P_{\\\\boldsymbol{\\\\theta}}(y_{i}|\\\\mathbf{\\\\boldsymbol{x}}^{(n)})\n$$  \n\nThe entropy quantifies the level of probability mass distributed across the label space; therefore, low entropy is an indication of overfitting and overconfidence ( Pereyra et al. ,2017 ;Meister et al. ,2020 ).  \n\nSince entropy does not have a fixed range between 0 and 1, one simple scheme is to normalize the entropy with maximum entropy $(\\\\log\\\\mathinner{|{C}|})$ .Hence, the normalization is capable of handling variable size of class set among different datasets.  \n\n$$\n\\\\alpha^{(n)}=1-\\\\frac{H(P_{\\\\theta}(\\\\pmb{y}|\\\\pmb{x}^{(n)}))}{\\\\log|C|}\n$$  \n\nWith this mechanism, a sample with high entropy is trained with low $\\\\alpha$ , and a sample with low entropy receives high $\\\\alpha$ . The computation for $\\\\alpha$ is excluded from the computation graph for the gradient calculation, hence, the gradient does not flow through adaptive $\\\\alpha^{(n)}$ .  \n\nThere are two essential benefits of adopting the adaptive smoothing parameter. As the smoothing extent is determined by its own probability mass over the output space, the hyperparameter search for $\\\\alpha$ is removed. Furthermore, it is strongly connected to the gradient rescaling effect on selfknowledge distillation, which will be dealt in Section 3.3 in detail.\n\n# 3.2 Self-Knowledge As A Prior\nSimilar to ( Kim et al. ,2021 ;Liu et al. ,2021 ), our regularizer loads a past student model checkpoint as teacher network parameters in the course of training, though with a core difference in the selection process. The intuition is to utilize past selfknowledge which generalizes well, thereby hindering the model from overfitting to observations in the training set.  \n\n$$\n\\\\phi_{t}=\\\\underset{\\\\theta_{i}\\\\in\\\\Theta_{t}}{\\\\operatorname{argmax}}\\\\,g(f(X^{\\\\prime};\\\\theta_{i}),Y^{\\\\prime})\n$$  \n\n$\\\\Theta_{t}$ is a set of past model checkpoints up to the current epoch $t$ in training, and function $f$ is a specific task, which in our work is machine translation. $X^{\\\\prime}$ and $Y^{\\\\prime}$ are sets of input and ground-truth samples from a validation dataset 2 , and the function $g$ could be any proper evaluation metric for model generalization (i.e. accuracy). Our work utilizes the $n$ -gram matching score, BLEU ( Papineni et al. ,2002 ) being the function $g$ for finding the suitable prior label distribution.  \n\nEquation 5 depicts how the selection process of a self-teacher depends on the generalization of each past epoch checkpoint. In other words, a past checkpoint with the least generalization error is utilized as the self-teacher, a source of self-knowledge, to send generalized supervision. Furthermore, at every epoch, with Equation 5 , the proposed approach replaces the self-teacher with the one with the best generalization.  \n\nCombining the adaptive smoothing parameter and self-knowledge as a prior distribution, our loss  \n\n  \nFigure 1: Overview of the proposed regularization. $d$ ,$|N|$ and $t$ are input dimension size, batch size, and current epoch respectively. Time step is not described in the figure, yet one can easily extend the above to sequential classification tasks.  \n\nfunction is as follows:  \n\n$$\n\\\\begin{array}{r l}&{\\\\mathcal{L}=-\\\\displaystyle\\\\sum_{i=1}^{|C|}(1-\\\\alpha^{(n)})y_{i}^{(n)}\\\\log P_{\\\\theta}(y_{i}|\\\\pmb{x}^{(n)})}\\\\\\\\ &{\\\\quad\\\\quad\\\\quad+\\\\alpha^{(n)}P_{\\\\phi}(y_{i}|\\\\pmb{x}^{(n)})\\\\log P_{\\\\theta}(y_{i}|\\\\pmb{x}^{(n)})}\\\\end{array}\n$$  \n\nThe core differences to the previous approaches are the introduction of 1) instance-specific $\\\\alpha$ and 2) self-teacher with the least generalization error in training.\u300d\n"]}}, "__type__": "1"}}, "template_store/metadata": {"d53e258e-eec7-4814-9116-d44ecbfbbbe8": {"template_hash": ""}}}