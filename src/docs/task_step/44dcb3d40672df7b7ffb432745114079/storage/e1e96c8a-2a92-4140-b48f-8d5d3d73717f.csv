角色,内容,分镜
e1e96c8a-2a92-4140-b48f-8d5d3d73717f,优势与局限性,4
e1e96c8a-2a92-4140-b48f-8d5d3d73717f,"### 问题

在模型蒸馏的过程中，尽管学生模型具有计算效率高和性能接近教师模型的优势，但仍然存在性能差距和训练复杂性的局限性。请结合模型蒸馏的基本概念和蒸馏过程，分析以下问题：

1. **为什么学生模型的性能可能仍略低于教师模型？**  
2. **在训练过程中，如何通过调整损失函数或引入其他技术来缩小学生模型与教师模型之间的性能差距？**  
3. **针对训练复杂性的局限性，有哪些策略可以简化或优化教师模型的训练过程，从而降低整体蒸馏的复杂性？**

通过这些问题，可以更深入地理解模型蒸馏的优势与局限性，并探讨如何在实际应用中克服这些挑战。",4
e1e96c8a-2a92-4140-b48f-8d5d3d73717f,"ref_ids: 454847819752024066, chunk_ids: 2, Score: 0.5117, Text: # 4 Evolving Teacher to Student Models via Pruning
As the previous section presents, the Knowledge Distillation approach to model compression has two main drawbacks in the private world:  

•Drop in accuracy: There is a considerable drop in the accuracy between the teacher and the student models.   
•Good initialization of students is crucial: The best performance is obtained by students who already have a good initialization; in our experiments, pre-trained DistilBERT mostly achieved the best student performance.  

Finding a good initialization can be challenging in practice. Often, the student architectures are chosen to suit the hardware and latency requirements of the application for which the model is being deployed, using neural architecture search [ 18 ]. Hence, finding a good initialization for every student architecture via pre-training can be expensive and in most cases impossible. Our zero-shot initialization strategies alleviate this problem to a certain degree, yet fall short of closing the gap between the teacher and the student performances. Moreover, DPKD requires that (i) the teacher is trained with DPSGD and (ii) the student is distilled via DPSGD. This two-step approach creates additional overheads in terms of training. Given these limitations, it is natural to ask: Can we evolve the teacher to a student model while fine-tuning with DPSGD? In this section, we explore an answer to this via structured and unstructured pruning with privacy, which allows us to obtain student models that are as good as the teacher models.

# 4.1 Model Compression via Pruning
Pruning algorithms are a broad class of model compression techniques where one drops the parameters from a model during or after the training process. Many works have shown that eliminating unnecessary parameters of neural networks via pruning can lead to sparser and compressed models that have shorter inference times without loss in performance [ 30 ,25 ,24 ]. For example, in magnitude pruning , one of the most widely used pruning techniques, we prune a fraction of parameters with the lowest magnitude. However, there are several pruning strategies, and we refer the readers to [ 33 ] for more details and references.  

Pruning can be implemented in both structured and unstructured ways. In structured pruning, all the pruned weights belong to a single building block of the model. For example, a 6-layer $\\frac{1}{2}$ -BERT can be obtained by pruning 6 layers from the full BERT model, which consists of 12 transformer blocks. On the other hand, in unstructured pruning, pruned weights may be spread across all the layers of the network. In unstructured pruning, it is possible to obtain a $50\\%$ sparse student model while still having all the 12 layers of BERT. Depending on the hardware architectures, inference latency between models with structured and unstructured sparsity could be quite different. However, in this section, we use sparsity as the main measure of model compression , which is also well accepted in the community [ 33 ,27 ].

# 4.2 Iterative Magnitude Pruning (IMP)
Private pruning techniques we study in this section are based on the Iterative Magnitude Pruning (IMP) method, which is a specific pruning technique proposed in a recent work on Lottery Ticket Hypothesis [ 19 ]. The idea behind IMP is rather simple: As we train a deep learning model, after every $N$ iterations we prune an $\\alpha\\%$ of the weights with the lowest magnitude .  

We repeat this process until we achieve the desired sparsity. Here, both $N$ and $\\alpha$ are hyperparameters that need to be tuned. For example, to achieve $50\\%$ sparsity, one can perform $5N$ iterations where after every $N$ iterations additional $10\\%$ of the weights with the least magnitudes are dropped. As specified IMP produces unstructured sparsity. However, we consider a simple modification of the IMP algorithm to produce structured sparsity as well.

# Algorithm 2 Structured DPIMP
Input: Teacher model $\\tau$ , numb r of la ers to prune $L$ , hype rams $\\alpha$ ,$N$ and $M$   
Output e student model Swith Llayers pruned from T$\\tau$   
1: 2: Set for ${\\mathcal{S}}:={\\mathcal{T}}$ $j=1$ to T$L$ do   
3: Fin $\\boldsymbol{S}$ for $N$ itera s with DPSGD   
4: Set $W_{\\mathrm{min}}$ consisting of $\\alpha\\%$ of the remaining model weights with the least magnitude   
5: Set $W_{i}$ as the weights of layer $i$   
6: Drop the layer $i^{*}$ from $\\boldsymbol{S}$ satisfying $i^{*}:=\\arg\\operatorname*{max}_{i}\\left\\{W_{i}\\cap W_{\\operatorname*{min}}\\right\\}$   
7: end for   
8: Fine-tu $\\boldsymbol{S}$ for $M$ more iterations with DPSGD   
9: return S

# 4.3 Structured DPIMP
We first attempt to obtain a student model from the teacher model via a structured IMP technique, using the following modification: During fine-tuning the teacher model with DPSGD, we progressively drop an appropriately chosen transformer block from the teacher model at the end of every $N$ iterations. We repeat this process until we obtain the student model with the required sparsity. The layer to drop is chosen using the following heuristic: Let $\\alpha>0$ be a hyperparameter. At the end of $N$ iterations, fix bottom (by magnitude) $\\alpha\\%$ of all model weights, and denote it by $W_{\\mathrm{min}}$ . For the $i^{t h}$ transformer block, let $W_{i}$ denote the set of model weights belonging to that block. Among all the transformers blocks we find the block $i^{*}$ that has the highest number of weights from the set $W_{\\mathrm{min}}$ ; Formally, $i^{*}:=\\arg\\operatorname*{max}_{i}$ $\\{W_{i}\\cap W_{\\operatorname*{min}}\\}$ , and we prune the transformer layer $i^{*}$ . We present this in Algorithm 2 .  

Empirical Evaluation We evaluate our structured pruning algorithm with the same setup described in Section 3.3 , and provide the hyperparameters in Appendix A.2 . We split the privacy budget equally among all the iterations of the algorithm. Our goal is to produce a student model which has $\\frac{1}{2}$ as many layers as the full BERT model. Table 4 shows the results for this setting where we compare structured DPIMP to private fine-tuning of the pre-trained DistilBERT and the full BERT model. The main takeaway from this experiment is:  

Table 4: Comparing performance of 6-layer $\\frac{1}{2}$ -BERT student model produced by structured DPIMP with 12-layer BERT teacher model and pre-trained DistillBERT. All our models have the same privacy budget bounded by $\\epsilon<4.25\\$ .  


<html><body><table><tr><td>Model</td><td>MNLI QQP</td><td>QNLI</td><td>SST-2</td><td>Avg</td></tr><tr><td>BERT</td><td>77.8</td><td>84.7 87.8</td><td>90.5</td><td>85.2</td></tr><tr><td>DistilBERT</td><td>73.0</td><td>84.3 82.8</td><td>87.7</td><td>81.9</td></tr><tr><td>-BERT</td><td>72.9</td><td>83.1 82.5</td><td>85.7</td><td>81.0</td></tr></table></body></html>  

•DP structured pruning algorithm produces a student model that has performance comparable to that of DistilBERT. Further, it avoids the pre-training cost associated with DistilBERT.",4
e1e96c8a-2a92-4140-b48f-8d5d3d73717f,"ref_ids: 454846524788653356, chunk_ids: 3, Score: 0.4434, Text: # 4 Experimental Results

# 4.1 Implementation
We use the CLIP model, ViT-g-14 (ViT-G), provided by OpenCLIP [25] as the teacher model. The student model is built upon ViT-S and its state-of-the-art (SOTA) variants, DAT-T [48] and Swin-T [34], where we replace the classification head with a feature projection head. For training with student models in stage 1, we use AdamW optimizer with a base learning rate of $10^{-4}$ and weight decay of 0.05. We use a cosine learning rate scheduler which decays the learning rate to $5\\times10^{-6}$ over 120 epochs. In stage 2, we reduce the base learning rate to $10^{-6}$ .For the threshold $\\tau_{c}$ , we empirically set it at 0 .25 considering the training data utilization and data noise. For the CLIP text encoder, we use the text prompt “a photo of a {scene category}."" or “a satellite image of a {scene category}."" depending on the dataset as suggested by [39]. For quantized models, we report static quantization ts. For the triplet loss $\\mathcal{L}_{c}$ , we use a margin $m=0.3$ and a negative set size J= 3 .",4
e1e96c8a-2a92-4140-b48f-8d5d3d73717f,"ref_ids: 455037545741550556, chunk_ids: 6, Score: 0.4316, Text: # 4 Methods
To overcome the aforementioned limitations, we introduce our L2T framework, Learning Good Teacher Matters (LGTM) to enable more effective knowledge distillation. We first introduce distillation influence , which estimates how much will the student’s performance on validation data change if we put one training sample in the knowledge distillation process.  

Afterwards, we introduce an efficient training method based on finite difference approximation for incorporating distillation influence into the teacher’s update. Finally, we interpret current L2T methods from the perspective of influence function.  

Distillation influence Influence function ( Pruthi et al. ,2020 ;Koh and Liang ,2017 ) is a way of measuring the influence of training samples on the model’s predictions. It can be utilized to identify instances that have a disproportionate effect on the model’s behavior, whether due to their status as outliers or due to incorrect labeling ( Jia et al. ,2019 ;Ghorbani and Zou ,2019 ;Hara et al. ,2019 ). By calculating the influence function for a particular example, it is possible to estimate the extent to which the model’s prediction would be altered as a result of operations on that sample.  

In vanilla distillation, for the student model, we derive the distillation influence of $\\boldsymbol{z}_{i}^{r}$ as the gradient similarity between the training sample $z_{i}^{r}$ and the validation batch $z^{e}$ :  

$$
\\begin{array}{r l}&{\\mathcal{Z}_{\\mathrm{distill}}(z_{i}^{r},z^{e})=\\!\\nabla_{\\theta_{s}}\\mathcal{L}_{\\mathrm{ce}}(T(\\pmb{x}_{i}^{r};\\theta_{t}^{m}),S(\\pmb{x}_{i}^{r};\\theta_{s}^{m}))^{\\intercal}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\nabla_{\\theta_{s}}\\mathcal{L}_{\\mathrm{ce}}(\\pmb{y}^{e},S(\\pmb{x}^{e};\\theta_{s}^{m+1}))}\\end{array}
$$  

The detailed derivation can be found in appendix A .The influence reflects how well the knowledge gained from a particular sample generalizes. It follows that the teacher should focus on teaching the student to capture training samples that have the highest distillation influences.  

In order to incorporate the per-sample influence into knowledge distillation, we adjust the loss weight of each sample based on its distillation influence. This allows us to determine the relative importance of each sample, and helps to control how much each sample contributes to the teacher’s learning process. Samples that are deemed to be more beneficial for the student’s generalization are assigned higher weights. Then we propose training the teacher using the following objective:  

$$
\\mathcal{L}_{\\mathrm{influence}}=\\frac{1}{B^{r}}\\sum_{i=1}^{B^{r}}w_{i}\\mathcal{L}_{\\mathrm{ce}}((T(\\pmb{x}_{i}^{r};\\theta_{t}^{m}),S(\\pmb{x}_{i}^{r};\\theta_{s}^{m})),
$$  

where $w_{i}\\,=\\,\\mathcal{T}_{\\mathrm{distill}}(z_{i}^{r},z^{e})$ . By including the influence in the knowledge distillation loss function, we can tailor the training process to better suit the characteristics of the target task.  

<html><body><table><tr><td>Algorithm1 LGTM</td></tr><tr><td>Require: student Os, teacher Ot, training set Dtrain, validation set Dval Require: Ms, Nt: learning rate for the student and the teacher Require: e:a small scalar Require:M: the maximum number of the training steps 1:while step<Mdo 2: Sample a batch of training set zr = (α"", y""） ～ Dtrain 3: Copystudentparameter0stostudent0' 4: Update 0': 0s < 0s - nsV Ls(0', 0t, 2"") 5: Sample a batch of validation set z = (°, y°) ～ Dval 6: Calculate θ: 0 = 0s ± ∈Lce(y, S(∞;θs)) 7: Calculate the Distillation Infuence with z', 0t, 0 and e: Linfluence (01) ·b< 8: Update θt:θt←0t-ntVotLt(0t,0s,) >eq.(11) 9: Update original 0s: 0s < 0s - ns Vos Cs(0s, 0t, 2r) 10: step ← step + 1 11: end while</td></tr></table></body></html>  

Finite difference approximation For standard neural network training, we often compute a consolidated gradient for a mini-batch of $B^{r}$ training samples to enhance computational efficiency. However, in the context of determining the distillation influence for each sample, the computation will slow down the training by a factor of of per-sample gradient $\\mathcal{L}_{\\mathrm{ce}}(T(\\pmb{x}_{i}^{r};\\theta_{t}^{m}),S(\\pmb{x}_{i}^{r};\\theta_{s}^{m}))$ $B^{r}$ .In addition, a naive implementation is memory intensive, because it requires to keep a copy of $\\nabla_{\\theta_{s}}\\mathcal{L}_{\\mathrm{ce}}(\\pmb{y}^{e},S(\\pmb{x}^{e};\\theta_{s}^{m+1}))$ .  

To address this, we propose an efficient method for updating the teacher with the distillation influence by utilizing finite difference ( Gleich ,2005 ), a technique commonly used in numerical analysis for approximating the derivative of a function at a given point. Similar to ( Pham et al. ,2021 ;Liu et al. ,2018 ), we approximate Linfluence by  

$$
\\begin{array}{r l r}{\\lefteqn{\\mathcal{L}_{\\mathrm{influence}}\\approx\\hat{\\mathcal{L}}_{\\mathrm{influence}}=\\frac{1}{B^{r}}\\sum_{i=1}^{B^{r}}\\Big[\\frac{\\mathcal{L}_{\\mathrm{ce}}(T(x_{i};\\theta_{t}^{m}),S(x_{i};\\theta_{s}^{+}))}{2\\epsilon}}\\\\ &{}&{{\\displaystyle-\\:\\frac{\\mathcal{L}_{\\mathrm{ce}}(T(x_{i};\\theta_{t}^{m}),S(x_{i};\\theta_{s}^{-}))}{2\\epsilon}\\Big],\\;\\;}\\end{array}
$$  

where $\\theta_{s}^{\\pm}=\\theta_{s}\\pm\\epsilon\\mathcal{L}_{\\mathrm{ce}}(\\pmb{y}^{e},S(\\pmb{x}^{e};\\theta_{s}^{m+1}))$ ±Land $\\epsilon$ is a small scalar. Our proposed method for evaluating the finite difference is computationally efficient, as it only requires two forward passes for $\\theta_{s}$ and one backward pass for $\\theta_{t}$ for a single batch, as opposed to a naive implementation which requires $B^{r}$ forward and backward passes for $\\theta_{s}$ and one backward pass for $\\theta_{t}$ . We provide more details of the derivation in appendix B.  

Teacher’s auxiliary loss Inspired by ( Pham et al. ,2021 ), in order to balance the trade-off between self-evolution and transferability of the teacher model, we incorporate the loss with respect to the ground truth as ${\\mathcal{L}}_{\\mathrm{aux}}$ into the final objective:  

  
Figure 2: Performance comparison between Meta Distill ( Zhou et al. ,2022 ) and LGTM on the MNLI validation set. We observe that for LGTM, student model does not suffer from overfitting (thanks to distillation influence), and the teacher can balance its own evolution and effective knowledge transfer (thanks to auxiliary loss).  

$$
\\begin{array}{r l}{\\mathcal{L}_{\\mathfrak{t}}(\\theta_{t}\\ |\\ \\theta_{s},z^{r})=}&{\\hat{\\mathcal{L}}_{\\mathrm{influence}}+\\mathcal{L}_{\\mathrm{aux}},}\\\\ {\\mathcal{L}_{\\mathrm{aux}}=}&{\\alpha\\mathcal{L}_{\\mathrm{ce}}(y^{r},T(x^{r};\\theta_{t}))+}\\\\ &{(1-\\alpha)\\mathcal{L}_{\\mathrm{ce}}(T(x^{r};\\theta_{t}),S(x^{r};\\theta_{s})).}\\end{array}
$$  

where $\\alpha$ is the loss ratio.  

Overall, our method allows the teacher to adapt to the student’s abilities and provide more personalized guidance while improving the student’s generalization capability. We present the algorithm of LGTM in algorithm 1 .  

Relationship with other L2T methods Here we interpret current learning to teach methods from the perspective of influence function.  

In the case of online distillation, it is assumed that all training samples possess an equivalent distillation influence and that the teacher model is responsible for reducing the transfer difficulty of all training samples.  

In contrast, the key differentiating factor between meta distillation and online distillation is the utilization of a dynamic loss weight. We interpret this weight as a measure of the distillation influence of the current training batch $z^{r}$ on the generalization ability of the student model. Specifi- cally, it reflects the similarity between the gradients of the training and validation batches, indicating the effect of the current training batch $z^{r}$ on the validation batch $z^{e}$ (as detailed in appendix C). However, it should be noted that this weight functions primarily as an adaptive learning rate, adjusting the gradient step proportionally to the degree of similarity in gradients. We illustrate the general workflow of vanilla distillation, online distillation, meta distillation and LGTM in fig. 1 .",4
