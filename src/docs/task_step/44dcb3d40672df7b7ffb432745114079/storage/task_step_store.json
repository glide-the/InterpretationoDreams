{"task_step_store/data": {"fa33ea01-c2fd-4f5d-8d46-184313ddea1c": {"__data__": {"id_": "fa33ea01-c2fd-4f5d-8d46-184313ddea1c", "metadata": {}, "relationships": {}, "hash": "", "start_task_context": "\u6a21\u578b\u84b8\u998f\u7684\u662f\u4ec0\u4e48\uff1f", "aemo_representation_context": "\u6a21\u578b\u84b8\u998f\uff08Model Distillation\uff09\u662f\u4e00\u79cd\u5728\u673a\u5668\u5b66\u4e60\u9886\u57df\u5e7f\u6cdb\u4f7f\u7528\u7684\u6280\u672f\uff0c\u4e3b\u8981\u7528\u4e8e\u5c06\u4e00\u4e2a\u5927\u578b\u3001\u590d\u6742\u4e14\u9ad8\u6027\u80fd\u7684\u6a21\u578b\uff08\u901a\u5e38\u79f0\u4e3a\u6559\u5e08\u6a21\u578b\uff09\u7684\u77e5\u8bc6\u8fc1\u79fb\u5230\u4e00\u4e2a\u66f4\u5c0f\u3001\u66f4\u7b80\u5355\u4e14\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u7684\u6a21\u578b\uff08\u901a\u5e38\u79f0\u4e3a\u5b66\u751f\u6a21\u578b\uff09\u4e2d\u3002\u4ee5\u4e0b\u662f\u6a21\u578b\u84b8\u998f\u7684\u8be6\u7ec6\u5206\u89e3\uff1a\n\n### 1. **\u57fa\u672c\u6982\u5ff5**\n- **\u6559\u5e08\u6a21\u578b\uff08Teacher Model\uff09**\uff1a\u4e00\u4e2a\u8bad\u7ec3\u597d\u7684\u3001\u6027\u80fd\u4f18\u5f02\u4f46\u53ef\u80fd\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u7684\u6a21\u578b\u3002\n- **\u5b66\u751f\u6a21\u578b\uff08Student Model\uff09**\uff1a\u4e00\u4e2a\u66f4\u5c0f\u3001\u66f4\u7b80\u5355\u3001\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u7684\u6a21\u578b\uff0c\u76ee\u6807\u662f\u6a21\u4eff\u6559\u5e08\u6a21\u578b\u7684\u8868\u73b0\u3002\n\n### 2. **\u84b8\u998f\u8fc7\u7a0b**\n- **\u8bad\u7ec3\u6559\u5e08\u6a21\u578b**\uff1a\u9996\u5148\uff0c\u4f7f\u7528\u5927\u91cf\u6570\u636e\u8bad\u7ec3\u4e00\u4e2a\u9ad8\u6027\u80fd\u7684\u6559\u5e08\u6a21\u578b\u3002\n- **\u751f\u6210\u8f6f\u6807\u7b7e**\uff1a\u5229\u7528\u6559\u5e08\u6a21\u578b\u5bf9\u8bad\u7ec3\u6570\u636e\u8fdb\u884c\u9884\u6d4b\uff0c\u751f\u6210\u8f6f\u6807\u7b7e\uff08\u5373\u6982\u7387\u5206\u5e03\uff09\uff0c\u800c\u4e0d\u662f\u7b80\u5355\u7684\u786c\u6807\u7b7e\uff08\u5373\u7c7b\u522b\u6807\u7b7e\uff09\u3002\n- **\u8bad\u7ec3\u5b66\u751f\u6a21\u578b**\uff1a\u4f7f\u7528\u539f\u59cb\u6570\u636e\u548c\u6559\u5e08\u6a21\u578b\u751f\u6210\u7684\u8f6f\u6807\u7b7e\u6765\u8bad\u7ec3\u5b66\u751f\u6a21\u578b\u3002\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u5b66\u751f\u6a21\u578b\u7684\u635f\u5931\u51fd\u6570\u901a\u5e38\u5305\u62ec\u4e24\u90e8\u5206\uff1a\n  - **\u786c\u6807\u7b7e\u635f\u5931**\uff1a\u5b66\u751f\u6a21\u578b\u9884\u6d4b\u4e0e\u771f\u5b9e\u6807\u7b7e\u4e4b\u95f4\u7684\u5dee\u5f02\u3002\n  - **\u8f6f\u6807\u7b7e\u635f\u5931**\uff1a\u5b66\u751f\u6a21\u578b\u9884\u6d4b\u4e0e\u6559\u5e08\u6a21\u578b\u9884\u6d4b\u4e4b\u95f4\u7684\u5dee\u5f02\u3002\n\n### 3. **\u5173\u952e\u6280\u672f\u70b9**\n- **\u6e29\u5ea6\u8c03\u8282**\uff1a\u5728\u751f\u6210\u8f6f\u6807\u7b7e\u65f6\uff0c\u901a\u5e38\u5f15\u5165\u4e00\u4e2a\u6e29\u5ea6\u53c2\u6570\uff08Temperature\uff09\u6765\u5e73\u6ed1\u6982\u7387\u5206\u5e03\uff0c\u4f7f\u5f97\u8f6f\u6807\u7b7e\u5305\u542b\u66f4\u591a\u7684\u4fe1\u606f\u3002\n- **\u635f\u5931\u51fd\u6570\u8bbe\u8ba1**\uff1a\u7ed3\u5408\u786c\u6807\u7b7e\u635f\u5931\u548c\u8f6f\u6807\u7b7e\u635f\u5931\uff0c\u5e73\u8861\u4e24\u8005\u7684\u91cd\u8981\u6027\u3002\n\n### 4. **\u5e94\u7528\u573a\u666f**\n- **\u6a21\u578b\u538b\u7f29**\uff1a\u5c06\u5927\u578b\u6a21\u578b\u538b\u7f29\u6210\u5c0f\u578b\u6a21\u578b\uff0c\u4fbf\u4e8e\u90e8\u7f72\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8bbe\u5907\u4e0a\u3002\n- **\u77e5\u8bc6\u8fc1\u79fb**\uff1a\u5c06\u4e00\u4e2a\u4efb\u52a1\u4e0a\u5b66\u5230\u7684\u77e5\u8bc6\u8fc1\u79fb\u5230\u53e6\u4e00\u4e2a\u4efb\u52a1\u4e0a\u3002\n- **\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b**\uff1a\u901a\u8fc7\u5b66\u4e60\u6559\u5e08\u6a21\u578b\u7684\u8f93\u51fa\uff0c\u5b66\u751f\u6a21\u578b\u53ef\u80fd\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u5177\u6709\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002\n\n### 5. **\u4f18\u52bf\u4e0e\u5c40\u9650\u6027**\n- **\u4f18\u52bf**\uff1a\n  - **\u8ba1\u7b97\u6548\u7387\u9ad8**\uff1a\u5b66\u751f\u6a21\u578b\u66f4\u5c0f\uff0c\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u4f4e\u3002\n  - **\u6027\u80fd\u63a5\u8fd1**\uff1a\u5b66\u751f\u6a21\u578b\u80fd\u63a5\u8fd1\u6559\u5e08\u6a21\u578b\u7684\u6027\u80fd\u3002\n- **\u5c40\u9650\u6027**\uff1a\n  - **\u6027\u80fd\u5dee\u8ddd**\uff1a\u5b66\u751f\u6a21\u578b\u6027\u80fd\u53ef\u80fd\u4ecd\u7565\u4f4e\u4e8e\u6559\u5e08\u6a21\u578b\u3002\n  - **\u8bad\u7ec3\u590d\u6742**\uff1a\u9700\u8981\u5148\u8bad\u7ec3\u4e00\u4e2a\u9ad8\u6027\u80fd\u7684\u6559\u5e08\u6a21\u578b\u3002\n\n### 6. **\u6700\u65b0\u8fdb\u5c55**\n- **\u591a\u4efb\u52a1\u84b8\u998f**\uff1a\u5728\u4e00\u4e2a\u5b66\u751f\u6a21\u578b\u4e0a\u540c\u65f6\u84b8\u998f\u591a\u4e2a\u6559\u5e08\u6a21\u578b\u7684\u77e5\u8bc6\u3002\n- **\u81ea\u84b8\u998f**\uff1a\u6a21\u578b\u81ea\u8eab\u4f5c\u4e3a\u6559\u5e08\u6a21\u578b\u8fdb\u884c\u84b8\u998f\u3002\n- **\u8de8\u6a21\u6001\u84b8\u998f**\uff1a\u5728\u4e0d\u540c\u6a21\u6001\uff08\u5982\u56fe\u50cf\u548c\u6587\u672c\uff09\u4e4b\u95f4\u8fdb\u884c\u77e5\u8bc6\u84b8\u998f\u3002\n\n### 7. **\u672a\u6765\u7814\u7a76\u65b9\u5411**\n- **\u66f4\u9ad8\u6548\u7684\u84b8\u998f\u65b9\u6cd5**\uff1a\u63a2\u7d22\u66f4\u6709\u6548\u7684\u77e5\u8bc6\u8fc1\u79fb\u6280\u672f\u3002\n- **\u6cdb\u5316\u80fd\u529b\u7684\u63d0\u5347**\uff1a\u7814\u7a76\u5982\u4f55\u901a\u8fc7\u84b8\u998f\u63d0\u9ad8\u6a21\u578b\u5728\u672a\u89c1\u6570\u636e\u4e0a\u7684\u8868\u73b0\u3002\n- **\u591a\u6a21\u6001\u84b8\u998f\u7684\u62d3\u5c55**\uff1a\u63a2\u7d22\u66f4\u591a\u8de8\u6a21\u6001\u77e5\u8bc6\u84b8\u998f\u7684\u5e94\u7528\u573a\u666f\u3002\n\n\u901a\u8fc7\u4ee5\u4e0a\u6b65\u9aa4\uff0c\u53ef\u4ee5\u5168\u9762\u7406\u89e3\u6a21\u578b\u84b8\u998f\u7684\u6982\u5ff5\u3001\u8fc7\u7a0b\u3001\u6280\u672f\u7ec6\u8282\u3001\u5e94\u7528\u573a\u666f\u53ca\u5176\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u7814\u7a76\u4e2d\u7684\u91cd\u8981\u6027\u3002\u6a21\u578b\u84b8\u998f\u4e0d\u4ec5\u662f\u4e00\u79cd\u6709\u6548\u7684\u6a21\u578b\u538b\u7f29\u6280\u672f\uff0c\u4e5f\u662f\u77e5\u8bc6\u8fc1\u79fb\u548c\u63d0\u5347\u6a21\u578b\u6027\u80fd\u7684\u91cd\u8981\u624b\u6bb5\u3002", "task_step_name": "\u57fa\u672c\u6982\u5ff5", "task_step_description": "\u6559\u5e08\u6a21\u578b\uff08Teacher Model\uff09\uff1a\u4e00\u4e2a\u8bad\u7ec3\u597d\u7684\u3001\u6027\u80fd\u4f18\u5f02\u4f46\u53ef\u80fd\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u7684\u6a21\u578b\u3002\u5b66\u751f\u6a21\u578b\uff08Student Model\uff09\uff1a\u4e00\u4e2a\u66f4\u5c0f\u3001\u66f4\u7b80\u5355\u3001\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u7684\u6a21\u578b\uff0c\u76ee\u6807\u662f\u6a21\u4eff\u6559\u5e08\u6a21\u578b\u7684\u8868\u73b0\u3002", "task_step_level": "0", "task_step_question": "### \u95ee\u9898\n\n\u5728\u6a21\u578b\u84b8\u998f\u7684\u57fa\u672c\u6982\u5ff5\u4e2d\uff0c\u6559\u5e08\u6a21\u578b\u548c\u5b66\u751f\u6a21\u578b\u7684\u4e3b\u8981\u533a\u522b\u662f\u4ec0\u4e48\uff1f\u8bf7\u7b80\u8981\u63cf\u8ff0\u5b83\u4eec\u5404\u81ea\u7684\u7279\u70b9\u548c\u4f5c\u7528\u3002", "task_step_question_context": [{"ref_id": "454846524788653356", "chunk_id": "3", "score": 0.494140625, "text": "# (3) Students Guiding Each Other\nAs is evident, students in real-world scenarios can also learn from each other. Accordingly, the works in [36] ,[65] apply this approach to KD to improve the performance of student models in OD. Kang et al. [36] proposed an oracle knowledge extraction framework based on a neural structure search to solve the capacity and complexity problems of the integration model. Moreover, teacher-student KD [65] involves not only a teacher model guiding multiple students, but also mutual guidance between student models. For knowledge sharing between students, the final objective function is as follows:  \n\n$$\n\\begin{array}{r l}&{L_{s_{1}}^{k D}=L_{s_{1}}^{k T}+\\sigma\\left(s_{1}\\right)L^{k s}\\left(s_{1}\\|s_{2}\\right)\\!,}\\\\ &{L_{S_{2}}^{K D}=L_{S_{2}}^{K T}+\\beta\\cdot\\sigma\\left(S_{2}\\right)L^{K S}\\left(S_{2}\\|S_{1}\\right)\\!,}\\end{array}\n$$  \n\nwhere $S_{1}$ and $S_{2}$ are a slow and a smart student model respectively. The smarter the student model is, the better it will learn the knowledge.  \n\n5) Self-Feature Distillation: Traditional KD methods usually require a pre-trained teacher model to train a student model. To reduce the dependence of the student model on the teacher model, many scholars have designed so-called teacherfree KD models. The existing teacher-free KD methods include self-distillation, cooperative learning, and label regularization, among others. In addition, there is another type of KD, namely feature distillation, which is dissimilar to logits methods (object distillation). The student model in logits methods only learns the teacher model\u2019s logits as the resulting knowledge, rather than the middle-layer features. Many scholars have applied feature distillation to self-feature distillation, enabling a KD scheme without teachers to be devised.  \n\nSelf-feature distillation does not require teacher networks, which reduces training cost. Moreover, these methods carry out one-to-one feature transformation learning between layers. For example, LGD [57] is a self-distillation model used for OD. The knowledge learned by networks can be transmitted between different layers through top-down distillation [38] ,and attention maps are used to capture more effective feature information and thereby promote the development of OD. An auxiliary classifier is added to the intermediate feature layers to strengthen self-supervision and enable student models to learn more effective feature representations [56] . Progressive self-knowledge distillation (PS-KD) [39] carries out feature learning by gradually extracting the knowledge from the features the model obtains. Notably, since all the lower layers imitate the attention maps of upper layers, the attention information of the lower layers may be lost. Accordingly, Bi-SAD [18] is proposed to solve this problem. Moreover, in YOLOv6 [148] ,a simple self-distillation technique using a pre-trained student model as the teacher model is introduced to minimize the data distribution difference between the predictions of the teacher and student models.  \n\nInordertosolvetheproblemofmodelseasilyfallingintooverfitting due to limited sample availability during few-shot OD, Li et al. [139] utilized the self-feature KD strategy to improve the generalization ability of the student model by designing an attention loss, which includes classification, regression, and class-specific features in a small number of examples. The core concept is that the student model can learn the feature mapping functions to approximate the original model through location and category feature transformation. Moreover, to improve the generalization ability of OD models, Wu et al. realized crossdomain OD based on Single-Domain through self-distillation technology [58] . Cyclic-disentangled self-distillation is proposed to continuously strip the scene information of the objects during model training, as well as to extract the shared feature expression suitable for OD in different domains. Specifically, the fine-grained location and classification information contained in multiple convolution layers of the teacher model is used to guide the backbone network (student model) to learn crossdomain features, thereby improving the generalization ability of OD models and achieving cross-domain OD. Similarly, selfdistillation technology was used by He et al. for cross-domain OD [149] . These authors designed two self-distilling branches to learn the shared proposal features from the source and target domain. In addition, there are also some distillation frameworks that use self-distillation as part of their models, such as the methods in [129] and [118] , which have successfully integrated self-feature distillation patterns into their models to achieve performance improvements.  \n\n6) Specific Information Guidance for OD: General OD models based on KD make full use of a heavyweight teacher model to transfer the complex knowledge learned from a specific dataset to a lightweight student model. In this section, we will introduce several models in which some specific knowledge information extracted by the teacher models is transferred to the student models. Among these models are the mask-guided OD models that focus on the local features of images, along with the textual guided models that introduce textual prior knowledge information for improving the student models\u2019 ability to learn the visual features; there are also some other models that pay attention to the semantic information of different object relationships in images through semantic guidance, etc.\n\n# (1) Mask-Guided KD-Based OD\nThe mask-guided network [32] is designed based on the two-stage OD model structure. Here, the mask information is used to guide the student model to pay attention to the global and local features, and the loss function of KD is given by combining global and local loss. Similarly, the method proposed by Wang et al. [17] uses the ground truth in the initial generation model as a mask to guide student models in learning the features of the objects of interest and the adjacent objects. For fine-feature imitation, a combination of the adaptation layer and the generated mask enables the student model to simulate the teacher model\u2019sattentiontolocalfeaturesandnearbyobjects.Limitation loss combined with testing loss is proposed for training the student model. In current KD-based OD models, student models largely depend on the outputs of teacher models, or transitional trust teacher models. However, in real-world scenarios, teacher models may not be able to provide very reliable output features or prediction results. Therefore, student models should learn the knowledge from teacher models selectively. Several methods have been developed to rank the outputs (including feature maps, proposals or predictions) of the teacher models based on their own quality measurements, and select a few key predictive regions or reliable predictions to guide the student OD models [128] ,[143] ,[150] ."}, {"ref_id": "455037545741550556", "chunk_id": "6", "score": 0.44140625, "text": "# (3) Interactive Distillation Between Multi-Teacher and MultiStudent Models for OD\nDistillation learning is similar to the pattern of human learning. Thus, as in real-world scenarios, a student model can be guided by multiple teacher models for feature learning. At the same time, a task can also be completed by multiple student models and student models can learn from each other. In existing methods, a teacher model can guide multiple student models to conduct feature learning, then select the student model with superiorperformance [65] .Multi-teachermodelscanalsojointly train student models in two stages [37] ; alternatively, multiple teacher models can be combined to train student models by means of weight fusion [53] . These methods make full use of the advantages of multiple teacher models to train a superior student model and achieve good results. However, it is worth conducting further research into how knowledge might be seamlessly transferred from multiple teacher models to student models. In addition, it is important to investigate how the teacher model and student model interact with each other through certain mechanisms or technologies during joint model training, which may also be an effective way to improve the performance of both teacher and student models. Finally, as multiple teacher models can train multiple different types of student models, the question of how to select the optimal student model or combine multiple student models for OD should receive attention in the future.  \n\n(4) New Knowledge and Multiple Modal Features Distillation for OD  \n\nThe introduction of new knowledge is a very effective way to improve OD models\u2019 performance, and the ability to extract new knowledge from other modes is a major advantage of KD. Existing methods of this kind take a variety of multimodal data (RGB images [35] , thermal images [55] , depth images [14] ,[35] ,textual information [19] ,[64] , etc.) as the input of the teacher model, then use the teacher model to extract relevant features from these multimodal data for guiding the student model to learn the visual features from 2D RGB images. However, a question that merits future exploration is that of how to minimize the gaps between the different types of features learned by the teacher model from multimodal data and the visual features for OD. In addition, using the features of other modal data as the prior knowledge of weakly supervised OD could also be consideredwhendesigningweaklysupervisedODmodelsbased on KD.\n\n# (5) Model Compression for 3D OD\n3D OD is mostly used in automatic driving and other fields. The OD models dealing with 3D image data are more complex than those of 2D data. It is therefore of great practical significance to use KD technology to compress the 3D OD models, and there are many innovative works on this topic worth exploring. First, as there are large-scale network parameters in 3D OD models, model compression based on KD is an important research field that needs to be considered. For example, we could use a complex deep neural network as the teacher model to guide a finely designed lightweight student model. However, another key question is that of how to improve the accuracy of OD models. Future research into KD-based 3D OD could focus on the image data and the models. For 3D image data, the models using the point cloud data as input can adopt self-distillation to improve the accuracy of 3D OD. For the models using multiple forms of data as input, the initial detection results can first be obtained from simple 2D images, after which they could be used as weakly supervised tags to optimize the 3D OD models with complex 3D data. In addition, we can use more sophisticated complex models to optimize 3D OD models, such as the KD method that combines the segmentation model and detection model. It would also be beneficial to design a suitable KD framework that is specifically tailored for the unique context of 3D OD.  \n\nFinally, KD has great advantages in model compression and model performance improvement, and has been widely used in multiple computer vision tasks. In recent years, the use of KD technology for OD-related tasks has attracted increasing research attention; at the same time, KD-based OD has also encountered many challenges. In the future, KD should be extended to a wide range of visual detection tasks, such as 3D OD, weakly supervised/unsupervised OD, visual relationship detection, social relationship detection, and so on. In addition, KD-based OD can be further applied to other specific types of data, such as multi-source remote sensing images, multi-modal images, textual data, audio data, etc.\n\n# VI. C ONCLUSION\nThis survey reviews KD-based OD models. First, we detailed the basic principles for designing OD models based on KD. We thensummarizedandanalyzedthepreviousworksintermsofthe KD-based OD tasks, the KD strategies employed in OD models, the related problems to be solved, and the datasets associated with model application. Finally, we discussed the promising possible research directions to be further explored in the future. As shown by the above comprehensive analysis of current KD-based OD models, KD brings great potential to traditional OD models in terms of model compression and performance improvement. Therefore, there are many novel ideas and techniques in this research field that merit further exploration."}, {"ref_id": "454845516158078890", "chunk_id": "13", "score": 0.4296875, "text": "# 2 RELATED WORK\n\n# 2.1 Knowledge distillation\nThe concept of knowledge distillation is introduced by Hinton et al. [5 ] based on a teacher-student framework. This method transfers knowledge from the trained teacher to the student network. Recently, it has been applied mainly to two areas: model compression [ 13 ] and knowledge transfer [ 14 ]. For model compression, a compact small student model is trained to mimic the pre-trained cumbersome teacher model.  \n\nMost knowledge distillation methods explore distilled knowledge in order to guide the student network, including instance feature, instance feature relationship and feature space transformation, etc. For instance feature, the related methods [ 5 ], [ 15 ] in the early time distill logits at the end of the network. The logits reflect the class distribution and contain more information than one-hot label. In this manner, the student network can be improved by learning more information. After that, features containing richer spatial information from intermediate layers [ 16 ], [ 17 ], [ 18 ] are extracted as the distilled knowledge. For example, FitNet [ 16 ]extracts the feature maps of the intermediate layers as well as the final output to teach the student network. Zagoruyko et al. [17 ]define Attention Transfer (AT) based on attention maps to improve the performance of the student network. More recently, structural knowledge [ 19 ], [ 20 ], [ 21 ], e.g., instance feature relationship and feature space transformation, has been presented, which represents more comprehensive information. For example, Liu et al. [19 ] propose the Instance Relationship Graph (IRG) to represent instance feature relationship and feature space transformation. It considers the geometry of the feature spaces and allows for dimensionagnostic transfer of knowledge. Yim et al. [21 ] present the Flow of Solution Procedure (FSP) to transfer the inference procedure of the teacher, which can be seen as a feature space transformation rather than the intermediate layer results.  \n\nThough the above methods have reached a milestone in knowledge distillation, all of them follow a classic single-teachersingle-student framework. Recently, some works have explored new frameworks for knowledge distillation. For instance, [ 22 ]and [ 23 ] propose a mutual learning framework where multiple peer networks learn from each other. The papers [ 24 ] and [ 25 ]present self-distillation frameworks that enable the network to distill from itself. Meta learning methods are adopted to design new frameworks. Jang et al. [26 ] make use of meta learning to determine which information should be transferred during knowledge transfer. Liu et al. [27 ] directly learn soft targets via a meta network for self-distillation. However, nearly all of the previous works perform optimization with a fixed student network. A better resource-performance trade-off can be achieved, if the architecture design is considered during training.\n\n# 2.2 Structured sparsity pruning\nIn model compression, structured sparsity pruning directly removes redundant neurons and channels rather than irregular weights. Thus, it is hardware-friendly and has been widely applied in recent years. Some works [ 28 ], [ 29 ], [ 30 ], [ 31 ], [ 32 ] aim to exploit a criterion of the filter importance and prune the unimportant filters, while some other works [ 33 ], [ 34 ], [ 35 ], [ 36 ]devote to training the network with additional sparse constraints and removing the sparse part of the network. For example, Li et al. [28 ] consider that the parameters with small $L_{1}$ -norm are less important. He et al. [29 ] calculate the geometric median of the filters within the same layer and prune the filters near the geometric median. Afterwards, HRank [ 30 ] uses rank to assess the filter importance and pruned filters with low-rank feature maps. He et al. [31 ], [ 32 ] exploit a measure of the filter importance. The unimportant filters are pruned in a soft manner. In particular, the unimportant filters are just set to be zero but they may still be updated in the next training epoch. In contrast, some works [ 33 ], [34 ] impose sparse regularization to learn the importance of each channel. Huang et al. [35 ] present a scaling factor to scale the outputs of specific structures and add sparsity constraints on these factors, so that the structure corresponding to a zero-value scaling factor can be removed. ThiNet [ 36 ] regards filter pruning as an optimization problem, and prune each filter layer using statistical information from their next layer.  \n\nMore recently, some works [ 37 ], [ 38 ], [ 39 ], [ 40 ], [ 41 ], [ 42 ]learn the sparse allocation of pruning, to meet budget constraints. For example, Gordon et al. [37 ] propose a general technique, i.e. , MorphNet, for resource-constrained optimization of DNN architecture. But the width multiplier that uniformly expands all layer sizes does not consider the difference among layers so that the resource allocation may not be optimal. ECC [ 38 ] introduces an energy consumption model to optimize the DNN compression problem and update the pruned ratio, under an energy constraint. ADMM is leveraged to solve the gradient-based learning problem. Besides, some works [ 39 ], [ 40 ], [ 41 ], [ 42 ] automatically learn the pruned ratio of each DNN layer. For instance, AMC [ 39 ] uses reinforcement learning to find a proper sparsity ratio for each layer. MetaPruning [ 41 ] constructs a meta network to directly generate the weights of the compressed model, given the sparse allocated ratios. Ning et al. [42 ] present a differentiable pruning process to learn the sparse allocation. ADMM is also used for the budgeted pruning problem. Though these previous works use complex optimization processes to meet the compression budget, no extra operation is adopted to enhance the model performance.  \n\nRecent works [ 10 ], [ 11 ], [ 12 ] combine knowledge distillation and model compression to obtain a compact model with high accuracy. Li et al. [11 ] first compress a teacher network to obtain a student network, and then add a $1\\times1$ convolution layer at the end of each block to make the student mimic the teacher. After that, they merged the $1\\times1$ convolution layer into the previous layer. Bai et al. [12 ] combine cross distillation and network pruning by adding regularization to a loss function. However, these methods either treat knowledge distillation and model compression as two independent stages or simply combine the loss functions. Without a framework-level re-design, it is difficult to achieve an optimal trade-off between performance and model complexity."}], "task_step_question_answer": "\u5728\u84b8\u998f\u8fc7\u7a0b\u4e2d\uff0c\u6559\u5e08\u6a21\u578b\u548c\u5b66\u751f\u6a21\u578b\u7684\u4e92\u52a8\u6027\u4f53\u73b0\u5728\u6559\u5e08\u6a21\u578b\u4e0d\u4ec5\u63d0\u4f9b\u8f6f\u6807\u7b7e\uff0c\u8fd8\u901a\u8fc7\u5176\u590d\u6742\u7684\u7ed3\u6784\u548c\u6027\u80fd\u6765\u63d0\u4f9b\u9ad8\u8d28\u91cf\u7684\u77e5\u8bc6\u8f93\u51fa\u3002\u8fd9\u79cd\u53cc\u5411\u6027\u7684\u63cf\u8ff0\u53ef\u4ee5\u66f4\u5168\u9762\u5730\u5c55\u793a\u84b8\u998f\u8fc7\u7a0b\u7684\u4e92\u52a8\u6027\u3002\u6e29\u5ea6\u53c2\u6570\u7684\u5f15\u5165\u5e73\u6ed1\u4e86\u6982\u7387\u5206\u5e03\uff0c\u4f7f\u5f97\u8f6f\u6807\u7b7e\u5305\u542b\u66f4\u591a\u7684\u4fe1\u606f\uff0c\u800c\u635f\u5931\u51fd\u6570\u7684\u8bbe\u8ba1\u5219\u5e73\u8861\u4e86\u786c\u6807\u7b7e\u635f\u5931\u548c\u8f6f\u6807\u7b7e\u635f\u5931\u3002\u8fd9\u4e9b\u6280\u672f\u7ec6\u8282\u7684\u5f15\u5165\u8fdb\u4e00\u6b65\u4f18\u5316\u4e86\u84b8\u998f\u8fc7\u7a0b\uff0c\u4f7f\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u66f4\u52a0\u7a33\u5065\u3002\u84b8\u998f\u8fc7\u7a0b\u7684\u5e94\u7528\u573a\u666f\u4e0d\u4ec5\u9650\u4e8e\u6a21\u578b\u538b\u7f29\uff0c\u8fd8\u5305\u62ec\u77e5\u8bc6\u8fc1\u79fb\u548c\u591a\u4efb\u52a1\u84b8\u998f\uff0c\u5176\u4f18\u52bf\u5728\u4e8e\u8ba1\u7b97\u6548\u7387\u9ad8\u548c\u6027\u80fd\u63a5\u8fd1\u6559\u5e08\u6a21\u578b\u3002\u901a\u8fc7\u77e5\u8bc6\u8fc1\u79fb\u5b9e\u73b0\u6a21\u578b\u6027\u80fd\u7684\u63d0\u5347\u548c\u590d\u6742\u5ea6\u7684\u964d\u4f4e\uff0c\u540c\u65f6\u4fdd\u6301\u8f83\u9ad8\u7684\u8ba1\u7b97\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\u3002", "ref_task_step_id": ""}, "__type__": "task_step_node"}, "73280fb7-b826-4157-9dfd-3771e5ec66d8": {"__data__": {"id_": "73280fb7-b826-4157-9dfd-3771e5ec66d8", "metadata": {}, "relationships": {}, "hash": "", "start_task_context": "\u6a21\u578b\u84b8\u998f\u7684\u662f\u4ec0\u4e48\uff1f", "aemo_representation_context": "\u6a21\u578b\u84b8\u998f\uff08Model Distillation\uff09\u662f\u4e00\u79cd\u5728\u673a\u5668\u5b66\u4e60\u9886\u57df\u5e7f\u6cdb\u4f7f\u7528\u7684\u6280\u672f\uff0c\u4e3b\u8981\u7528\u4e8e\u5c06\u4e00\u4e2a\u5927\u578b\u3001\u590d\u6742\u4e14\u9ad8\u6027\u80fd\u7684\u6a21\u578b\uff08\u901a\u5e38\u79f0\u4e3a\u6559\u5e08\u6a21\u578b\uff09\u7684\u77e5\u8bc6\u8fc1\u79fb\u5230\u4e00\u4e2a\u66f4\u5c0f\u3001\u66f4\u7b80\u5355\u4e14\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u7684\u6a21\u578b\uff08\u901a\u5e38\u79f0\u4e3a\u5b66\u751f\u6a21\u578b\uff09\u4e2d\u3002\u4ee5\u4e0b\u662f\u6a21\u578b\u84b8\u998f\u7684\u8be6\u7ec6\u5206\u89e3\uff1a\n\n### 1. **\u57fa\u672c\u6982\u5ff5**\n- **\u6559\u5e08\u6a21\u578b\uff08Teacher Model\uff09**\uff1a\u4e00\u4e2a\u8bad\u7ec3\u597d\u7684\u3001\u6027\u80fd\u4f18\u5f02\u4f46\u53ef\u80fd\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u7684\u6a21\u578b\u3002\n- **\u5b66\u751f\u6a21\u578b\uff08Student Model\uff09**\uff1a\u4e00\u4e2a\u66f4\u5c0f\u3001\u66f4\u7b80\u5355\u3001\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u7684\u6a21\u578b\uff0c\u76ee\u6807\u662f\u6a21\u4eff\u6559\u5e08\u6a21\u578b\u7684\u8868\u73b0\u3002\n\n### 2. **\u84b8\u998f\u8fc7\u7a0b**\n- **\u8bad\u7ec3\u6559\u5e08\u6a21\u578b**\uff1a\u9996\u5148\uff0c\u4f7f\u7528\u5927\u91cf\u6570\u636e\u8bad\u7ec3\u4e00\u4e2a\u9ad8\u6027\u80fd\u7684\u6559\u5e08\u6a21\u578b\u3002\n- **\u751f\u6210\u8f6f\u6807\u7b7e**\uff1a\u5229\u7528\u6559\u5e08\u6a21\u578b\u5bf9\u8bad\u7ec3\u6570\u636e\u8fdb\u884c\u9884\u6d4b\uff0c\u751f\u6210\u8f6f\u6807\u7b7e\uff08\u5373\u6982\u7387\u5206\u5e03\uff09\uff0c\u800c\u4e0d\u662f\u7b80\u5355\u7684\u786c\u6807\u7b7e\uff08\u5373\u7c7b\u522b\u6807\u7b7e\uff09\u3002\n- **\u8bad\u7ec3\u5b66\u751f\u6a21\u578b**\uff1a\u4f7f\u7528\u539f\u59cb\u6570\u636e\u548c\u6559\u5e08\u6a21\u578b\u751f\u6210\u7684\u8f6f\u6807\u7b7e\u6765\u8bad\u7ec3\u5b66\u751f\u6a21\u578b\u3002\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u5b66\u751f\u6a21\u578b\u7684\u635f\u5931\u51fd\u6570\u901a\u5e38\u5305\u62ec\u4e24\u90e8\u5206\uff1a\n  - **\u786c\u6807\u7b7e\u635f\u5931**\uff1a\u5b66\u751f\u6a21\u578b\u9884\u6d4b\u4e0e\u771f\u5b9e\u6807\u7b7e\u4e4b\u95f4\u7684\u5dee\u5f02\u3002\n  - **\u8f6f\u6807\u7b7e\u635f\u5931**\uff1a\u5b66\u751f\u6a21\u578b\u9884\u6d4b\u4e0e\u6559\u5e08\u6a21\u578b\u9884\u6d4b\u4e4b\u95f4\u7684\u5dee\u5f02\u3002\n\n### 3. **\u5173\u952e\u6280\u672f\u70b9**\n- **\u6e29\u5ea6\u8c03\u8282**\uff1a\u5728\u751f\u6210\u8f6f\u6807\u7b7e\u65f6\uff0c\u901a\u5e38\u5f15\u5165\u4e00\u4e2a\u6e29\u5ea6\u53c2\u6570\uff08Temperature\uff09\u6765\u5e73\u6ed1\u6982\u7387\u5206\u5e03\uff0c\u4f7f\u5f97\u8f6f\u6807\u7b7e\u5305\u542b\u66f4\u591a\u7684\u4fe1\u606f\u3002\n- **\u635f\u5931\u51fd\u6570\u8bbe\u8ba1**\uff1a\u7ed3\u5408\u786c\u6807\u7b7e\u635f\u5931\u548c\u8f6f\u6807\u7b7e\u635f\u5931\uff0c\u5e73\u8861\u4e24\u8005\u7684\u91cd\u8981\u6027\u3002\n\n### 4. **\u5e94\u7528\u573a\u666f**\n- **\u6a21\u578b\u538b\u7f29**\uff1a\u5c06\u5927\u578b\u6a21\u578b\u538b\u7f29\u6210\u5c0f\u578b\u6a21\u578b\uff0c\u4fbf\u4e8e\u90e8\u7f72\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8bbe\u5907\u4e0a\u3002\n- **\u77e5\u8bc6\u8fc1\u79fb**\uff1a\u5c06\u4e00\u4e2a\u4efb\u52a1\u4e0a\u5b66\u5230\u7684\u77e5\u8bc6\u8fc1\u79fb\u5230\u53e6\u4e00\u4e2a\u4efb\u52a1\u4e0a\u3002\n- **\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b**\uff1a\u901a\u8fc7\u5b66\u4e60\u6559\u5e08\u6a21\u578b\u7684\u8f93\u51fa\uff0c\u5b66\u751f\u6a21\u578b\u53ef\u80fd\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u5177\u6709\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002\n\n### 5. **\u4f18\u52bf\u4e0e\u5c40\u9650\u6027**\n- **\u4f18\u52bf**\uff1a\n  - **\u8ba1\u7b97\u6548\u7387\u9ad8**\uff1a\u5b66\u751f\u6a21\u578b\u66f4\u5c0f\uff0c\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u4f4e\u3002\n  - **\u6027\u80fd\u63a5\u8fd1**\uff1a\u5b66\u751f\u6a21\u578b\u80fd\u63a5\u8fd1\u6559\u5e08\u6a21\u578b\u7684\u6027\u80fd\u3002\n- **\u5c40\u9650\u6027**\uff1a\n  - **\u6027\u80fd\u5dee\u8ddd**\uff1a\u5b66\u751f\u6a21\u578b\u6027\u80fd\u53ef\u80fd\u4ecd\u7565\u4f4e\u4e8e\u6559\u5e08\u6a21\u578b\u3002\n  - **\u8bad\u7ec3\u590d\u6742**\uff1a\u9700\u8981\u5148\u8bad\u7ec3\u4e00\u4e2a\u9ad8\u6027\u80fd\u7684\u6559\u5e08\u6a21\u578b\u3002\n\n### 6. **\u6700\u65b0\u8fdb\u5c55**\n- **\u591a\u4efb\u52a1\u84b8\u998f**\uff1a\u5728\u4e00\u4e2a\u5b66\u751f\u6a21\u578b\u4e0a\u540c\u65f6\u84b8\u998f\u591a\u4e2a\u6559\u5e08\u6a21\u578b\u7684\u77e5\u8bc6\u3002\n- **\u81ea\u84b8\u998f**\uff1a\u6a21\u578b\u81ea\u8eab\u4f5c\u4e3a\u6559\u5e08\u6a21\u578b\u8fdb\u884c\u84b8\u998f\u3002\n- **\u8de8\u6a21\u6001\u84b8\u998f**\uff1a\u5728\u4e0d\u540c\u6a21\u6001\uff08\u5982\u56fe\u50cf\u548c\u6587\u672c\uff09\u4e4b\u95f4\u8fdb\u884c\u77e5\u8bc6\u84b8\u998f\u3002\n\n### 7. **\u672a\u6765\u7814\u7a76\u65b9\u5411**\n- **\u66f4\u9ad8\u6548\u7684\u84b8\u998f\u65b9\u6cd5**\uff1a\u63a2\u7d22\u66f4\u6709\u6548\u7684\u77e5\u8bc6\u8fc1\u79fb\u6280\u672f\u3002\n- **\u6cdb\u5316\u80fd\u529b\u7684\u63d0\u5347**\uff1a\u7814\u7a76\u5982\u4f55\u901a\u8fc7\u84b8\u998f\u63d0\u9ad8\u6a21\u578b\u5728\u672a\u89c1\u6570\u636e\u4e0a\u7684\u8868\u73b0\u3002\n- **\u591a\u6a21\u6001\u84b8\u998f\u7684\u62d3\u5c55**\uff1a\u63a2\u7d22\u66f4\u591a\u8de8\u6a21\u6001\u77e5\u8bc6\u84b8\u998f\u7684\u5e94\u7528\u573a\u666f\u3002\n\n\u901a\u8fc7\u4ee5\u4e0a\u6b65\u9aa4\uff0c\u53ef\u4ee5\u5168\u9762\u7406\u89e3\u6a21\u578b\u84b8\u998f\u7684\u6982\u5ff5\u3001\u8fc7\u7a0b\u3001\u6280\u672f\u7ec6\u8282\u3001\u5e94\u7528\u573a\u666f\u53ca\u5176\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u7814\u7a76\u4e2d\u7684\u91cd\u8981\u6027\u3002\u6a21\u578b\u84b8\u998f\u4e0d\u4ec5\u662f\u4e00\u79cd\u6709\u6548\u7684\u6a21\u578b\u538b\u7f29\u6280\u672f\uff0c\u4e5f\u662f\u77e5\u8bc6\u8fc1\u79fb\u548c\u63d0\u5347\u6a21\u578b\u6027\u80fd\u7684\u91cd\u8981\u624b\u6bb5\u3002", "task_step_name": "\u84b8\u998f\u8fc7\u7a0b", "task_step_description": "\u8bad\u7ec3\u6559\u5e08\u6a21\u578b\uff1a\u9996\u5148\uff0c\u4f7f\u7528\u5927\u91cf\u6570\u636e\u8bad\u7ec3\u4e00\u4e2a\u9ad8\u6027\u80fd\u7684\u6559\u5e08\u6a21\u578b\u3002\u751f\u6210\u8f6f\u6807\u7b7e\uff1a\u5229\u7528\u6559\u5e08\u6a21\u578b\u5bf9\u8bad\u7ec3\u6570\u636e\u8fdb\u884c\u9884\u6d4b\uff0c\u751f\u6210\u8f6f\u6807\u7b7e\uff08\u5373\u6982\u7387\u5206\u5e03\uff09\uff0c\u800c\u4e0d\u662f\u7b80\u5355\u7684\u786c\u6807\u7b7e\uff08\u5373\u7c7b\u522b\u6807\u7b7e\uff09\u3002\u8bad\u7ec3\u5b66\u751f\u6a21\u578b\uff1a\u4f7f\u7528\u539f\u59cb\u6570\u636e\u548c\u6559\u5e08\u6a21\u578b\u751f\u6210\u7684\u8f6f\u6807\u7b7e\u6765\u8bad\u7ec3\u5b66\u751f\u6a21\u578b\u3002\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u5b66\u751f\u6a21\u578b\u7684\u635f\u5931\u51fd\u6570\u901a\u5e38\u5305\u62ec\u4e24\u90e8\u5206\uff1a\u786c\u6807\u7b7e\u635f\u5931\uff1a\u5b66\u751f\u6a21\u578b\u9884\u6d4b\u4e0e\u771f\u5b9e\u6807\u7b7e\u4e4b\u95f4\u7684\u5dee\u5f02\u3002\u8f6f\u6807\u7b7e\u635f\u5931\uff1a\u5b66\u751f\u6a21\u578b\u9884\u6d4b\u4e0e\u6559\u5e08\u6a21\u578b\u9884\u6d4b\u4e4b\u95f4\u7684\u5dee\u5f02\u3002", "task_step_level": "1", "task_step_question": "### \u95ee\u9898\uff1a\n\u5728\u6a21\u578b\u84b8\u998f\u7684\u201c\u84b8\u998f\u8fc7\u7a0b\u201d\u4e2d\uff0c\u4e3a\u4ec0\u4e48\u5728\u8bad\u7ec3\u5b66\u751f\u6a21\u578b\u65f6\uff0c\u9664\u4e86\u4f7f\u7528\u786c\u6807\u7b7e\u635f\u5931\uff08\u5b66\u751f\u6a21\u578b\u9884\u6d4b\u4e0e\u771f\u5b9e\u6807\u7b7e\u4e4b\u95f4\u7684\u5dee\u5f02\uff09\u5916\uff0c\u8fd8\u9700\u8981\u5f15\u5165\u8f6f\u6807\u7b7e\u635f\u5931\uff08\u5b66\u751f\u6a21\u578b\u9884\u6d4b\u4e0e\u6559\u5e08\u6a21\u578b\u9884\u6d4b\u4e4b\u95f4\u7684\u5dee\u5f02\uff09\uff1f\u8fd9\u4e24\u90e8\u5206\u635f\u5931\u51fd\u6570\u5404\u81ea\u7684\u4f5c\u7528\u662f\u4ec0\u4e48\uff0c\u5b83\u4eec\u5982\u4f55\u5171\u540c\u5e2e\u52a9\u5b66\u751f\u6a21\u578b\u66f4\u597d\u5730\u6a21\u4eff\u6559\u5e08\u6a21\u578b\u7684\u6027\u80fd\uff1f", "task_step_question_context": [{"ref_id": "454846109840627474", "chunk_id": "5", "score": 0.54296875, "text": "# (2) Soft Label-Based KD for OD\nIn order to improve the performance of student models and reduce their dependence on ground truth labels, related works typically guide student models using soft labels or pseudo-labels output by teacher models [16] ,[19] ,[78] . In addition, we need to successfully use the predictions of teacher models as soft labels for student models, and we should also know how to reasonably assign the labels of teacher models, even the hard labels. For example,LADin [72] cansignificantlyimprovetheperformance of a student model with a lightweight teacher model, and the performance of the student model will in fact be superior to that of its teacher. LAD requires a trained teacher to provide guidance (soft and hard labels) for its student. However, in practice, it is difficult to obtain excellent teacher models capable of providing effective guidance information. Therefore, Zhang et al. [57] proposed a self-distillation framework for OD, called label-guided self-distillation (LGD), which can generate the required guidance information using only the internal relationship between objects. Moreover, there are also some other methods that have tried to obtain better OD results by combining soft and hard labels [39] ,[71] .  \n\nIn label guided KD-based OD models, pseudo-labels can also play important roles in the training of student models. For example, Feng et al. [97] proposed an adaptive pseudo-label selection strategy to selectively calculate the distillation loss using the pseudo-labels. The student model carries out feature learning by using pseudo-labels of the teacher model first, and fine-tunes the network according to the ground truth labels. This KD strategy can not only reduce the demand for labeled samples [59] ,[60] , but also achieves higher OD performance than traditional methods based on the single-stage KD strategy [142] .In addition, for KD-based 3D OD models, using high-beam point clouds with pseudo-labels to train student models is a good way to solve the problem of high-cost 3D sample data labeling; thus, low-beam pseudo-LiDAR needs to be generated by down-sampling high-beam point clouds [108] .  \n\n2) Feature Distillation for OD: Another KD strategy for feature distillation in intermediate feature layers can also effectively improve the performance of OD models. In this subsection, we will provide an overview of different feature distillation strategies embedded in OD models, including the basic ideas of feature distillation, full trust feature distillation, selective trust feature distillation, and so on.  \n\nIn general, the methods based on feature distillation use the features output from the middle layers of teacher models to supervise the training of student models, so that the student models can mimic the features output from the teacher models to the greatest extent possible. The essence of the idea is to continuously optimize the loss function consisting of both the activation functions of the teacher and student models\u2019 feature layers. Therefore, feature distillation is carried out by using a loss function to train the student model. A general formulation of the loss function for feature distillation has been provided by Gou et al. [10] :  \n\n$$\n{\\cal L}={\\cal L}_{F e a D}(f_{t}(x),f_{s}(x))={\\cal L}_{F}(\\Phi_{t}(f_{t}(x)),\\Phi_{s}(f_{s}(x))),\n$$  \n\nwhere $f_{t}(x)$ and $f_{s}(x)$ are the output features from the middle layers of the teacher and student models respectively. Considering that the different network structures of the teacher and student models may lead to different sizes of the output features from their middle layers, a transformation function $\\Phi(.)$ is used to match these features. $L_{F}(.)$ denotes the loss function between the features of teacher and student models.  \n\nAccording to the related works on feature distillation published in recent years, there are two main distillation strategies employed for feature distillation on OD models: full trust feature distillation and selective trust feature distillation. In the below, we will provide a description of these methods in detail.  \n\n(1) Full Trust Feature Distillation  \n\nFull trust feature distillation means that the student model learns all the knowledge from the teacher model unconditionally, without considering whether the knowledge to be learned is correct or not. Methods based on full trust feature distillation can further be divided into global feature distillation and local feature distillation.  \n\nGlobal Feature Distillation is an approach in which the student model imitates the entire feature maps of the middle layers of the teacher model. For example, the fast scene text detector uses all feature maps of the teacher model to guide the training of the student model [92] . In addition, the related works in [115] ,[116] , and [93] also designed their models through the strategy of full trust feature distillation. These methods are in essence the applications of the most basic feature distillation strategy; they do not make any improvements to the feature maps, but instead simply guide the student models to learn all feature information directly from the teacher models. However, there are limitations on the capacity to improve the performance of student models by learning the global features of the teacher model indiscriminately. Therefore, researchers have gradually explored optimizing the output feature maps to further improve the performance of student models. For example, Qi et al. [24] aligned the feature maps at different resolutions using a feature pyramid, dynamically fused these features, and finally extracted these fused features from the teacher model to provide better guidance to the student model. The work of He et al. [88] ranks the feature maps by calculating the channel strengths of these feature maps in the teacher and student models for feature distillation. The feature distillation strategies proposed in [24] and [88] perform knowledge learning between the same layers of the teacher and student models; moreover, the multi-layer feature maps of the teacher model can also be used to guide the single-layer feature learning of the student model [28] ,[89] .  \n\nLocal Feature Distillation refers to the simulation of a student model learning those local features that are more helpful for the final prediction, rather than the entire feature maps of the teacher model. In recent years, an increasing number of related works have explored corresponding distillation strategies for local feature learning; these methods mainly try to learn the visual features at key locations in the feature maps. Chen et al. [70] used a region distillation strategy to train a lightweight pedestrian detector that crops features corresponding to RoI regions, after which the cropped local feature maps are used as the guidance information for the student model. In addition, the anchors in object detectors are widely used to locate the key local features for training student models [17] ,[91] ,[95] ; these anchors can also be ranked to enable the student model to learn feature maps with different significance [143] . It can be readily observedthatthelocalfeaturedistillationintheabovementioned methods is performed directly around the feature maps. The attentional mechanisms can also be used to make the student models pay more attention to the key local visual features. For example, spatial attention on feature maps is introduced for local feature distillation [144] , while the attention mechanism used to highlight foreground regions as well as contextual information is also helpful for OD [47] . Furthermore, Yang et al. [76] considered that the teacher and student models pay different levels of attention to the foreground and background, while the uneven differences in the feature maps in turn impact the effect of KD. Therefore, these authors proposed a strategy combining focal distillation and global distillation, in which focal distillation is guided to the student model by using spatial and channel attention masks during model training. The goal here is to make the student model focus only on the key pixels and channels on the feature map, thereby improving the performance of the student model. There are also some other local feature distillation strategies, such as the key proposal generation of the student and teacher models for local feature distillation [75] .  \n\nThese above mentioned OD methods based on whether global feature distillation or local feature distillation place full trust in the guidance information of the teacher model. However, it shouldbenotedthattheguidancefeatureinformationusedbythe teacher model to supervise the training of the student model may be incomplete or even incorrect, which has a negative impact on the performance improvements of the student model.  \n\n(2) Selective Trust Feature Distillation  \n\nTo address the detrimental effects on the student model of incorrectinformationprovidedbytheteachermodel,thestrategy of selective trust feature distillation is introduced in KD-based OD models. Selective trust feature distillation means that the guidance information provided by the teacher model to the student model should be selected first: in short, it is necessary to remove the incorrect information and leave only the feature information that has a positive impact on the detection performance. For example, Heo et al. [145] proposed a margin ReLu to suppress the unfavorable feature information from the teachermodel,sothatthestudentmodellearnsonlythefavorable features and thus achieves performance improvement.  \n\nIn summary, whether full trust feature distillation or selective trust feature distillation is employed, this is ultimately optimized through the loss function. However, it is challenging to quickly and accurately select the substantial and beneficial features from the large amount of prior guidance information provided by the teacher model. There are thus many scientific problems worthy of further study associated with OD models based on selective trust KD.  \n\n3) Various Network Structures of Teacher-Student Models: ThissectionwillexploretheKDstrategyfromanewperspective. Specifically, we found that different network structures can be designed for the teacher and student models respectively, and the knowledge extracted from multimodal data can facilitate significant performance improvements on the part of the student model. Therefore, this section will summarize and analyze the network structures of the teacher-student models and the feature learning from multimodal data.  \n\nItisacommonKDstrategythattheteacherandstudentmodels have similar network structures. Many different backbone networks have been adopted by teacher and student models, such as ResNet [18] ,[58] , ResNext [146] , SSD [80] , VGG [146] ,and so on. In addition, some studies do not directly use the classical network model as the backbone network, but instead adjust existing networks [32] ,[138] . However, these methods areKD-basedODmodelsusingtraditionaldistillationstrategies, whether they directly use typical networks or employ adjusted networks as the backbone of the teacher and student models. Most KD-based OD models with similar teacher-student network structures extract the knowledge from single-modal data (RGB images), although there also are some methods that try to learn knowledge from other modalities for guiding the lightweight student model. For example, the student model can learn semantic knowledge provided by the teacher model [111] ,learn the textual and visual features extracted by the teacher model on text information [147] , or jointly learn the visual features from RGB images and heat-like images under the guidance of the trained teacher model on these two modes of data [14] .  \n\nFurthermore, for 3D OD, there are also common approaches involving teacher and student models using similar network structures as their backbones. For example, Wei et al. [108] opt to use the KD strategy to generate a lightweight 3D detector; here, the network structure of teacher and student models in the distillation framework is the same 3D convolutional neural network. In addition, ItKD, designed by Cho et al. [106] ,uses an autoencoder in combination with KD to improve the performance of a 3D object detector. The teacher and student models in ItKD are composed of the same backbone CenterPoint and autoencoder, and the same point cloud data is used for training the teacher and student networks. In the KD-based 3D OD tasks, multi-modal data is also used as the input of the distillation models. In methods employing this strategy, the student model is trained using 3D point cloud data, while the teacher model is trained using other modalities. For example, Qin et al. [31] proposed a cross-modal KD method, in which RGB images are used to train the teacher model and the point cloud is used to train the student model. This method aims to transfer the knowledge from the RGB domain to the point cloud domain, thereby reducing the labeling cost of 3D OD. Moreover, multimodal data can be used to train the teacher model, which is more beneficial to the performance of the student model [45] ,[109] . Multimodal data (LiDAR-image, which consists of point clouds and RGB images after segmentation) is used for training the teacher model in [45] ,[109] ; here, the student model is expected to learn the knowledge from the teacher model and to obtain the similar outputs to the teacher model using only LiDAR.  \n\n(2) Different Network Structures of Teacher and Student Models  \n\nAnother KD strategy involves the teacher and student models using different network structures as their backbones. For example, the method in [82] uses DarkNet-53 based SSD as the teacher model\u2019s backbone and MobileNet v2/ShuffleNet v1 as the student model\u2019s backbone. Su et al. [55] use ResNet-based networksastheteachermodel\u2019sbackboneandaself-built3-layer CNN as the student model\u2019s backbone. A similar strategy is used in [5] . In addition to those listed above, there are also many more similar methods with different combinations of teacher-student models. Notably, while the teacher and student models can choose various networks as their own backbones, it is necessary to choose the appropriate networks according to the specific problems to be solved, especially given the lack of capacity of the student model. Similarly, different network structures can also be used for KD-based 3D OD tasks. Sautier et al. [46] use a 2D-to-3D distillation strategy to improve 3D OD in an autonomous driving context. The backbone of the teacher model is ResNet50 trained with RGB images, and the student model uses U-Net as its backbone trained with LiDAR data. The final experiments show that the model with this strategy outperforms the state-of-the-art methods.  \n\n  \nFig. 5. The structure of multiple teacher models guiding one student model to learn the knowledge.  \n\nThis section lists several KD-based OD models using various teacher-student model network structures. Similar/different networks are used as the backbones of the teacher and student models to extract features from multimodal data. Through analysis of the existing relevant methods, we determine that the methods using different network structures as the backbone of the teacher model to extract knowledge from the multi-modal data have relatively more advantages, when it comes to guiding the feature learning of the student models. However, there are no fixed KD strategies of combining teacher-student model structures and multimodal data, we should design/choose appropriate networks for KD according to the specific tasks.  \n\n4) Multiple Teacher Models: KD is similar to the learning processes used by humans. The traditional technology of KD involves a teacher model guiding a student model to learn the knowledge. Notably, however, human teaching activities involve morepatternsthantraditionalKD.Therefore,severalKDmodels based on human teaching patterns have been proposed.  \n\n(1) Multiple Teachers Guiding One Student  \n\nAs previously discussed, the first commonly used human teaching mode is that in which multiple teachers teach one student. For example, if teachers with different areas of specialization all teach one student, the student can acquire higher-quality knowledge. Similarly, we can use different types of teacher models to learn different knowledge from large-scale datasets, then try to transfer the learned knowledge to one student model, so as that this student model can learn more comprehensive and significant visual features, as shown in Fig. 5 . For example, in [37] ,[53] ,[54] , multiple teacher models are used to guide one student model to improve its OD performance. There are some key differences between these three works: Kuang et al. [53] carried out weighted fusion of different teacher models to improve the accuracy of OD networks; Chen et al. [37] used two teacher networks trained with different strategies to ensure that the knowledge could be fully transferred to the student network; Li et al. [54] designed an asymmetric two-path learning framework to train the student model.  \n\n(2) One Teacher Guiding Multiple Students  \n\nThe second commonly used teaching pattern is that one teacher teaches multiple students. For example, multiple student models are guided by one teacher model, after which one student model with the best performance is selected from all the student models. The work in [65] uses a KD framework in which one teacher model guides multiple student models to solve the problem of Siamese trackers being limited by high cost."}, {"ref_id": "454845516142612392", "chunk_id": "12", "score": 0.54296875, "text": "# Learning From Biased Soft Labels\nHua Yuan, Ning Xu, Yu Shi, Xin Geng and Yong Rui\n\n# Abstract\nKnowledge distillation has been widely adopted in a variety of tasks and has achieved remarkable successes. Since its inception, many researchers have been intrigued by the dark knowledge hidden in the outputs of the teacher model. Recently, a study has demonstrated that knowledge distillation and label smoothing can be unified as learning from soft labels. Consequently, how to measure the effectiveness of the soft labels becomes an important question. Most existing theories have stringent constraints on the teacher model or data distribution, and many assumptions imply that the soft labels are close to the ground-truth labels. This paper studies whether biased soft labels are still effective. We present two more comprehensive indicators to measure the effectiveness of such soft labels. Based on the two indicators, we give sufficient conditions to ensure biased soft label based learners are classifier-consistent and ERM learnable . The theory is applied to three weakly-supervised frameworks. Experimental results validate that biased soft labels can also teach good students, which corroborates the soundness of the theory.\n\n# 1 Introduction\nRecently, knowledge distillation Bucilu\u02c7a et al. (2006 ); Ba and Caruana (2014 ); Hinton et al. (2015 )has engendered remarkable achievements in a wide range of applications. Although it was firstly proposed for model compression by distilling knowledge from the big model (teacher) to the small model (student), considerable efforts have been devoted to figuring out the dark knowledge hidden in the outputs of the teacher model. The dark knowledge is compatibly utilized for transfer learning Vapnik et al. (2015 ); Zagoruyko and Komodakis (2016a ); Noroozi et al. (2018 ).  \n\nIn practice, the student loss is defined as the tradeoffbetween imitating the ground-truth label and imitating the output of the teacher model. Many studies Rusu et al. (2015 ); Furlanello et al. (2018 ) have demonstrated that learning from the teacher model can be more effective than the ground-truth labels. This seems counterintuitive since it challenges the correctness of the groundtruth labels. Apart from knowledge distillation, label smoothing Szegedy et al. (2016 ); Zoph et al. (2018 ) also softens the labels by incorporating uniform noise, which is a useful trick to improve generalization. Knowledge distillation and label smoothing are often analyzed together, and Yuan et al. (2020 ) elucidates that they can be unified as learning from soft labels. The essence of both is why the soft labels are effective.  \n\nIn this paper, we mainly focus on the effectiveness of these soft labels. To be clarified, this paper investigates when the soft labels are effective, rather than when the soft labels are superior to ground-truth labels. It is apparent that, when the soft labels are close to the ground-truth labels, the student model will have an adequate performance. A straightforward question is,  \n\n  \nFigure 1: (a) Images of the birds in CIFAR-10. (b) Defective knowledge distillation with $\\alpha=0.9$ and $\\tau=20$ . (c) Label smoothing with $\\alpha=0.9$ . (d) Our customized soft labels.  \n\nYuan et al. (2020 ) empirically demonstrates the poorly-trained teacher model can also improve the student model. However, it sets the tradeoff $\\alpha=0.9$ and temperature $\\tau=20$ , which means the defective soft labels are still close to the ground-truth label. Figure 1 illustrates defective soft labels, label smoothing (with $\\alpha=0.9$ ) and our customized soft labels (detailed in subsection 3.3 ).  \n\nTo measure the effectiveness of the soft labels, without accuracy, we propose two intuitive indicators, unreliability degree and ambiguity degree. Furthermore, based on the two indicators, we prove that learning from the biased soft labels is classifier-consistent and Empirical Risk Minimizing (ERM) learnable under a moderate condition. The theory is applicable not only to learning from poor teachers, but to all soft label based learners. This result significantly extends the application scope of soft labels. We apply it to three classic weakly-supervised frameworks: parital label learning, learning with additive noise, learning with incomplete data.  \n\nAmong the weakly-supervised frameworks, soft labels of parital label learning and additive noise are spoiled by human or during collection. In incomplete supervision, where only part of the data is labeled, the typical strategy is to label the unlabeled data and learn with all data iteratively. Soft labels in these weakly-supervised frameworks are usually biased and we provide a guarantee for the learners in these fields. Specifically, for the incomplete data, we delineate the dynamics of the model performance with an ideal accuracy funtion and give conditions to ensure the existence of the final accuracy.  \n\nTo illustrate the soundness of our theory, we train the teacher models with some heuristic losses to generate soft labels with low accuracy but fulfilling the criteria in Theorem 3.2 . Training on these biased soft labels, the student model can achieve an adequate performance as if training on ground-truth labels, which is consistent with our theory. In addition, the experiments of weaklysupervised learning also validate the effectiveness of biased soft labels. Our contributions can be summarized as follows:  \n\n\u2022We focus on the effectiveness of soft labels and find that learning from biased soft labels may also achieve an adequate performance. A heuristic method is devised to generate biased soft labels that can train a good student.   \n\u2022We give sufficient conditions to guarantee the effectiveness of the soft labels. It is proved that learning from such soft labels is classifier-consistent and ERM learnable. Experimental results validate our theory.   \n\u2022Our theory is applied to three weakly-supervised frameworks where the soft labels are biased. In incomplete supervision, We delineate the dynamics of the model performance with an ideal  \n\naccuracy function, and give the final accuracy."}, {"ref_id": "454846239382763602", "chunk_id": "3", "score": 0.54296875, "text": "# Model Optimization\nTraining. Our framework first undergoes a preprocessing phase in which the teacher model is trained on real seen samples. The teacher model then extracts knowledge, either in the form of logits or as intermediate features. This extracted knowledge is subsequently used to guide the training of the student model during the distillation process. The cross-entropy loss in the preprocessing phase is as follows:  \n\n$$\n\\mathcal{L}_{p r e}(x,y)=-\\sum_{i=1}^{S}y^{(i)}\\log\\frac{\\exp(\\phi(x)^{(i)})/\\tau_{o})}{\\sum_{k=1}^{S}\\exp(\\phi(x)^{(k)})/\\tau_{o})}.\n$$  \n\nAfter completing the pre-training of the OOD detection model, we jointly train the FG, $\\mathrm{ID^{2}\\bar{S}D}$ and $\\mathrm{O^{2}D B D}$ end-toend. We utilize the real seen samples $x^{\\prime}$ and the unseen samples $x^{\\prime\\prime}$ generated by the FG as inputs for the $\\mathrm{ID^{2}S D}$ module. Then, we calculate the OOD confidence labels from the output of the teacher network, and map the softmax probability of student network to the OOD representation embedding space. Thus, the total loss of $\\mathrm{D^{3}G Z\\bar{S}L}$ is formulated as:  \n\n$$\n\\operatorname*{min}_{G,E_{s},C_{s},H}\\mathcal{L}_{g e n}+\\lambda(\\mathcal{L}_{i d}+\\mathcal{L}_{o d}),\n$$  \n\n<html><body><table><tr><td rowspan=\"2\">Method</td><td colspan=\"3\">AWA1</td><td colspan=\"3\">AWA2</td><td colspan=\"3\">CUB</td><td colspan=\"3\">FLO</td></tr><tr><td>U</td><td>S</td><td>H</td><td>U</td><td>S</td><td>H</td><td>U</td><td>S</td><td>H</td><td>U</td><td>S</td><td>H</td></tr><tr><td>cycle-CLSWGAN (Felixet al. 2018)</td><td>56.9</td><td>64.0</td><td>60.2</td><td>-</td><td>-</td><td></td><td>45.7</td><td>61.0</td><td>52.3</td><td>59.2</td><td>72.5</td><td>65.1</td></tr><tr><td>CADA-VAE (Schonfeld et al.2019)</td><td>57.3</td><td>72.8</td><td>64.1</td><td>55.8</td><td>75.0</td><td>63.9</td><td>51.6</td><td>53.5</td><td>52.4</td><td></td><td></td><td></td></tr><tr><td>LisGAN (Li et al.2019)</td><td>52.6</td><td>76.3</td><td>62.3</td><td></td><td></td><td></td><td>46.5</td><td>57.9</td><td>51.6</td><td>57.7</td><td>83.8</td><td>68.3</td></tr><tr><td>IZF (Shen et al. 2020)</td><td>61.3</td><td>80.5</td><td>69.6</td><td>60.6</td><td>77.5</td><td>68.0</td><td>52.7</td><td>68.0</td><td>59.4</td><td></td><td></td><td>-</td></tr><tr><td>SE-GZSL (Kim,Shim,and Shim 2022)</td><td>61.3</td><td>76.7</td><td>68.1</td><td>59.9</td><td>80.7</td><td>68.8</td><td>53.1</td><td>60.3</td><td>56.4</td><td></td><td></td><td></td></tr><tr><td>TDCSS (Feng et al. 2022)</td><td>54.4</td><td>69.8</td><td>60.9</td><td>59.2</td><td>74.9</td><td>66.1</td><td>44.2</td><td>62.8</td><td>51.9</td><td>54.1</td><td>85.1</td><td>66.2</td></tr><tr><td>DUET (Chen et al. 2023)</td><td></td><td></td><td>-</td><td>63.7</td><td>84.7</td><td>72.7</td><td>62.9</td><td>72.8</td><td>67.5</td><td></td><td></td><td>1</td></tr><tr><td>GKU (Guo et al. 2023)</td><td></td><td></td><td></td><td></td><td>-</td><td>-</td><td>52.3</td><td>71.1</td><td>60.3</td><td></td><td></td><td></td></tr><tr><td>f-CLSWGAN(Xianet al.2018b)</td><td>57.9</td><td>61.4</td><td>59.6</td><td></td><td></td><td></td><td>43.7</td><td>57.7</td><td>49.7</td><td>59.0</td><td>73.8</td><td>65.6</td></tr><tr><td>f-CLSWGAN+D?GZSL</td><td>57.1</td><td>69.8</td><td>62.8</td><td></td><td></td><td></td><td>52.3</td><td>61.5</td><td>56.5</td><td>61.1</td><td>86.7</td><td>71.7</td></tr><tr><td>TF-VAEGAN (Narayan et al. 2020)</td><td></td><td>-</td><td>-</td><td>59.8</td><td>75.1</td><td>66.6</td><td>52.8</td><td>64.7</td><td>58.1</td><td>62.5</td><td>84.1</td><td>71.7</td></tr><tr><td>TF-VAEGAN+D?GZSL</td><td>-</td><td></td><td></td><td>60.2</td><td>74.9</td><td>66.8</td><td>57.3</td><td>64.5</td><td>60.7</td><td>65.6</td><td>81.4</td><td>72.7</td></tr><tr><td>DDGAN</td><td>58.1</td><td>63.5</td><td>60.7</td><td>61.7</td><td>68.4</td><td>64.9</td><td>47.5</td><td>61.5</td><td>53.6</td><td>61.1</td><td>85.2</td><td>71.2</td></tr><tr><td>DDGAN+D?GZSL</td><td>59.5</td><td>68.3</td><td>63.6</td><td>62.9</td><td>67.7</td><td>65.2</td><td>54.2</td><td>59.7</td><td>56.8</td><td>63.4</td><td>82.1</td><td>71.5</td></tr><tr><td>CE-GZSL(Han et al.2021)</td><td>65.3</td><td>73.4</td><td>69.1</td><td>63.1</td><td>78.6</td><td>70.0</td><td>63.9</td><td>66.8</td><td>65.3</td><td>69.0</td><td>78.7</td><td>73.5</td></tr><tr><td>CE-GZSL+D?GZSL</td><td>65.7</td><td>76.2</td><td>70.5</td><td>64.6</td><td>76.7</td><td>70.1</td><td>66.7</td><td>69.1</td><td>67.8</td><td>68.6</td><td>80.9</td><td>74.2</td></tr></table></body></html>  \n\nwhe and L$\\mathcal{L}_{o d}$ $\\lambda$ towards the generator. is the hyper-parameters indicating the effect of $\\mathcal{L}_{i d}$  \n\nInference. We no longer train a separate classifier for classification. Once the training is completed, we map the $D_{t e}=\\{x_{i},y_{i}\\}_{i=N+1}^{N+M}$ to the embedding space using the embedding function $E_{s}$ of the student network. Then, we employ the classifier $C_{s}$ to predict the class label $\\hat{y}$ :  \n\n$$\n\\hat{y}=\\arg\\operatorname*{max}_{i}\\frac{\\exp(\\psi(x)^{(i)})}{\\sum_{k=1}^{S+U}\\exp(\\psi(x)^{(k)})}.\n$$\n\n# Experiment\nDatasets. We perform experiments on four ZSL benchmark datasets that are widely used: the Animals with Attributes1&2 (AWA1 (Lampert, Nickisch, and Harmeling 2013) & AWA2 (Xian et al. 2018a)) dataset, Caltech-UCSD Birds-200-2011 (CUB (Wah et al. 2011)) dataset, and Oxford Flowers (FLO (Nilsback and Zisserman 2008)) dataset.  \n\nEvaluation Protocols. We evaluate the top-1 accuracy separately on both seen classes $(S)$ and unseen classes $(U)$ in the generalized zero-shot learning (GZSL). We also use mean of these two accuracies $\\langle H=(2\\times S\\times$ $U)/(S+U))$ ) as a performance measure for GZSL.  \n\nImplementation Details. We set the embedding dimension $z$ to 2048 on all datasets. The classifier $C_{s}$ outputs logits on all classes, and the classifier $C_{o}$ outputs logits on seen classes. The projector $H$ maps softmax probabilities into a two-dimensional space that encodes both ID and OOD information. The input noise dimension $w$ in the generator is equal to that of the corresponding attributes. In batch distillation, instances of the same class within a batch serve as positive samples for each other, while those of different class are treated as negative samples. Here are some of the parameter settings when employing f-CLSWGAN as the baseline model. We set batch size of 4096 for AWA1, 256 for CUB, 512 for FLO. The number of generated samples for each unseen category is as follows: 200 for AWA1, 5 for CUB, and 30 for FLO. We empirically set the loss weights $\\lambda=0.0001$ for AWA1, CUB. We set $\\lambda=0.001$ for FLO.\n\n# Comparisons with Previous Methods\nIn Table 1, we applied our framework to three previous baseline methods and the DDGAN which is a new generative method of ZSL we introduced to demonstrate the improvement of our framework on diffusion models. The results show that we achieved improvements on the AWA1, AWA2, CUB, and FLO datasets. The most significant improvement was observed with the f-CLSWGAN method, with increases of $3.2\\%$ on the AWA1 dataset, $6.8\\%$ on the CUB dataset, and $6.1\\%$ on the FLO dataset. The best-performing dataset was CUB, with improvements of $6.8\\%$ ,$2.6\\%$ ,$2.5\\%$ , and $3.2\\%$ on the four baseline methods, respectively.  \n\nWithin the CE-GZSL baseline, our $H$ metric delivered the top performance on the AWA1, CUB, and FLO datasets, while ranking second on the AWA2 dataset, only surpassed by DUET (Chen et al. 2023). Strikingly, when compared to the $S$ metric, our $U$ metric demonstrated a substantial improvement, achieving the highest scores on AWA1, AWA2 and CUB, with $65.7\\%$ ,$64.6\\%$ and $66.7\\%$ respectively. It also secured the second-highest performance on the FLO dataset, recording $68.6\\%$ . The experimental results demonstrate that aligning the distribution of generated samples with that of real samples through out-of-distribution detection is an effective method to address seen bias.\n\n# Ablation Study\nTraining Strategy Analysis. In this section, we compare the outcomes of three different experimental groups. Table 2 shows the comparison results. The first comprises our proposed one-stage end-to-end training method. The second involves a Two-Stage (TS) classification method based on OOD detection. The third represents an idealized version of the Two-Stage (IV-TS) classification method based on OOD detection. This idealized experiment is designed to simulate the performance of seen and unseen expert classifiers under conditions where the process of OOD detection can classify seen and unseen samples with complete accuracy. In an ideal scenario, the results of the domain expert classifiers surpass those of our method. In fact, the OOD detection in TS cannot achieve a $100\\%$ accuracy rate. Some data that is not within the training distribution is mistakenly assigned to the expert classifiers for classification. This results in the performance of the TS approach being inferior to that of our proposed method because of the compounding of errors from OOD detection and the expert classifiers (error accumulation).  \n\n<html><body><table><tr><td rowspan=\"2\">Method</td><td rowspan=\"2\">IV-TS</td><td rowspan=\"2\">TS</td><td rowspan=\"2\">Ours</td><td colspan=\"3\">AWA1</td><td colspan=\"3\">AWA2</td><td colspan=\"3\">CUB</td><td colspan=\"3\">FLO</td></tr><tr><td>U</td><td>S</td><td>H</td><td>U</td><td>S</td><td>H</td><td>U</td><td>S</td><td>H</td><td>U</td><td>S</td><td>H</td></tr><tr><td rowspan=\"3\">f-CLSWGAN</td><td>\u221a</td><td></td><td></td><td>67.4</td><td>88.9</td><td></td><td></td><td></td><td></td><td>56.1</td><td>70.0</td><td></td><td>65.2</td><td>87.4</td><td></td></tr><tr><td></td><td>\u221a</td><td></td><td>58.9</td><td>71.8</td><td>64.7</td><td></td><td></td><td></td><td>39.7</td><td>46.1</td><td>42.7</td><td>60.6</td><td>62.3</td><td>61.4</td></tr><tr><td></td><td></td><td>\u4eba</td><td>57.1</td><td>69.8</td><td>62.8</td><td></td><td></td><td></td><td>52.3</td><td>61.5</td><td>56.5</td><td>61.1</td><td>86.7</td><td>71.7</td></tr><tr><td rowspan=\"3\">CE-GZSL</td><td></td><td></td><td></td><td>69.2</td><td>88.3</td><td></td><td>69.7</td><td>91.8</td><td>=</td><td>78.7</td><td>73.5</td><td></td><td>68.1</td><td>89.4</td><td></td></tr><tr><td></td><td>\u221a</td><td></td><td>57.5</td><td>72.3</td><td>64.1</td><td>55.5</td><td>76.7</td><td>64.4</td><td>61.1</td><td>42.8</td><td>50.3</td><td>58.3</td><td>70.8</td><td>64.0</td></tr><tr><td></td><td></td><td>\u221a</td><td>65.7</td><td>76.2</td><td>70.5</td><td>64.6</td><td>76.7</td><td>70.1</td><td>66.7</td><td>69.1</td><td>67.8</td><td>68.6</td><td>80.9</td><td>74.2</td></tr></table></body></html>\n\nTable 2: The performance comparison of our proposed $\\mathrm{D^{3}G Z S L}$ framework (our), the two-stage classification method based on OOD detection (TS) and idealized version of the two-stage classification method based on OOD detection (IV-TS).  \n\n<html><body><table><tr><td>Datasets</td><td>Baseline ID\u00b2SD</td><td>O'DBD</td><td>U</td><td>S</td><td>H</td></tr><tr><td rowspan=\"4\">CUB</td><td>\u00d7</td><td>\u00d7</td><td>50.4</td><td>59.8</td><td>54.7</td></tr><tr><td>\u221a</td><td>\u00d7</td><td>49.4</td><td>63.8</td><td>55.6</td></tr><tr><td>\u00d7</td><td></td><td>51.3</td><td>59.6</td><td>55.1</td></tr><tr><td>\u4eba \u4eba</td><td>\u4eba</td><td>52.3</td><td>61.5</td><td>56.5</td></tr><tr><td rowspan=\"4\">FLO</td><td>\u221a</td><td>\u00d7</td><td>\u00d7</td><td>58.6</td><td>83.3</td><td>68.8</td></tr><tr><td></td><td>\u4eba</td><td>\u00d7</td><td>61.8</td><td>84.3</td><td>71.3</td></tr><tr><td></td><td>\u00d7</td><td></td><td>59.9</td><td>86.3</td><td>70.7</td></tr><tr><td>\u221a</td><td></td><td>\u221a</td><td>61.1</td><td>86.7</td><td>71.7</td></tr></table></body></html>  \n\nTable 3: The baseline model includes the FG and the classification los losses, while O $\\mathrm{O^{2}D B D}$ $\\mathcal{L}_{c l s}$ DBD represents the use of $\\mathrm{ID^{2}S D}$ indicates the us L$\\mathcal{L}_{o d}$ loss. $\\mathcal{L}_{b e}$ and $\\mathcal{L}_{k l}$   \nTable 4: The performance of our framework is demonstrated under various OOD detection methods, employing f-CLSWGAN as the baseline model in our framework.   \n\n\n<html><body><table><tr><td rowspan=\"2\">Method</td><td colspan=\"3\">CUB</td><td colspan=\"3\">FLO</td></tr><tr><td>U</td><td>S</td><td>H</td><td>U</td><td>S</td><td>H</td></tr><tr><td>Baseline</td><td>43.7</td><td>57.7</td><td>49.7</td><td>59.0</td><td>73.8</td><td>65.6</td></tr><tr><td>Energy</td><td>49.1</td><td>62.3</td><td>54.9</td><td>60.3</td><td>87.2</td><td>71.3</td></tr><tr><td>Softmax</td><td>52.3</td><td>61.5</td><td>56.5</td><td>61.1</td><td>86.7</td><td>71.7</td></tr></table></body></html>  \n\nComponent Analysis. Here, we set up an ablation study to examine the impact of various components on our $\\mathrm{D^{3}G Z S L}$ framework. The baseline model includes the FG and the classification loss experiments on two benchmark datasets to validate the in$\\mathcal{L}_{c l s}$ . We conducted three sets of dividual and combined effects of our $\\mathrm{ID^{2}S D}$ and $\\mathrm{O^{2}D B D}$ modules. Through the experimental results presented in Table 3, we can draw the following conclusions: (1) Employing modules $\\mathrm{ID^{2}S D}$ and $\\mathrm{O^{2}D B\\bar{D}}$ separately has led to an enhancement in our performance over the baseline method. (2) When the two modules operate in conjunction, there is a marked enhancement in performance on the $H$ metric. This suggests that our framework is effective in reducing the discrepancy between the distribution of generated samples and the distribution of real samples. It accomplishes this by optimizing both the in-distribution and out-of-distribution aspects, thereby creating a more cohesive alignment between the two distributions.  \n\nOOD Scoring Strategy Analysis. In this paper, we experimented with two different architectures to verify the impact of using different OOD detection methods on our framework. We conducted experiments using two methods, Softmax score (Hendrycks and Gimpel 2016) and Energy (Liu et al. 2020), on the f-CLSWGAN baseline model. The experimental results in Table 4 showed that no matter which architecture was used, the performance was significantly improved. This provides strong evidence for our framework, demonstrating its effective adaptability and scalability, and its ability to be successfully applied to different architectures."}], "task_step_question_answer": "\u5728\u84b8\u998f\u8fc7\u7a0b\u4e2d\uff0c\u6e29\u5ea6\u53c2\u6570\u7684\u5f15\u5165\u662f\u4e00\u4e2a\u5173\u952e\u7684\u6280\u672f\u70b9\u3002\u6e29\u5ea6\u53c2\u6570\u901a\u8fc7\u5e73\u6ed1\u6982\u7387\u5206\u5e03\uff0c\u4f7f\u5f97\u8f6f\u6807\u7b7e\u5305\u542b\u66f4\u591a\u7684\u4fe1\u606f\uff0c\u4ece\u800c\u5e2e\u52a9\u5b66\u751f\u6a21\u578b\u66f4\u597d\u5730\u5b66\u4e60\u6559\u5e08\u6a21\u578b\u7684\u8f93\u51fa\u5206\u5e03\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6e29\u5ea6\u53c2\u6570T\u5728softmax\u51fd\u6570\u4e2d\u8d77\u5230\u8c03\u8282\u4f5c\u7528\uff0c\u4f7f\u5f97\u8f93\u51fa\u6982\u7387\u5206\u5e03\u66f4\u52a0\u5e73\u6ed1\uff0c\u4ece\u800c\u4f7f\u5f97\u5b66\u751f\u6a21\u578b\u80fd\u591f\u66f4\u597d\u5730\u6355\u6349\u5230\u6559\u5e08\u6a21\u578b\u7684\u9690\u542b\u77e5\u8bc6\u3002\u6b64\u5916\uff0c\u635f\u5931\u51fd\u6570\u7684\u8bbe\u8ba1\u4e5f\u662f\u84b8\u998f\u8fc7\u7a0b\u4e2d\u7684\u4e00\u4e2a\u91cd\u8981\u73af\u8282\u3002\u901a\u5e38\uff0c\u635f\u5931\u51fd\u6570\u4f1a\u7ed3\u5408\u786c\u6807\u7b7e\u635f\u5931\u548c\u8f6f\u6807\u7b7e\u635f\u5931\uff0c\u786c\u6807\u7b7e\u635f\u5931\u786e\u4fdd\u5b66\u751f\u6a21\u578b\u5bf9\u771f\u5b9e\u6807\u7b7e\u7684\u51c6\u786e\u6027\uff0c\u800c\u8f6f\u6807\u7b7e\u635f\u5931\u5219\u5e2e\u52a9\u5b66\u751f\u6a21\u578b\u5b66\u4e60\u6559\u5e08\u6a21\u578b\u7684\u590d\u6742\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002\u901a\u8fc7\u5408\u7406\u8bbe\u8ba1\u635f\u5931\u51fd\u6570\uff0c\u53ef\u4ee5\u786e\u4fdd\u5b66\u751f\u6a21\u578b\u5728\u84b8\u998f\u8fc7\u7a0b\u4e2d\u65e2\u80fd\u4fdd\u6301\u5bf9\u771f\u5b9e\u6570\u636e\u7684\u62df\u5408\uff0c\u53c8\u80fd\u4ece\u6559\u5e08\u6a21\u578b\u4e2d\u5b66\u4e60\u5230\u66f4\u4e30\u5bcc\u7684\u77e5\u8bc6\u3002\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u84b8\u998f\u8fc7\u7a0b\u4e0d\u4ec5\u9650\u4e8e\u6a21\u578b\u538b\u7f29\uff0c\u8fd8\u5305\u62ec\u77e5\u8bc6\u8fc1\u79fb\u548c\u591a\u4efb\u52a1\u84b8\u998f\u3002\u901a\u8fc7\u77e5\u8bc6\u8fc1\u79fb\u5b9e\u73b0\u6a21\u578b\u6027\u80fd\u7684\u63d0\u5347\u548c\u590d\u6742\u5ea6\u7684\u964d\u4f4e\uff0c\u540c\u65f6\u4fdd\u6301\u8f83\u9ad8\u7684\u8ba1\u7b97\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\u3002\u4f8b\u5982\uff0c\u5728\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u4e2d\uff0c\u8f6f\u6807\u7b7e\u635f\u5931\u53ef\u4ee5\u901a\u8fc7\u6559\u5e08\u6a21\u578b\u8f93\u51fa\u7684\u4f2a\u6807\u7b7e\u6765\u6307\u5bfc\u5b66\u751f\u6a21\u578b\u7684\u7279\u5f81\u5b66\u4e60\uff0c\u800c\u786c\u6807\u7b7e\u635f\u5931\u5219\u786e\u4fdd\u5b66\u751f\u6a21\u578b\u5bf9\u771f\u5b9e\u6807\u7b7e\u7684\u51c6\u786e\u6027\u3002\u8fd9\u79cd\u7ed3\u5408\u7b56\u7565\u4e0d\u4ec5\u51cf\u5c11\u4e86\u5bf9\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u8fd8\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u6027\u80fd\u3002", "ref_task_step_id": ""}, "__type__": "task_step_node"}, "bc7eecf8-6996-4c90-a203-0c062c7e5ea8": {"__data__": {"id_": "bc7eecf8-6996-4c90-a203-0c062c7e5ea8", "metadata": {}, "relationships": {}, "hash": "", "start_task_context": "\u6a21\u578b\u84b8\u998f\u7684\u662f\u4ec0\u4e48\uff1f", "aemo_representation_context": "\u6a21\u578b\u84b8\u998f\uff08Model Distillation\uff09\u662f\u4e00\u79cd\u5728\u673a\u5668\u5b66\u4e60\u9886\u57df\u5e7f\u6cdb\u4f7f\u7528\u7684\u6280\u672f\uff0c\u4e3b\u8981\u7528\u4e8e\u5c06\u4e00\u4e2a\u5927\u578b\u3001\u590d\u6742\u4e14\u9ad8\u6027\u80fd\u7684\u6a21\u578b\uff08\u901a\u5e38\u79f0\u4e3a\u6559\u5e08\u6a21\u578b\uff09\u7684\u77e5\u8bc6\u8fc1\u79fb\u5230\u4e00\u4e2a\u66f4\u5c0f\u3001\u66f4\u7b80\u5355\u4e14\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u7684\u6a21\u578b\uff08\u901a\u5e38\u79f0\u4e3a\u5b66\u751f\u6a21\u578b\uff09\u4e2d\u3002\u4ee5\u4e0b\u662f\u6a21\u578b\u84b8\u998f\u7684\u8be6\u7ec6\u5206\u89e3\uff1a\n\n### 1. **\u57fa\u672c\u6982\u5ff5**\n- **\u6559\u5e08\u6a21\u578b\uff08Teacher Model\uff09**\uff1a\u4e00\u4e2a\u8bad\u7ec3\u597d\u7684\u3001\u6027\u80fd\u4f18\u5f02\u4f46\u53ef\u80fd\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u7684\u6a21\u578b\u3002\n- **\u5b66\u751f\u6a21\u578b\uff08Student Model\uff09**\uff1a\u4e00\u4e2a\u66f4\u5c0f\u3001\u66f4\u7b80\u5355\u3001\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u7684\u6a21\u578b\uff0c\u76ee\u6807\u662f\u6a21\u4eff\u6559\u5e08\u6a21\u578b\u7684\u8868\u73b0\u3002\n\n### 2. **\u84b8\u998f\u8fc7\u7a0b**\n- **\u8bad\u7ec3\u6559\u5e08\u6a21\u578b**\uff1a\u9996\u5148\uff0c\u4f7f\u7528\u5927\u91cf\u6570\u636e\u8bad\u7ec3\u4e00\u4e2a\u9ad8\u6027\u80fd\u7684\u6559\u5e08\u6a21\u578b\u3002\n- **\u751f\u6210\u8f6f\u6807\u7b7e**\uff1a\u5229\u7528\u6559\u5e08\u6a21\u578b\u5bf9\u8bad\u7ec3\u6570\u636e\u8fdb\u884c\u9884\u6d4b\uff0c\u751f\u6210\u8f6f\u6807\u7b7e\uff08\u5373\u6982\u7387\u5206\u5e03\uff09\uff0c\u800c\u4e0d\u662f\u7b80\u5355\u7684\u786c\u6807\u7b7e\uff08\u5373\u7c7b\u522b\u6807\u7b7e\uff09\u3002\n- **\u8bad\u7ec3\u5b66\u751f\u6a21\u578b**\uff1a\u4f7f\u7528\u539f\u59cb\u6570\u636e\u548c\u6559\u5e08\u6a21\u578b\u751f\u6210\u7684\u8f6f\u6807\u7b7e\u6765\u8bad\u7ec3\u5b66\u751f\u6a21\u578b\u3002\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u5b66\u751f\u6a21\u578b\u7684\u635f\u5931\u51fd\u6570\u901a\u5e38\u5305\u62ec\u4e24\u90e8\u5206\uff1a\n  - **\u786c\u6807\u7b7e\u635f\u5931**\uff1a\u5b66\u751f\u6a21\u578b\u9884\u6d4b\u4e0e\u771f\u5b9e\u6807\u7b7e\u4e4b\u95f4\u7684\u5dee\u5f02\u3002\n  - **\u8f6f\u6807\u7b7e\u635f\u5931**\uff1a\u5b66\u751f\u6a21\u578b\u9884\u6d4b\u4e0e\u6559\u5e08\u6a21\u578b\u9884\u6d4b\u4e4b\u95f4\u7684\u5dee\u5f02\u3002\n\n### 3. **\u5173\u952e\u6280\u672f\u70b9**\n- **\u6e29\u5ea6\u8c03\u8282**\uff1a\u5728\u751f\u6210\u8f6f\u6807\u7b7e\u65f6\uff0c\u901a\u5e38\u5f15\u5165\u4e00\u4e2a\u6e29\u5ea6\u53c2\u6570\uff08Temperature\uff09\u6765\u5e73\u6ed1\u6982\u7387\u5206\u5e03\uff0c\u4f7f\u5f97\u8f6f\u6807\u7b7e\u5305\u542b\u66f4\u591a\u7684\u4fe1\u606f\u3002\n- **\u635f\u5931\u51fd\u6570\u8bbe\u8ba1**\uff1a\u7ed3\u5408\u786c\u6807\u7b7e\u635f\u5931\u548c\u8f6f\u6807\u7b7e\u635f\u5931\uff0c\u5e73\u8861\u4e24\u8005\u7684\u91cd\u8981\u6027\u3002\n\n### 4. **\u5e94\u7528\u573a\u666f**\n- **\u6a21\u578b\u538b\u7f29**\uff1a\u5c06\u5927\u578b\u6a21\u578b\u538b\u7f29\u6210\u5c0f\u578b\u6a21\u578b\uff0c\u4fbf\u4e8e\u90e8\u7f72\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8bbe\u5907\u4e0a\u3002\n- **\u77e5\u8bc6\u8fc1\u79fb**\uff1a\u5c06\u4e00\u4e2a\u4efb\u52a1\u4e0a\u5b66\u5230\u7684\u77e5\u8bc6\u8fc1\u79fb\u5230\u53e6\u4e00\u4e2a\u4efb\u52a1\u4e0a\u3002\n- **\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b**\uff1a\u901a\u8fc7\u5b66\u4e60\u6559\u5e08\u6a21\u578b\u7684\u8f93\u51fa\uff0c\u5b66\u751f\u6a21\u578b\u53ef\u80fd\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u5177\u6709\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002\n\n### 5. **\u4f18\u52bf\u4e0e\u5c40\u9650\u6027**\n- **\u4f18\u52bf**\uff1a\n  - **\u8ba1\u7b97\u6548\u7387\u9ad8**\uff1a\u5b66\u751f\u6a21\u578b\u66f4\u5c0f\uff0c\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u4f4e\u3002\n  - **\u6027\u80fd\u63a5\u8fd1**\uff1a\u5b66\u751f\u6a21\u578b\u80fd\u63a5\u8fd1\u6559\u5e08\u6a21\u578b\u7684\u6027\u80fd\u3002\n- **\u5c40\u9650\u6027**\uff1a\n  - **\u6027\u80fd\u5dee\u8ddd**\uff1a\u5b66\u751f\u6a21\u578b\u6027\u80fd\u53ef\u80fd\u4ecd\u7565\u4f4e\u4e8e\u6559\u5e08\u6a21\u578b\u3002\n  - **\u8bad\u7ec3\u590d\u6742**\uff1a\u9700\u8981\u5148\u8bad\u7ec3\u4e00\u4e2a\u9ad8\u6027\u80fd\u7684\u6559\u5e08\u6a21\u578b\u3002\n\n### 6. **\u6700\u65b0\u8fdb\u5c55**\n- **\u591a\u4efb\u52a1\u84b8\u998f**\uff1a\u5728\u4e00\u4e2a\u5b66\u751f\u6a21\u578b\u4e0a\u540c\u65f6\u84b8\u998f\u591a\u4e2a\u6559\u5e08\u6a21\u578b\u7684\u77e5\u8bc6\u3002\n- **\u81ea\u84b8\u998f**\uff1a\u6a21\u578b\u81ea\u8eab\u4f5c\u4e3a\u6559\u5e08\u6a21\u578b\u8fdb\u884c\u84b8\u998f\u3002\n- **\u8de8\u6a21\u6001\u84b8\u998f**\uff1a\u5728\u4e0d\u540c\u6a21\u6001\uff08\u5982\u56fe\u50cf\u548c\u6587\u672c\uff09\u4e4b\u95f4\u8fdb\u884c\u77e5\u8bc6\u84b8\u998f\u3002\n\n### 7. **\u672a\u6765\u7814\u7a76\u65b9\u5411**\n- **\u66f4\u9ad8\u6548\u7684\u84b8\u998f\u65b9\u6cd5**\uff1a\u63a2\u7d22\u66f4\u6709\u6548\u7684\u77e5\u8bc6\u8fc1\u79fb\u6280\u672f\u3002\n- **\u6cdb\u5316\u80fd\u529b\u7684\u63d0\u5347**\uff1a\u7814\u7a76\u5982\u4f55\u901a\u8fc7\u84b8\u998f\u63d0\u9ad8\u6a21\u578b\u5728\u672a\u89c1\u6570\u636e\u4e0a\u7684\u8868\u73b0\u3002\n- **\u591a\u6a21\u6001\u84b8\u998f\u7684\u62d3\u5c55**\uff1a\u63a2\u7d22\u66f4\u591a\u8de8\u6a21\u6001\u77e5\u8bc6\u84b8\u998f\u7684\u5e94\u7528\u573a\u666f\u3002\n\n\u901a\u8fc7\u4ee5\u4e0a\u6b65\u9aa4\uff0c\u53ef\u4ee5\u5168\u9762\u7406\u89e3\u6a21\u578b\u84b8\u998f\u7684\u6982\u5ff5\u3001\u8fc7\u7a0b\u3001\u6280\u672f\u7ec6\u8282\u3001\u5e94\u7528\u573a\u666f\u53ca\u5176\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u7814\u7a76\u4e2d\u7684\u91cd\u8981\u6027\u3002\u6a21\u578b\u84b8\u998f\u4e0d\u4ec5\u662f\u4e00\u79cd\u6709\u6548\u7684\u6a21\u578b\u538b\u7f29\u6280\u672f\uff0c\u4e5f\u662f\u77e5\u8bc6\u8fc1\u79fb\u548c\u63d0\u5347\u6a21\u578b\u6027\u80fd\u7684\u91cd\u8981\u624b\u6bb5\u3002", "task_step_name": "\u5173\u952e\u6280\u672f\u70b9", "task_step_description": "\u6e29\u5ea6\u8c03\u8282\uff1a\u5728\u751f\u6210\u8f6f\u6807\u7b7e\u65f6\uff0c\u901a\u5e38\u5f15\u5165\u4e00\u4e2a\u6e29\u5ea6\u53c2\u6570\uff08Temperature\uff09\u6765\u5e73\u6ed1\u6982\u7387\u5206\u5e03\uff0c\u4f7f\u5f97\u8f6f\u6807\u7b7e\u5305\u542b\u66f4\u591a\u7684\u4fe1\u606f\u3002\u635f\u5931\u51fd\u6570\u8bbe\u8ba1\uff1a\u7ed3\u5408\u786c\u6807\u7b7e\u635f\u5931\u548c\u8f6f\u6807\u7b7e\u635f\u5931\uff0c\u5e73\u8861\u4e24\u8005\u7684\u91cd\u8981\u6027\u3002", "task_step_level": "2", "task_step_question": "### \u95ee\u9898\n\n\u5728\u6a21\u578b\u84b8\u998f\u7684\u201c\u5173\u952e\u6280\u672f\u70b9\u201d\u4e2d\uff0c\u6e29\u5ea6\u8c03\u8282\uff08Temperature\uff09\u7684\u4f5c\u7528\u662f\u4ec0\u4e48\uff1f\u5b83\u5982\u4f55\u5f71\u54cd\u8f6f\u6807\u7b7e\u7684\u751f\u6210\u4ee5\u53ca\u5b66\u751f\u6a21\u578b\u7684\u8bad\u7ec3\u8fc7\u7a0b\uff1f", "task_step_question_context": [{"ref_id": "454849384555410342", "chunk_id": "1", "score": 0.333984375, "text": "# Attention Temperature Matters in Abstractive Summarization Distillation\nShengqiang Zhang 1 \u2217\u2020 , Xingxing Zhang 2 \u2217, Hangbo Bao 2 \u2020, Furu Wei 2 1 Peking University 2 Microsoft Research Asia  {xizhang,t-habao,fuwei}@microsoft.com\n\n# Abstract\nRecent progress of abstractive text summarization largely relies on large pre-trained sequence-to-sequence Transformer models, which are computationally expensive. This paper aims to distill these large models into smaller ones for faster inference and with minimal performance loss.Pseudo-labelingbased methods are popular in sequence-tosequence model distillation. In this paper, we find simply manipulating attention temperatures in Transformers can make pseudo labels easier to learn for student models. Our experiments on three summarization datasets show our proposed method consistently improves vanilla pseudo-labeling based methods. Further empirical analysis shows that both pseudo labels and summaries produced by our students are shorter and more abstractive. Our code is available at https://github. com/Shengqiang-Zhang/plate .\n\n# 1 Introduction\nAutomatic document summarization is the task of rewriting a long document into its shorter form while still retaining its most important content. In the literature, there are mainly two kinds of methods for summarization: extractive summarization and abstractive summarization (Nenkova and McKeown ,2011 ). In this work, we focus on abstractive summarization, which is viewed as a sequence-tosequence (Seq2Seq) learning problem, since recent abstractive models outperform their extractive counterparts and can produce more concise summaries ( Raffel et al. ,2020 ;Lewis et al. ,2020 ;Zhang et al. ,2020 ;Liu and Lapata ,2019 ). Recent progress of abstractive summarization largely relies on large pre-trained Transformer models ( Raffel et al. ,2020 ;Lewis et al. ,2020 ;Zhang et al. ,2020 ;Liu and Lapata ,2019 ;Bao et al. ,2020 ). With these extremely large models, we can obtain state-of-theart summarization results, but they are slow for online inference, which makes them difficult to be used in the production environment even with cutting-edge hardware. This paper aims to distill these large Transformer summarization models into smaller ones with minimal loss in performance.  \n\nKnowledge distillation is a class of methods that leverage the output of a (large) teacher model to guide the training of a (small) student model. In classification tasks, it is typically done by minimizing the distance between the teacher and student predictions ( Hinton et al. ,2015 ). As to Seq2Seq models, an effective distillation method is called pseudo-labeling ( Kim and Rush ,2016 ), where the teacher model generates pseudo summaries for all documents in the training set and the resulting document\u2013 pseudo -summary pairs are used to train the student model.  \n\nIn this paper, we argue that attention distributions of a Seq2Seq teacher model might be too sharp. As a result, pseudo labels generated from it are sub-optimal for student models. In the summarization task, we observe that 1) pseudo summaries generated from our teacher model copy more continuous text spans from original documents than reference summaries ( $56\\%$ 4-grams in pseudo summaries and $15\\%$ 4-grams in reference summaries are copied from their original documents on CNN/DailyMail dataset); 2) pseudo summaries tend to summarize the leading part of a document (measured on CNN/DailyMail, $74\\%$ of sentences in pseudo summaries and $64\\%$ of sentences in reference summaries are from the leading $40\\%$ sentences in original documents). We obtain the two numbers above by matching each sentence in a summary with the sentence in its original document that can produce maximum ROUGE ( Lin ,2004 ) score between them. We call the two biases above the copy bias and the leading bias . In order to have an intuitive feeling, we select a representative example 1 and visualize its cross attention weights 2 (see the left graph in Figure 1 ). We observe that attention weights form three \u201clines\u201d, which indicates very time the decoder predicts the next word, its attention points to the next word in the input document. That may be the reason why multiple continuous spans of text are copied. Another phenomenon we observe is that all high-value attention weights (in deeper color) concentrate on the first 200 words in the input document, which reflects the leading bias. In either case, the attention distribution is too sharp (i.e., attention weights of the next word position or the leading part is much larger than other positions), which means our teacher model is over-confident.  \n\nBased on the observations above, we propose a simple method called P LATE (as shorthand for Pseudo-labeling with Larger A ttention TE mperature) to smooth attention distributions of teacher models. Specifically, we re-scale attention weights in all attention modules with a higher temperature, which leads to softer attention distributions. Figure 1 intuitively shows the effect of using higher attention temperatures. Compared with the left graph, the right graph with higher attention temperature has shorter lines (less copy bias) with high attention weights, and positions of high attention weights extend to the first 450 words (less leading bias). Less copy bias in pseudo summaries encourages student models to be more abstractive, while less leading bias in pseudo summaries encourages student models to take advantage of longer context in documents.  \n\nExperiments on CNN/DailyMail, XSum, and New York Times datasets with student models of different sizes show P LATE consistently outperforms vanilla pseudo-labeling methods. Further empirical analysis shows that, with P LATE , both pseudo summaries generated by teacher models and summaries generated by student models are shorter and more abstractive, which matches the goal of abstractive summarization."}, {"ref_id": "454847845063601170", "chunk_id": "1", "score": 0.296875, "text": "# 2. Related Work\nKnowledge distillation [ 13 ] is designed to transfer the \u201cdark\u201d knowledge from a cumbersome teacher model to a lightweight student model. By learning from the soft labels of teacher, student can achieve better performance than training on hard labels only. The traditional method trains a student by minimizing a difference such as KL divergence between its predicted probability and the teacher\u2019s. The prediction of probability is commonly approximated by the softmax of logit output. KD algorithms can be classified into three types, i.e., logit-based [ 3 ,13 ,17 ,22 ,28 ,47 ,49 ,50 ], feature-based [ 1 ,4 ,5 ,10 ,12 ,23 ,25 ,27 ,31 ,37 ,44 ], and relation-based [ 15 ,19 ,29 ,30 ,39 ,43 ] methods.  \n\nA temperature is introduced to flatten the probabilities in logit-based methods. Several works [ 2 ,13 ,26 ] explore its properties and effects. They reach an identical conclusion that temperature controls how much attention student pays on those logits more negative than average. A very low temperature makes student ignore other logits and instead mainly focus on the largest logit of teacher. However, they do not discuss why teacher and student share a globally predefined temperature. It was unknown whether temperature can be determined in an instance-wise level until CTKD [ 24 ] proposed predicting sample-wise temperatures by leveraging adversarial learning. However, it assumes that teacher and student should share temperatures. It was still undiscovered whether teacher and student can have divergent temperatures. ATKD [ 9 ] proposes a sharpness metric and chooses adaptive temperature by reducing the gap between teacher and student. However, their assumption of a zero logit mean relies on numerical approximation and limits its performance. Additionally, they do not thoroughly discuss where the temperature is derived from and whether distinct temperatures can be assigned. In this work, we provide an analytical derivation based on the entropy-maximization principle, demonstrating that students and teachers do not necessarily share a temperature. It is also found sufficient to preserve the innate relationship of prediction, instead of exact logit values of teacher [ 15 ]. However, the existing logit-based KD pipelines still implicitly mandate an exact match between teacher and student logits. We thus define the temperature to be the weighted standard deviation of logit to alleviate the issue and facilitate the existing logit-based KD approaches.\n\n# 3. Background and Notation\nSuppose ansfer d $\\mathcal{D}$ aini $N$ samples are the image and label respectively for the {$\\{\\mathbf{x}_{n},y_{n}\\}_{n=1}^{N}$ }, where $\\mathbf{x}_{n}\\in\\mathbb{R}^{H\\times W}$ \u2208an n$y_{n}\\in[1,K]$ -th sample. \u2208The notations of $H$ ,$W$ and $K$ are image height, width and $f_{T}$ and student number of $f_{S}$ respectively predict logit vectors sses. Given an input $\\{\\mathbf{x}_{n},y_{n}\\}$ , teacher ${\\bf v}_{n}$ and $\\mathbf{z}_{n}\\in\\mathbb{R}^{1\\times K}$ . Namely, ${\\bf z}_{n}=f_{S}({\\bf x}_{n})$ and $\\mathbf{v}_{n}=f_{T}(\\mathbf{x}_{n})$ .  \n\nIt is widely accepted that a softmax function involving vectors a temperature $q(\\mathbf{z}_{n})$ or $\\tau$ $q{\\bf(v}_{n})$ is used to convert th such that their klogit to probability -th items have  \n\n$$\n\\begin{array}{r l}&{q(\\mathbf{z}_{n})^{(k)}=\\frac{\\exp(\\mathbf{z}_{n}^{(k)}/\\mathcal{T})}{\\sum_{m=1}^{K}\\exp(\\mathbf{z}_{n}^{(m)}/\\mathcal{T})},}\\\\ &{q(\\mathbf{v}_{n})^{(k)}=\\frac{\\exp(\\mathbf{v}_{n}^{(k)}/\\mathcal{T})}{\\sum_{m=1}^{K}\\exp(\\mathbf{v}_{n}^{(m)}/\\mathcal{T})},}\\end{array}\n$$  \n\nwhere ${\\bf z}_{n}^{(k)}$ and ${\\bf v}_{n}^{(k)}$ are the $k$ -th item of ${\\bf z}_{n}$ and ${\\bf v}_{n}$ respectively. A knowledge distillation process is essentially letting $q({\\bf z}_{n})^{(k)}$ mimic $q\\bar{(}\\mathbf{v}_{n})^{(k)}$ for any class and all samples. The objective is realized by minimizing $\\mathrm{KL}$ divergence  \n\n$$\n\\mathcal{L}_{\\mathrm{KL}}\\left(q(\\mathbf{v}_{n})\\vert\\vert q(\\mathbf{z}_{n})\\right)=\\sum_{k=1}^{K}q(\\mathbf{v}_{n})^{(k)}\\log\\left(\\frac{q(\\mathbf{v}_{n})^{(k)}}{q(\\mathbf{z}_{n})^{(k)}}\\right),\n$$  \n\nwhich is theoretically equivalent to a cross-entropy loss when optimizing solely on $\\mathbf{z}$ ,  \n\n$$\n\\mathcal{L}_{\\mathrm{CE}}\\left(q(\\mathbf{v}_{n}),q(\\mathbf{z}_{n})\\right)=-\\sum_{k=1}^{K}q(\\mathbf{v}_{n})^{(k)}\\log q(\\mathbf{z}_{n})^{(k)}.\n$$  \n\nNote that they are empirically nonequivalent as their gradients diverge due to the negative entropy term of $q({\\bf v}_{n})$ .\n\n# 4. Methodology\nIt is widely accepted that in Eq. 1 and Eq. 2 . In contrast, in Sec. $\\tau$ is shared for teacher and student 4.1 , we show the irrelevance between the temperatures of teacher and student, as well as across different samples. Guaranteed that temperatures can be different between teacher and student and among sample, we further show two side-effect drawbacks of shared-temperatures setting in conventional KD pipelines in Sec. 4.2 . In Sec. 4.3 , we propose leveraging logit standard deviation as a factor in temperature and derive a preprocess of logit standardization.\n\n# 4.1. Irrelevance between Temperatures\nIn Sec. 4.1.1 and 4.1.2 , we first give a derivation of the temperature-involved softmax function in classification and KD based on the entropy-maximization principle in information theory. This implies the temperatures of student and teacher can be distinct and sample-wisely different.\n\n# 4.1.1 Derivation of softmax in Classification\nThe softmax function in classification can be proved to be the unique solution of maximizing entropy subject to the normalization condition of probability and a constraint on the expectation of states in information theory [ 16 ]. The derivation is also leveraged in confidence calibration to formulate temperature scaling [ 8 ]. Suppose we have the following constrained entropy-maximization optimization,  \n\n$$\n\\begin{array}{c}{\\displaystyle\\operatorname*{max}_{q}\\mathcal{L}_{1}=-\\sum_{n=1}^{N}\\sum_{k=1}^{K}q(\\mathbf{v}_{n})^{(k)}\\log q(\\mathbf{v}_{n})^{(k)}}\\\\ {\\displaystyle\\left\\{\\sum_{k=1}^{K}q(\\mathbf{v}_{n})^{(k)}=1,\\quad\\forall n\\in\\mathbb{N}^{2},\\quad\\forall n\\in\\mathbb{N}^{2},\\quad\\forall n}\\\\ {\\mathbb{E}_{q}[\\mathbf{v}_{n}]=\\sum_{k=1}^{K}\\mathbf{v}_{n}^{(k)}q(\\mathbf{v}_{n})^{(k)}=\\mathbf{v}_{n}^{(y_{n})},\\quad\\forall n.}\\end{array}\n$$  \n\nThe first constraint holds due to the requirement of discrete \uf8f3probability density, while the second constraint controls the scope of the distribution such that model accurately predicts the target class. Suppose ${\\hat{q}}_{n}$ to be the one-hot hard probability distribution whose values are all zero except at the target index $\\hat{q}_{n}^{(y_{n})}=1$ . The second constraint is then actually $\\begin{array}{r}{\\mathbb{E}_{q}[\\mathbf{v}_{n}]=\\sum_{k=1}^{K}\\mathbf{v}_{n}^{(k)}\\hat{q}_{n}^{(k)}=\\mathbf{v}_{n}^{(y_{n})}}\\end{array}$ . This is equivalent to making model predict the correct label $y_{n}$ . By applying Lagrangian multipliers $\\{\\alpha_{1,i}\\}_{i=1}^{N}$ and $\\{\\alpha_{2,i}\\}_{i=1}^{N}$ , it gives  \n\n$$\n\\begin{array}{l}{\\displaystyle\\mathcal{L}_{T}=\\mathcal{L}_{1}+\\sum_{n=1}^{N}\\alpha_{1,n}\\left(\\sum_{k=1}^{K}q(\\mathbf{v}_{n})^{(k)}-1\\right)}\\\\ {\\displaystyle+\\sum_{n=1}^{N}\\alpha_{2,n}\\left(\\sum_{k=1}^{K}\\mathbf{v}_{n}^{(k)}q(\\mathbf{v}_{n})^{(k)}-\\mathbf{v}_{n}^{(y_{n})}\\right).}\\end{array}\n$$  \n\nTaking the partial derivative with respective to $\\alpha_{1,n}$ and $\\alpha_{2,n}$ yields back the constraints. In contrast, taking the derivative with respective to $q(\\mathbf{v}_{n})^{(k)}$ gives  \n\n$$\n\\frac{\\partial\\mathcal{L}_{T}}{\\partial q(\\mathbf{v}_{n})^{(k)}}=-1-\\log q(\\mathbf{v}_{n})^{(k)}+\\alpha_{1,n}+\\alpha_{2,n}\\mathbf{v}_{n}^{(k)},\n$$  \n\nwhich leads to a solution by making the derivative zero:  \n\n$$\nq(\\mathbf{v}_{n})^{(k)}=\\exp\\left(\\alpha_{2,n}\\mathbf{v}_{n}^{(k)}\\right)/Z_{T},\n$$  \n\nwhere $\\begin{array}{r}{Z_{T}=\\exp\\left(1-\\alpha_{1,n}\\right)=\\sum_{m=1}^{K}\\exp\\left(\\alpha_{2,n}\\mathbf{v}_{n}^{(m)}\\right)}\\end{array}$ \u0010\u0011is the partition function to fulfill the normalization condition."}, {"ref_id": "454918605807027478", "chunk_id": "0", "score": 0.294921875, "text": "# (2) Soft Label-Based KD for OD\nIn order to improve the performance of student models and reduce their dependence on ground truth labels, related works typically guide student models using soft labels or pseudo-labels output by teacher models [16] ,[19] ,[78] . In addition, we need to successfully use the predictions of teacher models as soft labels for student models, and we should also know how to reasonably assign the labels of teacher models, even the hard labels. For example,LADin [72] cansignificantlyimprovetheperformance of a student model with a lightweight teacher model, and the performance of the student model will in fact be superior to that of its teacher. LAD requires a trained teacher to provide guidance (soft and hard labels) for its student. However, in practice, it is difficult to obtain excellent teacher models capable of providing effective guidance information. Therefore, Zhang et al. [57] proposed a self-distillation framework for OD, called label-guided self-distillation (LGD), which can generate the required guidance information using only the internal relationship between objects. Moreover, there are also some other methods that have tried to obtain better OD results by combining soft and hard labels [39] ,[71] .  \n\nIn label guided KD-based OD models, pseudo-labels can also play important roles in the training of student models. For example, Feng et al. [97] proposed an adaptive pseudo-label selection strategy to selectively calculate the distillation loss using the pseudo-labels. The student model carries out feature learning by using pseudo-labels of the teacher model first, and fine-tunes the network according to the ground truth labels. This KD strategy can not only reduce the demand for labeled samples [59] ,[60] , but also achieves higher OD performance than traditional methods based on the single-stage KD strategy [142] .In addition, for KD-based 3D OD models, using high-beam point clouds with pseudo-labels to train student models is a good way to solve the problem of high-cost 3D sample data labeling; thus, low-beam pseudo-LiDAR needs to be generated by down-sampling high-beam point clouds [108] .  \n\n2) Feature Distillation for OD: Another KD strategy for feature distillation in intermediate feature layers can also effectively improve the performance of OD models. In this subsection, we will provide an overview of different feature distillation strategies embedded in OD models, including the basic ideas of feature distillation, full trust feature distillation, selective trust feature distillation, and so on.  \n\nIn general, the methods based on feature distillation use the features output from the middle layers of teacher models to supervise the training of student models, so that the student models can mimic the features output from the teacher models to the greatest extent possible. The essence of the idea is to continuously optimize the loss function consisting of both the activation functions of the teacher and student models\u2019 feature layers. Therefore, feature distillation is carried out by using a loss function to train the student model. A general formulation of the loss function for feature distillation has been provided by Gou et al. [10] :  \n\n$$\n{\\cal L}={\\cal L}_{F e a D}(f_{t}(x),f_{s}(x))={\\cal L}_{F}(\\Phi_{t}(f_{t}(x)),\\Phi_{s}(f_{s}(x))),\n$$  \n\nwhere $f_{t}(x)$ and $f_{s}(x)$ are the output features from the middle layers of the teacher and student models respectively. Considering that the different network structures of the teacher and student models may lead to different sizes of the output features from their middle layers, a transformation function $\\Phi(.)$ is used to match these features. $L_{F}(.)$ denotes the loss function between the features of teacher and student models.  \n\nAccording to the related works on feature distillation published in recent years, there are two main distillation strategies employed for feature distillation on OD models: full trust feature distillation and selective trust feature distillation. In the below, we will provide a description of these methods in detail.  \n\n(1) Full Trust Feature Distillation  \n\nFull trust feature distillation means that the student model learns all the knowledge from the teacher model unconditionally, without considering whether the knowledge to be learned is correct or not. Methods based on full trust feature distillation can further be divided into global feature distillation and local feature distillation.  \n\nGlobal Feature Distillation is an approach in which the student model imitates the entire feature maps of the middle layers of the teacher model. For example, the fast scene text detector uses all feature maps of the teacher model to guide the training of the student model [92] . In addition, the related works in [115] ,[116] , and [93] also designed their models through the strategy of full trust feature distillation. These methods are in essence the applications of the most basic feature distillation strategy; they do not make any improvements to the feature maps, but instead simply guide the student models to learn all feature information directly from the teacher models. However, there are limitations on the capacity to improve the performance of student models by learning the global features of the teacher model indiscriminately. Therefore, researchers have gradually explored optimizing the output feature maps to further improve the performance of student models. For example, Qi et al. [24] aligned the feature maps at different resolutions using a feature pyramid, dynamically fused these features, and finally extracted these fused features from the teacher model to provide better guidance to the student model. The work of He et al. [88] ranks the feature maps by calculating the channel strengths of these feature maps in the teacher and student models for feature distillation. The feature distillation strategies proposed in [24] and [88] perform knowledge learning between the same layers of the teacher and student models; moreover, the multi-layer feature maps of the teacher model can also be used to guide the single-layer feature learning of the student model [28] ,[89] .  \n\nLocal Feature Distillation refers to the simulation of a student model learning those local features that are more helpful for the final prediction, rather than the entire feature maps of the teacher model. In recent years, an increasing number of related works have explored corresponding distillation strategies for local feature learning; these methods mainly try to learn the visual features at key locations in the feature maps. Chen et al. [70] used a region distillation strategy to train a lightweight pedestrian detector that crops features corresponding to RoI regions, after which the cropped local feature maps are used as the guidance information for the student model. In addition, the anchors in object detectors are widely used to locate the key local features for training student models [17] ,[91] ,[95] ; these anchors can also be ranked to enable the student model to learn feature maps with different significance [143] . It can be readily observedthatthelocalfeaturedistillationintheabovementioned methods is performed directly around the feature maps. The attentional mechanisms can also be used to make the student models pay more attention to the key local visual features. For example, spatial attention on feature maps is introduced for local feature distillation [144] , while the attention mechanism used to highlight foreground regions as well as contextual information is also helpful for OD [47] . Furthermore, Yang et al. [76] considered that the teacher and student models pay different levels of attention to the foreground and background, while the uneven differences in the feature maps in turn impact the effect of KD. Therefore, these authors proposed a strategy combining focal distillation and global distillation, in which focal distillation is guided to the student model by using spatial and channel attention masks during model training. The goal here is to make the student model focus only on the key pixels and channels on the feature map, thereby improving the performance of the student model. There are also some other local feature distillation strategies, such as the key proposal generation of the student and teacher models for local feature distillation [75] .  \n\nThese above mentioned OD methods based on whether global feature distillation or local feature distillation place full trust in the guidance information of the teacher model. However, it shouldbenotedthattheguidancefeatureinformationusedbythe teacher model to supervise the training of the student model may be incomplete or even incorrect, which has a negative impact on the performance improvements of the student model.  \n\n(2) Selective Trust Feature Distillation  \n\nTo address the detrimental effects on the student model of incorrectinformationprovidedbytheteachermodel,thestrategy of selective trust feature distillation is introduced in KD-based OD models. Selective trust feature distillation means that the guidance information provided by the teacher model to the student model should be selected first: in short, it is necessary to remove the incorrect information and leave only the feature information that has a positive impact on the detection performance. For example, Heo et al. [145] proposed a margin ReLu to suppress the unfavorable feature information from the teachermodel,sothatthestudentmodellearnsonlythefavorable features and thus achieves performance improvement.  \n\nIn summary, whether full trust feature distillation or selective trust feature distillation is employed, this is ultimately optimized through the loss function. However, it is challenging to quickly and accurately select the substantial and beneficial features from the large amount of prior guidance information provided by the teacher model. There are thus many scientific problems worthy of further study associated with OD models based on selective trust KD.  \n\n3) Various Network Structures of Teacher-Student Models: ThissectionwillexploretheKDstrategyfromanewperspective. Specifically, we found that different network structures can be designed for the teacher and student models respectively, and the knowledge extracted from multimodal data can facilitate significant performance improvements on the part of the student model. Therefore, this section will summarize and analyze the network structures of the teacher-student models and the feature learning from multimodal data.  \n\nItisacommonKDstrategythattheteacherandstudentmodels have similar network structures. Many different backbone networks have been adopted by teacher and student models, such as ResNet [18] ,[58] , ResNext [146] , SSD [80] , VGG [146] ,and so on. In addition, some studies do not directly use the classical network model as the backbone network, but instead adjust existing networks [32] ,[138] . However, these methods areKD-basedODmodelsusingtraditionaldistillationstrategies, whether they directly use typical networks or employ adjusted networks as the backbone of the teacher and student models. Most KD-based OD models with similar teacher-student network structures extract the knowledge from single-modal data (RGB images), although there also are some methods that try to learn knowledge from other modalities for guiding the lightweight student model. For example, the student model can learn semantic knowledge provided by the teacher model [111] ,learn the textual and visual features extracted by the teacher model on text information [147] , or jointly learn the visual features from RGB images and heat-like images under the guidance of the trained teacher model on these two modes of data [14] .  \n\nFurthermore, for 3D OD, there are also common approaches involving teacher and student models using similar network structures as their backbones. For example, Wei et al. [108] opt to use the KD strategy to generate a lightweight 3D detector; here, the network structure of teacher and student models in the distillation framework is the same 3D convolutional neural network. In addition, ItKD, designed by Cho et al. [106] ,uses an autoencoder in combination with KD to improve the performance of a 3D object detector. The teacher and student models in ItKD are composed of the same backbone CenterPoint and autoencoder, and the same point cloud data is used for training the teacher and student networks. In the KD-based 3D OD tasks, multi-modal data is also used as the input of the distillation models. In methods employing this strategy, the student model is trained using 3D point cloud data, while the teacher model is trained using other modalities. For example, Qin et al. [31] proposed a cross-modal KD method, in which RGB images are used to train the teacher model and the point cloud is used to train the student model. This method aims to transfer the knowledge from the RGB domain to the point cloud domain, thereby reducing the labeling cost of 3D OD. Moreover, multimodal data can be used to train the teacher model, which is more beneficial to the performance of the student model [45] ,[109] . Multimodal data (LiDAR-image, which consists of point clouds and RGB images after segmentation) is used for training the teacher model in [45] ,[109] ; here, the student model is expected to learn the knowledge from the teacher model and to obtain the similar outputs to the teacher model using only LiDAR.  \n\n(2) Different Network Structures of Teacher and Student Models  \n\nAnother KD strategy involves the teacher and student models using different network structures as their backbones. For example, the method in [82] uses DarkNet-53 based SSD as the teacher model\u2019s backbone and MobileNet v2/ShuffleNet v1 as the student model\u2019s backbone. Su et al. [55] use ResNet-based networksastheteachermodel\u2019sbackboneandaself-built3-layer CNN as the student model\u2019s backbone. A similar strategy is used in [5] . In addition to those listed above, there are also many more similar methods with different combinations of teacher-student models. Notably, while the teacher and student models can choose various networks as their own backbones, it is necessary to choose the appropriate networks according to the specific problems to be solved, especially given the lack of capacity of the student model. Similarly, different network structures can also be used for KD-based 3D OD tasks. Sautier et al. [46] use a 2D-to-3D distillation strategy to improve 3D OD in an autonomous driving context. The backbone of the teacher model is ResNet50 trained with RGB images, and the student model uses U-Net as its backbone trained with LiDAR data. The final experiments show that the model with this strategy outperforms the state-of-the-art methods.  \n\n  \nFig. 5. The structure of multiple teacher models guiding one student model to learn the knowledge.  \n\nThis section lists several KD-based OD models using various teacher-student model network structures. Similar/different networks are used as the backbones of the teacher and student models to extract features from multimodal data. Through analysis of the existing relevant methods, we determine that the methods using different network structures as the backbone of the teacher model to extract knowledge from the multi-modal data have relatively more advantages, when it comes to guiding the feature learning of the student models. However, there are no fixed KD strategies of combining teacher-student model structures and multimodal data, we should design/choose appropriate networks for KD according to the specific tasks.  \n\n4) Multiple Teacher Models: KD is similar to the learning processes used by humans. The traditional technology of KD involves a teacher model guiding a student model to learn the knowledge. Notably, however, human teaching activities involve morepatternsthantraditionalKD.Therefore,severalKDmodels based on human teaching patterns have been proposed.  \n\n(1) Multiple Teachers Guiding One Student  \n\nAs previously discussed, the first commonly used human teaching mode is that in which multiple teachers teach one student. For example, if teachers with different areas of specialization all teach one student, the student can acquire higher-quality knowledge. Similarly, we can use different types of teacher models to learn different knowledge from large-scale datasets, then try to transfer the learned knowledge to one student model, so as that this student model can learn more comprehensive and significant visual features, as shown in Fig. 5 . For example, in [37] ,[53] ,[54] , multiple teacher models are used to guide one student model to improve its OD performance. There are some key differences between these three works: Kuang et al. [53] carried out weighted fusion of different teacher models to improve the accuracy of OD networks; Chen et al. [37] used two teacher networks trained with different strategies to ensure that the knowledge could be fully transferred to the student network; Li et al. [54] designed an asymmetric two-path learning framework to train the student model.  \n\n(2) One Teacher Guiding Multiple Students  \n\nThe second commonly used teaching pattern is that one teacher teaches multiple students. For example, multiple student models are guided by one teacher model, after which one student model with the best performance is selected from all the student models. The work in [65] uses a KD framework in which one teacher model guides multiple student models to solve the problem of Siamese trackers being limited by high cost."}], "task_step_question_answer": "\u5728\u6a21\u578b\u84b8\u998f\u4e2d\uff0c\u6e29\u5ea6\u8c03\u8282\u7684\u6570\u5b66\u5f62\u5f0f\u901a\u8fc7softmax\u51fd\u6570\u7684\u6269\u5c55\u6765\u7406\u89e3\uff0c\u5f15\u5165\u6e29\u5ea6\u53c2\u6570\u03c4\u540e\uff0csoftmax\u51fd\u6570\u7684\u8f93\u51fa\u6982\u7387\u5206\u5e03\u53ef\u4ee5\u8868\u793a\u4e3a\uff1a\n\\[\np_i = \\frac{\\exp(z_i / \\tau)}{\\sum_j \\exp(z_j / \\tau)}\n\\]\n\u5176\u4e2d\uff0c\\( p_i \\) \u662f\u7b2ci\u4e2a\u7c7b\u522b\u7684\u6982\u7387\uff0c\\( z_i \\) \u662flogits\uff0c\u03c4\u662f\u6e29\u5ea6\u53c2\u6570\u3002\u6e29\u5ea6\u53c2\u6570\u03c4\u7684\u4f5c\u7528\u673a\u5236\u5728\u4e8e\u5b83\u80fd\u591f\u5e73\u6ed1\u6216\u9510\u5316\u6982\u7387\u5206\u5e03\u3002\u5f53\u03c4\u8f83\u5927\u65f6\uff0csoftmax\u8f93\u51fa\u7684\u6982\u7387\u5206\u5e03\u4f1a\u66f4\u52a0\u5e73\u6ed1\uff0c\u8fd9\u610f\u5473\u7740\u6559\u5e08\u6a21\u578b\u5bf9\u5404\u4e2a\u7c7b\u522b\u7684\u7f6e\u4fe1\u5ea6\u5dee\u5f02\u51cf\u5c0f\uff0c\u5b66\u751f\u6a21\u578b\u53ef\u4ee5\u4ece\u66f4\u591a\u7684\u7c7b\u522b\u4e2d\u5b66\u4e60\u77e5\u8bc6\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u5173\u6ce8\u6700\u53ef\u80fd\u7684\u7c7b\u522b\u3002\u76f8\u53cd\uff0c\u5f53\u03c4\u8f83\u5c0f\u65f6\uff0c\u6982\u7387\u5206\u5e03\u4f1a\u66f4\u52a0\u5c16\u9510\uff0c\u5b66\u751f\u6a21\u578b\u4f1a\u66f4\u591a\u5730\u5173\u6ce8\u6559\u5e08\u6a21\u578b\u8ba4\u4e3a\u6700\u53ef\u80fd\u7684\u7c7b\u522b\u3002\u8fd9\u79cd\u5e73\u6ed1\u6216\u9510\u5316\u7684\u8fc7\u7a0b\u5e2e\u52a9\u5b66\u751f\u6a21\u578b\u5b66\u4e60\u5230\u66f4\u591a\u7684\u201c\u6697\u77e5\u8bc6\u201d\uff0c\u5373\u7c7b\u522b\u4e4b\u95f4\u7684\u9690\u542b\u5173\u7cfb\u3002\n\n\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u6e29\u5ea6\u7684\u9009\u62e9\u901a\u5e38\u4f9d\u8d56\u4e8e\u4efb\u52a1\u7684\u5177\u4f53\u9700\u6c42\u3002\u5e38\u89c1\u7684\u6e29\u5ea6\u503c\u8303\u56f4\u5728\u03c4=1\u5230\u03c4=20\u4e4b\u95f4\u3002\u5bf9\u4e8e\u9700\u8981\u6a21\u578b\u5b66\u4e60\u66f4\u591a\u201c\u6697\u77e5\u8bc6\u201d\u7684\u4efb\u52a1\uff0c\u8f83\u9ad8\u7684\u6e29\u5ea6\uff08\u5982\u03c4=10\uff09\u53ef\u80fd\u66f4\u4e3a\u5408\u9002\uff1b\u800c\u5bf9\u4e8e\u9700\u8981\u6a21\u578b\u4e13\u6ce8\u4e8e\u9ad8\u7f6e\u4fe1\u5ea6\u9884\u6d4b\u7684\u4efb\u52a1\uff0c\u8f83\u4f4e\u7684\u6e29\u5ea6\uff08\u5982\u03c4=1\uff09\u53ef\u80fd\u66f4\u4e3a\u6709\u6548\u3002\u4f8b\u5982\uff0c\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0c\u8f83\u9ad8\u7684\u6e29\u5ea6\u53ef\u4ee5\u5e2e\u52a9\u6a21\u578b\u66f4\u597d\u5730\u5904\u7406\u7c7b\u522b\u4e4b\u95f4\u7684\u6a21\u7cca\u8fb9\u754c\uff0c\u800c\u5728\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u4e2d\uff0c\u8f83\u4f4e\u7684\u6e29\u5ea6\u53ef\u80fd\u66f4\u6709\u5229\u4e8e\u6a21\u578b\u4e13\u6ce8\u4e8e\u9ad8\u7f6e\u4fe1\u5ea6\u7684\u76ee\u6807\u68c0\u6d4b\u3002\n\n\u635f\u5931\u51fd\u6570\u7684\u8bbe\u8ba1\u5728\u6a21\u578b\u84b8\u998f\u4e2d\u4e5f\u81f3\u5173\u91cd\u8981\u3002\u901a\u5e38\uff0c\u635f\u5931\u51fd\u6570\u4f1a\u7ed3\u5408\u786c\u6807\u7b7e\u635f\u5931\u548c\u8f6f\u6807\u7b7e\u635f\u5931\uff0c\u5176\u5f62\u5f0f\u53ef\u4ee5\u8868\u793a\u4e3a\uff1a\n\\[\n\\mathcal{L} = \\alpha \\cdot \\mathcal{L}_{\\text{hard}} + (1 - \\alpha) \\cdot \\mathcal{L}_{\\text{soft}}\n\\]\n\u5176\u4e2d\uff0c\\( \\mathcal{L}_{\\text{hard}} \\) \u662f\u786c\u6807\u7b7e\u635f\u5931\uff0c\\( \\mathcal{L}_{\\text{soft}} \\) \u662f\u8f6f\u6807\u7b7e\u635f\u5931\uff0c\u03b1\u662f\u6743\u91cd\u53c2\u6570\u3002\u901a\u8fc7\u8c03\u6574\u03b1\u7684\u503c\uff0c\u53ef\u4ee5\u63a7\u5236\u786c\u6807\u7b7e\u548c\u8f6f\u6807\u7b7e\u5728\u8bad\u7ec3\u4e2d\u7684\u76f8\u5bf9\u91cd\u8981\u6027\u3002\u4f8b\u5982\uff0c\u5728\u6570\u636e\u6807\u6ce8\u8f83\u4e3a\u51c6\u786e\u7684\u4efb\u52a1\u4e2d\uff0c\u53ef\u4ee5\u9002\u5f53\u589e\u52a0\u03b1\u7684\u503c\uff0c\u4f7f\u6a21\u578b\u66f4\u4f9d\u8d56\u4e8e\u786c\u6807\u7b7e\uff1b\u800c\u5728\u6570\u636e\u6807\u6ce8\u8f83\u4e3a\u6a21\u7cca\u6216\u566a\u58f0\u8f83\u591a\u7684\u4efb\u52a1\u4e2d\uff0c\u53ef\u4ee5\u9002\u5f53\u51cf\u5c11\u03b1\u7684\u503c\uff0c\u4f7f\u6a21\u578b\u66f4\u4f9d\u8d56\u4e8e\u8f6f\u6807\u7b7e\u3002\n\n\u6e29\u5ea6\u8c03\u8282\u4e0e\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u7684\u5173\u7cfb\u53ef\u4ee5\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u636e\u8fdb\u4e00\u6b65\u8bf4\u660e\u3002\u7814\u7a76\u8868\u660e\uff0c\u5408\u9002\u7684\u6e29\u5ea6\u8c03\u8282\u80fd\u591f\u4f7f\u6a21\u578b\u5728\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u66f4\u597d\u5730\u5e73\u8861\u9ad8\u7f6e\u4fe1\u5ea6\u9884\u6d4b\u548c\u6574\u4f53\u6982\u7387\u5206\u5e03\uff0c\u4ece\u800c\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002\u4f8b\u5982\uff0c\u5728\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0c\u901a\u8fc7\u8c03\u6574\u03c4\u503c\uff0c\u53ef\u4ee5\u89c2\u5bdf\u5230\u6a21\u578b\u5728\u9a8c\u8bc1\u96c6\u4e0a\u7684\u8868\u73b0\u663e\u8457\u63d0\u5347\u3002\u7136\u800c\uff0c\u6e29\u5ea6\u8c03\u8282\u5e76\u975e\u5728\u6240\u6709\u4efb\u52a1\u4e2d\u90fd\u80fd\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0c\u8fc7\u9ad8\u7684\u6e29\u5ea6\u53ef\u80fd\u5bfc\u81f4\u6a21\u578b\u5b66\u4e60\u5230\u8fc7\u591a\u7684\u566a\u58f0\u4fe1\u606f\uff0c\u800c\u8fc7\u4f4e\u7684\u6e29\u5ea6\u5219\u53ef\u80fd\u4f7f\u6a21\u578b\u8fc7\u4e8e\u4f9d\u8d56\u5355\u4e00\u9884\u6d4b\uff0c\u5931\u53bb\u6cdb\u5316\u80fd\u529b\u3002\u4f8b\u5982\uff0c\u5728\u6570\u636e\u5206\u5e03\u975e\u5e38\u4e0d\u5e73\u8861\u7684\u4efb\u52a1\u4e2d\uff0c\u6e29\u5ea6\u8c03\u8282\u53ef\u80fd\u65e0\u6cd5\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u56e0\u4e3a\u6a21\u578b\u53ef\u80fd\u8fc7\u4e8e\u5173\u6ce8\u5c11\u6570\u7c7b\u522b\uff0c\u800c\u5ffd\u7565\u4e86\u591a\u6570\u7c7b\u522b\u3002", "ref_task_step_id": ""}, "__type__": "task_step_node"}, "1bc44462-cdfe-4df1-bfb3-cb4ec397081c": {"__data__": {"id_": "1bc44462-cdfe-4df1-bfb3-cb4ec397081c", "metadata": {}, "relationships": {}, "hash": "", "start_task_context": "\u6a21\u578b\u84b8\u998f\u7684\u662f\u4ec0\u4e48\uff1f", "aemo_representation_context": "\u6a21\u578b\u84b8\u998f\uff08Model Distillation\uff09\u662f\u4e00\u79cd\u5728\u673a\u5668\u5b66\u4e60\u9886\u57df\u5e7f\u6cdb\u4f7f\u7528\u7684\u6280\u672f\uff0c\u4e3b\u8981\u7528\u4e8e\u5c06\u4e00\u4e2a\u5927\u578b\u3001\u590d\u6742\u4e14\u9ad8\u6027\u80fd\u7684\u6a21\u578b\uff08\u901a\u5e38\u79f0\u4e3a\u6559\u5e08\u6a21\u578b\uff09\u7684\u77e5\u8bc6\u8fc1\u79fb\u5230\u4e00\u4e2a\u66f4\u5c0f\u3001\u66f4\u7b80\u5355\u4e14\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u7684\u6a21\u578b\uff08\u901a\u5e38\u79f0\u4e3a\u5b66\u751f\u6a21\u578b\uff09\u4e2d\u3002\u4ee5\u4e0b\u662f\u6a21\u578b\u84b8\u998f\u7684\u8be6\u7ec6\u5206\u89e3\uff1a\n\n### 1. **\u57fa\u672c\u6982\u5ff5**\n- **\u6559\u5e08\u6a21\u578b\uff08Teacher Model\uff09**\uff1a\u4e00\u4e2a\u8bad\u7ec3\u597d\u7684\u3001\u6027\u80fd\u4f18\u5f02\u4f46\u53ef\u80fd\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u7684\u6a21\u578b\u3002\n- **\u5b66\u751f\u6a21\u578b\uff08Student Model\uff09**\uff1a\u4e00\u4e2a\u66f4\u5c0f\u3001\u66f4\u7b80\u5355\u3001\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u7684\u6a21\u578b\uff0c\u76ee\u6807\u662f\u6a21\u4eff\u6559\u5e08\u6a21\u578b\u7684\u8868\u73b0\u3002\n\n### 2. **\u84b8\u998f\u8fc7\u7a0b**\n- **\u8bad\u7ec3\u6559\u5e08\u6a21\u578b**\uff1a\u9996\u5148\uff0c\u4f7f\u7528\u5927\u91cf\u6570\u636e\u8bad\u7ec3\u4e00\u4e2a\u9ad8\u6027\u80fd\u7684\u6559\u5e08\u6a21\u578b\u3002\n- **\u751f\u6210\u8f6f\u6807\u7b7e**\uff1a\u5229\u7528\u6559\u5e08\u6a21\u578b\u5bf9\u8bad\u7ec3\u6570\u636e\u8fdb\u884c\u9884\u6d4b\uff0c\u751f\u6210\u8f6f\u6807\u7b7e\uff08\u5373\u6982\u7387\u5206\u5e03\uff09\uff0c\u800c\u4e0d\u662f\u7b80\u5355\u7684\u786c\u6807\u7b7e\uff08\u5373\u7c7b\u522b\u6807\u7b7e\uff09\u3002\n- **\u8bad\u7ec3\u5b66\u751f\u6a21\u578b**\uff1a\u4f7f\u7528\u539f\u59cb\u6570\u636e\u548c\u6559\u5e08\u6a21\u578b\u751f\u6210\u7684\u8f6f\u6807\u7b7e\u6765\u8bad\u7ec3\u5b66\u751f\u6a21\u578b\u3002\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u5b66\u751f\u6a21\u578b\u7684\u635f\u5931\u51fd\u6570\u901a\u5e38\u5305\u62ec\u4e24\u90e8\u5206\uff1a\n  - **\u786c\u6807\u7b7e\u635f\u5931**\uff1a\u5b66\u751f\u6a21\u578b\u9884\u6d4b\u4e0e\u771f\u5b9e\u6807\u7b7e\u4e4b\u95f4\u7684\u5dee\u5f02\u3002\n  - **\u8f6f\u6807\u7b7e\u635f\u5931**\uff1a\u5b66\u751f\u6a21\u578b\u9884\u6d4b\u4e0e\u6559\u5e08\u6a21\u578b\u9884\u6d4b\u4e4b\u95f4\u7684\u5dee\u5f02\u3002\n\n### 3. **\u5173\u952e\u6280\u672f\u70b9**\n- **\u6e29\u5ea6\u8c03\u8282**\uff1a\u5728\u751f\u6210\u8f6f\u6807\u7b7e\u65f6\uff0c\u901a\u5e38\u5f15\u5165\u4e00\u4e2a\u6e29\u5ea6\u53c2\u6570\uff08Temperature\uff09\u6765\u5e73\u6ed1\u6982\u7387\u5206\u5e03\uff0c\u4f7f\u5f97\u8f6f\u6807\u7b7e\u5305\u542b\u66f4\u591a\u7684\u4fe1\u606f\u3002\n- **\u635f\u5931\u51fd\u6570\u8bbe\u8ba1**\uff1a\u7ed3\u5408\u786c\u6807\u7b7e\u635f\u5931\u548c\u8f6f\u6807\u7b7e\u635f\u5931\uff0c\u5e73\u8861\u4e24\u8005\u7684\u91cd\u8981\u6027\u3002\n\n### 4. **\u5e94\u7528\u573a\u666f**\n- **\u6a21\u578b\u538b\u7f29**\uff1a\u5c06\u5927\u578b\u6a21\u578b\u538b\u7f29\u6210\u5c0f\u578b\u6a21\u578b\uff0c\u4fbf\u4e8e\u90e8\u7f72\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8bbe\u5907\u4e0a\u3002\n- **\u77e5\u8bc6\u8fc1\u79fb**\uff1a\u5c06\u4e00\u4e2a\u4efb\u52a1\u4e0a\u5b66\u5230\u7684\u77e5\u8bc6\u8fc1\u79fb\u5230\u53e6\u4e00\u4e2a\u4efb\u52a1\u4e0a\u3002\n- **\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b**\uff1a\u901a\u8fc7\u5b66\u4e60\u6559\u5e08\u6a21\u578b\u7684\u8f93\u51fa\uff0c\u5b66\u751f\u6a21\u578b\u53ef\u80fd\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u5177\u6709\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002\n\n### 5. **\u4f18\u52bf\u4e0e\u5c40\u9650\u6027**\n- **\u4f18\u52bf**\uff1a\n  - **\u8ba1\u7b97\u6548\u7387\u9ad8**\uff1a\u5b66\u751f\u6a21\u578b\u66f4\u5c0f\uff0c\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u4f4e\u3002\n  - **\u6027\u80fd\u63a5\u8fd1**\uff1a\u5b66\u751f\u6a21\u578b\u80fd\u63a5\u8fd1\u6559\u5e08\u6a21\u578b\u7684\u6027\u80fd\u3002\n- **\u5c40\u9650\u6027**\uff1a\n  - **\u6027\u80fd\u5dee\u8ddd**\uff1a\u5b66\u751f\u6a21\u578b\u6027\u80fd\u53ef\u80fd\u4ecd\u7565\u4f4e\u4e8e\u6559\u5e08\u6a21\u578b\u3002\n  - **\u8bad\u7ec3\u590d\u6742**\uff1a\u9700\u8981\u5148\u8bad\u7ec3\u4e00\u4e2a\u9ad8\u6027\u80fd\u7684\u6559\u5e08\u6a21\u578b\u3002\n\n### 6. **\u6700\u65b0\u8fdb\u5c55**\n- **\u591a\u4efb\u52a1\u84b8\u998f**\uff1a\u5728\u4e00\u4e2a\u5b66\u751f\u6a21\u578b\u4e0a\u540c\u65f6\u84b8\u998f\u591a\u4e2a\u6559\u5e08\u6a21\u578b\u7684\u77e5\u8bc6\u3002\n- **\u81ea\u84b8\u998f**\uff1a\u6a21\u578b\u81ea\u8eab\u4f5c\u4e3a\u6559\u5e08\u6a21\u578b\u8fdb\u884c\u84b8\u998f\u3002\n- **\u8de8\u6a21\u6001\u84b8\u998f**\uff1a\u5728\u4e0d\u540c\u6a21\u6001\uff08\u5982\u56fe\u50cf\u548c\u6587\u672c\uff09\u4e4b\u95f4\u8fdb\u884c\u77e5\u8bc6\u84b8\u998f\u3002\n\n### 7. **\u672a\u6765\u7814\u7a76\u65b9\u5411**\n- **\u66f4\u9ad8\u6548\u7684\u84b8\u998f\u65b9\u6cd5**\uff1a\u63a2\u7d22\u66f4\u6709\u6548\u7684\u77e5\u8bc6\u8fc1\u79fb\u6280\u672f\u3002\n- **\u6cdb\u5316\u80fd\u529b\u7684\u63d0\u5347**\uff1a\u7814\u7a76\u5982\u4f55\u901a\u8fc7\u84b8\u998f\u63d0\u9ad8\u6a21\u578b\u5728\u672a\u89c1\u6570\u636e\u4e0a\u7684\u8868\u73b0\u3002\n- **\u591a\u6a21\u6001\u84b8\u998f\u7684\u62d3\u5c55**\uff1a\u63a2\u7d22\u66f4\u591a\u8de8\u6a21\u6001\u77e5\u8bc6\u84b8\u998f\u7684\u5e94\u7528\u573a\u666f\u3002\n\n\u901a\u8fc7\u4ee5\u4e0a\u6b65\u9aa4\uff0c\u53ef\u4ee5\u5168\u9762\u7406\u89e3\u6a21\u578b\u84b8\u998f\u7684\u6982\u5ff5\u3001\u8fc7\u7a0b\u3001\u6280\u672f\u7ec6\u8282\u3001\u5e94\u7528\u573a\u666f\u53ca\u5176\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u7814\u7a76\u4e2d\u7684\u91cd\u8981\u6027\u3002\u6a21\u578b\u84b8\u998f\u4e0d\u4ec5\u662f\u4e00\u79cd\u6709\u6548\u7684\u6a21\u578b\u538b\u7f29\u6280\u672f\uff0c\u4e5f\u662f\u77e5\u8bc6\u8fc1\u79fb\u548c\u63d0\u5347\u6a21\u578b\u6027\u80fd\u7684\u91cd\u8981\u624b\u6bb5\u3002", "task_step_name": "\u5e94\u7528\u573a\u666f", "task_step_description": "\u6a21\u578b\u538b\u7f29\uff1a\u5c06\u5927\u578b\u6a21\u578b\u538b\u7f29\u6210\u5c0f\u578b\u6a21\u578b\uff0c\u4fbf\u4e8e\u90e8\u7f72\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8bbe\u5907\u4e0a\u3002\u77e5\u8bc6\u8fc1\u79fb\uff1a\u5c06\u4e00\u4e2a\u4efb\u52a1\u4e0a\u5b66\u5230\u7684\u77e5\u8bc6\u8fc1\u79fb\u5230\u53e6\u4e00\u4e2a\u4efb\u52a1\u4e0a\u3002\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\uff1a\u901a\u8fc7\u5b66\u4e60\u6559\u5e08\u6a21\u578b\u7684\u8f93\u51fa\uff0c\u5b66\u751f\u6a21\u578b\u53ef\u80fd\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u5177\u6709\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "task_step_level": "3", "task_step_question": "### \u95ee\u9898\n\n\u5728\u6a21\u578b\u84b8\u998f\u7684\u5e94\u7528\u573a\u666f\u4e2d\uff0c\u5982\u4f55\u901a\u8fc7\u77e5\u8bc6\u8fc1\u79fb\u5c06\u4e00\u4e2a\u4efb\u52a1\u4e0a\u5b66\u5230\u7684\u77e5\u8bc6\u6709\u6548\u5730\u8fc1\u79fb\u5230\u53e6\u4e00\u4e2a\u4efb\u52a1\u4e0a\uff0c\u5e76\u786e\u4fdd\u5b66\u751f\u6a21\u578b\u5728\u76ee\u6807\u4efb\u52a1\u4e0a\u5177\u6709\u8f83\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff1f", "task_step_question_context": [{"ref_id": "454846429944424370", "chunk_id": "5", "score": 0.6015625, "text": "# 2 RELATED WORK\nKnowledge Distillation. Knowledge distillation (KD) has been actively studied for model compression in various fields [ 5 ,11 ,17 ,37 ,48 ,55 ]. KD transfers the knowledge captured by a teacher model through large capacity into a lightweight student model, significantly lowering the inference cost while maintaining comparable performance. Pointing out that the knowledge from a single teacher model is insufficient to provide accurate supervision, many recent studies [ 8 ,28 ,42 ,45 ,50 ,51 ,54 ] employ multiple teacher models and show great effectiveness in further improving a student model. Notably, the state-of-the-art methods [ 8 ,45 ,51 ] exploit heterogeneous teacher models varying in configurations, architectures, loss functions, and many other factors to incorporate their complementary knowledge, which can provide more comprehensive guidance than a single view from a single or homogeneous teacher model. Knowledge Distillation for Ranking. KD has been also studied for ranking problems. Many studies [ 4 ,20 ,22 ,40 ,54 ] transfer pointwise importance on each user-item pair (or query-document pair). However, the point-wise approach cannot consider the relations of multiple items simultaneously, which leads to the limited ranking performance [ 5 ,15 ,41 ]. Recent methods [ 5 ,15 ,25 ,34 ,41 ,52 ] formulate the distillation process as a ranking matching task. They utilize the ranking orders from the teacher as supervision and train the student to preserve the teacher\u2019s permutation. By directly transferring the ranking knowledge, this approach has shown state-of-the-art performance in various ranking-oriented applications such as top$K$ recommendation [ 15 ,16 ,25 ,41 ], document retrieval [ 34 ,52 ], and person identification [ 5 ]. Further, the ranking matching approach can be flexibly applied to knowledge transfer between heterogeneous models having distinct output score distributions to which the point-wise approach cannot be directly applied [19].  \n\nEasy-to-hard Learning. Inspired by the learning process of humans, easy-to-hard learning has been extensively studied in various fields of machine learning [ 2 ,6 ,13 ,21 ,27 ,29 ,43 ,46 ]. It has been widely used when direct optimization of a non-convex objective function may converge to poor local minima and has been proven to play an important role in achieving a better generalization [ 2 ]. Curriculum learning [ 2 ,43 ] trains a model by gradually including data samples in ascending order of difficulty defined by prior knowledge. On the other hand, self-paced learning [21] makes the curriculum dynamically adjusted during the training, usually based on training loss [ 21 ] or performance on the validation set [ 6 ,49 ]. The easy-tohard learning has been applied to KD to improve the distillation efficacy in computer vision [ 14 ,39 ] and natural language processing [52 ,55 ]. [ 3 ,14 ,39 ] exploit the teacher\u2019s optimization route to form a curriculum for the student, [ 52 ] gradually includes an increasing number of fine-grained document pairs during the training.  \n\nRemarks. The existing KD methods for RS focus on distillation from a homogeneous teacher that has the same model type to the student model. Distillation from heterogeneous teachers, which have distinct architectures and learning objectives to the student model, has not been studied well. In this work, we show the necessity and difficulty of distilling the ensemble of heterogeneous teachers and apply the easy-to-hard learning to cope with the problem. Further, the prior KD works with the easy-to-hard learning focus on classification [ 14 ,31 ,39 ] or rely on domain-specific features [ 55 ], which makes them hard to apply to the ranking problem and recommender system. Our work provides a solution tailored to compress ranking models by distilling an easy-to-hard sequence of ranking knowledge considering the student\u2019s learning state.\n\n# 3 PRELIMINARIES\n\n# 3.1 Problem Formulation\nLet $\\mathbf{\\nabla}\\mathcal{U}$ and $\\boldsymbol{\\mathcal{I}}$ denote the user and item sets, respectively. Given implicit user-item interaction (e.g., click) history, a recommendation model $f:\\mathcal{U}\\times\\mathcal{I}\\rightarrow\\mathbb{R}$ learns the ranking score of each user-item pair. Based on the predicted scores, the recommender system provides a ranked list of top $\\cal{K}$ unobserved items for each user, called as top$K$ recommendation. Given a set of cumbersome teacher models $\\bar{\\mathcal{F}}=\\{f^{1},f^{2},...,f^{M}\\}$ , our goal is to effectively compress an ensemble of the teachers into a lightweight student model $f$ . The student model has a significantly reduced computational cost for inference, and thus it is more suitable for real-time services and resourceconstrained environments. We pursue a model-agnostic solution, which enables any kind of recommendation model can be flexibly used for both teacher and student, allowing service providers to use any preferred model according to their environments.  \n\nWe exploit heterogeneous teacher models with various architectures and loss functions. In this work, we choose six representative types of models extensively studied for RS: MF (Matrix Factorization) [ 36 ], ML (Metric Learning) [ 12 ], DNN (Deep Neural Network) [10 ], GNN (Graph Neural Network) [ 9 ], AE (AutoEncoder) [ 26 ], IAE (Item-based AE) [ 38 ]. A detailed analysis of the teacher models and their ensemble is provided in Appendix A.2.  \n\nNotations. Given a ranked list (i.e., permutation of items) $\\pi$ ,$\\pi_{k}$ denotes the $k$ -th item in $\\pi$ , and $r(\\pi,i)$ denotes the ranking of item \ud835\udc56in $\\pi$ where a lower value indicates a higher position, i.e., $r(\\pi,i)=0$ is the highest ranking. Note that $\\pi$ is defined for each user $u$ . For notational simplicity, we omit $u$ from $\\pi$ throughout the paper."}, {"ref_id": "454845510461164774", "chunk_id": "2", "score": 0.578125, "text": "# 2 RELATED WORK\n\n# 2.1 Knowledge distillation\nThe concept of knowledge distillation is introduced by Hinton et al. [5 ] based on a teacher-student framework. This method transfers knowledge from the trained teacher to the student network. Recently, it has been applied mainly to two areas: model compression [ 13 ] and knowledge transfer [ 14 ]. For model compression, a compact small student model is trained to mimic the pre-trained cumbersome teacher model.  \n\nMost knowledge distillation methods explore distilled knowledge in order to guide the student network, including instance feature, instance feature relationship and feature space transformation, etc. For instance feature, the related methods [ 5 ], [ 15 ] in the early time distill logits at the end of the network. The logits reflect the class distribution and contain more information than one-hot label. In this manner, the student network can be improved by learning more information. After that, features containing richer spatial information from intermediate layers [ 16 ], [ 17 ], [ 18 ] are extracted as the distilled knowledge. For example, FitNet [ 16 ]extracts the feature maps of the intermediate layers as well as the final output to teach the student network. Zagoruyko et al. [17 ]define Attention Transfer (AT) based on attention maps to improve the performance of the student network. More recently, structural knowledge [ 19 ], [ 20 ], [ 21 ], e.g., instance feature relationship and feature space transformation, has been presented, which represents more comprehensive information. For example, Liu et al. [19 ] propose the Instance Relationship Graph (IRG) to represent instance feature relationship and feature space transformation. It considers the geometry of the feature spaces and allows for dimensionagnostic transfer of knowledge. Yim et al. [21 ] present the Flow of Solution Procedure (FSP) to transfer the inference procedure of the teacher, which can be seen as a feature space transformation rather than the intermediate layer results.  \n\nThough the above methods have reached a milestone in knowledge distillation, all of them follow a classic single-teachersingle-student framework. Recently, some works have explored new frameworks for knowledge distillation. For instance, [ 22 ]and [ 23 ] propose a mutual learning framework where multiple peer networks learn from each other. The papers [ 24 ] and [ 25 ]present self-distillation frameworks that enable the network to distill from itself. Meta learning methods are adopted to design new frameworks. Jang et al. [26 ] make use of meta learning to determine which information should be transferred during knowledge transfer. Liu et al. [27 ] directly learn soft targets via a meta network for self-distillation. However, nearly all of the previous works perform optimization with a fixed student network. A better resource-performance trade-off can be achieved, if the architecture design is considered during training.\n\n# 2.2 Structured sparsity pruning\nIn model compression, structured sparsity pruning directly removes redundant neurons and channels rather than irregular weights. Thus, it is hardware-friendly and has been widely applied in recent years. Some works [ 28 ], [ 29 ], [ 30 ], [ 31 ], [ 32 ] aim to exploit a criterion of the filter importance and prune the unimportant filters, while some other works [ 33 ], [ 34 ], [ 35 ], [ 36 ]devote to training the network with additional sparse constraints and removing the sparse part of the network. For example, Li et al. [28 ] consider that the parameters with small $L_{1}$ -norm are less important. He et al. [29 ] calculate the geometric median of the filters within the same layer and prune the filters near the geometric median. Afterwards, HRank [ 30 ] uses rank to assess the filter importance and pruned filters with low-rank feature maps. He et al. [31 ], [ 32 ] exploit a measure of the filter importance. The unimportant filters are pruned in a soft manner. In particular, the unimportant filters are just set to be zero but they may still be updated in the next training epoch. In contrast, some works [ 33 ], [34 ] impose sparse regularization to learn the importance of each channel. Huang et al. [35 ] present a scaling factor to scale the outputs of specific structures and add sparsity constraints on these factors, so that the structure corresponding to a zero-value scaling factor can be removed. ThiNet [ 36 ] regards filter pruning as an optimization problem, and prune each filter layer using statistical information from their next layer.  \n\nMore recently, some works [ 37 ], [ 38 ], [ 39 ], [ 40 ], [ 41 ], [ 42 ]learn the sparse allocation of pruning, to meet budget constraints. For example, Gordon et al. [37 ] propose a general technique, i.e. , MorphNet, for resource-constrained optimization of DNN architecture. But the width multiplier that uniformly expands all layer sizes does not consider the difference among layers so that the resource allocation may not be optimal. ECC [ 38 ] introduces an energy consumption model to optimize the DNN compression problem and update the pruned ratio, under an energy constraint. ADMM is leveraged to solve the gradient-based learning problem. Besides, some works [ 39 ], [ 40 ], [ 41 ], [ 42 ] automatically learn the pruned ratio of each DNN layer. For instance, AMC [ 39 ] uses reinforcement learning to find a proper sparsity ratio for each layer. MetaPruning [ 41 ] constructs a meta network to directly generate the weights of the compressed model, given the sparse allocated ratios. Ning et al. [42 ] present a differentiable pruning process to learn the sparse allocation. ADMM is also used for the budgeted pruning problem. Though these previous works use complex optimization processes to meet the compression budget, no extra operation is adopted to enhance the model performance.  \n\nRecent works [ 10 ], [ 11 ], [ 12 ] combine knowledge distillation and model compression to obtain a compact model with high accuracy. Li et al. [11 ] first compress a teacher network to obtain a student network, and then add a $1\\times1$ convolution layer at the end of each block to make the student mimic the teacher. After that, they merged the $1\\times1$ convolution layer into the previous layer. Bai et al. [12 ] combine cross distillation and network pruning by adding regularization to a loss function. However, these methods either treat knowledge distillation and model compression as two independent stages or simply combine the loss functions. Without a framework-level re-design, it is difficult to achieve an optimal trade-off between performance and model complexity."}, {"ref_id": "454846429879674798", "chunk_id": "3", "score": 0.578125, "text": "# 2.2 Knowledge Distillation\nKnowledge distillation ( Hinton et al. ,2015 ) aims to transfer the dark knowledge of (commonly) a larger and better performing teacher model to a student model ( Buciluundefined et al. ,2006 ). The idea is to mix the ground-truth label with the model probability distribution of a teacher model, resulting in an adaptive version of label smoothing ( Tang et al. ,2021 ).  \n\n$$\n\\begin{array}{r}{\\mathcal{L}_{k d}=-\\displaystyle\\sum_{i=1}^{|C|}(1-\\alpha)y_{i}^{(n)}\\log P_{\\theta}(y_{i}|\\pmb{x}^{(n)})}\\\\ {+\\,\\alpha\\bar{P}_{\\phi}(y_{i}|\\pmb{x}^{(n)})\\log\\bar{P}_{\\theta}(y_{i}|\\pmb{x}^{(n)})~~~~~~~~}\\end{array}\n$$  \n\n$\\phi$ and $\\theta$ denote the parameters of a teacher model and a student model respectively. $\\bar{P}$ indicates a probability distribution smoothed with a temperature. Similar to label smoothing, $\\phi$ remains unchanged in training; thus a student model is learned to minimize the KL divergence between its probability distribution and that of the teacher model. When $\\bar{P}_{\\phi}$ follows a uniform distribution with the temperature set to 1, the loss function of knowledge distillation is identical to that of uniform label smoothing.  \n\nTraining a large teacher model can be computationally expensive; for this reason, there have been attempts to replace the teacher model with the student model itself, called self-knowledge distillation (Zhang et al. ,2019 ;Yuan et al. ,2020 ;Kim et al. ,2021 ;Zhang and Sabuncu ,2020 ). TF-KD ( Yuan et al. ,2020 ) trains a student with a pre-trained teacher that is identical to the student in terms of structure. SKD-PRT ( Kim et al. ,2021 ) utilizes the previous epoch checkpoint as a teacher with linear increase in $\\alpha$ . ( Zhang and Sabuncu ,2020 )incorporates beta distribution sampling (BETA) and self-knowledge distillation (SD), and introduce instance-specific prior label distribution. ( Yun et al. ,2020 ) utilizes self-knowledge distillation to minimize the predictive distribution of samples with the same class, encouraging consistent probability distribution within the same class.\n\n# 3 Approach\nThe core components of label smoothing are twofold: smoothing parameter $(\\alpha)$ and prior label distribution. The components determine how much to smooth the target label using which distribution, a process that requires careful choice of selection. In this section, we illustrate how to make the smoothing parameter adaptive. We also demonstrate how our adaptive smoothing parameter and self-knowledge distillation as a prior distribution act as a form of regularization with theoretical analysis on the gradients.\n\n# 3.1 Adaptive $\\alpha$\nAn intuitive and ideal way of softening the hard target is to bring dynamic nature into choosing $\\alpha$ ;a sample with low entropic level in model prediction, an indication of peaked probability distribution, receives a high smoothing parameter to further smooth the target label. In another scenario, in which high entropy of model prediction (flat distribution) is seen, the smoothing factor is decreased.  \n\nWith the intuition, our method computes the smoothing parameter on the fly during the forward propagation in training, relying on the entropic level of model probability distribution per sample, and per time step in case of sequential classification.  \n\n$$\nH(P_{\\boldsymbol{\\theta}}(\\mathbf{\\boldsymbol{y}}|\\mathbf{\\boldsymbol{x}}^{(n)}))=-\\sum_{i=1}^{|C|}P_{\\boldsymbol{\\theta}}(y_{i}|\\mathbf{\\boldsymbol{x}}^{(n)})\n$$  \n\nThe entropy quantifies the level of probability mass distributed across the label space; therefore, low entropy is an indication of overfitting and overconfidence ( Pereyra et al. ,2017 ;Meister et al. ,2020 ).  \n\nSince entropy does not have a fixed range between 0 and 1, one simple scheme is to normalize the entropy with maximum entropy $(\\log\\mathinner{|{C}|})$ .Hence, the normalization is capable of handling variable size of class set among different datasets.  \n\n$$\n\\alpha^{(n)}=1-\\frac{H(P_{\\theta}(\\pmb{y}|\\pmb{x}^{(n)}))}{\\log|C|}\n$$  \n\nWith this mechanism, a sample with high entropy is trained with low $\\alpha$ , and a sample with low entropy receives high $\\alpha$ . The computation for $\\alpha$ is excluded from the computation graph for the gradient calculation, hence, the gradient does not flow through adaptive $\\alpha^{(n)}$ .  \n\nThere are two essential benefits of adopting the adaptive smoothing parameter. As the smoothing extent is determined by its own probability mass over the output space, the hyperparameter search for $\\alpha$ is removed. Furthermore, it is strongly connected to the gradient rescaling effect on selfknowledge distillation, which will be dealt in Section 3.3 in detail.\n\n# 3.2 Self-Knowledge As A Prior\nSimilar to ( Kim et al. ,2021 ;Liu et al. ,2021 ), our regularizer loads a past student model checkpoint as teacher network parameters in the course of training, though with a core difference in the selection process. The intuition is to utilize past selfknowledge which generalizes well, thereby hindering the model from overfitting to observations in the training set.  \n\n$$\n\\phi_{t}=\\underset{\\theta_{i}\\in\\Theta_{t}}{\\operatorname{argmax}}\\,g(f(X^{\\prime};\\theta_{i}),Y^{\\prime})\n$$  \n\n$\\Theta_{t}$ is a set of past model checkpoints up to the current epoch $t$ in training, and function $f$ is a specific task, which in our work is machine translation. $X^{\\prime}$ and $Y^{\\prime}$ are sets of input and ground-truth samples from a validation dataset 2 , and the function $g$ could be any proper evaluation metric for model generalization (i.e. accuracy). Our work utilizes the $n$ -gram matching score, BLEU ( Papineni et al. ,2002 ) being the function $g$ for finding the suitable prior label distribution.  \n\nEquation 5 depicts how the selection process of a self-teacher depends on the generalization of each past epoch checkpoint. In other words, a past checkpoint with the least generalization error is utilized as the self-teacher, a source of self-knowledge, to send generalized supervision. Furthermore, at every epoch, with Equation 5 , the proposed approach replaces the self-teacher with the one with the best generalization.  \n\nCombining the adaptive smoothing parameter and self-knowledge as a prior distribution, our loss  \n\n  \nFigure 1: Overview of the proposed regularization. $d$ ,$|N|$ and $t$ are input dimension size, batch size, and current epoch respectively. Time step is not described in the figure, yet one can easily extend the above to sequential classification tasks.  \n\nfunction is as follows:  \n\n$$\n\\begin{array}{r l}&{\\mathcal{L}=-\\displaystyle\\sum_{i=1}^{|C|}(1-\\alpha^{(n)})y_{i}^{(n)}\\log P_{\\theta}(y_{i}|\\pmb{x}^{(n)})}\\\\ &{\\quad\\quad\\quad+\\alpha^{(n)}P_{\\phi}(y_{i}|\\pmb{x}^{(n)})\\log P_{\\theta}(y_{i}|\\pmb{x}^{(n)})}\\end{array}\n$$  \n\nThe core differences to the previous approaches are the introduction of 1) instance-specific $\\alpha$ and 2) self-teacher with the least generalization error in training."}], "task_step_question_answer": "\u5728\u672a\u6765\u7684\u7814\u7a76\u4e2d\uff0c\u6a21\u578b\u84b8\u998f\u7684\u5e94\u7528\u573a\u666f\u5c06\u8fdb\u4e00\u6b65\u62d3\u5c55\u3002\u4f8b\u5982\uff0c\u8de8\u6a21\u6001\u84b8\u998f\u5c06\u6210\u4e3a\u4e00\u4e2a\u91cd\u8981\u7684\u7814\u7a76\u65b9\u5411\uff0c\u901a\u8fc7\u5c06\u4e0d\u540c\u6a21\u6001\uff08\u5982\u56fe\u50cf\u3001\u6587\u672c\u3001\u97f3\u9891\uff09\u7684\u77e5\u8bc6\u8fdb\u884c\u8fc1\u79fb\uff0c\u53ef\u4ee5\u6784\u5efa\u66f4\u52a0\u901a\u7528\u548c\u5f3a\u5927\u7684\u6a21\u578b\u3002\u6b64\u5916\uff0c\u968f\u7740\u786c\u4ef6\u6280\u672f\u7684\u8fdb\u6b65\uff0c\u6a21\u578b\u84b8\u998f\u5728\u66f4\u5e7f\u6cdb\u7684\u8bbe\u5907\u4e0a\u7684\u5e94\u7528\u4e5f\u5c06\u6210\u4e3a\u53ef\u80fd\uff0c\u4f8b\u5982\u5728\u53ef\u7a7f\u6234\u8bbe\u5907\u548c\u65e0\u4eba\u673a\u4e0a\u7684\u5e94\u7528\u3002\u8fd9\u4e9b\u8bbe\u5907\u901a\u5e38\u5177\u6709\u66f4\u4e25\u683c\u7684\u8d44\u6e90\u9650\u5236\uff0c\u56e0\u6b64\u84b8\u998f\u6280\u672f\u5c06\u5728\u8fd9\u4e9b\u9886\u57df\u4e2d\u53d1\u6325\u66f4\u5927\u7684\u4f5c\u7528\u3002\u540c\u65f6\uff0c\u968f\u7740\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u4e0d\u65ad\u590d\u6742\u5316\uff0c\u5982\u4f55\u5728\u4e0d\u635f\u5931\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\u8fdb\u4e00\u6b65\u538b\u7f29\u6a21\u578b\u4e5f\u5c06\u6210\u4e3a\u4e00\u4e2a\u91cd\u8981\u7684\u7814\u7a76\u8bfe\u9898\u3002\u672a\u6765\u7684\u7814\u7a76\u53ef\u80fd\u4f1a\u63a2\u7d22\u66f4\u52a0\u9ad8\u6548\u7684\u84b8\u998f\u65b9\u6cd5\uff0c\u4f8b\u5982\u57fa\u4e8e\u795e\u7ecf\u67b6\u6784\u641c\u7d22\u7684\u84b8\u998f\u6280\u672f\uff0c\u6216\u8005\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u7684\u84b8\u998f\u7b56\u7565\uff0c\u4ee5\u8fdb\u4e00\u6b65\u63d0\u9ad8\u6a21\u578b\u7684\u6027\u80fd\u548c\u6548\u7387\u3002", "ref_task_step_id": ""}, "__type__": "task_step_node"}, "e1e96c8a-2a92-4140-b48f-8d5d3d73717f": {"__data__": {"id_": "e1e96c8a-2a92-4140-b48f-8d5d3d73717f", "metadata": {}, "relationships": {}, "hash": "", "start_task_context": "\u6a21\u578b\u84b8\u998f\u7684\u662f\u4ec0\u4e48\uff1f", "aemo_representation_context": "\u6a21\u578b\u84b8\u998f\uff08Model Distillation\uff09\u662f\u4e00\u79cd\u5728\u673a\u5668\u5b66\u4e60\u9886\u57df\u5e7f\u6cdb\u4f7f\u7528\u7684\u6280\u672f\uff0c\u4e3b\u8981\u7528\u4e8e\u5c06\u4e00\u4e2a\u5927\u578b\u3001\u590d\u6742\u4e14\u9ad8\u6027\u80fd\u7684\u6a21\u578b\uff08\u901a\u5e38\u79f0\u4e3a\u6559\u5e08\u6a21\u578b\uff09\u7684\u77e5\u8bc6\u8fc1\u79fb\u5230\u4e00\u4e2a\u66f4\u5c0f\u3001\u66f4\u7b80\u5355\u4e14\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u7684\u6a21\u578b\uff08\u901a\u5e38\u79f0\u4e3a\u5b66\u751f\u6a21\u578b\uff09\u4e2d\u3002\u4ee5\u4e0b\u662f\u6a21\u578b\u84b8\u998f\u7684\u8be6\u7ec6\u5206\u89e3\uff1a\n\n### 1. **\u57fa\u672c\u6982\u5ff5**\n- **\u6559\u5e08\u6a21\u578b\uff08Teacher Model\uff09**\uff1a\u4e00\u4e2a\u8bad\u7ec3\u597d\u7684\u3001\u6027\u80fd\u4f18\u5f02\u4f46\u53ef\u80fd\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u7684\u6a21\u578b\u3002\n- **\u5b66\u751f\u6a21\u578b\uff08Student Model\uff09**\uff1a\u4e00\u4e2a\u66f4\u5c0f\u3001\u66f4\u7b80\u5355\u3001\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u7684\u6a21\u578b\uff0c\u76ee\u6807\u662f\u6a21\u4eff\u6559\u5e08\u6a21\u578b\u7684\u8868\u73b0\u3002\n\n### 2. **\u84b8\u998f\u8fc7\u7a0b**\n- **\u8bad\u7ec3\u6559\u5e08\u6a21\u578b**\uff1a\u9996\u5148\uff0c\u4f7f\u7528\u5927\u91cf\u6570\u636e\u8bad\u7ec3\u4e00\u4e2a\u9ad8\u6027\u80fd\u7684\u6559\u5e08\u6a21\u578b\u3002\n- **\u751f\u6210\u8f6f\u6807\u7b7e**\uff1a\u5229\u7528\u6559\u5e08\u6a21\u578b\u5bf9\u8bad\u7ec3\u6570\u636e\u8fdb\u884c\u9884\u6d4b\uff0c\u751f\u6210\u8f6f\u6807\u7b7e\uff08\u5373\u6982\u7387\u5206\u5e03\uff09\uff0c\u800c\u4e0d\u662f\u7b80\u5355\u7684\u786c\u6807\u7b7e\uff08\u5373\u7c7b\u522b\u6807\u7b7e\uff09\u3002\n- **\u8bad\u7ec3\u5b66\u751f\u6a21\u578b**\uff1a\u4f7f\u7528\u539f\u59cb\u6570\u636e\u548c\u6559\u5e08\u6a21\u578b\u751f\u6210\u7684\u8f6f\u6807\u7b7e\u6765\u8bad\u7ec3\u5b66\u751f\u6a21\u578b\u3002\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u5b66\u751f\u6a21\u578b\u7684\u635f\u5931\u51fd\u6570\u901a\u5e38\u5305\u62ec\u4e24\u90e8\u5206\uff1a\n  - **\u786c\u6807\u7b7e\u635f\u5931**\uff1a\u5b66\u751f\u6a21\u578b\u9884\u6d4b\u4e0e\u771f\u5b9e\u6807\u7b7e\u4e4b\u95f4\u7684\u5dee\u5f02\u3002\n  - **\u8f6f\u6807\u7b7e\u635f\u5931**\uff1a\u5b66\u751f\u6a21\u578b\u9884\u6d4b\u4e0e\u6559\u5e08\u6a21\u578b\u9884\u6d4b\u4e4b\u95f4\u7684\u5dee\u5f02\u3002\n\n### 3. **\u5173\u952e\u6280\u672f\u70b9**\n- **\u6e29\u5ea6\u8c03\u8282**\uff1a\u5728\u751f\u6210\u8f6f\u6807\u7b7e\u65f6\uff0c\u901a\u5e38\u5f15\u5165\u4e00\u4e2a\u6e29\u5ea6\u53c2\u6570\uff08Temperature\uff09\u6765\u5e73\u6ed1\u6982\u7387\u5206\u5e03\uff0c\u4f7f\u5f97\u8f6f\u6807\u7b7e\u5305\u542b\u66f4\u591a\u7684\u4fe1\u606f\u3002\n- **\u635f\u5931\u51fd\u6570\u8bbe\u8ba1**\uff1a\u7ed3\u5408\u786c\u6807\u7b7e\u635f\u5931\u548c\u8f6f\u6807\u7b7e\u635f\u5931\uff0c\u5e73\u8861\u4e24\u8005\u7684\u91cd\u8981\u6027\u3002\n\n### 4. **\u5e94\u7528\u573a\u666f**\n- **\u6a21\u578b\u538b\u7f29**\uff1a\u5c06\u5927\u578b\u6a21\u578b\u538b\u7f29\u6210\u5c0f\u578b\u6a21\u578b\uff0c\u4fbf\u4e8e\u90e8\u7f72\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8bbe\u5907\u4e0a\u3002\n- **\u77e5\u8bc6\u8fc1\u79fb**\uff1a\u5c06\u4e00\u4e2a\u4efb\u52a1\u4e0a\u5b66\u5230\u7684\u77e5\u8bc6\u8fc1\u79fb\u5230\u53e6\u4e00\u4e2a\u4efb\u52a1\u4e0a\u3002\n- **\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b**\uff1a\u901a\u8fc7\u5b66\u4e60\u6559\u5e08\u6a21\u578b\u7684\u8f93\u51fa\uff0c\u5b66\u751f\u6a21\u578b\u53ef\u80fd\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u5177\u6709\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002\n\n### 5. **\u4f18\u52bf\u4e0e\u5c40\u9650\u6027**\n- **\u4f18\u52bf**\uff1a\n  - **\u8ba1\u7b97\u6548\u7387\u9ad8**\uff1a\u5b66\u751f\u6a21\u578b\u66f4\u5c0f\uff0c\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u4f4e\u3002\n  - **\u6027\u80fd\u63a5\u8fd1**\uff1a\u5b66\u751f\u6a21\u578b\u80fd\u63a5\u8fd1\u6559\u5e08\u6a21\u578b\u7684\u6027\u80fd\u3002\n- **\u5c40\u9650\u6027**\uff1a\n  - **\u6027\u80fd\u5dee\u8ddd**\uff1a\u5b66\u751f\u6a21\u578b\u6027\u80fd\u53ef\u80fd\u4ecd\u7565\u4f4e\u4e8e\u6559\u5e08\u6a21\u578b\u3002\n  - **\u8bad\u7ec3\u590d\u6742**\uff1a\u9700\u8981\u5148\u8bad\u7ec3\u4e00\u4e2a\u9ad8\u6027\u80fd\u7684\u6559\u5e08\u6a21\u578b\u3002\n\n### 6. **\u6700\u65b0\u8fdb\u5c55**\n- **\u591a\u4efb\u52a1\u84b8\u998f**\uff1a\u5728\u4e00\u4e2a\u5b66\u751f\u6a21\u578b\u4e0a\u540c\u65f6\u84b8\u998f\u591a\u4e2a\u6559\u5e08\u6a21\u578b\u7684\u77e5\u8bc6\u3002\n- **\u81ea\u84b8\u998f**\uff1a\u6a21\u578b\u81ea\u8eab\u4f5c\u4e3a\u6559\u5e08\u6a21\u578b\u8fdb\u884c\u84b8\u998f\u3002\n- **\u8de8\u6a21\u6001\u84b8\u998f**\uff1a\u5728\u4e0d\u540c\u6a21\u6001\uff08\u5982\u56fe\u50cf\u548c\u6587\u672c\uff09\u4e4b\u95f4\u8fdb\u884c\u77e5\u8bc6\u84b8\u998f\u3002\n\n### 7. **\u672a\u6765\u7814\u7a76\u65b9\u5411**\n- **\u66f4\u9ad8\u6548\u7684\u84b8\u998f\u65b9\u6cd5**\uff1a\u63a2\u7d22\u66f4\u6709\u6548\u7684\u77e5\u8bc6\u8fc1\u79fb\u6280\u672f\u3002\n- **\u6cdb\u5316\u80fd\u529b\u7684\u63d0\u5347**\uff1a\u7814\u7a76\u5982\u4f55\u901a\u8fc7\u84b8\u998f\u63d0\u9ad8\u6a21\u578b\u5728\u672a\u89c1\u6570\u636e\u4e0a\u7684\u8868\u73b0\u3002\n- **\u591a\u6a21\u6001\u84b8\u998f\u7684\u62d3\u5c55**\uff1a\u63a2\u7d22\u66f4\u591a\u8de8\u6a21\u6001\u77e5\u8bc6\u84b8\u998f\u7684\u5e94\u7528\u573a\u666f\u3002\n\n\u901a\u8fc7\u4ee5\u4e0a\u6b65\u9aa4\uff0c\u53ef\u4ee5\u5168\u9762\u7406\u89e3\u6a21\u578b\u84b8\u998f\u7684\u6982\u5ff5\u3001\u8fc7\u7a0b\u3001\u6280\u672f\u7ec6\u8282\u3001\u5e94\u7528\u573a\u666f\u53ca\u5176\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u7814\u7a76\u4e2d\u7684\u91cd\u8981\u6027\u3002\u6a21\u578b\u84b8\u998f\u4e0d\u4ec5\u662f\u4e00\u79cd\u6709\u6548\u7684\u6a21\u578b\u538b\u7f29\u6280\u672f\uff0c\u4e5f\u662f\u77e5\u8bc6\u8fc1\u79fb\u548c\u63d0\u5347\u6a21\u578b\u6027\u80fd\u7684\u91cd\u8981\u624b\u6bb5\u3002", "task_step_name": "\u4f18\u52bf\u4e0e\u5c40\u9650\u6027", "task_step_description": "\u4f18\u52bf\uff1a\u8ba1\u7b97\u6548\u7387\u9ad8\uff1a\u5b66\u751f\u6a21\u578b\u66f4\u5c0f\uff0c\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u4f4e\u3002\u6027\u80fd\u63a5\u8fd1\uff1a\u5b66\u751f\u6a21\u578b\u80fd\u63a5\u8fd1\u6559\u5e08\u6a21\u578b\u7684\u6027\u80fd\u3002\u5c40\u9650\u6027\uff1a\u6027\u80fd\u5dee\u8ddd\uff1a\u5b66\u751f\u6a21\u578b\u6027\u80fd\u53ef\u80fd\u4ecd\u7565\u4f4e\u4e8e\u6559\u5e08\u6a21\u578b\u3002\u8bad\u7ec3\u590d\u6742\uff1a\u9700\u8981\u5148\u8bad\u7ec3\u4e00\u4e2a\u9ad8\u6027\u80fd\u7684\u6559\u5e08\u6a21\u578b\u3002", "task_step_level": "4", "task_step_question": "### \u95ee\u9898\n\n\u5728\u6a21\u578b\u84b8\u998f\u7684\u8fc7\u7a0b\u4e2d\uff0c\u5c3d\u7ba1\u5b66\u751f\u6a21\u578b\u5177\u6709\u8ba1\u7b97\u6548\u7387\u9ad8\u548c\u6027\u80fd\u63a5\u8fd1\u6559\u5e08\u6a21\u578b\u7684\u4f18\u52bf\uff0c\u4f46\u4ecd\u7136\u5b58\u5728\u6027\u80fd\u5dee\u8ddd\u548c\u8bad\u7ec3\u590d\u6742\u6027\u7684\u5c40\u9650\u6027\u3002\u8bf7\u7ed3\u5408\u6a21\u578b\u84b8\u998f\u7684\u57fa\u672c\u6982\u5ff5\u548c\u84b8\u998f\u8fc7\u7a0b\uff0c\u5206\u6790\u4ee5\u4e0b\u95ee\u9898\uff1a\n\n1. **\u4e3a\u4ec0\u4e48\u5b66\u751f\u6a21\u578b\u7684\u6027\u80fd\u53ef\u80fd\u4ecd\u7565\u4f4e\u4e8e\u6559\u5e08\u6a21\u578b\uff1f**  \n2. **\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u5982\u4f55\u901a\u8fc7\u8c03\u6574\u635f\u5931\u51fd\u6570\u6216\u5f15\u5165\u5176\u4ed6\u6280\u672f\u6765\u7f29\u5c0f\u5b66\u751f\u6a21\u578b\u4e0e\u6559\u5e08\u6a21\u578b\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u8ddd\uff1f**  \n3. **\u9488\u5bf9\u8bad\u7ec3\u590d\u6742\u6027\u7684\u5c40\u9650\u6027\uff0c\u6709\u54ea\u4e9b\u7b56\u7565\u53ef\u4ee5\u7b80\u5316\u6216\u4f18\u5316\u6559\u5e08\u6a21\u578b\u7684\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u4ece\u800c\u964d\u4f4e\u6574\u4f53\u84b8\u998f\u7684\u590d\u6742\u6027\uff1f**\n\n\u901a\u8fc7\u8fd9\u4e9b\u95ee\u9898\uff0c\u53ef\u4ee5\u66f4\u6df1\u5165\u5730\u7406\u89e3\u6a21\u578b\u84b8\u998f\u7684\u4f18\u52bf\u4e0e\u5c40\u9650\u6027\uff0c\u5e76\u63a2\u8ba8\u5982\u4f55\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u514b\u670d\u8fd9\u4e9b\u6311\u6218\u3002", "task_step_question_context": [{"ref_id": "454847819752024066", "chunk_id": "2", "score": 0.51171875, "text": "# 4 Evolving Teacher to Student Models via Pruning\nAs the previous section presents, the Knowledge Distillation approach to model compression has two main drawbacks in the private world:  \n\n\u2022Drop in accuracy: There is a considerable drop in the accuracy between the teacher and the student models.   \n\u2022Good initialization of students is crucial: The best performance is obtained by students who already have a good initialization; in our experiments, pre-trained DistilBERT mostly achieved the best student performance.  \n\nFinding a good initialization can be challenging in practice. Often, the student architectures are chosen to suit the hardware and latency requirements of the application for which the model is being deployed, using neural architecture search [ 18 ]. Hence, finding a good initialization for every student architecture via pre-training can be expensive and in most cases impossible. Our zero-shot initialization strategies alleviate this problem to a certain degree, yet fall short of closing the gap between the teacher and the student performances. Moreover, DPKD requires that (i) the teacher is trained with DPSGD and (ii) the student is distilled via DPSGD. This two-step approach creates additional overheads in terms of training. Given these limitations, it is natural to ask: Can we evolve the teacher to a student model while fine-tuning with DPSGD? In this section, we explore an answer to this via structured and unstructured pruning with privacy, which allows us to obtain student models that are as good as the teacher models.\n\n# 4.1 Model Compression via Pruning\nPruning algorithms are a broad class of model compression techniques where one drops the parameters from a model during or after the training process. Many works have shown that eliminating unnecessary parameters of neural networks via pruning can lead to sparser and compressed models that have shorter inference times without loss in performance [ 30 ,25 ,24 ]. For example, in magnitude pruning , one of the most widely used pruning techniques, we prune a fraction of parameters with the lowest magnitude. However, there are several pruning strategies, and we refer the readers to [ 33 ] for more details and references.  \n\nPruning can be implemented in both structured and unstructured ways. In structured pruning, all the pruned weights belong to a single building block of the model. For example, a 6-layer $\\frac{1}{2}$ -BERT can be obtained by pruning 6 layers from the full BERT model, which consists of 12 transformer blocks. On the other hand, in unstructured pruning, pruned weights may be spread across all the layers of the network. In unstructured pruning, it is possible to obtain a $50\\%$ sparse student model while still having all the 12 layers of BERT. Depending on the hardware architectures, inference latency between models with structured and unstructured sparsity could be quite different. However, in this section, we use sparsity as the main measure of model compression , which is also well accepted in the community [ 33 ,27 ].\n\n# 4.2 Iterative Magnitude Pruning (IMP)\nPrivate pruning techniques we study in this section are based on the Iterative Magnitude Pruning (IMP) method, which is a specific pruning technique proposed in a recent work on Lottery Ticket Hypothesis [ 19 ]. The idea behind IMP is rather simple: As we train a deep learning model, after every $N$ iterations we prune an $\\alpha\\%$ of the weights with the lowest magnitude .  \n\nWe repeat this process until we achieve the desired sparsity. Here, both $N$ and $\\alpha$ are hyperparameters that need to be tuned. For example, to achieve $50\\%$ sparsity, one can perform $5N$ iterations where after every $N$ iterations additional $10\\%$ of the weights with the least magnitudes are dropped. As specified IMP produces unstructured sparsity. However, we consider a simple modification of the IMP algorithm to produce structured sparsity as well.\n\n# Algorithm 2 Structured DPIMP\nInput: Teacher model $\\tau$ , numb r of la ers to prune $L$ , hype rams $\\alpha$ ,$N$ and $M$   \nOutput e student model Swith Llayers pruned from T$\\tau$   \n1: 2: Set for ${\\mathcal{S}}:={\\mathcal{T}}$ $j=1$ to T$L$ do   \n3: Fin $\\boldsymbol{S}$ for $N$ itera s with DPSGD   \n4: Set $W_{\\mathrm{min}}$ consisting of $\\alpha\\%$ of the remaining model weights with the least magnitude   \n5: Set $W_{i}$ as the weights of layer $i$   \n6: Drop the layer $i^{*}$ from $\\boldsymbol{S}$ satisfying $i^{*}:=\\arg\\operatorname*{max}_{i}\\left\\{W_{i}\\cap W_{\\operatorname*{min}}\\right\\}$   \n7: end for   \n8: Fine-tu $\\boldsymbol{S}$ for $M$ more iterations with DPSGD   \n9: return S\n\n# 4.3 Structured DPIMP\nWe first attempt to obtain a student model from the teacher model via a structured IMP technique, using the following modification: During fine-tuning the teacher model with DPSGD, we progressively drop an appropriately chosen transformer block from the teacher model at the end of every $N$ iterations. We repeat this process until we obtain the student model with the required sparsity. The layer to drop is chosen using the following heuristic: Let $\\alpha>0$ be a hyperparameter. At the end of $N$ iterations, fix bottom (by magnitude) $\\alpha\\%$ of all model weights, and denote it by $W_{\\mathrm{min}}$ . For the $i^{t h}$ transformer block, let $W_{i}$ denote the set of model weights belonging to that block. Among all the transformers blocks we find the block $i^{*}$ that has the highest number of weights from the set $W_{\\mathrm{min}}$ ; Formally, $i^{*}:=\\arg\\operatorname*{max}_{i}$ $\\{W_{i}\\cap W_{\\operatorname*{min}}\\}$ , and we prune the transformer layer $i^{*}$ . We present this in Algorithm 2 .  \n\nEmpirical Evaluation We evaluate our structured pruning algorithm with the same setup described in Section 3.3 , and provide the hyperparameters in Appendix A.2 . We split the privacy budget equally among all the iterations of the algorithm. Our goal is to produce a student model which has $\\frac{1}{2}$ as many layers as the full BERT model. Table 4 shows the results for this setting where we compare structured DPIMP to private fine-tuning of the pre-trained DistilBERT and the full BERT model. The main takeaway from this experiment is:  \n\nTable 4: Comparing performance of 6-layer $\\frac{1}{2}$ -BERT student model produced by structured DPIMP with 12-layer BERT teacher model and pre-trained DistillBERT. All our models have the same privacy budget bounded by $\\epsilon<4.25\\$ .  \n\n\n<html><body><table><tr><td>Model</td><td>MNLI QQP</td><td>QNLI</td><td>SST-2</td><td>Avg</td></tr><tr><td>BERT</td><td>77.8</td><td>84.7 87.8</td><td>90.5</td><td>85.2</td></tr><tr><td>DistilBERT</td><td>73.0</td><td>84.3 82.8</td><td>87.7</td><td>81.9</td></tr><tr><td>-BERT</td><td>72.9</td><td>83.1 82.5</td><td>85.7</td><td>81.0</td></tr></table></body></html>  \n\n\u2022DP structured pruning algorithm produces a student model that has performance comparable to that of DistilBERT. Further, it avoids the pre-training cost associated with DistilBERT."}, {"ref_id": "454846524788653356", "chunk_id": "3", "score": 0.443359375, "text": "# 4 Experimental Results\n\n# 4.1 Implementation\nWe use the CLIP model, ViT-g-14 (ViT-G), provided by OpenCLIP [25] as the teacher model. The student model is built upon ViT-S and its state-of-the-art (SOTA) variants, DAT-T [48] and Swin-T [34], where we replace the classification head with a feature projection head. For training with student models in stage 1, we use AdamW optimizer with a base learning rate of $10^{-4}$ and weight decay of 0.05. We use a cosine learning rate scheduler which decays the learning rate to $5\\times10^{-6}$ over 120 epochs. In stage 2, we reduce the base learning rate to $10^{-6}$ .For the threshold $\\tau_{c}$ , we empirically set it at 0 .25 considering the training data utilization and data noise. For the CLIP text encoder, we use the text prompt \u201ca photo of a {scene category}.\" or \u201ca satellite image of a {scene category}.\" depending on the dataset as suggested by [39]. For quantized models, we report static quantization ts. For the triplet loss $\\mathcal{L}_{c}$ , we use a margin $m=0.3$ and a negative set size J= 3 ."}, {"ref_id": "455037545741550556", "chunk_id": "6", "score": 0.431640625, "text": "# 4 Methods\nTo overcome the aforementioned limitations, we introduce our L2T framework, Learning Good Teacher Matters (LGTM) to enable more effective knowledge distillation. We first introduce distillation influence , which estimates how much will the student\u2019s performance on validation data change if we put one training sample in the knowledge distillation process.  \n\nAfterwards, we introduce an efficient training method based on finite difference approximation for incorporating distillation influence into the teacher\u2019s update. Finally, we interpret current L2T methods from the perspective of influence function.  \n\nDistillation influence Influence function ( Pruthi et al. ,2020 ;Koh and Liang ,2017 ) is a way of measuring the influence of training samples on the model\u2019s predictions. It can be utilized to identify instances that have a disproportionate effect on the model\u2019s behavior, whether due to their status as outliers or due to incorrect labeling ( Jia et al. ,2019 ;Ghorbani and Zou ,2019 ;Hara et al. ,2019 ). By calculating the influence function for a particular example, it is possible to estimate the extent to which the model\u2019s prediction would be altered as a result of operations on that sample.  \n\nIn vanilla distillation, for the student model, we derive the distillation influence of $\\boldsymbol{z}_{i}^{r}$ as the gradient similarity between the training sample $z_{i}^{r}$ and the validation batch $z^{e}$ :  \n\n$$\n\\begin{array}{r l}&{\\mathcal{Z}_{\\mathrm{distill}}(z_{i}^{r},z^{e})=\\!\\nabla_{\\theta_{s}}\\mathcal{L}_{\\mathrm{ce}}(T(\\pmb{x}_{i}^{r};\\theta_{t}^{m}),S(\\pmb{x}_{i}^{r};\\theta_{s}^{m}))^{\\intercal}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\nabla_{\\theta_{s}}\\mathcal{L}_{\\mathrm{ce}}(\\pmb{y}^{e},S(\\pmb{x}^{e};\\theta_{s}^{m+1}))}\\end{array}\n$$  \n\nThe detailed derivation can be found in appendix A .The influence reflects how well the knowledge gained from a particular sample generalizes. It follows that the teacher should focus on teaching the student to capture training samples that have the highest distillation influences.  \n\nIn order to incorporate the per-sample influence into knowledge distillation, we adjust the loss weight of each sample based on its distillation influence. This allows us to determine the relative importance of each sample, and helps to control how much each sample contributes to the teacher\u2019s learning process. Samples that are deemed to be more beneficial for the student\u2019s generalization are assigned higher weights. Then we propose training the teacher using the following objective:  \n\n$$\n\\mathcal{L}_{\\mathrm{influence}}=\\frac{1}{B^{r}}\\sum_{i=1}^{B^{r}}w_{i}\\mathcal{L}_{\\mathrm{ce}}((T(\\pmb{x}_{i}^{r};\\theta_{t}^{m}),S(\\pmb{x}_{i}^{r};\\theta_{s}^{m})),\n$$  \n\nwhere $w_{i}\\,=\\,\\mathcal{T}_{\\mathrm{distill}}(z_{i}^{r},z^{e})$ . By including the influence in the knowledge distillation loss function, we can tailor the training process to better suit the characteristics of the target task.  \n\n<html><body><table><tr><td>Algorithm1 LGTM</td></tr><tr><td>Require: student Os, teacher Ot, training set Dtrain, validation set Dval Require: Ms, Nt: learning rate for the student and the teacher Require: e:a small scalar Require:M: the maximum number of the training steps 1:while step<Mdo 2: Sample a batch of training set zr = (\u03b1\", y\"\uff09 \uff5e Dtrain 3: Copystudentparameter0stostudent0' 4: Update 0': 0s < 0s - nsV Ls(0', 0t, 2\") 5: Sample a batch of validation set z = (\u00b0, y\u00b0) \uff5e Dval 6: Calculate \u03b8: 0 = 0s \u00b1 \u2208Lce(y, S(\u221e;\u03b8s)) 7: Calculate the Distillation Infuence with z', 0t, 0 and e: Linfluence (01) \u00b7b< 8: Update \u03b8t:\u03b8t\u21900t-ntVotLt(0t,0s,) >eq.(11) 9: Update original 0s: 0s < 0s - ns Vos Cs(0s, 0t, 2r) 10: step \u2190 step + 1 11: end while</td></tr></table></body></html>  \n\nFinite difference approximation For standard neural network training, we often compute a consolidated gradient for a mini-batch of $B^{r}$ training samples to enhance computational efficiency. However, in the context of determining the distillation influence for each sample, the computation will slow down the training by a factor of of per-sample gradient $\\mathcal{L}_{\\mathrm{ce}}(T(\\pmb{x}_{i}^{r};\\theta_{t}^{m}),S(\\pmb{x}_{i}^{r};\\theta_{s}^{m}))$ $B^{r}$ .In addition, a naive implementation is memory intensive, because it requires to keep a copy of $\\nabla_{\\theta_{s}}\\mathcal{L}_{\\mathrm{ce}}(\\pmb{y}^{e},S(\\pmb{x}^{e};\\theta_{s}^{m+1}))$ .  \n\nTo address this, we propose an efficient method for updating the teacher with the distillation influence by utilizing finite difference ( Gleich ,2005 ), a technique commonly used in numerical analysis for approximating the derivative of a function at a given point. Similar to ( Pham et al. ,2021 ;Liu et al. ,2018 ), we approximate Linfluence by  \n\n$$\n\\begin{array}{r l r}{\\lefteqn{\\mathcal{L}_{\\mathrm{influence}}\\approx\\hat{\\mathcal{L}}_{\\mathrm{influence}}=\\frac{1}{B^{r}}\\sum_{i=1}^{B^{r}}\\Big[\\frac{\\mathcal{L}_{\\mathrm{ce}}(T(x_{i};\\theta_{t}^{m}),S(x_{i};\\theta_{s}^{+}))}{2\\epsilon}}\\\\ &{}&{{\\displaystyle-\\:\\frac{\\mathcal{L}_{\\mathrm{ce}}(T(x_{i};\\theta_{t}^{m}),S(x_{i};\\theta_{s}^{-}))}{2\\epsilon}\\Big],\\;\\;}\\end{array}\n$$  \n\nwhere $\\theta_{s}^{\\pm}=\\theta_{s}\\pm\\epsilon\\mathcal{L}_{\\mathrm{ce}}(\\pmb{y}^{e},S(\\pmb{x}^{e};\\theta_{s}^{m+1}))$ \u00b1Land $\\epsilon$ is a small scalar. Our proposed method for evaluating the finite difference is computationally efficient, as it only requires two forward passes for $\\theta_{s}$ and one backward pass for $\\theta_{t}$ for a single batch, as opposed to a naive implementation which requires $B^{r}$ forward and backward passes for $\\theta_{s}$ and one backward pass for $\\theta_{t}$ . We provide more details of the derivation in appendix B.  \n\nTeacher\u2019s auxiliary loss Inspired by ( Pham et al. ,2021 ), in order to balance the trade-off between self-evolution and transferability of the teacher model, we incorporate the loss with respect to the ground truth as ${\\mathcal{L}}_{\\mathrm{aux}}$ into the final objective:  \n\n  \nFigure 2: Performance comparison between Meta Distill ( Zhou et al. ,2022 ) and LGTM on the MNLI validation set. We observe that for LGTM, student model does not suffer from overfitting (thanks to distillation influence), and the teacher can balance its own evolution and effective knowledge transfer (thanks to auxiliary loss).  \n\n$$\n\\begin{array}{r l}{\\mathcal{L}_{\\mathfrak{t}}(\\theta_{t}\\ |\\ \\theta_{s},z^{r})=}&{\\hat{\\mathcal{L}}_{\\mathrm{influence}}+\\mathcal{L}_{\\mathrm{aux}},}\\\\ {\\mathcal{L}_{\\mathrm{aux}}=}&{\\alpha\\mathcal{L}_{\\mathrm{ce}}(y^{r},T(x^{r};\\theta_{t}))+}\\\\ &{(1-\\alpha)\\mathcal{L}_{\\mathrm{ce}}(T(x^{r};\\theta_{t}),S(x^{r};\\theta_{s})).}\\end{array}\n$$  \n\nwhere $\\alpha$ is the loss ratio.  \n\nOverall, our method allows the teacher to adapt to the student\u2019s abilities and provide more personalized guidance while improving the student\u2019s generalization capability. We present the algorithm of LGTM in algorithm 1 .  \n\nRelationship with other L2T methods Here we interpret current learning to teach methods from the perspective of influence function.  \n\nIn the case of online distillation, it is assumed that all training samples possess an equivalent distillation influence and that the teacher model is responsible for reducing the transfer difficulty of all training samples.  \n\nIn contrast, the key differentiating factor between meta distillation and online distillation is the utilization of a dynamic loss weight. We interpret this weight as a measure of the distillation influence of the current training batch $z^{r}$ on the generalization ability of the student model. Specifi- cally, it reflects the similarity between the gradients of the training and validation batches, indicating the effect of the current training batch $z^{r}$ on the validation batch $z^{e}$ (as detailed in appendix C). However, it should be noted that this weight functions primarily as an adaptive learning rate, adjusting the gradient step proportionally to the degree of similarity in gradients. We illustrate the general workflow of vanilla distillation, online distillation, meta distillation and LGTM in fig. 1 ."}], "task_step_question_answer": "\u5728\u672a\u6765\u7684\u7814\u7a76\u4e2d\uff0c\u6a21\u578b\u84b8\u998f\u7684\u5e94\u7528\u573a\u666f\u5c06\u8fdb\u4e00\u6b65\u62d3\u5c55\u3002\u4f8b\u5982\uff0c\u8de8\u6a21\u6001\u84b8\u998f\u5c06\u6210\u4e3a\u4e00\u4e2a\u91cd\u8981\u7684\u7814\u7a76\u65b9\u5411\uff0c\u901a\u8fc7\u5c06\u4e0d\u540c\u6a21\u6001\uff08\u5982\u56fe\u50cf\u3001\u6587\u672c\u3001\u97f3\u9891\uff09\u7684\u77e5\u8bc6\u8fdb\u884c\u8fc1\u79fb\uff0c\u53ef\u4ee5\u6784\u5efa\u66f4\u52a0\u901a\u7528\u548c\u5f3a\u5927\u7684\u6a21\u578b\u3002\u6b64\u5916\uff0c\u968f\u7740\u786c\u4ef6\u6280\u672f\u7684\u8fdb\u6b65\uff0c\u6a21\u578b\u84b8\u998f\u5728\u66f4\u5e7f\u6cdb\u7684\u8bbe\u5907\u4e0a\u7684\u5e94\u7528\u4e5f\u5c06\u6210\u4e3a\u53ef\u80fd\uff0c\u4f8b\u5982\u5728\u53ef\u7a7f\u6234\u8bbe\u5907\u548c\u65e0\u4eba\u673a\u4e0a\u7684\u5e94\u7528\u3002\u8fd9\u4e9b\u8bbe\u5907\u901a\u5e38\u5177\u6709\u66f4\u4e25\u683c\u7684\u8d44\u6e90\u9650\u5236\uff0c\u56e0\u6b64\u84b8\u998f\u6280\u672f\u5c06\u5728\u8fd9\u4e9b\u9886\u57df\u4e2d\u53d1\u6325\u66f4\u5927\u7684\u4f5c\u7528\u3002\u540c\u65f6\uff0c\u968f\u7740\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u4e0d\u65ad\u590d\u6742\u5316\uff0c\u5982\u4f55\u5728\u4e0d\u635f\u5931\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\u8fdb\u4e00\u6b65\u538b\u7f29\u6a21\u578b\u4e5f\u5c06\u6210\u4e3a\u4e00\u4e2a\u91cd\u8981\u7684\u7814\u7a76\u8bfe\u9898\u3002\u672a\u6765\u7684\u7814\u7a76\u53ef\u80fd\u4f1a\u63a2\u7d22\u66f4\u52a0\u9ad8\u6548\u7684\u84b8\u998f\u65b9\u6cd5\uff0c\u4f8b\u5982\u57fa\u4e8e\u795e\u7ecf\u67b6\u6784\u641c\u7d22\u7684\u84b8\u998f\u6280\u672f\uff0c\u6216\u8005\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u7684\u84b8\u998f\u7b56\u7565\uff0c\u4ee5\u8fdb\u4e00\u6b65\u63d0\u9ad8\u6a21\u578b\u7684\u6027\u80fd\u548c\u6548\u7387\u3002", "ref_task_step_id": ""}, "__type__": "task_step_node"}, "d7aba3a0-4cfb-4815-9e43-475a4c78758d": {"__data__": {"id_": "d7aba3a0-4cfb-4815-9e43-475a4c78758d", "metadata": {}, "relationships": {}, "hash": "", "start_task_context": "\u6a21\u578b\u84b8\u998f\u7684\u662f\u4ec0\u4e48\uff1f", "aemo_representation_context": "\u6a21\u578b\u84b8\u998f\uff08Model Distillation\uff09\u662f\u4e00\u79cd\u5728\u673a\u5668\u5b66\u4e60\u9886\u57df\u5e7f\u6cdb\u4f7f\u7528\u7684\u6280\u672f\uff0c\u4e3b\u8981\u7528\u4e8e\u5c06\u4e00\u4e2a\u5927\u578b\u3001\u590d\u6742\u4e14\u9ad8\u6027\u80fd\u7684\u6a21\u578b\uff08\u901a\u5e38\u79f0\u4e3a\u6559\u5e08\u6a21\u578b\uff09\u7684\u77e5\u8bc6\u8fc1\u79fb\u5230\u4e00\u4e2a\u66f4\u5c0f\u3001\u66f4\u7b80\u5355\u4e14\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u7684\u6a21\u578b\uff08\u901a\u5e38\u79f0\u4e3a\u5b66\u751f\u6a21\u578b\uff09\u4e2d\u3002\u4ee5\u4e0b\u662f\u6a21\u578b\u84b8\u998f\u7684\u8be6\u7ec6\u5206\u89e3\uff1a\n\n### 1. **\u57fa\u672c\u6982\u5ff5**\n- **\u6559\u5e08\u6a21\u578b\uff08Teacher Model\uff09**\uff1a\u4e00\u4e2a\u8bad\u7ec3\u597d\u7684\u3001\u6027\u80fd\u4f18\u5f02\u4f46\u53ef\u80fd\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u7684\u6a21\u578b\u3002\n- **\u5b66\u751f\u6a21\u578b\uff08Student Model\uff09**\uff1a\u4e00\u4e2a\u66f4\u5c0f\u3001\u66f4\u7b80\u5355\u3001\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u7684\u6a21\u578b\uff0c\u76ee\u6807\u662f\u6a21\u4eff\u6559\u5e08\u6a21\u578b\u7684\u8868\u73b0\u3002\n\n### 2. **\u84b8\u998f\u8fc7\u7a0b**\n- **\u8bad\u7ec3\u6559\u5e08\u6a21\u578b**\uff1a\u9996\u5148\uff0c\u4f7f\u7528\u5927\u91cf\u6570\u636e\u8bad\u7ec3\u4e00\u4e2a\u9ad8\u6027\u80fd\u7684\u6559\u5e08\u6a21\u578b\u3002\n- **\u751f\u6210\u8f6f\u6807\u7b7e**\uff1a\u5229\u7528\u6559\u5e08\u6a21\u578b\u5bf9\u8bad\u7ec3\u6570\u636e\u8fdb\u884c\u9884\u6d4b\uff0c\u751f\u6210\u8f6f\u6807\u7b7e\uff08\u5373\u6982\u7387\u5206\u5e03\uff09\uff0c\u800c\u4e0d\u662f\u7b80\u5355\u7684\u786c\u6807\u7b7e\uff08\u5373\u7c7b\u522b\u6807\u7b7e\uff09\u3002\n- **\u8bad\u7ec3\u5b66\u751f\u6a21\u578b**\uff1a\u4f7f\u7528\u539f\u59cb\u6570\u636e\u548c\u6559\u5e08\u6a21\u578b\u751f\u6210\u7684\u8f6f\u6807\u7b7e\u6765\u8bad\u7ec3\u5b66\u751f\u6a21\u578b\u3002\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u5b66\u751f\u6a21\u578b\u7684\u635f\u5931\u51fd\u6570\u901a\u5e38\u5305\u62ec\u4e24\u90e8\u5206\uff1a\n  - **\u786c\u6807\u7b7e\u635f\u5931**\uff1a\u5b66\u751f\u6a21\u578b\u9884\u6d4b\u4e0e\u771f\u5b9e\u6807\u7b7e\u4e4b\u95f4\u7684\u5dee\u5f02\u3002\n  - **\u8f6f\u6807\u7b7e\u635f\u5931**\uff1a\u5b66\u751f\u6a21\u578b\u9884\u6d4b\u4e0e\u6559\u5e08\u6a21\u578b\u9884\u6d4b\u4e4b\u95f4\u7684\u5dee\u5f02\u3002\n\n### 3. **\u5173\u952e\u6280\u672f\u70b9**\n- **\u6e29\u5ea6\u8c03\u8282**\uff1a\u5728\u751f\u6210\u8f6f\u6807\u7b7e\u65f6\uff0c\u901a\u5e38\u5f15\u5165\u4e00\u4e2a\u6e29\u5ea6\u53c2\u6570\uff08Temperature\uff09\u6765\u5e73\u6ed1\u6982\u7387\u5206\u5e03\uff0c\u4f7f\u5f97\u8f6f\u6807\u7b7e\u5305\u542b\u66f4\u591a\u7684\u4fe1\u606f\u3002\n- **\u635f\u5931\u51fd\u6570\u8bbe\u8ba1**\uff1a\u7ed3\u5408\u786c\u6807\u7b7e\u635f\u5931\u548c\u8f6f\u6807\u7b7e\u635f\u5931\uff0c\u5e73\u8861\u4e24\u8005\u7684\u91cd\u8981\u6027\u3002\n\n### 4. **\u5e94\u7528\u573a\u666f**\n- **\u6a21\u578b\u538b\u7f29**\uff1a\u5c06\u5927\u578b\u6a21\u578b\u538b\u7f29\u6210\u5c0f\u578b\u6a21\u578b\uff0c\u4fbf\u4e8e\u90e8\u7f72\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8bbe\u5907\u4e0a\u3002\n- **\u77e5\u8bc6\u8fc1\u79fb**\uff1a\u5c06\u4e00\u4e2a\u4efb\u52a1\u4e0a\u5b66\u5230\u7684\u77e5\u8bc6\u8fc1\u79fb\u5230\u53e6\u4e00\u4e2a\u4efb\u52a1\u4e0a\u3002\n- **\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b**\uff1a\u901a\u8fc7\u5b66\u4e60\u6559\u5e08\u6a21\u578b\u7684\u8f93\u51fa\uff0c\u5b66\u751f\u6a21\u578b\u53ef\u80fd\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u5177\u6709\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002\n\n### 5. **\u4f18\u52bf\u4e0e\u5c40\u9650\u6027**\n- **\u4f18\u52bf**\uff1a\n  - **\u8ba1\u7b97\u6548\u7387\u9ad8**\uff1a\u5b66\u751f\u6a21\u578b\u66f4\u5c0f\uff0c\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u4f4e\u3002\n  - **\u6027\u80fd\u63a5\u8fd1**\uff1a\u5b66\u751f\u6a21\u578b\u80fd\u63a5\u8fd1\u6559\u5e08\u6a21\u578b\u7684\u6027\u80fd\u3002\n- **\u5c40\u9650\u6027**\uff1a\n  - **\u6027\u80fd\u5dee\u8ddd**\uff1a\u5b66\u751f\u6a21\u578b\u6027\u80fd\u53ef\u80fd\u4ecd\u7565\u4f4e\u4e8e\u6559\u5e08\u6a21\u578b\u3002\n  - **\u8bad\u7ec3\u590d\u6742**\uff1a\u9700\u8981\u5148\u8bad\u7ec3\u4e00\u4e2a\u9ad8\u6027\u80fd\u7684\u6559\u5e08\u6a21\u578b\u3002\n\n### 6. **\u6700\u65b0\u8fdb\u5c55**\n- **\u591a\u4efb\u52a1\u84b8\u998f**\uff1a\u5728\u4e00\u4e2a\u5b66\u751f\u6a21\u578b\u4e0a\u540c\u65f6\u84b8\u998f\u591a\u4e2a\u6559\u5e08\u6a21\u578b\u7684\u77e5\u8bc6\u3002\n- **\u81ea\u84b8\u998f**\uff1a\u6a21\u578b\u81ea\u8eab\u4f5c\u4e3a\u6559\u5e08\u6a21\u578b\u8fdb\u884c\u84b8\u998f\u3002\n- **\u8de8\u6a21\u6001\u84b8\u998f**\uff1a\u5728\u4e0d\u540c\u6a21\u6001\uff08\u5982\u56fe\u50cf\u548c\u6587\u672c\uff09\u4e4b\u95f4\u8fdb\u884c\u77e5\u8bc6\u84b8\u998f\u3002\n\n### 7. **\u672a\u6765\u7814\u7a76\u65b9\u5411**\n- **\u66f4\u9ad8\u6548\u7684\u84b8\u998f\u65b9\u6cd5**\uff1a\u63a2\u7d22\u66f4\u6709\u6548\u7684\u77e5\u8bc6\u8fc1\u79fb\u6280\u672f\u3002\n- **\u6cdb\u5316\u80fd\u529b\u7684\u63d0\u5347**\uff1a\u7814\u7a76\u5982\u4f55\u901a\u8fc7\u84b8\u998f\u63d0\u9ad8\u6a21\u578b\u5728\u672a\u89c1\u6570\u636e\u4e0a\u7684\u8868\u73b0\u3002\n- **\u591a\u6a21\u6001\u84b8\u998f\u7684\u62d3\u5c55**\uff1a\u63a2\u7d22\u66f4\u591a\u8de8\u6a21\u6001\u77e5\u8bc6\u84b8\u998f\u7684\u5e94\u7528\u573a\u666f\u3002\n\n\u901a\u8fc7\u4ee5\u4e0a\u6b65\u9aa4\uff0c\u53ef\u4ee5\u5168\u9762\u7406\u89e3\u6a21\u578b\u84b8\u998f\u7684\u6982\u5ff5\u3001\u8fc7\u7a0b\u3001\u6280\u672f\u7ec6\u8282\u3001\u5e94\u7528\u573a\u666f\u53ca\u5176\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u7814\u7a76\u4e2d\u7684\u91cd\u8981\u6027\u3002\u6a21\u578b\u84b8\u998f\u4e0d\u4ec5\u662f\u4e00\u79cd\u6709\u6548\u7684\u6a21\u578b\u538b\u7f29\u6280\u672f\uff0c\u4e5f\u662f\u77e5\u8bc6\u8fc1\u79fb\u548c\u63d0\u5347\u6a21\u578b\u6027\u80fd\u7684\u91cd\u8981\u624b\u6bb5\u3002", "task_step_name": "\u6700\u65b0\u8fdb\u5c55", "task_step_description": "\u591a\u4efb\u52a1\u84b8\u998f\uff1a\u5728\u4e00\u4e2a\u5b66\u751f\u6a21\u578b\u4e0a\u540c\u65f6\u84b8\u998f\u591a\u4e2a\u6559\u5e08\u6a21\u578b\u7684\u77e5\u8bc6\u3002\u81ea\u84b8\u998f\uff1a\u6a21\u578b\u81ea\u8eab\u4f5c\u4e3a\u6559\u5e08\u6a21\u578b\u8fdb\u884c\u84b8\u998f\u3002\u8de8\u6a21\u6001\u84b8\u998f\uff1a\u5728\u4e0d\u540c\u6a21\u6001\uff08\u5982\u56fe\u50cf\u548c\u6587\u672c\uff09\u4e4b\u95f4\u8fdb\u884c\u77e5\u8bc6\u84b8\u998f\u3002", "task_step_level": "5", "task_step_question": "### \u95ee\u9898\uff1a\n\u5728\u6a21\u578b\u84b8\u998f\u7684\u6700\u65b0\u8fdb\u5c55\u4e2d\uff0c\u8de8\u6a21\u6001\u84b8\u998f\uff08Cross-Modal Distillation\uff09\u662f\u5982\u4f55\u5728\u4e0d\u540c\u6a21\u6001\uff08\u5982\u56fe\u50cf\u548c\u6587\u672c\uff09\u4e4b\u95f4\u5b9e\u73b0\u77e5\u8bc6\u8fc1\u79fb\u7684\uff1f\u8fd9\u79cd\u6280\u672f\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u6709\u54ea\u4e9b\u6311\u6218\u548c\u6f5c\u5728\u4f18\u52bf\uff1f", "task_step_question_context": [{"ref_id": "454846907942800944", "chunk_id": "6", "score": 0.54296875, "text": "# Discussion & Conclusion\nOur experiments demonstrate our approach can distill knowledge between two or more models regardless of architecture, algorithm, feature overlap, and under small or large data settings. Since our method targets specific weaknesses of each model, we can distill knowledge between any combination of high and/or low-performance models, compared to traditional knowledge distillation techniques which tend to only distill knowledge from a single high-performance model to a low-performance model (Hinton 2015)(Gou et al. 2021b). Though our method performed well on real-world data sets it does have some assumptions. It assumes there is some overlap between the features of the data sets and most importantly, our method works best when the distribution of the datasets used to train models are significantly different from each other. Further, our method is fundamentally limited by the strength of counterfactual generation. Counterfactual explanations are easy to compute on tabular data but their performance on more complex data, such as images is more challenging. However, more recent approaches have found success in more basic image networks (Goyal et al. 2019) (Sauer and Geiger 2021), so as research progresses, we believe this limitation will be removed.  \n\nWe show in Figure 5 the number of instances each model teaches to the others. Interestingly, this quantity is asymmetrical which will motivate future work to better understand the mechanisms of how each model teaches the others.  \n\nConclusion We present a novel form of knowledge distillation that can be used between multiple models, in multiple directions and is focused. Each model simultaneously acts as teacher and student, distilling knowledge to the other by encoding learned information into virtual counterfactual instances and passing them into the training sets of other models. Unlike other knowledge distillation algorithms, which always distill knowledge from the teacher to student, we use a targeting mechanism to ensure that teachers only distill correct knowledge tailored to a student\u2019s deficiencies.  \n\nIn our four main experiments, our method beats the competitors studied, including state-of-the-art knowledge distillation algorithms. In a stress test to determine if knowledge could be distilled between many (10) models, our model surpasses all but one competitor and remains competitive. We find our method particularly useful in the setting where models can be freely shared, but raw data cannot, and the data sets share some features. This is common in medical imaging or finance communities where data is confidential. Given our method\u2019s strong performance on experiments simulating the aforementioned setting, we believe this to be a viable approach to knowledge distillation under such circumstances.\n#\nAhn, S.; Hu, S. X.; Damianou, A.; Lawrence, N. D.; and Dai, Z. 2019. Variational Information Distillation for Knowledge Transfer. arXiv:1904.05835.   \nAllison, P. D. 2001. Missing Data . Sage Publications. Alsenani, D. 2020. US Cars Dataset: Online Car Auction in North American. Retrieved from https://www.kaggle.com/doaaalsenani/usa-cers-dataset. Ashrapov, I. 2020. Tabular GANs for uneven distribution. arXiv preprint arXiv:2010.00638 .  \nChen, D.; Mei, J.-P.; Wang, C.; Feng, Y.; and Chen, C. 2020. Online Knowledge Distillation With Diverse Peers. In Proceedings of the AAAI Conference on Artificial Intelligence ,volume 34, 3430\u20133437.   \nChen, D.; Mei, J.-P.; Zhang, Y.; Wang, C.; Wang, Z.; Feng, Y.; and Chen, C. 2021. Cross-layer distillation with semantic calibration. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 35.   \nDua, D.; and Graff, C. 2017. UCI Machine Learning Repository. Http://archive.ics.uci.edu/ml.   \nGou, J.; Yu, B.; Maybank, S. J.; and Tao, D. 2021a. Knowledge distillation: A survey. International Journal of Computer Vision , 129(6): 1789\u20131819.   \nGou, J.; Yu, B.; Maybank, S. J.; and Tao, D. 2021b. Knowledge Distillation: A Survey. International Journal of Computer Vision , 129(6).   \nGoyal, Y.; Wu, Z.; Ernst, J.; Batra, D.; Parikh, D.; and Lee, S. 2019. Counterfactual visual explanations. In International Conference on Machine Learning , 2376\u20132384. PMLR. Gromski, P. S.; Xu, Y.; Kotze, H. L.; Correa, E.; Ellis, D. I.; Armitage, E. G.; Turner, M. L.; and Goodacre, R. 2014. Influence of Missing Values Substitutes on Multivariate Analysis of Metabolomics Data. Metabolites , 4(2): 433\u2013452. Heo, B.; Lee, M.; Yun, S.; and Choi, J. Y. 2019. Knowledge Transfer via Distillation of Activation Boundaries Formed by Hidden Neurons. Proceedings of the AAAI Conference on Artificial Intelligence , 33(01).   \nHinton, G. e. A. 2015. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531 .  \nJanosi, A. e. a. 1988. Heart Disease Data Sets.   \nKeane, M. T.; and Smyth, B. 2020. Good counterfactuals and where to find them: A case-based technique for generating counterfactuals for explainable ai (xai). In International Conference on Case-Based Reasoning . Springer.   \nKennedy, J.; and Eberhart, R. 1995. Particle swarm optimization. In Proceedings of ICNN\u201995-international conference on neural networks , volume 4, 1942\u20131948. IEEE. Kim, J.; and Park, S. 2020. Paraphrasing complex network: Network compression via factor transfer. arXiv preprint arXiv:1802.04977 .  \nLiu, Y.; Cao, J.; Li, B.; Yuan, C.; Hu, W.; Li, Y.; and Duan, Y. 2019. Knowledge Distillation via Instance Relationship Graph. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) .Mirzadeh, S. I.; Farajtabar, M.; Li, A.; Levine, N.; Matsukawa, A.; and Ghasemzadeh, H. 2020. Improved knowledge distillation via teacher assistant. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 34, 5191\u20135198.   \nMital, A. 2020. US Used cars dataset. Https://www.kaggle.com/ananaymital/us-used-cars-dataset. Molnar, C. 2019. Interpretable Machine Learning . Independently published.   \nPan, S. J.; and Yang, Q. 2010. A Survey on Transfer Learning. IEEE Transactions on Knowledge and Data Engineering , 22(10): 1345\u20131359.   \nRadford, A.; Metz, L.; and Chintala, S. 2016. Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434 .Reese, A. 2021. Used Cars Dataset Vehicles listings from Craigslist.org. Https://www.kaggle.com/austinreese/craigslist-carstrucksdata.   \nRomero, A.; Ballas, N.; Kahou, S. E.; Chassang, A.; Gatta, C.; and Bengio, Y. 2014. Fitnets: Hints for thin deep nets. arXiv preprint arXiv:1412.6550 .  \nSauer, A.; and Geiger, A. 2021. Counterfactual generative networks. arXiv preprint arXiv:2101.06046 .  \nTung, F.; and Mori, G. 2019. Similarity-preserving knowledge distillation. In Proceedings of the IEEE/CVF International Conference on Computer Vision , 1365\u20131374.   \nXiao, H.; Rasul, K.; and Vollgraf, R. 2017. Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms. arXiv:1708.07747.   \nXie, Q.; Luong, M.-T.; Hovy, E.; and Le, Q. V. 2020. Selftraining with noisy student improves imagenet classification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 10687\u201310698.   \nYim, J.; Joo, D.; Bae, J.; and Kim, J. 2017. A Gift From Knowledge Distillation: Fast Optimization, Network Minimization and Transfer Learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) .  \nZhang, H.; Cisse, M.; Dauphin, Y. N.; and Lopez-Paz, D. 2017. mixup: Beyond Empirical Risk Minimization.   \nZhang, H.; Hu, Z.; Qin, W.; Xu, M.; and Wang, M. 2021. Adversarial co-distillation learning for image recognition. Pattern Recognition , 111: 107659.   \nZhang, Y.; Xiang, T.; Hospedales, T. M.; and Lu, H. 2018. Deep mutual learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , 4320\u2013 4328.   \nZhang, Z.; and Sabuncu, M. R. 2020. Self-distillation as instance-specific label smoothing. arXiv preprint arXiv:2006.05065 .  \nZhuang, F.; Qi, Z.; Duan, K.; Xi, D.; Zhu, Y.; Zhu, H.; Xiong, H.; and He, Q. 2020. A comprehensive survey on transfer learning. Proceedings of the IEEE , 109(1): 43\u201376."}, {"ref_id": "454898870263482564", "chunk_id": "0", "score": 0.51953125, "text": "# B. Challenges\n(1) The Balance Between Model Compression and Performance  \n\nKDisaneffectivemodelcompressiontechnique,andeffective knowledge transfer helps with the construction of a lightweight network model; such lightweight models are more suitable for practical applications with particular model efficiency requirements and device performance limitations [4] ,[61] ,[62] .Complex networks have large-scale network parameters and a long running time, making them unsuitable for such practical applications [63] . However, it is also challenging for lightweight networks to learn effective visual features from large-scale complex datasets [4] . A balance must therefore be struck between model compression and model performance improvement.  \n\n(2) The Imbalance Between Multi-Modal Data Features  \n\nAt present, most KD-based OD models focus on transferring knowledge within the image domain; only a few works have attempted to extract additional features from other modal data (infrared images, depth images, text, index data, etc.) [19] ,[35] ,[55] . The introduction of multi-modal features is beneficial to KD. However, a more challenging problem is that of how to deal with the imbalance between multi-modal data features. Here, \u201cimbalance\u201d means that significant differences exist between the feature dimensions and semantic information of different modalities. For example, the dimensions of features extracted from visual data and index data were significantly different. In addition, the semantic feature gaps between the textual and visualfeaturesofRGBimagesalsomakeitdifficulttousetextinformation to guide the visual feature learning of student models. Therefore, another challenge is that of combining imbalanced multi-modal features to guide student models for feature learning [19] ,[64] ; new feature fusion mechanisms or multi-modal information-guided mechanisms need to be designed for student model learning.  \n\n(3) Designing or Selecting Superior Teacher and Student Models  \n\nKDtechnologyisutilizedtotransfertheknowledgelearnedby complex teacher models to lightweight student models. An optimal teacher model or model combination has a very favourable influence on guiding the feature learning of a student model. Therefore, the selection of teacher and student models makes a very important contribution to the performance of the final student models [53] ,[65] . However, as there are many complex and lightweight models for OD, it is difficult to choose the appropriate teacher and student models for specific OD problems."}, {"ref_id": "454845510461164774", "chunk_id": "2", "score": 0.45703125, "text": "# 2 Related Work\n\n# 2.1 Multi-modal Machine Translation\nAs an intersection of multimedia and neural machine translation (NMT), MMT has drawn great attention in the research community. Technically, existing methods mainly focus on how to better integrate visual information into the framework of NMT. 1) Calixto et al. (2017 ) propose a doublyattentive decoder to incorporate two separate attention over the source words and visual features. 2) Ive et al. (2019 ) propose a translate-and-refine approach to refine draft translations by visual features. 3) Yao and Wan (2020 ) propose the multimodal Transformer to induce the image representations from the text under the guide of image-aware attention. 4) Yin et al. (2020 )employs a unified multimodal graph to capture various semantic interactions between multimodal semantic units.  \n\nHowever, the quantity and quality of the annotated images limit the development of this task, which is scarce and expensive. In this work, we aim to perform the MMT in an image-free manner, which has the ability to break data constraints.\n\n# 2.2 Knowledge Distillation\nKnowledge distillation (KD) ( Buciluco et al. ,2006 ;Hinton et al. ,2015 ) aims to use a knowledgerich teacher network to guide the parameter learning of the student network. In fact, KD has been investigated in a wide range of fields. Romero et al. (2014 ) transfer knowledge through an intermediate hidden layer to extend the KD. Yim et al. (2017 ) define the distilled knowledge to be transferred in terms of flow between layers, which is calculated by the inner product between features from two layers. In the multimedia field, Gupta et al. (2016 ) first introduce the technique that transfers supervision between images from different modalities. Yuan and Peng (2018 )propose the symmetric distillation networks for the text-to-image synthesis task.  \n\nInspired by these pioneering efforts, our IKDMMT framework is intents to take full advantage of KD to generate a multimodal feature to overcome triplet data constraints.\n\n# 3 IKD-MMT Model\nAs illustrated in Figure 2 , the proposed framework consists of two components: an image-free MMT backbone and a multimodal feature generator.  \n\n  \nFigure 2: The framework of our IKD-MMT model. The multimodal feature generator, multimodal student network and visual teacher network are the most critical modules, which help break the dataset constraints of image-must.\n\n# 3.1 Image-Free MMT Backbone\nGiven a source sentence $X{=}\\left(x_{1},\\ldots,x_{I}\\right)$ , each token $x_{i}$ is mapped into a word embedding vector $E_{x_{i}}\\,\\in\\,\\mathbb{R}^{d_{w}}$ through the textual embedding w position encoding ( Gehring et al. ,2017 ). $d_{w}$ and $t=(E_{x_{1}},\\ldots,E_{x_{I}})$ are the word embedding dimension and the textual feature, respectively.  \n\nThen, we feed the text feature $t$ together with the multimodal feature $m$ (detail in Section 3.2.1) into the multimodal transformer encoder ( Yao and Wan ,2020 ). In the multimodal encoder layer, we cascade the multimodal feature $m$ and the text feature $t$ to reorganize a new multimodal feature $\\widetilde{x}$ as the query vector:  \n\n$$\n\\widetilde{x}{=}[t;m W^{m}]\\in\\mathbb{R}^{(I+P)*d},\n$$  \n\nwhere $I$ is the length of source sentence, and e$P$ is the size of multimodal feature. Here, we can understand this modal fusion from the perspective of nodes and graphs. If we treat each source token as a node, each region of the multimodal feature can also be regarded as a pseudo-token and added to the source token graph for modal fusion. The key and value vectors are preserved as the text feature $t$ , and the multimodal encoder layer is calculated as follows:  \n\n$$\n\\begin{array}{c}{{c_{k}=\\displaystyle\\sum_{i=1}^{I}\\tilde{\\alpha}_{k i}\\left(t_{i}W^{V}\\right),}}\\\\ {{\\tilde{\\alpha}_{k i}=s o f t m a x\\left(\\frac{\\left(\\tilde{\\alpha}_{k}W^{Q}\\right)\\left(t_{i}W^{K}\\right)^{\\top}}{\\sqrt{d}}\\right).}}\\end{array}\n$$  \n\nIn this paper, we directly adopt the Transformer decoder 2 (Vaswani et al. ,2017 ) for translation.  \n\nGiven a target sentence $Y{=}\\left(y_{1},\\ldots,y_{J}\\right)$ , our framework outputs the predicted probability of the target word $y_{j}$ as follow:  \n\n$$\np\\left(y_{j}|y_{<j},\\mathbf{X},m\\right)\\propto\\exp\\left(W^{h}H_{j}^{L}+b^{h}\\right),\n$$  \n\nwhere $H_{j}^{L}$ represents the top output of the decoder at $j$ -th decoding time step, $W^{h}$ and $b^{h}$ are learnable multi-layer perceptrons, and $\\exp()$ is a Softmax layer.\n\n# 3.2 Multimodal Feature Generation\n\n# 3.2.1 Preliminaries\nIn this part, we introduce the frame, symbol definitions and task goal of multimodal feature generation in advance.  \n\nThe frame is composed of a multimodal feature generator $F$ , a visual teacher model $T$ and a multimodal student model $S$ .The detailed architecture of each module is shown in Table 7 of the appendix. The model parameters of $S$ are denoted as $\\theta^{s}$ . When the global text feature $\\bar{t}$ is fed into by the $l$ -th layer is denoted as $S$ , the hidden representation produced $\\varphi_{l}^{S}\\left(\\bar{t},\\theta_{l}^{s}\\right)$ \u0000\u0001. The $F$ outputs a multimodal feature $m$ , and the $S$ produces an inverse feature $I_{s}$ after the $S.$ -conv1 layer. The real image and the inverse feature are $\\{I_{s},I_{r}\\}\\in\\mathbb{R}^{m*n*3}$ . Given a feature $I$ s input, the hidden representation produced by the l-th layer of $T$ is denoted as $\\varphi_{l}^{T}(I)$ .  \n\nOur goal is to generate multimodal features from the source text to break the image-must restriction in testing. The visual perception of this multimodal feature is extracted from the visual distillation of the teacher-student model, while the textual semantic of that is derived from the text translation of the input text."}], "task_step_question_answer": "\u8de8\u6a21\u6001\u84b8\u998f\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u8fd8\u53ef\u4ee5\u63a2\u7d22\u5982\u4f55\u66f4\u597d\u5730\u878d\u5408\u4e0d\u540c\u6a21\u6001\u7684\u7279\u5f81\uff0c\u7279\u522b\u662f\u5728\u6a21\u6001\u7279\u5f81\u4e0d\u5e73\u8861\u7684\u60c5\u51b5\u4e0b\u3002\u4f8b\u5982\uff0c\u901a\u8fc7\u5f15\u5165\u81ea\u9002\u5e94\u7279\u5f81\u5bf9\u9f50\u673a\u5236\uff0c\u53ef\u4ee5\u52a8\u6001\u8c03\u6574\u4e0d\u540c\u6a21\u6001\u7279\u5f81\u7684\u6743\u91cd\uff0c\u4ece\u800c\u66f4\u6709\u6548\u5730\u8fdb\u884c\u77e5\u8bc6\u8fc1\u79fb\u3002\u6b64\u5916\uff0c\u8de8\u6a21\u6001\u84b8\u998f\u5728\u6570\u636e\u9690\u79c1\u4fdd\u62a4\u65b9\u9762\u7684\u6f5c\u529b\u4e5f\u503c\u5f97\u8fdb\u4e00\u6b65\u6316\u6398\u3002\u901a\u8fc7\u77e5\u8bc6\u8fc1\u79fb\uff0c\u53ef\u4ee5\u5728\u4e0d\u76f4\u63a5\u8bbf\u95ee\u539f\u59cb\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u6a21\u578b\u7684\u4f18\u5316\u548c\u6027\u80fd\u63d0\u5347\u3002\u672a\u6765\u7814\u7a76\u8fd8\u53ef\u4ee5\u5173\u6ce8\u5982\u4f55\u5c06\u8de8\u6a21\u6001\u84b8\u998f\u4e0e\u5176\u4ed6\u5148\u8fdb\u6280\u672f\uff08\u5982\u8054\u90a6\u5b66\u4e60\u3001\u5143\u5b66\u4e60\uff09\u7ed3\u5408\uff0c\u4ee5\u8fdb\u4e00\u6b65\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u5e94\u7528\u8303\u56f4\u3002", "ref_task_step_id": ""}, "__type__": "task_step_node"}, "61a16cbb-0e64-4546-8bf4-5ab34231e80c": {"__data__": {"id_": "61a16cbb-0e64-4546-8bf4-5ab34231e80c", "metadata": {}, "relationships": {}, "hash": "", "start_task_context": "\u6a21\u578b\u84b8\u998f\u7684\u662f\u4ec0\u4e48\uff1f", "aemo_representation_context": "\u6a21\u578b\u84b8\u998f\uff08Model Distillation\uff09\u662f\u4e00\u79cd\u5728\u673a\u5668\u5b66\u4e60\u9886\u57df\u5e7f\u6cdb\u4f7f\u7528\u7684\u6280\u672f\uff0c\u4e3b\u8981\u7528\u4e8e\u5c06\u4e00\u4e2a\u5927\u578b\u3001\u590d\u6742\u4e14\u9ad8\u6027\u80fd\u7684\u6a21\u578b\uff08\u901a\u5e38\u79f0\u4e3a\u6559\u5e08\u6a21\u578b\uff09\u7684\u77e5\u8bc6\u8fc1\u79fb\u5230\u4e00\u4e2a\u66f4\u5c0f\u3001\u66f4\u7b80\u5355\u4e14\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u7684\u6a21\u578b\uff08\u901a\u5e38\u79f0\u4e3a\u5b66\u751f\u6a21\u578b\uff09\u4e2d\u3002\u4ee5\u4e0b\u662f\u6a21\u578b\u84b8\u998f\u7684\u8be6\u7ec6\u5206\u89e3\uff1a\n\n### 1. **\u57fa\u672c\u6982\u5ff5**\n- **\u6559\u5e08\u6a21\u578b\uff08Teacher Model\uff09**\uff1a\u4e00\u4e2a\u8bad\u7ec3\u597d\u7684\u3001\u6027\u80fd\u4f18\u5f02\u4f46\u53ef\u80fd\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u7684\u6a21\u578b\u3002\n- **\u5b66\u751f\u6a21\u578b\uff08Student Model\uff09**\uff1a\u4e00\u4e2a\u66f4\u5c0f\u3001\u66f4\u7b80\u5355\u3001\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u7684\u6a21\u578b\uff0c\u76ee\u6807\u662f\u6a21\u4eff\u6559\u5e08\u6a21\u578b\u7684\u8868\u73b0\u3002\n\n### 2. **\u84b8\u998f\u8fc7\u7a0b**\n- **\u8bad\u7ec3\u6559\u5e08\u6a21\u578b**\uff1a\u9996\u5148\uff0c\u4f7f\u7528\u5927\u91cf\u6570\u636e\u8bad\u7ec3\u4e00\u4e2a\u9ad8\u6027\u80fd\u7684\u6559\u5e08\u6a21\u578b\u3002\n- **\u751f\u6210\u8f6f\u6807\u7b7e**\uff1a\u5229\u7528\u6559\u5e08\u6a21\u578b\u5bf9\u8bad\u7ec3\u6570\u636e\u8fdb\u884c\u9884\u6d4b\uff0c\u751f\u6210\u8f6f\u6807\u7b7e\uff08\u5373\u6982\u7387\u5206\u5e03\uff09\uff0c\u800c\u4e0d\u662f\u7b80\u5355\u7684\u786c\u6807\u7b7e\uff08\u5373\u7c7b\u522b\u6807\u7b7e\uff09\u3002\n- **\u8bad\u7ec3\u5b66\u751f\u6a21\u578b**\uff1a\u4f7f\u7528\u539f\u59cb\u6570\u636e\u548c\u6559\u5e08\u6a21\u578b\u751f\u6210\u7684\u8f6f\u6807\u7b7e\u6765\u8bad\u7ec3\u5b66\u751f\u6a21\u578b\u3002\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u5b66\u751f\u6a21\u578b\u7684\u635f\u5931\u51fd\u6570\u901a\u5e38\u5305\u62ec\u4e24\u90e8\u5206\uff1a\n  - **\u786c\u6807\u7b7e\u635f\u5931**\uff1a\u5b66\u751f\u6a21\u578b\u9884\u6d4b\u4e0e\u771f\u5b9e\u6807\u7b7e\u4e4b\u95f4\u7684\u5dee\u5f02\u3002\n  - **\u8f6f\u6807\u7b7e\u635f\u5931**\uff1a\u5b66\u751f\u6a21\u578b\u9884\u6d4b\u4e0e\u6559\u5e08\u6a21\u578b\u9884\u6d4b\u4e4b\u95f4\u7684\u5dee\u5f02\u3002\n\n### 3. **\u5173\u952e\u6280\u672f\u70b9**\n- **\u6e29\u5ea6\u8c03\u8282**\uff1a\u5728\u751f\u6210\u8f6f\u6807\u7b7e\u65f6\uff0c\u901a\u5e38\u5f15\u5165\u4e00\u4e2a\u6e29\u5ea6\u53c2\u6570\uff08Temperature\uff09\u6765\u5e73\u6ed1\u6982\u7387\u5206\u5e03\uff0c\u4f7f\u5f97\u8f6f\u6807\u7b7e\u5305\u542b\u66f4\u591a\u7684\u4fe1\u606f\u3002\n- **\u635f\u5931\u51fd\u6570\u8bbe\u8ba1**\uff1a\u7ed3\u5408\u786c\u6807\u7b7e\u635f\u5931\u548c\u8f6f\u6807\u7b7e\u635f\u5931\uff0c\u5e73\u8861\u4e24\u8005\u7684\u91cd\u8981\u6027\u3002\n\n### 4. **\u5e94\u7528\u573a\u666f**\n- **\u6a21\u578b\u538b\u7f29**\uff1a\u5c06\u5927\u578b\u6a21\u578b\u538b\u7f29\u6210\u5c0f\u578b\u6a21\u578b\uff0c\u4fbf\u4e8e\u90e8\u7f72\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8bbe\u5907\u4e0a\u3002\n- **\u77e5\u8bc6\u8fc1\u79fb**\uff1a\u5c06\u4e00\u4e2a\u4efb\u52a1\u4e0a\u5b66\u5230\u7684\u77e5\u8bc6\u8fc1\u79fb\u5230\u53e6\u4e00\u4e2a\u4efb\u52a1\u4e0a\u3002\n- **\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b**\uff1a\u901a\u8fc7\u5b66\u4e60\u6559\u5e08\u6a21\u578b\u7684\u8f93\u51fa\uff0c\u5b66\u751f\u6a21\u578b\u53ef\u80fd\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u5177\u6709\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002\n\n### 5. **\u4f18\u52bf\u4e0e\u5c40\u9650\u6027**\n- **\u4f18\u52bf**\uff1a\n  - **\u8ba1\u7b97\u6548\u7387\u9ad8**\uff1a\u5b66\u751f\u6a21\u578b\u66f4\u5c0f\uff0c\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u4f4e\u3002\n  - **\u6027\u80fd\u63a5\u8fd1**\uff1a\u5b66\u751f\u6a21\u578b\u80fd\u63a5\u8fd1\u6559\u5e08\u6a21\u578b\u7684\u6027\u80fd\u3002\n- **\u5c40\u9650\u6027**\uff1a\n  - **\u6027\u80fd\u5dee\u8ddd**\uff1a\u5b66\u751f\u6a21\u578b\u6027\u80fd\u53ef\u80fd\u4ecd\u7565\u4f4e\u4e8e\u6559\u5e08\u6a21\u578b\u3002\n  - **\u8bad\u7ec3\u590d\u6742**\uff1a\u9700\u8981\u5148\u8bad\u7ec3\u4e00\u4e2a\u9ad8\u6027\u80fd\u7684\u6559\u5e08\u6a21\u578b\u3002\n\n### 6. **\u6700\u65b0\u8fdb\u5c55**\n- **\u591a\u4efb\u52a1\u84b8\u998f**\uff1a\u5728\u4e00\u4e2a\u5b66\u751f\u6a21\u578b\u4e0a\u540c\u65f6\u84b8\u998f\u591a\u4e2a\u6559\u5e08\u6a21\u578b\u7684\u77e5\u8bc6\u3002\n- **\u81ea\u84b8\u998f**\uff1a\u6a21\u578b\u81ea\u8eab\u4f5c\u4e3a\u6559\u5e08\u6a21\u578b\u8fdb\u884c\u84b8\u998f\u3002\n- **\u8de8\u6a21\u6001\u84b8\u998f**\uff1a\u5728\u4e0d\u540c\u6a21\u6001\uff08\u5982\u56fe\u50cf\u548c\u6587\u672c\uff09\u4e4b\u95f4\u8fdb\u884c\u77e5\u8bc6\u84b8\u998f\u3002\n\n### 7. **\u672a\u6765\u7814\u7a76\u65b9\u5411**\n- **\u66f4\u9ad8\u6548\u7684\u84b8\u998f\u65b9\u6cd5**\uff1a\u63a2\u7d22\u66f4\u6709\u6548\u7684\u77e5\u8bc6\u8fc1\u79fb\u6280\u672f\u3002\n- **\u6cdb\u5316\u80fd\u529b\u7684\u63d0\u5347**\uff1a\u7814\u7a76\u5982\u4f55\u901a\u8fc7\u84b8\u998f\u63d0\u9ad8\u6a21\u578b\u5728\u672a\u89c1\u6570\u636e\u4e0a\u7684\u8868\u73b0\u3002\n- **\u591a\u6a21\u6001\u84b8\u998f\u7684\u62d3\u5c55**\uff1a\u63a2\u7d22\u66f4\u591a\u8de8\u6a21\u6001\u77e5\u8bc6\u84b8\u998f\u7684\u5e94\u7528\u573a\u666f\u3002\n\n\u901a\u8fc7\u4ee5\u4e0a\u6b65\u9aa4\uff0c\u53ef\u4ee5\u5168\u9762\u7406\u89e3\u6a21\u578b\u84b8\u998f\u7684\u6982\u5ff5\u3001\u8fc7\u7a0b\u3001\u6280\u672f\u7ec6\u8282\u3001\u5e94\u7528\u573a\u666f\u53ca\u5176\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u7814\u7a76\u4e2d\u7684\u91cd\u8981\u6027\u3002\u6a21\u578b\u84b8\u998f\u4e0d\u4ec5\u662f\u4e00\u79cd\u6709\u6548\u7684\u6a21\u578b\u538b\u7f29\u6280\u672f\uff0c\u4e5f\u662f\u77e5\u8bc6\u8fc1\u79fb\u548c\u63d0\u5347\u6a21\u578b\u6027\u80fd\u7684\u91cd\u8981\u624b\u6bb5\u3002", "task_step_name": "\u672a\u6765\u7814\u7a76\u65b9\u5411", "task_step_description": "\u66f4\u9ad8\u6548\u7684\u84b8\u998f\u65b9\u6cd5\uff1a\u63a2\u7d22\u66f4\u6709\u6548\u7684\u77e5\u8bc6\u8fc1\u79fb\u6280\u672f\u3002\u6cdb\u5316\u80fd\u529b\u7684\u63d0\u5347\uff1a\u7814\u7a76\u5982\u4f55\u901a\u8fc7\u84b8\u998f\u63d0\u9ad8\u6a21\u578b\u5728\u672a\u89c1\u6570\u636e\u4e0a\u7684\u8868\u73b0\u3002\u591a\u6a21\u6001\u84b8\u998f\u7684\u62d3\u5c55\uff1a\u63a2\u7d22\u66f4\u591a\u8de8\u6a21\u6001\u77e5\u8bc6\u84b8\u998f\u7684\u5e94\u7528\u573a\u666f\u3002", "task_step_level": "6", "task_step_question": "### \u95ee\u9898\uff1a\n\u5728\u6a21\u578b\u84b8\u998f\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e2d\uff0c\u5982\u4f55\u8bbe\u8ba1\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u84b8\u998f\u65b9\u6cd5\uff0c\u4ee5\u786e\u4fdd\u5b66\u751f\u6a21\u578b\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\uff0c\u80fd\u591f\u66f4\u6709\u6548\u5730\u4ece\u6559\u5e08\u6a21\u578b\u4e2d\u8fc1\u79fb\u77e5\u8bc6\uff0c\u5e76\u63d0\u5347\u5176\u5728\u672a\u89c1\u6570\u636e\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\uff1f\u6b64\u5916\uff0c\u591a\u6a21\u6001\u84b8\u998f\u7684\u62d3\u5c55\u5982\u4f55\u5728\u4e0d\u540c\u6a21\u6001\uff08\u5982\u56fe\u50cf\u548c\u6587\u672c\uff09\u4e4b\u95f4\u5b9e\u73b0\u66f4\u6709\u6548\u7684\u77e5\u8bc6\u8fc1\u79fb\uff0c\u4ee5\u5e94\u5bf9\u590d\u6742\u591a\u6a21\u6001\u4efb\u52a1\u7684\u9700\u6c42\uff1f", "task_step_question_context": [{"ref_id": "454965221526340508", "chunk_id": "0", "score": 0.62890625, "text": "# 6 CONCLUSION\nIn this paper, we proposed a new method for model compression, which simultaneously considers model accuracy and model size. In particular, we first revisited model compression and analyzed the factors that influence the performance of model compression. Specifically, we found that there are two attributes for model compression: distillability and sparsablity. Distillability indicates how much useful knowledge can be extracted from a teacher network. Sparsablity indicates the extent to which the model can be pruned. By exploring distillability and sparsablity, a guide for model compression during training can be obtained. Inspired by our observations, we proposed a novel dynamically distillability-and-sparsablity learning framework (DDSL), comprising a teacher, a student and a dean. The teacher guides the student using the distilled knowledge, while the student learns to become more accurate and more compact. In addition, the dean controls the whole learning process. In this manner, the supervisions are dynamically adjusted and a good balance is achieved. In order to optimize the proposed framework, an ADMM-based knowledge distillation-with-pruning (KDP) joint optimization algorithm was presented for updating the model. The joint optimization improves the accuracy and the pruned ratio of our method. Finally, experimental results showed that our method outperforms 24 state-of-the-art methods in terms of accuracy and pruned ratio on both small-scale and large-scale databases.  \n\n  \nFig. 15. The test Top-1 accuracy curves and pruned ratio PR FLOPS curves at different outer loop learning rates $\\zeta_{\\mathrm{out}}$ and batch sizes. Note that the inter loop learning rate $\\zeta_{\\mathrm{in}}$ is commonly set to 0.1. (a) The curves of pruning ResNet101 on ImageNet. (b) The curves of pruning MobileNetV2 on ImageNet.  \n\nWe foresee three directions for future research in this area. First, it would be promising to extend our method to datafree situation. Second, the acceleration and simplification of the training procedure is another future work, for making it more convenient and efficient. Third, in addition to the convolutional neural network, it is interesting to extend our method to other architectures, such as Transformer [ 70 ].\n\n# A CKNOWLEDGMENTS\nThis work was supported by the National Key Research and Development Program of China (Grant No. 2020AAA0106800), the Natural Science Foundation of China (Grant No.61902401, No. 62192785, No. 61972071, No. U1936204, No. 62122086, No. 62036011, No. 62192782 and No. 61721004), the Beijing Natural Science Foundation No. M22005, the CAS Key Research Program of Frontier Sciences (Grant No. QYZDJ-SSW-JSC040). The work of Bing Li was also supported by the Youth Innovation Promotion Association, CAS."}, {"ref_id": "454846429944424370", "chunk_id": "5", "score": 0.59375, "text": "# 2. Related Work\nKnowledge distillation (KD) was initially introduced as a model compression technique [ 7 ], where the goal is to train a smaller student model from the output of a teacher model [ 23 ]. While early work focused on predicting the final outputs of a classification model, the idea was rapidly extended to other forms of distillation, such as distilling intermediate representations [ 1 ,21 ,22 ,49 ,73 ,75 ,79 ]. These methods perform well but require careful layer selection and loss balancing [ 21 ]. In our work, instead of matching layer-wise representations between the student and teacher architectures, we add shortcut connections from intermediate layers of the student to the loss of each teacher.  \n\nMulti-teacher knowledge distillation. KD can naturally be extended to an ensemble of teachers so that student can benefit from their potential complementarity. While the final outputs of teachers trained for the same task can simply be averaged [ 3 ,15 ,23 ,75 ], multi-teacher distillation with teachers trained for different tasks is more challenging. UDON [ 76 ] first trains domain-specialist teachers which are subsequently distilled in a student model using adaptive data sampling for balancing the different domains. In [ 60 ], contrastive learning is used for ensemble distillation while [ 56 ] proposes a framework tailored for teachers trained with masked image modeling and contrastive learning. But such approaches are not straightforward to extend to teachers learned differently. Similarly, [ 71 ] combines self-supervised teachers from arbitrary heterogeneous pretext tasks. [13 ,16 ,51 ] focus on jointly utilizing pseudo- and true labels for multi-teacher distillation. Roth et al .[51 ] formulate multi-teacher distillation as continual learning and further propose a novel method for data partitioning based on confidence. Here we develop a more generic method for combining teachers, that is not limited to certain types of teachers or losses, and, unlike [ 30 ,51 ], does not require labeled data, nor classifiers associated with each teacher for obtaining pseudo-labels.  \n\nLoss balancing is shown to be crucial in multi-task learning [ 11 ,24 ,26 ,78 ]. Similar strategies to automatically balance losses have also been proposed for multi-teacher distillation [ 15 ,32 ]. In [ 24 ], adaptive loss weights inversely proportional to the average of each loss are introduced, while [ 32 ]learns instance-level teacher importance weights using ground-truth labels. In [ 15 ], the random selection of one teacher per mini-batch is shown to help. Our experiments show that our proposed generalized teacher dropping strategy leads to better models compared to [ 15 ,24 ].  \n\nDistilling from a \u201cfoundation model\u201d like CLIP [43 ] or DINOv2 [ 39 ] is an effective approach for tasks with limited training data [ 36 ,42 ,67 ]. Distilling from multiple foundation models allows for more versatile students. Recent works like AMRADIO [ 46 ], SAM-CLIP [ 65 ], and Open Vocabulary SAM [ 77 ] combine the semantics captured by CLIP with the localization capabilities of models like DINOv2 [ 39 ] or SAM [ 27 ]. AM-RADIO [ 46 ] builds on the same base setup as our study, but employs no loss balancing. Another difference comes from the fact that their student encoder is only a part of the final model: AM-RADIO requires the teacher-specific projectors learned during distillation to also be used at test time, effectively increasing the parameters of the encoder with task-specific ones. Instead, our method performs well on multiple classification tasks out-ofthe-box , without any additional parameters.  \n\nCombining models beyond distillation. Other ways to combine pretrained models have been proposed. Works like [ 37 ,44 ,45 ,59 ,68 ] explore different weight averaging strategies. They typically only combine models that differ by their hyperparameter configuration. Aiming at generalization, [72 ] merges multiple ViTs, each specialized to a classification task, into a single encoder that solves all classification tasks jointly, via a gating network. Instead, our students are distilled from scratch, have a simple ViT architecture, and tackle diverse tasks with simple linear probing.  \n\nExpendable projectors are extra modules that act as buffers between the final encoder output and the space where the loss is computed. They have been successfully used for both self-supervised [ 9 ,10 ] and supervised learning [ 55 ,66 ]. We extend this idea and add projectors during training to intermediate layers as well. Roth et al . [ 50 ] use several such projectors of varied dimensionality for metric learning, but do not use features from intermediate layers. Moreover, we use a specific set of projectors per teacher, similar to [ 3 ,46 ]. This way, projectors become loss-specific ,i.e . they contribute to the loss for only one of the teachers.\n\n# 3. Improving multi-teacher distillation\nIn this section we first present the multi-teacher distillation setup we use as a basis for our analysis (Sec. 3.1 ) and a summary of our evaluation protocol (Sec. 3.2 ). We then delve into challenges around multi-teacher distillation of ViT encoders (Sec. 3.3 ), and offer improvements to the basic setup to overcome them, like enhanced expendable teacherspecific projectors heads (Sec. 3.4 ) and strategies to more equally learn from all teachers (Sec. 3.5 )."}, {"ref_id": "454845510461164774", "chunk_id": "2", "score": 0.55078125, "text": "# 2 Related Work\nLanguage Model Compression. Pre-trained language models ( Devlin et al. ,2019 ;Clark et al. ,2020 ;Brown et al. ,2020 ) perform remarkably well on various applications but at the cost of high computation and memory usage. To deploy these powerful models into resource-scarce scenarios, various attempts have been made to compress the language models into small ones. Quantization methods ( Zafrir et al. ,2019 ;Shen et al. ,2020 ;Zhang et al. ,2020 ;Bai et al. ,2021 ) convert the model parameters to lower precision. Pruning approaches identify then remove unimportant individual weights or structures ( Michel et al. ,2019 ;Fan et al. ,2019 ;Gordon et al. ,2020 ;Hou et al. ,2020 ). Weight sharing techniques ( Dehghani et al. ,2018 ;Lan et al. ,2019 ) allow the model to reuse the transformer layer multiple times to reduce parameters.  \n\nKnowledge Distillation. Knowledge distillation ( Hinton et al. ,2015 ) is another major line of research to do model compression, which is the main concentration in this paper. Hinton et al. (2015 )first proposed to minimize the KL-divergence between the predicted distributions of the teacher and the student. Sanh et al. (2019 ); Sun et al. (2019 ); Liang et al. (2020 ) adopted this objective to teach the student on masked language modeling or text classification tasks. Romero et al. (2014 ) proposed to directly match the feature activations of the teacher and the student. Jiao et al. (2020 ) followed the idea and took the intermediate representations in each transformer layer of the teacher as one of the knowledge to be transferred. Tian et al. (2019 ) proposed a contrastive distillation framework where the teacher\u2019s representations were treated as positives to the corresponding student\u2019s representations. Sun et al. (2020 ); Fu et al. (2021 ) customized this idea to language model compression and proved its effectiveness. Researchers also attempted to use the mutual relations of representations as the knowledge to transfer. In the literature of image classification, Peng et al. (2019 ); Tung and Mori (2019 ); Park et al. (2019 ) pointed out that the relations of the image representations of the teacher should be preserved in the student\u2019s feature space, and adopted a series of geometric measurements to model the sample relations. For distilling transformer models, Park et al. (2021 ) enforced the relations across tokens and layers between the teacher and the student to be consistent. Jiao et al. (2020 ); Wang et al. (2020 ,2021 ) used the attention dependencies between tokens to teach the student. In this paper, we propose to transfer the multi-granularity knowledge to the student. Different from previous works that only considered a single granularity of representations, we jointly transfer the token-level, span-level and sample-level structural knowledge. And compared with Shao and Chen (2021 ) which considered the multi-granularity visual features in an image as the knowledge, our method works in a different modality, presents a different definition of granularity, and prepares the multi-granularity knowledge as the structural relations among representations.  \n\n  \nFigure 1: The overall framework of MGSKD.\n\n# 3 Method\nWe propose MultiGranularity Structural Knowledge Distillation, a novel framework to distill the knowledge from a large transformer language model to a small one. Different from previous works that transferred the knowledge derived from either token-level or sample-level outputs, we prepare the knowledge in three semantic granularities: token-level, span-level and sample-level. Given some granularity of representations of the teacher model, we form the knowledge as the structural relations, i.e., the pair-wise interactions and the triplet-wise geometric angles, between the representations. We then distill the well-organized structural knowledge to the student hierarchically across layers, where the token-level and the span-level knowledge are transferred to the bottom layers to provide more syntactic guidance while the sample-level knowledge is transferred to the upper layers to offer more help of semantic understanding. The framework of MGSKD is illustrated in Figure 1 .\n\n# 3.1 Multi-granularity Representation\nNatural languages have multiple granularities of conceptual units. In the context of pre-trained transformers ( Devlin et al. ,2019 ), the basic unit is the tokens produced by sub-word tokenizers ( Wu et al. ,2016 ;Radford et al. ,2019 ). Several consecutive tokens become a text span, and the sample is comprised of all the tokens it contains. Existing knowledge distillation approaches ( Jiao et al. ,2020 ;Wang et al. ,2020 ;Sun et al. ,2020 ;Fu et al. ,2021 ) focused on one granularity of representation, neglecting that texts are built upon language units from multiple granularities. Intuitively, incorporating multi-granularity representations in knowledge distillation may provide more guidance since the student can be taught how to compose the semantic concepts from small granularities to larger ones. Therefore, we propose to gather multi-granularity representations for knowledge distillation. We construct three granularities of representations: tokens, spans that hold complete meanings, and samples.  \n\nToken Representation. The first granularity is the sub-word token, which is the foundation of high-level granularity. Given an input text, a tokenizer such as WordPiece ( Wu et al. ,2016 ) splits it into $n$ tokens $x\\,=\\,[t_{1},t_{2},.\\ldots,t_{n}]$ . The tokens are converted to a sequence of continuous representations $\\pmb{E}=[e_{1},e_{2},\\allowbreak...,e_{n}]\\in\\mathbb{R}^{n\\times d}$ through the embedding layer. For the sake of clarity, we treat the embedding layer as the 0 -th layer and set $H^{0}\\;=\\;E$ . Then the token embeddings $H^{0}$ are passed to $L$ stacked transformer layers. The $l$ -th layer takes the output representations $H^{l-1}$ of the previous layer as its input, and returns the updated representations $H^{l}$ using multi-head attention (MHA) and position-wise feed-forward network (FFN). Herein, we obtain $L{+1}$ layers of token representations $\\{H^{l}\\}_{l=0}^{L}$ where $\\pmb{H}^{l}\\in\\mathbb{R}^{n\\times d}$ .  \n\nSpan Representation. The second granularity is the span, which is comprised of several consecutive tokens. Different from SpanBERT ( Joshi et al. ,2020 ) that randomly selects token spans whose start positions and lengths are sampled from some distributions for masked language modeling, we propose to extract spans that have complete meanings. Widely adopted sub-word tokenizers in pre-trained transformers split some of the English words into several sub-word tokens. We consider these whole words consisting of multiple sub-word tokens, and phrases, as meaningful spans. Sub-word tokens for whole words are easy to obtain using WordPiece tokenizer ( Wu et al. ,2016 ). While for phrase identification, we train a classifier-based English chunker on CoNLL-2000 corpus ( Tjong Kim Sang and Buchholz ,2000 ) following the instructions 1 . We then use the trained chunker to extract noun phrases (NP), verb phrases (VP), and prepositional phrases (PP). These identified phrases are tokenized by WordPiece tokenizer to obtain tokens. Herein, we can obtain $n_{s}$ token spans $x_{\\mathrm{span}}=[s_{1},s_{2},...\\,,s_{n_{s}}]$ ,where $s_{i}=[t_{j},t_{j+1},\\ldots,t_{j+n_{s_{i}}-1}]$ denotes the $i$ -th span that starts at the $j$ -th token and contains $n_{s_{i}}$ tokens. We then build span representations based on token representations using mean pooling:  \n\n$$\n\\begin{array}{r}{\\hat{h}_{i}^{l}=\\operatorname{Pool}(H_{j:j+n_{s_{i}}}^{l}),}\\end{array}\n$$  \n\nvarious relations to other elements, we propose that the knowledge is better specified as the structural relations of the representations in a semantic space, instead of the individual representations themselves. Therefore, instead of directly matching each hidden representation between the teacher and the student, we propose to extract structural relations from multi-granularity representations as the knowledge to teach the student. We first project the representations into multiple sub-spaces, then we extract two types of structural knowledge: pairwise interactions and triplet-wise geometric angles.  \n\nSample Representation. The third granularity is the input text sample itself. Based on token representations again, we use mean-pooling to aggregate all the token representations in a text sample to form sample representation:  \n\nwhere $\\hat{h}_{i}^{l}\\,\\in\\,\\mathbb{R}^{d}$ \u2208is the repr ion of the $i$ -th span in layer l. We obtain $L+1$ layers of span representations as $\\{\\hat{H}^{l}\\}_{l=0}^{L}$ }where $\\hat{H}^{l}\\in\\mathbb{R}^{n_{s}\\times{d}}$ \u2208.  \n\nHerein, we get $L+1$ layers of sample representations as $\\{\\tilde{h}^{l}\\}_{l=0}^{L}$ where $\\bar{\\tilde{h}}^{l}\\in\\mathbb{R}^{d}$ .  \n\n$$\n\\tilde{h}^{l}=\\mathrm{Pool}({\\cal H}^{l}),\n$$"}], "task_step_question_answer": "\u968f\u7740\u786c\u4ef6\u6280\u672f\u7684\u8fdb\u6b65\uff0c\u6a21\u578b\u84b8\u998f\u5728\u66f4\u5e7f\u6cdb\u7684\u8bbe\u5907\u4e0a\u7684\u5e94\u7528\u4e5f\u5c06\u6210\u4e3a\u53ef\u80fd\uff0c\u4f8b\u5982\u5728\u53ef\u7a7f\u6234\u8bbe\u5907\u548c\u65e0\u4eba\u673a\u4e0a\u7684\u5e94\u7528\u3002\u8fd9\u4e9b\u8bbe\u5907\u901a\u5e38\u5177\u6709\u66f4\u4e25\u683c\u7684\u8d44\u6e90\u9650\u5236\uff0c\u56e0\u6b64\u84b8\u998f\u6280\u672f\u5c06\u5728\u8fd9\u4e9b\u9886\u57df\u4e2d\u53d1\u6325\u66f4\u5927\u7684\u4f5c\u7528\u3002\u540c\u65f6\uff0c\u968f\u7740\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u4e0d\u65ad\u590d\u6742\u5316\uff0c\u5982\u4f55\u5728\u4e0d\u635f\u5931\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\u8fdb\u4e00\u6b65\u538b\u7f29\u6a21\u578b\u4e5f\u5c06\u6210\u4e3a\u4e00\u4e2a\u91cd\u8981\u7684\u7814\u7a76\u8bfe\u9898\u3002\u672a\u6765\u7684\u7814\u7a76\u53ef\u80fd\u4f1a\u63a2\u7d22\u66f4\u52a0\u9ad8\u6548\u7684\u84b8\u998f\u65b9\u6cd5\uff0c\u4f8b\u5982\u57fa\u4e8e\u795e\u7ecf\u67b6\u6784\u641c\u7d22\u7684\u84b8\u998f\u6280\u672f\uff0c\u6216\u8005\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u7684\u84b8\u998f\u7b56\u7565\uff0c\u4ee5\u8fdb\u4e00\u6b65\u63d0\u9ad8\u6a21\u578b\u7684\u6027\u80fd\u548c\u6548\u7387\u3002\u6b64\u5916\uff0c\u8bad\u7ec3\u8fc7\u7a0b\u7684\u4f18\u5316\u4e5f\u662f\u672a\u6765\u7814\u7a76\u7684\u4e00\u4e2a\u91cd\u8981\u65b9\u5411\uff0c\u901a\u8fc7\u5f15\u5165\u81ea\u9002\u5e94\u5b66\u4e60\u7387\u8c03\u6574\u7b56\u7565\u6216\u6279\u91cf\u5927\u5c0f\u4f18\u5316\uff0c\u53ef\u4ee5\u663e\u8457\u51cf\u5c11\u8bad\u7ec3\u65f6\u95f4\u3002\u6a21\u578b\u67b6\u6784\u7684\u6269\u5c55\u4e5f\u5c06\u6210\u4e3a\u7814\u7a76\u70ed\u70b9\uff0c\u9664\u4e86\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff0c\u672a\u6765\u7684\u7814\u7a76\u8fd8\u53ef\u4ee5\u63a2\u7d22\u5c06\u84b8\u998f\u65b9\u6cd5\u6269\u5c55\u5230\u5176\u4ed6\u67b6\u6784\uff0c\u5982Transformer\uff0c\u4ee5\u5e94\u5bf9\u66f4\u5e7f\u6cdb\u7684\u4efb\u52a1\u9700\u6c42\u3002", "ref_task_step_id": ""}, "__type__": "task_step_node"}}, "task_step_store/ref_task_step_info": {"": {"node_ids": ["fa33ea01-c2fd-4f5d-8d46-184313ddea1c", "73280fb7-b826-4157-9dfd-3771e5ec66d8", "bc7eecf8-6996-4c90-a203-0c062c7e5ea8", "1bc44462-cdfe-4df1-bfb3-cb4ec397081c", "e1e96c8a-2a92-4140-b48f-8d5d3d73717f", "d7aba3a0-4cfb-4815-9e43-475a4c78758d", "61a16cbb-0e64-4546-8bf4-5ab34231e80c", "fa33ea01-c2fd-4f5d-8d46-184313ddea1c", "bc7eecf8-6996-4c90-a203-0c062c7e5ea8", "1bc44462-cdfe-4df1-bfb3-cb4ec397081c", "d7aba3a0-4cfb-4815-9e43-475a4c78758d", "73280fb7-b826-4157-9dfd-3771e5ec66d8", "e1e96c8a-2a92-4140-b48f-8d5d3d73717f", "fa33ea01-c2fd-4f5d-8d46-184313ddea1c", "bc7eecf8-6996-4c90-a203-0c062c7e5ea8", "d7aba3a0-4cfb-4815-9e43-475a4c78758d", "1bc44462-cdfe-4df1-bfb3-cb4ec397081c", "73280fb7-b826-4157-9dfd-3771e5ec66d8", "e1e96c8a-2a92-4140-b48f-8d5d3d73717f", "bc7eecf8-6996-4c90-a203-0c062c7e5ea8", "fa33ea01-c2fd-4f5d-8d46-184313ddea1c", "d7aba3a0-4cfb-4815-9e43-475a4c78758d", "1bc44462-cdfe-4df1-bfb3-cb4ec397081c", "73280fb7-b826-4157-9dfd-3771e5ec66d8", "fa33ea01-c2fd-4f5d-8d46-184313ddea1c", "fa33ea01-c2fd-4f5d-8d46-184313ddea1c", "61a16cbb-0e64-4546-8bf4-5ab34231e80c", "e1e96c8a-2a92-4140-b48f-8d5d3d73717f", "61a16cbb-0e64-4546-8bf4-5ab34231e80c", "d7aba3a0-4cfb-4815-9e43-475a4c78758d", "d7aba3a0-4cfb-4815-9e43-475a4c78758d", "1bc44462-cdfe-4df1-bfb3-cb4ec397081c", "1bc44462-cdfe-4df1-bfb3-cb4ec397081c", "bc7eecf8-6996-4c90-a203-0c062c7e5ea8", "bc7eecf8-6996-4c90-a203-0c062c7e5ea8", "73280fb7-b826-4157-9dfd-3771e5ec66d8", "73280fb7-b826-4157-9dfd-3771e5ec66d8", "e1e96c8a-2a92-4140-b48f-8d5d3d73717f", "e1e96c8a-2a92-4140-b48f-8d5d3d73717f", "61a16cbb-0e64-4546-8bf4-5ab34231e80c", "61a16cbb-0e64-4546-8bf4-5ab34231e80c", "61a16cbb-0e64-4546-8bf4-5ab34231e80c"], "metadata": {}}}, "task_step_store/metadata": {"fa33ea01-c2fd-4f5d-8d46-184313ddea1c": {"task_step_hash": "", "ref_task_step_id": ""}, "73280fb7-b826-4157-9dfd-3771e5ec66d8": {"task_step_hash": "", "ref_task_step_id": ""}, "bc7eecf8-6996-4c90-a203-0c062c7e5ea8": {"task_step_hash": "", "ref_task_step_id": ""}, "1bc44462-cdfe-4df1-bfb3-cb4ec397081c": {"task_step_hash": "", "ref_task_step_id": ""}, "e1e96c8a-2a92-4140-b48f-8d5d3d73717f": {"task_step_hash": "", "ref_task_step_id": ""}, "d7aba3a0-4cfb-4815-9e43-475a4c78758d": {"task_step_hash": "", "ref_task_step_id": ""}, "61a16cbb-0e64-4546-8bf4-5ab34231e80c": {"task_step_hash": "", "ref_task_step_id": ""}}}