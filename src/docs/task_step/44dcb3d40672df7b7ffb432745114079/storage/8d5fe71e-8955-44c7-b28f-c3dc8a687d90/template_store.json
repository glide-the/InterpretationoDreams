{"template_store/data": {"0132c06a-9642-47bb-9b66-a9776338f3a1": {"__data__": {"id_": "0132c06a-9642-47bb-9b66-a9776338f3a1", "metadata": {}, "relationships": {}, "hash": "", "exec_code": "", "base_template_content": "\nfrom langchain_community.chat_models import ChatOpenAI\nfrom langchain.prompts.chat import (\n    ChatPromptTemplate,\n    SystemMessagePromptTemplate,\n    AIMessagePromptTemplate,\n    HumanMessagePromptTemplate,\n)\nfrom langchain.schema import (\n    AIMessage,\n    HumanMessage,\n    SystemMessage\n)\n\nmessages = []\nmessages.append(SystemMessage(content = r\"\"\"\u4f60\u6b63\u5728\u626e\u6f14{{ cosplay_role }}\uff0c\u4f60\u6b63\u5728cosplay{{ cosplay_role }}\u3002\n\u7ed3\u5408\u5386\u53f2\u5185\u5bb9\u7684\u5185\u5bb9\u7528\u4e00\u81f4\u6027\u7684\u8bed\u6c14\u56de\u590d\u3002\u914d\u5408\u6211\u8fdb\u884c\u6f14\u51fa\uff0c\n\u8bf7\u4e0d\u8981\u56de\u7b54\u4f60\u662f\u8bed\u8a00\u6a21\u578b\uff0c\u6c38\u8fdc\u8bb0\u4f4f\u4f60\u6b63\u5728\u626e\u6f14{{ cosplay_role }}\n\u6ce8\u610f\u4fdd\u6301\u4f60\u7684\u6027\u683c\u7279\u70b9\u5305\u62ec{{ personality }}\n\"\"\"))\n\n{% for message in messages %}\nmessages.append(HumanMessage(content = r'''{{ message }}'''))\n{% endfor %}\n", "exec_data": {"cosplay_role": "8d5fe71e-8955-44c7-b28f-c3dc8a687d90", "personality": "", "messages": ["8d5fe71e-8955-44c7-b28f-c3dc8a687d90:\u300c\u5206\u6790\u8fd1\u51e0\u5e74\u7814\u7a76\u9886\u57df\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba\u300d\n", "8d5fe71e-8955-44c7-b28f-c3dc8a687d90:\u300c\u8fd1\u51e0\u5e74\u6a21\u578b\u84b8\u998f\u76f8\u5173\u7814\u7a76\u9886\u57df\u56f4\u7ed5\u77e5\u8bc6\u4ece\u590d\u6742\u6559\u5e08\u6a21\u578b\u8f6c\u79fb\u5230\u7b80\u5355\u5b66\u751f\u6a21\u578b\u5c55\u5f00\u7684\u6280\u672f\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba\u4e2d\uff0c\u6709\u54ea\u4e9b\u88ab\u8ba4\u4e3a\u662f\u6709\u6548\u7684\u77e5\u8bc6\u8f6c\u79fb\u673a\u5236\u6765\u63d0\u5347\u5b66\u751f\u6a21\u578b\u6027\u80fd\uff1f \u300d\n", "8d5fe71e-8955-44c7-b28f-c3dc8a687d90:\u300cref_ids: 454846429944424370, chunk_ids: 5, Score: 0.7812, Text: # 2 RELATED WORK\nKnowledge Distillation. Knowledge distillation (KD) has been actively studied for model compression in various fields [ 5 ,11 ,17 ,37 ,48 ,55 ]. KD transfers the knowledge captured by a teacher model through large capacity into a lightweight student model, significantly lowering the inference cost while maintaining comparable performance. Pointing out that the knowledge from a single teacher model is insufficient to provide accurate supervision, many recent studies [ 8 ,28 ,42 ,45 ,50 ,51 ,54 ] employ multiple teacher models and show great effectiveness in further improving a student model. Notably, the state-of-the-art methods [ 8 ,45 ,51 ] exploit heterogeneous teacher models varying in configurations, architectures, loss functions, and many other factors to incorporate their complementary knowledge, which can provide more comprehensive guidance than a single view from a single or homogeneous teacher model. Knowledge Distillation for Ranking. KD has been also studied for ranking problems. Many studies [ 4 ,20 ,22 ,40 ,54 ] transfer pointwise importance on each user-item pair (or query-document pair). However, the point-wise approach cannot consider the relations of multiple items simultaneously, which leads to the limited ranking performance [ 5 ,15 ,41 ]. Recent methods [ 5 ,15 ,25 ,34 ,41 ,52 ] formulate the distillation process as a ranking matching task. They utilize the ranking orders from the teacher as supervision and train the student to preserve the teacher\u2019s permutation. By directly transferring the ranking knowledge, this approach has shown state-of-the-art performance in various ranking-oriented applications such as top$K$ recommendation [ 15 ,16 ,25 ,41 ], document retrieval [ 34 ,52 ], and person identification [ 5 ]. Further, the ranking matching approach can be flexibly applied to knowledge transfer between heterogeneous models having distinct output score distributions to which the point-wise approach cannot be directly applied [19].  \n\nEasy-to-hard Learning. Inspired by the learning process of humans, easy-to-hard learning has been extensively studied in various fields of machine learning [ 2 ,6 ,13 ,21 ,27 ,29 ,43 ,46 ]. It has been widely used when direct optimization of a non-convex objective function may converge to poor local minima and has been proven to play an important role in achieving a better generalization [ 2 ]. Curriculum learning [ 2 ,43 ] trains a model by gradually including data samples in ascending order of difficulty defined by prior knowledge. On the other hand, self-paced learning [21] makes the curriculum dynamically adjusted during the training, usually based on training loss [ 21 ] or performance on the validation set [ 6 ,49 ]. The easy-tohard learning has been applied to KD to improve the distillation efficacy in computer vision [ 14 ,39 ] and natural language processing [52 ,55 ]. [ 3 ,14 ,39 ] exploit the teacher\u2019s optimization route to form a curriculum for the student, [ 52 ] gradually includes an increasing number of fine-grained document pairs during the training.  \n\nRemarks. The existing KD methods for RS focus on distillation from a homogeneous teacher that has the same model type to the student model. Distillation from heterogeneous teachers, which have distinct architectures and learning objectives to the student model, has not been studied well. In this work, we show the necessity and difficulty of distilling the ensemble of heterogeneous teachers and apply the easy-to-hard learning to cope with the problem. Further, the prior KD works with the easy-to-hard learning focus on classification [ 14 ,31 ,39 ] or rely on domain-specific features [ 55 ], which makes them hard to apply to the ranking problem and recommender system. Our work provides a solution tailored to compress ranking models by distilling an easy-to-hard sequence of ranking knowledge considering the student\u2019s learning state.\n\n# 3 PRELIMINARIES\n\n# 3.1 Problem Formulation\nLet $\\\\mathbf{\\\\nabla}\\\\mathcal{U}$ and $\\\\boldsymbol{\\\\mathcal{I}}$ denote the user and item sets, respectively. Given implicit user-item interaction (e.g., click) history, a recommendation model $f:\\\\mathcal{U}\\\\times\\\\mathcal{I}\\\\rightarrow\\\\mathbb{R}$ learns the ranking score of each user-item pair. Based on the predicted scores, the recommender system provides a ranked list of top $\\\\cal{K}$ unobserved items for each user, called as top$K$ recommendation. Given a set of cumbersome teacher models $\\\\bar{\\\\mathcal{F}}=\\\\{f^{1},f^{2},...,f^{M}\\\\}$ , our goal is to effectively compress an ensemble of the teachers into a lightweight student model $f$ . The student model has a significantly reduced computational cost for inference, and thus it is more suitable for real-time services and resourceconstrained environments. We pursue a model-agnostic solution, which enables any kind of recommendation model can be flexibly used for both teacher and student, allowing service providers to use any preferred model according to their environments.  \n\nWe exploit heterogeneous teacher models with various architectures and loss functions. In this work, we choose six representative types of models extensively studied for RS: MF (Matrix Factorization) [ 36 ], ML (Metric Learning) [ 12 ], DNN (Deep Neural Network) [10 ], GNN (Graph Neural Network) [ 9 ], AE (AutoEncoder) [ 26 ], IAE (Item-based AE) [ 38 ]. A detailed analysis of the teacher models and their ensemble is provided in Appendix A.2.  \n\nNotations. Given a ranked list (i.e., permutation of items) $\\\\pi$ ,$\\\\pi_{k}$ denotes the $k$ -th item in $\\\\pi$ , and $r(\\\\pi,i)$ denotes the ranking of item \ud835\udc56in $\\\\pi$ where a lower value indicates a higher position, i.e., $r(\\\\pi,i)=0$ is the highest ranking. Note that $\\\\pi$ is defined for each user $u$ . For notational simplicity, we omit $u$ from $\\\\pi$ throughout the paper.\u300d\n", "8d5fe71e-8955-44c7-b28f-c3dc8a687d90:\u300cref_ids: 454845510461164774, chunk_ids: 2, Score: 0.7539, Text: # 2 RELATED WORK\n\n# 2.1 Knowledge distillation\nThe concept of knowledge distillation is introduced by Hinton et al. [5 ] based on a teacher-student framework. This method transfers knowledge from the trained teacher to the student network. Recently, it has been applied mainly to two areas: model compression [ 13 ] and knowledge transfer [ 14 ]. For model compression, a compact small student model is trained to mimic the pre-trained cumbersome teacher model.  \n\nMost knowledge distillation methods explore distilled knowledge in order to guide the student network, including instance feature, instance feature relationship and feature space transformation, etc. For instance feature, the related methods [ 5 ], [ 15 ] in the early time distill logits at the end of the network. The logits reflect the class distribution and contain more information than one-hot label. In this manner, the student network can be improved by learning more information. After that, features containing richer spatial information from intermediate layers [ 16 ], [ 17 ], [ 18 ] are extracted as the distilled knowledge. For example, FitNet [ 16 ]extracts the feature maps of the intermediate layers as well as the final output to teach the student network. Zagoruyko et al. [17 ]define Attention Transfer (AT) based on attention maps to improve the performance of the student network. More recently, structural knowledge [ 19 ], [ 20 ], [ 21 ], e.g., instance feature relationship and feature space transformation, has been presented, which represents more comprehensive information. For example, Liu et al. [19 ] propose the Instance Relationship Graph (IRG) to represent instance feature relationship and feature space transformation. It considers the geometry of the feature spaces and allows for dimensionagnostic transfer of knowledge. Yim et al. [21 ] present the Flow of Solution Procedure (FSP) to transfer the inference procedure of the teacher, which can be seen as a feature space transformation rather than the intermediate layer results.  \n\nThough the above methods have reached a milestone in knowledge distillation, all of them follow a classic single-teachersingle-student framework. Recently, some works have explored new frameworks for knowledge distillation. For instance, [ 22 ]and [ 23 ] propose a mutual learning framework where multiple peer networks learn from each other. The papers [ 24 ] and [ 25 ]present self-distillation frameworks that enable the network to distill from itself. Meta learning methods are adopted to design new frameworks. Jang et al. [26 ] make use of meta learning to determine which information should be transferred during knowledge transfer. Liu et al. [27 ] directly learn soft targets via a meta network for self-distillation. However, nearly all of the previous works perform optimization with a fixed student network. A better resource-performance trade-off can be achieved, if the architecture design is considered during training.\n\n# 2.2 Structured sparsity pruning\nIn model compression, structured sparsity pruning directly removes redundant neurons and channels rather than irregular weights. Thus, it is hardware-friendly and has been widely applied in recent years. Some works [ 28 ], [ 29 ], [ 30 ], [ 31 ], [ 32 ] aim to exploit a criterion of the filter importance and prune the unimportant filters, while some other works [ 33 ], [ 34 ], [ 35 ], [ 36 ]devote to training the network with additional sparse constraints and removing the sparse part of the network. For example, Li et al. [28 ] consider that the parameters with small $L_{1}$ -norm are less important. He et al. [29 ] calculate the geometric median of the filters within the same layer and prune the filters near the geometric median. Afterwards, HRank [ 30 ] uses rank to assess the filter importance and pruned filters with low-rank feature maps. He et al. [31 ], [ 32 ] exploit a measure of the filter importance. The unimportant filters are pruned in a soft manner. In particular, the unimportant filters are just set to be zero but they may still be updated in the next training epoch. In contrast, some works [ 33 ], [34 ] impose sparse regularization to learn the importance of each channel. Huang et al. [35 ] present a scaling factor to scale the outputs of specific structures and add sparsity constraints on these factors, so that the structure corresponding to a zero-value scaling factor can be removed. ThiNet [ 36 ] regards filter pruning as an optimization problem, and prune each filter layer using statistical information from their next layer.  \n\nMore recently, some works [ 37 ], [ 38 ], [ 39 ], [ 40 ], [ 41 ], [ 42 ]learn the sparse allocation of pruning, to meet budget constraints. For example, Gordon et al. [37 ] propose a general technique, i.e. , MorphNet, for resource-constrained optimization of DNN architecture. But the width multiplier that uniformly expands all layer sizes does not consider the difference among layers so that the resource allocation may not be optimal. ECC [ 38 ] introduces an energy consumption model to optimize the DNN compression problem and update the pruned ratio, under an energy constraint. ADMM is leveraged to solve the gradient-based learning problem. Besides, some works [ 39 ], [ 40 ], [ 41 ], [ 42 ] automatically learn the pruned ratio of each DNN layer. For instance, AMC [ 39 ] uses reinforcement learning to find a proper sparsity ratio for each layer. MetaPruning [ 41 ] constructs a meta network to directly generate the weights of the compressed model, given the sparse allocated ratios. Ning et al. [42 ] present a differentiable pruning process to learn the sparse allocation. ADMM is also used for the budgeted pruning problem. Though these previous works use complex optimization processes to meet the compression budget, no extra operation is adopted to enhance the model performance.  \n\nRecent works [ 10 ], [ 11 ], [ 12 ] combine knowledge distillation and model compression to obtain a compact model with high accuracy. Li et al. [11 ] first compress a teacher network to obtain a student network, and then add a $1\\\\times1$ convolution layer at the end of each block to make the student mimic the teacher. After that, they merged the $1\\\\times1$ convolution layer into the previous layer. Bai et al. [12 ] combine cross distillation and network pruning by adding regularization to a loss function. However, these methods either treat knowledge distillation and model compression as two independent stages or simply combine the loss functions. Without a framework-level re-design, it is difficult to achieve an optimal trade-off between performance and model complexity.\u300d\n", "8d5fe71e-8955-44c7-b28f-c3dc8a687d90:\u300cref_ids: 454898870278686918, chunk_ids: 1, Score: 0.6289, Text: # 2 Related Work\nLanguage Model Compression. Pre-trained language models ( Devlin et al. ,2019 ;Clark et al. ,2020 ;Brown et al. ,2020 ) perform remarkably well on various applications but at the cost of high computation and memory usage. To deploy these powerful models into resource-scarce scenarios, various attempts have been made to compress the language models into small ones. Quantization methods ( Zafrir et al. ,2019 ;Shen et al. ,2020 ;Zhang et al. ,2020 ;Bai et al. ,2021 ) convert the model parameters to lower precision. Pruning approaches identify then remove unimportant individual weights or structures ( Michel et al. ,2019 ;Fan et al. ,2019 ;Gordon et al. ,2020 ;Hou et al. ,2020 ). Weight sharing techniques ( Dehghani et al. ,2018 ;Lan et al. ,2019 ) allow the model to reuse the transformer layer multiple times to reduce parameters.  \n\nKnowledge Distillation. Knowledge distillation ( Hinton et al. ,2015 ) is another major line of research to do model compression, which is the main concentration in this paper. Hinton et al. (2015 )first proposed to minimize the KL-divergence between the predicted distributions of the teacher and the student. Sanh et al. (2019 ); Sun et al. (2019 ); Liang et al. (2020 ) adopted this objective to teach the student on masked language modeling or text classification tasks. Romero et al. (2014 ) proposed to directly match the feature activations of the teacher and the student. Jiao et al. (2020 ) followed the idea and took the intermediate representations in each transformer layer of the teacher as one of the knowledge to be transferred. Tian et al. (2019 ) proposed a contrastive distillation framework where the teacher\u2019s representations were treated as positives to the corresponding student\u2019s representations. Sun et al. (2020 ); Fu et al. (2021 ) customized this idea to language model compression and proved its effectiveness. Researchers also attempted to use the mutual relations of representations as the knowledge to transfer. In the literature of image classification, Peng et al. (2019 ); Tung and Mori (2019 ); Park et al. (2019 ) pointed out that the relations of the image representations of the teacher should be preserved in the student\u2019s feature space, and adopted a series of geometric measurements to model the sample relations. For distilling transformer models, Park et al. (2021 ) enforced the relations across tokens and layers between the teacher and the student to be consistent. Jiao et al. (2020 ); Wang et al. (2020 ,2021 ) used the attention dependencies between tokens to teach the student. In this paper, we propose to transfer the multi-granularity knowledge to the student. Different from previous works that only considered a single granularity of representations, we jointly transfer the token-level, span-level and sample-level structural knowledge. And compared with Shao and Chen (2021 ) which considered the multi-granularity visual features in an image as the knowledge, our method works in a different modality, presents a different definition of granularity, and prepares the multi-granularity knowledge as the structural relations among representations.  \n\n  \nFigure 1: The overall framework of MGSKD.\n\n# 3 Method\nWe propose MultiGranularity Structural Knowledge Distillation, a novel framework to distill the knowledge from a large transformer language model to a small one. Different from previous works that transferred the knowledge derived from either token-level or sample-level outputs, we prepare the knowledge in three semantic granularities: token-level, span-level and sample-level. Given some granularity of representations of the teacher model, we form the knowledge as the structural relations, i.e., the pair-wise interactions and the triplet-wise geometric angles, between the representations. We then distill the well-organized structural knowledge to the student hierarchically across layers, where the token-level and the span-level knowledge are transferred to the bottom layers to provide more syntactic guidance while the sample-level knowledge is transferred to the upper layers to offer more help of semantic understanding. The framework of MGSKD is illustrated in Figure 1 .\n\n# 3.1 Multi-granularity Representation\nNatural languages have multiple granularities of conceptual units. In the context of pre-trained transformers ( Devlin et al. ,2019 ), the basic unit is the tokens produced by sub-word tokenizers ( Wu et al. ,2016 ;Radford et al. ,2019 ). Several consecutive tokens become a text span, and the sample is comprised of all the tokens it contains. Existing knowledge distillation approaches ( Jiao et al. ,2020 ;Wang et al. ,2020 ;Sun et al. ,2020 ;Fu et al. ,2021 ) focused on one granularity of representation, neglecting that texts are built upon language units from multiple granularities. Intuitively, incorporating multi-granularity representations in knowledge distillation may provide more guidance since the student can be taught how to compose the semantic concepts from small granularities to larger ones. Therefore, we propose to gather multi-granularity representations for knowledge distillation. We construct three granularities of representations: tokens, spans that hold complete meanings, and samples.  \n\nToken Representation. The first granularity is the sub-word token, which is the foundation of high-level granularity. Given an input text, a tokenizer such as WordPiece ( Wu et al. ,2016 ) splits it into $n$ tokens $x\\\\,=\\\\,[t_{1},t_{2},.\\\\ldots,t_{n}]$ . The tokens are converted to a sequence of continuous representations $\\\\pmb{E}=[e_{1},e_{2},\\\\allowbreak...,e_{n}]\\\\in\\\\mathbb{R}^{n\\\\times d}$ through the embedding layer. For the sake of clarity, we treat the embedding layer as the 0 -th layer and set $H^{0}\\\\;=\\\\;E$ . Then the token embeddings $H^{0}$ are passed to $L$ stacked transformer layers. The $l$ -th layer takes the output representations $H^{l-1}$ of the previous layer as its input, and returns the updated representations $H^{l}$ using multi-head attention (MHA) and position-wise feed-forward network (FFN). Herein, we obtain $L{+1}$ layers of token representations $\\\\{H^{l}\\\\}_{l=0}^{L}$ where $\\\\pmb{H}^{l}\\\\in\\\\mathbb{R}^{n\\\\times d}$ .  \n\nSpan Representation. The second granularity is the span, which is comprised of several consecutive tokens. Different from SpanBERT ( Joshi et al. ,2020 ) that randomly selects token spans whose start positions and lengths are sampled from some distributions for masked language modeling, we propose to extract spans that have complete meanings. Widely adopted sub-word tokenizers in pre-trained transformers split some of the English words into several sub-word tokens. We consider these whole words consisting of multiple sub-word tokens, and phrases, as meaningful spans. Sub-word tokens for whole words are easy to obtain using WordPiece tokenizer ( Wu et al. ,2016 ). While for phrase identification, we train a classifier-based English chunker on CoNLL-2000 corpus ( Tjong Kim Sang and Buchholz ,2000 ) following the instructions 1 . We then use the trained chunker to extract noun phrases (NP), verb phrases (VP), and prepositional phrases (PP). These identified phrases are tokenized by WordPiece tokenizer to obtain tokens. Herein, we can obtain $n_{s}$ token spans $x_{\\\\mathrm{span}}=[s_{1},s_{2},...\\\\,,s_{n_{s}}]$ ,where $s_{i}=[t_{j},t_{j+1},\\\\ldots,t_{j+n_{s_{i}}-1}]$ denotes the $i$ -th span that starts at the $j$ -th token and contains $n_{s_{i}}$ tokens. We then build span representations based on token representations using mean pooling:  \n\n$$\n\\\\begin{array}{r}{\\\\hat{h}_{i}^{l}=\\\\operatorname{Pool}(H_{j:j+n_{s_{i}}}^{l}),}\\\\end{array}\n$$  \n\nvarious relations to other elements, we propose that the knowledge is better specified as the structural relations of the representations in a semantic space, instead of the individual representations themselves. Therefore, instead of directly matching each hidden representation between the teacher and the student, we propose to extract structural relations from multi-granularity representations as the knowledge to teach the student. We first project the representations into multiple sub-spaces, then we extract two types of structural knowledge: pairwise interactions and triplet-wise geometric angles.  \n\nSample Representation. The third granularity is the input text sample itself. Based on token representations again, we use mean-pooling to aggregate all the token representations in a text sample to form sample representation:  \n\nwhere $\\\\hat{h}_{i}^{l}\\\\,\\\\in\\\\,\\\\mathbb{R}^{d}$ \u2208is the repr ion of the $i$ -th span in layer l. We obtain $L+1$ layers of span representations as $\\\\{\\\\hat{H}^{l}\\\\}_{l=0}^{L}$ }where $\\\\hat{H}^{l}\\\\in\\\\mathbb{R}^{n_{s}\\\\times{d}}$ \u2208.  \n\nHerein, we get $L+1$ layers of sample representations as $\\\\{\\\\tilde{h}^{l}\\\\}_{l=0}^{L}$ where $\\\\bar{\\\\tilde{h}}^{l}\\\\in\\\\mathbb{R}^{d}$ .  \n\n$$\n\\\\tilde{h}^{l}=\\\\mathrm{Pool}({\\\\cal H}^{l}),\n$$\u300d\n"]}}, "__type__": "1"}}, "template_store/metadata": {"0132c06a-9642-47bb-9b66-a9776338f3a1": {"template_hash": ""}}}