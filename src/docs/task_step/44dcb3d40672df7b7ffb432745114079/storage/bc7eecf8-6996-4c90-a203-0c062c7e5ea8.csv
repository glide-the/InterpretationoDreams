角色,内容,分镜
bc7eecf8-6996-4c90-a203-0c062c7e5ea8,关键技术点,2
bc7eecf8-6996-4c90-a203-0c062c7e5ea8,"### 问题

在模型蒸馏的“关键技术点”中，温度调节（Temperature）的作用是什么？它如何影响软标签的生成以及学生模型的训练过程？",2
bc7eecf8-6996-4c90-a203-0c062c7e5ea8,"ref_ids: 454849384555410342, chunk_ids: 1, Score: 0.3340, Text: # Attention Temperature Matters in Abstractive Summarization Distillation
Shengqiang Zhang 1 ∗† , Xingxing Zhang 2 ∗, Hangbo Bao 2 †, Furu Wei 2 1 Peking University 2 Microsoft Research Asia  {xizhang,t-habao,fuwei}@microsoft.com

# Abstract
Recent progress of abstractive text summarization largely relies on large pre-trained sequence-to-sequence Transformer models, which are computationally expensive. This paper aims to distill these large models into smaller ones for faster inference and with minimal performance loss.Pseudo-labelingbased methods are popular in sequence-tosequence model distillation. In this paper, we find simply manipulating attention temperatures in Transformers can make pseudo labels easier to learn for student models. Our experiments on three summarization datasets show our proposed method consistently improves vanilla pseudo-labeling based methods. Further empirical analysis shows that both pseudo labels and summaries produced by our students are shorter and more abstractive. Our code is available at https://github. com/Shengqiang-Zhang/plate .

# 1 Introduction
Automatic document summarization is the task of rewriting a long document into its shorter form while still retaining its most important content. In the literature, there are mainly two kinds of methods for summarization: extractive summarization and abstractive summarization (Nenkova and McKeown ,2011 ). In this work, we focus on abstractive summarization, which is viewed as a sequence-tosequence (Seq2Seq) learning problem, since recent abstractive models outperform their extractive counterparts and can produce more concise summaries ( Raffel et al. ,2020 ;Lewis et al. ,2020 ;Zhang et al. ,2020 ;Liu and Lapata ,2019 ). Recent progress of abstractive summarization largely relies on large pre-trained Transformer models ( Raffel et al. ,2020 ;Lewis et al. ,2020 ;Zhang et al. ,2020 ;Liu and Lapata ,2019 ;Bao et al. ,2020 ). With these extremely large models, we can obtain state-of-theart summarization results, but they are slow for online inference, which makes them difficult to be used in the production environment even with cutting-edge hardware. This paper aims to distill these large Transformer summarization models into smaller ones with minimal loss in performance.  

Knowledge distillation is a class of methods that leverage the output of a (large) teacher model to guide the training of a (small) student model. In classification tasks, it is typically done by minimizing the distance between the teacher and student predictions ( Hinton et al. ,2015 ). As to Seq2Seq models, an effective distillation method is called pseudo-labeling ( Kim and Rush ,2016 ), where the teacher model generates pseudo summaries for all documents in the training set and the resulting document– pseudo -summary pairs are used to train the student model.  

In this paper, we argue that attention distributions of a Seq2Seq teacher model might be too sharp. As a result, pseudo labels generated from it are sub-optimal for student models. In the summarization task, we observe that 1) pseudo summaries generated from our teacher model copy more continuous text spans from original documents than reference summaries ( $56\\%$ 4-grams in pseudo summaries and $15\\%$ 4-grams in reference summaries are copied from their original documents on CNN/DailyMail dataset); 2) pseudo summaries tend to summarize the leading part of a document (measured on CNN/DailyMail, $74\\%$ of sentences in pseudo summaries and $64\\%$ of sentences in reference summaries are from the leading $40\\%$ sentences in original documents). We obtain the two numbers above by matching each sentence in a summary with the sentence in its original document that can produce maximum ROUGE ( Lin ,2004 ) score between them. We call the two biases above the copy bias and the leading bias . In order to have an intuitive feeling, we select a representative example 1 and visualize its cross attention weights 2 (see the left graph in Figure 1 ). We observe that attention weights form three “lines”, which indicates very time the decoder predicts the next word, its attention points to the next word in the input document. That may be the reason why multiple continuous spans of text are copied. Another phenomenon we observe is that all high-value attention weights (in deeper color) concentrate on the first 200 words in the input document, which reflects the leading bias. In either case, the attention distribution is too sharp (i.e., attention weights of the next word position or the leading part is much larger than other positions), which means our teacher model is over-confident.  

Based on the observations above, we propose a simple method called P LATE (as shorthand for Pseudo-labeling with Larger A ttention TE mperature) to smooth attention distributions of teacher models. Specifically, we re-scale attention weights in all attention modules with a higher temperature, which leads to softer attention distributions. Figure 1 intuitively shows the effect of using higher attention temperatures. Compared with the left graph, the right graph with higher attention temperature has shorter lines (less copy bias) with high attention weights, and positions of high attention weights extend to the first 450 words (less leading bias). Less copy bias in pseudo summaries encourages student models to be more abstractive, while less leading bias in pseudo summaries encourages student models to take advantage of longer context in documents.  

Experiments on CNN/DailyMail, XSum, and New York Times datasets with student models of different sizes show P LATE consistently outperforms vanilla pseudo-labeling methods. Further empirical analysis shows that, with P LATE , both pseudo summaries generated by teacher models and summaries generated by student models are shorter and more abstractive, which matches the goal of abstractive summarization.",2
bc7eecf8-6996-4c90-a203-0c062c7e5ea8,"ref_ids: 454847845063601170, chunk_ids: 1, Score: 0.2969, Text: # 2. Related Work
Knowledge distillation [ 13 ] is designed to transfer the “dark” knowledge from a cumbersome teacher model to a lightweight student model. By learning from the soft labels of teacher, student can achieve better performance than training on hard labels only. The traditional method trains a student by minimizing a difference such as KL divergence between its predicted probability and the teacher’s. The prediction of probability is commonly approximated by the softmax of logit output. KD algorithms can be classified into three types, i.e., logit-based [ 3 ,13 ,17 ,22 ,28 ,47 ,49 ,50 ], feature-based [ 1 ,4 ,5 ,10 ,12 ,23 ,25 ,27 ,31 ,37 ,44 ], and relation-based [ 15 ,19 ,29 ,30 ,39 ,43 ] methods.  

A temperature is introduced to flatten the probabilities in logit-based methods. Several works [ 2 ,13 ,26 ] explore its properties and effects. They reach an identical conclusion that temperature controls how much attention student pays on those logits more negative than average. A very low temperature makes student ignore other logits and instead mainly focus on the largest logit of teacher. However, they do not discuss why teacher and student share a globally predefined temperature. It was unknown whether temperature can be determined in an instance-wise level until CTKD [ 24 ] proposed predicting sample-wise temperatures by leveraging adversarial learning. However, it assumes that teacher and student should share temperatures. It was still undiscovered whether teacher and student can have divergent temperatures. ATKD [ 9 ] proposes a sharpness metric and chooses adaptive temperature by reducing the gap between teacher and student. However, their assumption of a zero logit mean relies on numerical approximation and limits its performance. Additionally, they do not thoroughly discuss where the temperature is derived from and whether distinct temperatures can be assigned. In this work, we provide an analytical derivation based on the entropy-maximization principle, demonstrating that students and teachers do not necessarily share a temperature. It is also found sufficient to preserve the innate relationship of prediction, instead of exact logit values of teacher [ 15 ]. However, the existing logit-based KD pipelines still implicitly mandate an exact match between teacher and student logits. We thus define the temperature to be the weighted standard deviation of logit to alleviate the issue and facilitate the existing logit-based KD approaches.

# 3. Background and Notation
Suppose ansfer d $\\mathcal{D}$ aini $N$ samples are the image and label respectively for the {$\\{\\mathbf{x}_{n},y_{n}\\}_{n=1}^{N}$ }, where $\\mathbf{x}_{n}\\in\\mathbb{R}^{H\\times W}$ ∈an n$y_{n}\\in[1,K]$ -th sample. ∈The notations of $H$ ,$W$ and $K$ are image height, width and $f_{T}$ and student number of $f_{S}$ respectively predict logit vectors sses. Given an input $\\{\\mathbf{x}_{n},y_{n}\\}$ , teacher ${\\bf v}_{n}$ and $\\mathbf{z}_{n}\\in\\mathbb{R}^{1\\times K}$ . Namely, ${\\bf z}_{n}=f_{S}({\\bf x}_{n})$ and $\\mathbf{v}_{n}=f_{T}(\\mathbf{x}_{n})$ .  

It is widely accepted that a softmax function involving vectors a temperature $q(\\mathbf{z}_{n})$ or $\\tau$ $q{\\bf(v}_{n})$ is used to convert th such that their klogit to probability -th items have  

$$
\\begin{array}{r l}&{q(\\mathbf{z}_{n})^{(k)}=\\frac{\\exp(\\mathbf{z}_{n}^{(k)}/\\mathcal{T})}{\\sum_{m=1}^{K}\\exp(\\mathbf{z}_{n}^{(m)}/\\mathcal{T})},}\\\\ &{q(\\mathbf{v}_{n})^{(k)}=\\frac{\\exp(\\mathbf{v}_{n}^{(k)}/\\mathcal{T})}{\\sum_{m=1}^{K}\\exp(\\mathbf{v}_{n}^{(m)}/\\mathcal{T})},}\\end{array}
$$  

where ${\\bf z}_{n}^{(k)}$ and ${\\bf v}_{n}^{(k)}$ are the $k$ -th item of ${\\bf z}_{n}$ and ${\\bf v}_{n}$ respectively. A knowledge distillation process is essentially letting $q({\\bf z}_{n})^{(k)}$ mimic $q\\bar{(}\\mathbf{v}_{n})^{(k)}$ for any class and all samples. The objective is realized by minimizing $\\mathrm{KL}$ divergence  

$$
\\mathcal{L}_{\\mathrm{KL}}\\left(q(\\mathbf{v}_{n})\\vert\\vert q(\\mathbf{z}_{n})\\right)=\\sum_{k=1}^{K}q(\\mathbf{v}_{n})^{(k)}\\log\\left(\\frac{q(\\mathbf{v}_{n})^{(k)}}{q(\\mathbf{z}_{n})^{(k)}}\\right),
$$  

which is theoretically equivalent to a cross-entropy loss when optimizing solely on $\\mathbf{z}$ ,  

$$
\\mathcal{L}_{\\mathrm{CE}}\\left(q(\\mathbf{v}_{n}),q(\\mathbf{z}_{n})\\right)=-\\sum_{k=1}^{K}q(\\mathbf{v}_{n})^{(k)}\\log q(\\mathbf{z}_{n})^{(k)}.
$$  

Note that they are empirically nonequivalent as their gradients diverge due to the negative entropy term of $q({\\bf v}_{n})$ .

# 4. Methodology
It is widely accepted that in Eq. 1 and Eq. 2 . In contrast, in Sec. $\\tau$ is shared for teacher and student 4.1 , we show the irrelevance between the temperatures of teacher and student, as well as across different samples. Guaranteed that temperatures can be different between teacher and student and among sample, we further show two side-effect drawbacks of shared-temperatures setting in conventional KD pipelines in Sec. 4.2 . In Sec. 4.3 , we propose leveraging logit standard deviation as a factor in temperature and derive a preprocess of logit standardization.

# 4.1. Irrelevance between Temperatures
In Sec. 4.1.1 and 4.1.2 , we first give a derivation of the temperature-involved softmax function in classification and KD based on the entropy-maximization principle in information theory. This implies the temperatures of student and teacher can be distinct and sample-wisely different.

# 4.1.1 Derivation of softmax in Classification
The softmax function in classification can be proved to be the unique solution of maximizing entropy subject to the normalization condition of probability and a constraint on the expectation of states in information theory [ 16 ]. The derivation is also leveraged in confidence calibration to formulate temperature scaling [ 8 ]. Suppose we have the following constrained entropy-maximization optimization,  

$$
\\begin{array}{c}{\\displaystyle\\operatorname*{max}_{q}\\mathcal{L}_{1}=-\\sum_{n=1}^{N}\\sum_{k=1}^{K}q(\\mathbf{v}_{n})^{(k)}\\log q(\\mathbf{v}_{n})^{(k)}}\\\\ {\\displaystyle\\left\\{\\sum_{k=1}^{K}q(\\mathbf{v}_{n})^{(k)}=1,\\quad\\forall n\\in\\mathbb{N}^{2},\\quad\\forall n\\in\\mathbb{N}^{2},\\quad\\forall n}\\\\ {\\mathbb{E}_{q}[\\mathbf{v}_{n}]=\\sum_{k=1}^{K}\\mathbf{v}_{n}^{(k)}q(\\mathbf{v}_{n})^{(k)}=\\mathbf{v}_{n}^{(y_{n})},\\quad\\forall n.}\\end{array}
$$  

The first constraint holds due to the requirement of discrete probability density, while the second constraint controls the scope of the distribution such that model accurately predicts the target class. Suppose ${\\hat{q}}_{n}$ to be the one-hot hard probability distribution whose values are all zero except at the target index $\\hat{q}_{n}^{(y_{n})}=1$ . The second constraint is then actually $\\begin{array}{r}{\\mathbb{E}_{q}[\\mathbf{v}_{n}]=\\sum_{k=1}^{K}\\mathbf{v}_{n}^{(k)}\\hat{q}_{n}^{(k)}=\\mathbf{v}_{n}^{(y_{n})}}\\end{array}$ . This is equivalent to making model predict the correct label $y_{n}$ . By applying Lagrangian multipliers $\\{\\alpha_{1,i}\\}_{i=1}^{N}$ and $\\{\\alpha_{2,i}\\}_{i=1}^{N}$ , it gives  

$$
\\begin{array}{l}{\\displaystyle\\mathcal{L}_{T}=\\mathcal{L}_{1}+\\sum_{n=1}^{N}\\alpha_{1,n}\\left(\\sum_{k=1}^{K}q(\\mathbf{v}_{n})^{(k)}-1\\right)}\\\\ {\\displaystyle+\\sum_{n=1}^{N}\\alpha_{2,n}\\left(\\sum_{k=1}^{K}\\mathbf{v}_{n}^{(k)}q(\\mathbf{v}_{n})^{(k)}-\\mathbf{v}_{n}^{(y_{n})}\\right).}\\end{array}
$$  

Taking the partial derivative with respective to $\\alpha_{1,n}$ and $\\alpha_{2,n}$ yields back the constraints. In contrast, taking the derivative with respective to $q(\\mathbf{v}_{n})^{(k)}$ gives  

$$
\\frac{\\partial\\mathcal{L}_{T}}{\\partial q(\\mathbf{v}_{n})^{(k)}}=-1-\\log q(\\mathbf{v}_{n})^{(k)}+\\alpha_{1,n}+\\alpha_{2,n}\\mathbf{v}_{n}^{(k)},
$$  

which leads to a solution by making the derivative zero:  

$$
q(\\mathbf{v}_{n})^{(k)}=\\exp\\left(\\alpha_{2,n}\\mathbf{v}_{n}^{(k)}\\right)/Z_{T},
$$  

where $\\begin{array}{r}{Z_{T}=\\exp\\left(1-\\alpha_{1,n}\\right)=\\sum_{m=1}^{K}\\exp\\left(\\alpha_{2,n}\\mathbf{v}_{n}^{(m)}\\right)}\\end{array}$ is the partition function to fulfill the normalization condition.",2
bc7eecf8-6996-4c90-a203-0c062c7e5ea8,"ref_ids: 454918605807027478, chunk_ids: 0, Score: 0.2949, Text: # (2) Soft Label-Based KD for OD
In order to improve the performance of student models and reduce their dependence on ground truth labels, related works typically guide student models using soft labels or pseudo-labels output by teacher models [16] ,[19] ,[78] . In addition, we need to successfully use the predictions of teacher models as soft labels for student models, and we should also know how to reasonably assign the labels of teacher models, even the hard labels. For example,LADin [72] cansignificantlyimprovetheperformance of a student model with a lightweight teacher model, and the performance of the student model will in fact be superior to that of its teacher. LAD requires a trained teacher to provide guidance (soft and hard labels) for its student. However, in practice, it is difficult to obtain excellent teacher models capable of providing effective guidance information. Therefore, Zhang et al. [57] proposed a self-distillation framework for OD, called label-guided self-distillation (LGD), which can generate the required guidance information using only the internal relationship between objects. Moreover, there are also some other methods that have tried to obtain better OD results by combining soft and hard labels [39] ,[71] .  

In label guided KD-based OD models, pseudo-labels can also play important roles in the training of student models. For example, Feng et al. [97] proposed an adaptive pseudo-label selection strategy to selectively calculate the distillation loss using the pseudo-labels. The student model carries out feature learning by using pseudo-labels of the teacher model first, and fine-tunes the network according to the ground truth labels. This KD strategy can not only reduce the demand for labeled samples [59] ,[60] , but also achieves higher OD performance than traditional methods based on the single-stage KD strategy [142] .In addition, for KD-based 3D OD models, using high-beam point clouds with pseudo-labels to train student models is a good way to solve the problem of high-cost 3D sample data labeling; thus, low-beam pseudo-LiDAR needs to be generated by down-sampling high-beam point clouds [108] .  

2) Feature Distillation for OD: Another KD strategy for feature distillation in intermediate feature layers can also effectively improve the performance of OD models. In this subsection, we will provide an overview of different feature distillation strategies embedded in OD models, including the basic ideas of feature distillation, full trust feature distillation, selective trust feature distillation, and so on.  

In general, the methods based on feature distillation use the features output from the middle layers of teacher models to supervise the training of student models, so that the student models can mimic the features output from the teacher models to the greatest extent possible. The essence of the idea is to continuously optimize the loss function consisting of both the activation functions of the teacher and student models’ feature layers. Therefore, feature distillation is carried out by using a loss function to train the student model. A general formulation of the loss function for feature distillation has been provided by Gou et al. [10] :  

$$
{\\cal L}={\\cal L}_{F e a D}(f_{t}(x),f_{s}(x))={\\cal L}_{F}(\\Phi_{t}(f_{t}(x)),\\Phi_{s}(f_{s}(x))),
$$  

where $f_{t}(x)$ and $f_{s}(x)$ are the output features from the middle layers of the teacher and student models respectively. Considering that the different network structures of the teacher and student models may lead to different sizes of the output features from their middle layers, a transformation function $\\Phi(.)$ is used to match these features. $L_{F}(.)$ denotes the loss function between the features of teacher and student models.  

According to the related works on feature distillation published in recent years, there are two main distillation strategies employed for feature distillation on OD models: full trust feature distillation and selective trust feature distillation. In the below, we will provide a description of these methods in detail.  

(1) Full Trust Feature Distillation  

Full trust feature distillation means that the student model learns all the knowledge from the teacher model unconditionally, without considering whether the knowledge to be learned is correct or not. Methods based on full trust feature distillation can further be divided into global feature distillation and local feature distillation.  

Global Feature Distillation is an approach in which the student model imitates the entire feature maps of the middle layers of the teacher model. For example, the fast scene text detector uses all feature maps of the teacher model to guide the training of the student model [92] . In addition, the related works in [115] ,[116] , and [93] also designed their models through the strategy of full trust feature distillation. These methods are in essence the applications of the most basic feature distillation strategy; they do not make any improvements to the feature maps, but instead simply guide the student models to learn all feature information directly from the teacher models. However, there are limitations on the capacity to improve the performance of student models by learning the global features of the teacher model indiscriminately. Therefore, researchers have gradually explored optimizing the output feature maps to further improve the performance of student models. For example, Qi et al. [24] aligned the feature maps at different resolutions using a feature pyramid, dynamically fused these features, and finally extracted these fused features from the teacher model to provide better guidance to the student model. The work of He et al. [88] ranks the feature maps by calculating the channel strengths of these feature maps in the teacher and student models for feature distillation. The feature distillation strategies proposed in [24] and [88] perform knowledge learning between the same layers of the teacher and student models; moreover, the multi-layer feature maps of the teacher model can also be used to guide the single-layer feature learning of the student model [28] ,[89] .  

Local Feature Distillation refers to the simulation of a student model learning those local features that are more helpful for the final prediction, rather than the entire feature maps of the teacher model. In recent years, an increasing number of related works have explored corresponding distillation strategies for local feature learning; these methods mainly try to learn the visual features at key locations in the feature maps. Chen et al. [70] used a region distillation strategy to train a lightweight pedestrian detector that crops features corresponding to RoI regions, after which the cropped local feature maps are used as the guidance information for the student model. In addition, the anchors in object detectors are widely used to locate the key local features for training student models [17] ,[91] ,[95] ; these anchors can also be ranked to enable the student model to learn feature maps with different significance [143] . It can be readily observedthatthelocalfeaturedistillationintheabovementioned methods is performed directly around the feature maps. The attentional mechanisms can also be used to make the student models pay more attention to the key local visual features. For example, spatial attention on feature maps is introduced for local feature distillation [144] , while the attention mechanism used to highlight foreground regions as well as contextual information is also helpful for OD [47] . Furthermore, Yang et al. [76] considered that the teacher and student models pay different levels of attention to the foreground and background, while the uneven differences in the feature maps in turn impact the effect of KD. Therefore, these authors proposed a strategy combining focal distillation and global distillation, in which focal distillation is guided to the student model by using spatial and channel attention masks during model training. The goal here is to make the student model focus only on the key pixels and channels on the feature map, thereby improving the performance of the student model. There are also some other local feature distillation strategies, such as the key proposal generation of the student and teacher models for local feature distillation [75] .  

These above mentioned OD methods based on whether global feature distillation or local feature distillation place full trust in the guidance information of the teacher model. However, it shouldbenotedthattheguidancefeatureinformationusedbythe teacher model to supervise the training of the student model may be incomplete or even incorrect, which has a negative impact on the performance improvements of the student model.  

(2) Selective Trust Feature Distillation  

To address the detrimental effects on the student model of incorrectinformationprovidedbytheteachermodel,thestrategy of selective trust feature distillation is introduced in KD-based OD models. Selective trust feature distillation means that the guidance information provided by the teacher model to the student model should be selected first: in short, it is necessary to remove the incorrect information and leave only the feature information that has a positive impact on the detection performance. For example, Heo et al. [145] proposed a margin ReLu to suppress the unfavorable feature information from the teachermodel,sothatthestudentmodellearnsonlythefavorable features and thus achieves performance improvement.  

In summary, whether full trust feature distillation or selective trust feature distillation is employed, this is ultimately optimized through the loss function. However, it is challenging to quickly and accurately select the substantial and beneficial features from the large amount of prior guidance information provided by the teacher model. There are thus many scientific problems worthy of further study associated with OD models based on selective trust KD.  

3) Various Network Structures of Teacher-Student Models: ThissectionwillexploretheKDstrategyfromanewperspective. Specifically, we found that different network structures can be designed for the teacher and student models respectively, and the knowledge extracted from multimodal data can facilitate significant performance improvements on the part of the student model. Therefore, this section will summarize and analyze the network structures of the teacher-student models and the feature learning from multimodal data.  

ItisacommonKDstrategythattheteacherandstudentmodels have similar network structures. Many different backbone networks have been adopted by teacher and student models, such as ResNet [18] ,[58] , ResNext [146] , SSD [80] , VGG [146] ,and so on. In addition, some studies do not directly use the classical network model as the backbone network, but instead adjust existing networks [32] ,[138] . However, these methods areKD-basedODmodelsusingtraditionaldistillationstrategies, whether they directly use typical networks or employ adjusted networks as the backbone of the teacher and student models. Most KD-based OD models with similar teacher-student network structures extract the knowledge from single-modal data (RGB images), although there also are some methods that try to learn knowledge from other modalities for guiding the lightweight student model. For example, the student model can learn semantic knowledge provided by the teacher model [111] ,learn the textual and visual features extracted by the teacher model on text information [147] , or jointly learn the visual features from RGB images and heat-like images under the guidance of the trained teacher model on these two modes of data [14] .  

Furthermore, for 3D OD, there are also common approaches involving teacher and student models using similar network structures as their backbones. For example, Wei et al. [108] opt to use the KD strategy to generate a lightweight 3D detector; here, the network structure of teacher and student models in the distillation framework is the same 3D convolutional neural network. In addition, ItKD, designed by Cho et al. [106] ,uses an autoencoder in combination with KD to improve the performance of a 3D object detector. The teacher and student models in ItKD are composed of the same backbone CenterPoint and autoencoder, and the same point cloud data is used for training the teacher and student networks. In the KD-based 3D OD tasks, multi-modal data is also used as the input of the distillation models. In methods employing this strategy, the student model is trained using 3D point cloud data, while the teacher model is trained using other modalities. For example, Qin et al. [31] proposed a cross-modal KD method, in which RGB images are used to train the teacher model and the point cloud is used to train the student model. This method aims to transfer the knowledge from the RGB domain to the point cloud domain, thereby reducing the labeling cost of 3D OD. Moreover, multimodal data can be used to train the teacher model, which is more beneficial to the performance of the student model [45] ,[109] . Multimodal data (LiDAR-image, which consists of point clouds and RGB images after segmentation) is used for training the teacher model in [45] ,[109] ; here, the student model is expected to learn the knowledge from the teacher model and to obtain the similar outputs to the teacher model using only LiDAR.  

(2) Different Network Structures of Teacher and Student Models  

Another KD strategy involves the teacher and student models using different network structures as their backbones. For example, the method in [82] uses DarkNet-53 based SSD as the teacher model’s backbone and MobileNet v2/ShuffleNet v1 as the student model’s backbone. Su et al. [55] use ResNet-based networksastheteachermodel’sbackboneandaself-built3-layer CNN as the student model’s backbone. A similar strategy is used in [5] . In addition to those listed above, there are also many more similar methods with different combinations of teacher-student models. Notably, while the teacher and student models can choose various networks as their own backbones, it is necessary to choose the appropriate networks according to the specific problems to be solved, especially given the lack of capacity of the student model. Similarly, different network structures can also be used for KD-based 3D OD tasks. Sautier et al. [46] use a 2D-to-3D distillation strategy to improve 3D OD in an autonomous driving context. The backbone of the teacher model is ResNet50 trained with RGB images, and the student model uses U-Net as its backbone trained with LiDAR data. The final experiments show that the model with this strategy outperforms the state-of-the-art methods.  

  
Fig. 5. The structure of multiple teacher models guiding one student model to learn the knowledge.  

This section lists several KD-based OD models using various teacher-student model network structures. Similar/different networks are used as the backbones of the teacher and student models to extract features from multimodal data. Through analysis of the existing relevant methods, we determine that the methods using different network structures as the backbone of the teacher model to extract knowledge from the multi-modal data have relatively more advantages, when it comes to guiding the feature learning of the student models. However, there are no fixed KD strategies of combining teacher-student model structures and multimodal data, we should design/choose appropriate networks for KD according to the specific tasks.  

4) Multiple Teacher Models: KD is similar to the learning processes used by humans. The traditional technology of KD involves a teacher model guiding a student model to learn the knowledge. Notably, however, human teaching activities involve morepatternsthantraditionalKD.Therefore,severalKDmodels based on human teaching patterns have been proposed.  

(1) Multiple Teachers Guiding One Student  

As previously discussed, the first commonly used human teaching mode is that in which multiple teachers teach one student. For example, if teachers with different areas of specialization all teach one student, the student can acquire higher-quality knowledge. Similarly, we can use different types of teacher models to learn different knowledge from large-scale datasets, then try to transfer the learned knowledge to one student model, so as that this student model can learn more comprehensive and significant visual features, as shown in Fig. 5 . For example, in [37] ,[53] ,[54] , multiple teacher models are used to guide one student model to improve its OD performance. There are some key differences between these three works: Kuang et al. [53] carried out weighted fusion of different teacher models to improve the accuracy of OD networks; Chen et al. [37] used two teacher networks trained with different strategies to ensure that the knowledge could be fully transferred to the student network; Li et al. [54] designed an asymmetric two-path learning framework to train the student model.  

(2) One Teacher Guiding Multiple Students  

The second commonly used teaching pattern is that one teacher teaches multiple students. For example, multiple student models are guided by one teacher model, after which one student model with the best performance is selected from all the student models. The work in [65] uses a KD framework in which one teacher model guides multiple student models to solve the problem of Siamese trackers being limited by high cost.",2
