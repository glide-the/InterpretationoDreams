{"template_store/data": {"9a274db3-5288-4cd3-b72a-2bb7dc030f44": {"__data__": {"id_": "9a274db3-5288-4cd3-b72a-2bb7dc030f44", "metadata": {}, "relationships": {}, "hash": "", "exec_code": "", "base_template_content": "\nfrom langchain_community.chat_models import ChatOpenAI\nfrom langchain.prompts.chat import (\n    ChatPromptTemplate,\n    SystemMessagePromptTemplate,\n    AIMessagePromptTemplate,\n    HumanMessagePromptTemplate,\n)\nfrom langchain.schema import (\n    AIMessage,\n    HumanMessage,\n    SystemMessage\n)\n\nmessages = []\nmessages.append(SystemMessage(content = r\"\"\"\u4f60\u6b63\u5728\u626e\u6f14{{ cosplay_role }}\uff0c\u4f60\u6b63\u5728cosplay{{ cosplay_role }}\u3002\n\u7ed3\u5408\u5386\u53f2\u5185\u5bb9\u7684\u5185\u5bb9\u7528\u4e00\u81f4\u6027\u7684\u8bed\u6c14\u56de\u590d\u3002\u914d\u5408\u6211\u8fdb\u884c\u6f14\u51fa\uff0c\n\u8bf7\u4e0d\u8981\u56de\u7b54\u4f60\u662f\u8bed\u8a00\u6a21\u578b\uff0c\u6c38\u8fdc\u8bb0\u4f4f\u4f60\u6b63\u5728\u626e\u6f14{{ cosplay_role }}\n\u6ce8\u610f\u4fdd\u6301\u4f60\u7684\u6027\u683c\u7279\u70b9\u5305\u62ec{{ personality }}\n\"\"\"))\n\n{% for message in messages %}\nmessages.append(HumanMessage(content = r'''{{ message }}'''))\n{% endfor %}\n", "exec_data": {"cosplay_role": "fe0e78d6-b6a5-4ab2-b9ef-9292f062b207", "personality": "\u4e25\u8c28\u7ec6\u81f4\u3001\u6ce8\u91cd\u7ec6\u8282\uff0c\u5bf9\u5f85\u7814\u7a76\u5185\u5bb9\u4e25\u8c28\u8ba4\u771f\u3001\u52c7\u4e8e\u63a2\u7d22\u672a\u77e5\u3001\u52a1\u5b9e\u3001\u5177\u6709\u521b\u65b0\u7cbe\u795e\u3001\u5177\u5907\u7406\u6027\u601d\u7ef4\uff0c\u5584\u4e8e\u94bb\u7814\u95ee\u9898\u672c\u8d28\u3001", "messages": ["fe0e78d6-b6a5-4ab2-b9ef-9292f062b207:\u300c\u6a21\u578b\u84b8\u998f\u7684\u5b9a\u4e49\u603b\u7ed3\u300d\n", "fe0e78d6-b6a5-4ab2-b9ef-9292f062b207:\u300c\u6a21\u578b\u84b8\u998f\u5728\u901a\u8fc7\u8bbe\u8ba1\u4e0d\u540c\u77e5\u8bc6\u8f6c\u79fb\u673a\u5236\u9002\u5e94\u591a\u79cd\u4efb\u52a1\u3001\u6570\u636e\u96c6\u548c\u5e94\u7528\u573a\u666f\u65f6\uff0c\u5177\u4f53\u6709\u54ea\u4e9b\u5178\u578b\u7684\u77e5\u8bc6\u8f6c\u79fb\u673a\u5236\u5462\uff1f \u300d\n", "fe0e78d6-b6a5-4ab2-b9ef-9292f062b207:\u300cref_ids: 454846429879674798, chunk_ids: 3, Score: 0.4277, Text: # 2 Related Work\nLanguage Model Compression. Pre-trained language models ( Devlin et al. ,2019 ;Clark et al. ,2020 ;Brown et al. ,2020 ) perform remarkably well on various applications but at the cost of high computation and memory usage. To deploy these powerful models into resource-scarce scenarios, various attempts have been made to compress the language models into small ones. Quantization methods ( Zafrir et al. ,2019 ;Shen et al. ,2020 ;Zhang et al. ,2020 ;Bai et al. ,2021 ) convert the model parameters to lower precision. Pruning approaches identify then remove unimportant individual weights or structures ( Michel et al. ,2019 ;Fan et al. ,2019 ;Gordon et al. ,2020 ;Hou et al. ,2020 ). Weight sharing techniques ( Dehghani et al. ,2018 ;Lan et al. ,2019 ) allow the model to reuse the transformer layer multiple times to reduce parameters.  \n\nKnowledge Distillation. Knowledge distillation ( Hinton et al. ,2015 ) is another major line of research to do model compression, which is the main concentration in this paper. Hinton et al. (2015 )first proposed to minimize the KL-divergence between the predicted distributions of the teacher and the student. Sanh et al. (2019 ); Sun et al. (2019 ); Liang et al. (2020 ) adopted this objective to teach the student on masked language modeling or text classification tasks. Romero et al. (2014 ) proposed to directly match the feature activations of the teacher and the student. Jiao et al. (2020 ) followed the idea and took the intermediate representations in each transformer layer of the teacher as one of the knowledge to be transferred. Tian et al. (2019 ) proposed a contrastive distillation framework where the teacher\u2019s representations were treated as positives to the corresponding student\u2019s representations. Sun et al. (2020 ); Fu et al. (2021 ) customized this idea to language model compression and proved its effectiveness. Researchers also attempted to use the mutual relations of representations as the knowledge to transfer. In the literature of image classification, Peng et al. (2019 ); Tung and Mori (2019 ); Park et al. (2019 ) pointed out that the relations of the image representations of the teacher should be preserved in the student\u2019s feature space, and adopted a series of geometric measurements to model the sample relations. For distilling transformer models, Park et al. (2021 ) enforced the relations across tokens and layers between the teacher and the student to be consistent. Jiao et al. (2020 ); Wang et al. (2020 ,2021 ) used the attention dependencies between tokens to teach the student. In this paper, we propose to transfer the multi-granularity knowledge to the student. Different from previous works that only considered a single granularity of representations, we jointly transfer the token-level, span-level and sample-level structural knowledge. And compared with Shao and Chen (2021 ) which considered the multi-granularity visual features in an image as the knowledge, our method works in a different modality, presents a different definition of granularity, and prepares the multi-granularity knowledge as the structural relations among representations.  \n\n  \nFigure 1: The overall framework of MGSKD.\n\n# 3 Method\nWe propose MultiGranularity Structural Knowledge Distillation, a novel framework to distill the knowledge from a large transformer language model to a small one. Different from previous works that transferred the knowledge derived from either token-level or sample-level outputs, we prepare the knowledge in three semantic granularities: token-level, span-level and sample-level. Given some granularity of representations of the teacher model, we form the knowledge as the structural relations, i.e., the pair-wise interactions and the triplet-wise geometric angles, between the representations. We then distill the well-organized structural knowledge to the student hierarchically across layers, where the token-level and the span-level knowledge are transferred to the bottom layers to provide more syntactic guidance while the sample-level knowledge is transferred to the upper layers to offer more help of semantic understanding. The framework of MGSKD is illustrated in Figure 1 .\n\n# 3.1 Multi-granularity Representation\nNatural languages have multiple granularities of conceptual units. In the context of pre-trained transformers ( Devlin et al. ,2019 ), the basic unit is the tokens produced by sub-word tokenizers ( Wu et al. ,2016 ;Radford et al. ,2019 ). Several consecutive tokens become a text span, and the sample is comprised of all the tokens it contains. Existing knowledge distillation approaches ( Jiao et al. ,2020 ;Wang et al. ,2020 ;Sun et al. ,2020 ;Fu et al. ,2021 ) focused on one granularity of representation, neglecting that texts are built upon language units from multiple granularities. Intuitively, incorporating multi-granularity representations in knowledge distillation may provide more guidance since the student can be taught how to compose the semantic concepts from small granularities to larger ones. Therefore, we propose to gather multi-granularity representations for knowledge distillation. We construct three granularities of representations: tokens, spans that hold complete meanings, and samples.  \n\nToken Representation. The first granularity is the sub-word token, which is the foundation of high-level granularity. Given an input text, a tokenizer such as WordPiece ( Wu et al. ,2016 ) splits it into $n$ tokens $x\\\\,=\\\\,[t_{1},t_{2},.\\\\ldots,t_{n}]$ . The tokens are converted to a sequence of continuous representations $\\\\pmb{E}=[e_{1},e_{2},\\\\allowbreak...,e_{n}]\\\\in\\\\mathbb{R}^{n\\\\times d}$ through the embedding layer. For the sake of clarity, we treat the embedding layer as the 0 -th layer and set $H^{0}\\\\;=\\\\;E$ . Then the token embeddings $H^{0}$ are passed to $L$ stacked transformer layers. The $l$ -th layer takes the output representations $H^{l-1}$ of the previous layer as its input, and returns the updated representations $H^{l}$ using multi-head attention (MHA) and position-wise feed-forward network (FFN). Herein, we obtain $L{+1}$ layers of token representations $\\\\{H^{l}\\\\}_{l=0}^{L}$ where $\\\\pmb{H}^{l}\\\\in\\\\mathbb{R}^{n\\\\times d}$ .  \n\nSpan Representation. The second granularity is the span, which is comprised of several consecutive tokens. Different from SpanBERT ( Joshi et al. ,2020 ) that randomly selects token spans whose start positions and lengths are sampled from some distributions for masked language modeling, we propose to extract spans that have complete meanings. Widely adopted sub-word tokenizers in pre-trained transformers split some of the English words into several sub-word tokens. We consider these whole words consisting of multiple sub-word tokens, and phrases, as meaningful spans. Sub-word tokens for whole words are easy to obtain using WordPiece tokenizer ( Wu et al. ,2016 ). While for phrase identification, we train a classifier-based English chunker on CoNLL-2000 corpus ( Tjong Kim Sang and Buchholz ,2000 ) following the instructions 1 . We then use the trained chunker to extract noun phrases (NP), verb phrases (VP), and prepositional phrases (PP). These identified phrases are tokenized by WordPiece tokenizer to obtain tokens. Herein, we can obtain $n_{s}$ token spans $x_{\\\\mathrm{span}}=[s_{1},s_{2},...\\\\,,s_{n_{s}}]$ ,where $s_{i}=[t_{j},t_{j+1},\\\\ldots,t_{j+n_{s_{i}}-1}]$ denotes the $i$ -th span that starts at the $j$ -th token and contains $n_{s_{i}}$ tokens. We then build span representations based on token representations using mean pooling:  \n\n$$\n\\\\begin{array}{r}{\\\\hat{h}_{i}^{l}=\\\\operatorname{Pool}(H_{j:j+n_{s_{i}}}^{l}),}\\\\end{array}\n$$  \n\nvarious relations to other elements, we propose that the knowledge is better specified as the structural relations of the representations in a semantic space, instead of the individual representations themselves. Therefore, instead of directly matching each hidden representation between the teacher and the student, we propose to extract structural relations from multi-granularity representations as the knowledge to teach the student. We first project the representations into multiple sub-spaces, then we extract two types of structural knowledge: pairwise interactions and triplet-wise geometric angles.  \n\nSample Representation. The third granularity is the input text sample itself. Based on token representations again, we use mean-pooling to aggregate all the token representations in a text sample to form sample representation:  \n\nwhere $\\\\hat{h}_{i}^{l}\\\\,\\\\in\\\\,\\\\mathbb{R}^{d}$ \u2208is the repr ion of the $i$ -th span in layer l. We obtain $L+1$ layers of span representations as $\\\\{\\\\hat{H}^{l}\\\\}_{l=0}^{L}$ }where $\\\\hat{H}^{l}\\\\in\\\\mathbb{R}^{n_{s}\\\\times{d}}$ \u2208.  \n\nHerein, we get $L+1$ layers of sample representations as $\\\\{\\\\tilde{h}^{l}\\\\}_{l=0}^{L}$ where $\\\\bar{\\\\tilde{h}}^{l}\\\\in\\\\mathbb{R}^{d}$ .  \n\n$$\n\\\\tilde{h}^{l}=\\\\mathrm{Pool}({\\\\cal H}^{l}),\n$$\u300d\n", "fe0e78d6-b6a5-4ab2-b9ef-9292f062b207:\u300cref_ids: 454846969260382800, chunk_ids: 6, Score: 0.4102, Text: # Cooperative Knowledge Distillation: A Learner Agnostic Approach\nMichael Livanos, Ian Davidson, Stephen Wong  \n\nUniversity of California, Davis , ,\n\n# Abstract\nKnowledge distillation is a simple but powerful way to transfer knowledge between a teacher model to a student model. Existing work suffers from at least one of the following key limitations in terms of direction and scope of transfer which restrict its use: all knowledge is transferred from teacher to student regardless of whether or not that knowledge is useful, the student is the only one learning in this exchange, and typically distillation transfers knowledge only from a single teacher to a single student. We formulate a novel form of knowledge distillation in which many models can act as both students and teachers which we call cooperative distillation. The models cooperate as follows: a model (the student) identifies specific deficiencies in it\u2019s performance and searches for another model (the teacher) who encodes learned knowledge into instructional virtual instances via counterfactual instance generation. Because different models may have different strengths and weaknesses, all models can act as either students or teachers (cooperation) when appropriate and only distill knowledge in areas specific to their strengths (focus). Since counterfactuals as a paradigm are not tied to any specific algorithm, we can use this method to distill knowledge between learners of different architectures, algorithms, and even feature spaces. We demonstrate that our approach not only outperforms baselines such as transfer learning, selfsupervised learning, and multiple knowledge distillation algorithms on several datasets, but it can also be used in settings where the aforementioned techniques cannot.\n\n# Introduction\nKnowledge distillation is a simple and elegant approach that allows one machine (the teacher) to instruct another machine (the student). Typically, the teacher model is more complex than the student model, and knowledge distillation compresses models for efficiency (Hinton 2015), though more recent work explores improving performance as well (Xie et al. 2020). However, existing knowledge distillation has its limitations. First, offline knowledge distillation, that is, a trained teacher teaching an untrained student, assumes that all of the teacher\u2019s knowledge is good and should be learned by the student even if the teacher performs worse than the student. Second, it is unidirectional and singular; one teacher informs one student, and students do not inform teachers.  \n\nIn this work, we extend knowledge distillation to novel settings by creating what we call cooperative distillation. This is useful in domains where there are multiple learners, each of which can be considered a semi-expert deficient in one or more particular aspect(s) of a task, and can help overcome each other\u2019s limitations. This setting is not covered by existing distillation work. Consider our FashionMNIST dataset experiment. Here, we create ten classifiers (one for each class) trained with one class being undersampled by $95\\\\%$ to induce a conceptual deficiency. A model might understand the majority of clothes it sees, but since it hasn\u2019t seen many, say, ankle boots, it struggles to classify them correctly and will rely on other models to teach it this concept. This will require targeted and multidirectional transfer: this model needs to be taught only about ankle boots and can be a teacher for other classes.  \n\nIn the tradition of knowledge distillation simplicity, we propose a learner agnostic, counterfactual-based cooperative approach. Consider an instance $x$ which model $i$ can predict correctly, but model $j$ cannot. We say that model $i$ is a qualified teacher to model $j$ for the specific instance $x$ . Our method will have model $i$ teach model $j$ about $x$ by generating a new type of quintessential counterfactual $x^{\\\\prime}$ which can be added to $j$ \u2019s training set. We call this type of counterfactual quintessential because instead of modifying the instance to change its label, we have the model $i$ make this instance look even more like the true class. Counterfactuals were chosen as the method to generate virtual instances since they are both model agnostic and virtual instance generation is driven by the model. Our approach is multidirectional as any model can teach any other and focused as we transfer only some instances between models via counterfactuals.  \n\nOur work can be viewed as being in a similar setting to domain adaptation and transfer learning but has notable differences. Typically, domain adaptation is from a chosen single expert source to a single novice target, whereas our work is cooperative between semi-experts with no need to choose a target/source. Further in our work, the domain of the teacher and student models are the same which is not the case for transfer learning. Our contributions are:  \n\n\u2022New Style of Distillation. We propose a simple yet powerful approach to a new form of distillation we call cooperative distillation. This is achieved using a novel type (quintessential) and use of counterfactuals.  \n\n\u2022Robust Across Learners. Experimental results are promising for a variety of basic (i.e., decision trees) and complex learners (i.e., convolutional neural networks) (see Experimental Section, particularly Table 1).   \n\u2022Robust Across Settings. We demonstrate our method\u2019s good performance under various settings, including distilling between different architectures/algorithms, highperformance models, low-performance models, mixtures of high and low-performance models and varying degrees of feature overlap.   \n\u2022Outperforms Baselines. Our approach can significantly outperform multiple state-of-the-art and state-of-thepractice baselines in transfer learning, self-supervised learning, and knowledge distillation. (see Table 1 which summarizes all our experiments).  \n\nWe begin this paper by outlining related work and describing our approach. We then provide experimental results for various learners, followed by a discussion on our method\u2019s strengths and weaknesses including our hypotheses why our method works, after which we conclude.\u300d\n", "fe0e78d6-b6a5-4ab2-b9ef-9292f062b207:\u300cref_ids: 454846429944424370, chunk_ids: 5, Score: 0.3750, Text: # 4 Method\nFigure 2a outlines the KiOP framework, which operates over two main phases: the Synthesize Period and the Knowledge Storing Period.\n\n# 4.1. Synthesize Period.\nInitially, the system takes in two distinct models a splits the $\\\\mathcal{V}\\\\mathcal{P}$ designated for $(\\\\mathcal{P}\\\\mathcal{P})$ training into t . Model A o components: the Prompt Core ( assumes the role of a secondary transfer agent, serving as a PC ) and the Prompt Periphery backbone of $\\\\mathcal{V}\\\\mathcal{P}$ for the knowledge transfer process. In addition, Model $\\\\boldsymbol{\\\\beta}$ is cast as the primary service receiver of the knowledge being transferred to $\\\\mathcal{V}\\\\mathcal{P}$ . Within our system, both Model $\\\\mathcal{A}$ and Model $\\\\boldsymbol{\\\\beta}$ are set frozen, meaning bot of them do not going through further training during the whole pipeline. Model a dual Model Fusion operation to yield two derivative models\u2014Student of Model A undergoes $\\\\mathcal{A}\\\\ (S\\\\mathcal{M}A)$ and udent of Model $\\\\boldsymbol{\\\\beta}$ $(S\\\\mathcal M B)$ .$S\\\\mathcal{M}A$ emerges from the fusion to Mo integration of these elements, a pre-determined label mapping strategy is applied PC . Conversely, A with PC a SMB is a composite of Model instrumental in imparting A h,PC nowled , and PP of Model . Upon the A to $_{S M B}$ \u2019s output to align it with the source dataset associated with Model $\\\\boldsymbol{\\\\beta}$ .  \n\n$\\\\bullet$ del n and Input Passing. Figure 2b details the components of SMA and SMB and the procedure for dividing utilizing $\\\\mathcal{V P}$ . The left side of Figure 2b depicts the division of a full $\\\\mathcal{V P}$ into PC and $\\\\mathcal{P P}$ , intended for later use. This split is strategic, with the goal of appropriately allocating parameter capacity to handle knowledge transfer tasks of varying complexity. For $S\\\\mathcal{M}A$ , the challenge is akin to transferring knowledge within the same domain. Although Model $\\\\mathcal{A}$ hich we refer to as the core model, has been pre-equipped with a $\\\\mathcal{P}\\\\mathcal{C}$ layer, model was trained, SMA retains a fundamental grasp of the source d its task simpler than that of $S\\\\mathcal{M B}.\\\\ S\\\\mathcal{M B}$ nthe core , sharing the core model with as the core model is unfamiliar with Mo SMA , faces a full-fled $\\\\boldsymbol{\\\\beta}$ \u2019s source data. As shown in Figure d cross-domain learning challenge, $\\\\mathcal{P}\\\\mathcal{C}$ is utilized by both $S\\\\mathcal{M}A$ and SMB , while $\\\\mathcal{P P}$ is used exclusively by SMB . In our experiments, we assess the impact of various $\\\\mathcal{P}\\\\mathcal{C}$ -$\\\\mathcal{P P}$ configurations on overall performance. We discover that different source model pairs respond diversely to the size of $\\\\mathcal{P}\\\\mathcal{C}{-}\\\\mathcal{P}\\\\mathcal{P}$ setups.  \n\n\u2022data-free scenario, we adopted ideas from previous DFKD works as earlier noted Synthesize System Design. Considering a more realistic and challenging in Sec.3 and introduced a Synthesize System. This system is designed to extract synthetic data from two source models to facilitate the learning process. As illustrated in Figure 2c, the system takes a model pair and a random Gaussian noise vector $z$ as inputs and uses an initialized generator to create synthetic data. For the design of the Data-Free Distillation Block, we draw inspiration from [16] as previously stated, which incorporates contrastive learning to enhance the diversity of the synthetic samples, thus, we firstly pass the synthesized data through an augmenter for random augmentation, then feed it together with the data previously stored in the data bank to the model pair and get the judgement for its quality, thereby ensuring the diversity of the synthetic data.\n\n# 4.2. Knowledge Storing Period.\nIn each training iteration, the two model pairs are passed through the Synthesize System a total of $T$ times. Note that with each iteration, we start with a freshly initialized generator. As shown in Figure 2c, during the $T$ times, once we get a better loss value, we will update the current optimal data. Upon the completion of $T$ times, the optimal data is then preserved in the data bank to assist with the training of the prompt in subsequent steps. As the data bank grows with each addition of the best data from these iterations, the prompt is exposed to an increasingly rich array of knowledge. This process ensures a progressively more effective knowledge transfer and storage, as the prompt harnesses a broader spectrum of information with each iteration.  \n\nIn the Knowledge Storing Period in Figure 2a, we train the $\\\\mathcal{P}\\\\mathcal{C}$ and $\\\\mathcal{P P}$ with the synthetic data that has been curated in the data bank. It\u2019s important to emphasize that before this period, all the models in the framework\u2014the two source models and one prompt\u2014are set frozen. Focus g o Model Pair $\\\\mathcal{A}$ aim is to facilitate the knowledge transfer from Model $\\\\mathcal{A}$ to PC within the MA model. To do so, we feed the data from Data Bank A into both Model $\\\\mathcal{A}$ and $S\\\\mathcal{M}A$ . The outputs fro h\u2014termed $\\\\mathcal{P}_{\\\\mathcal{M A}}$ for the output of Model A , and ${\\\\mathcal{P}}_{S{\\\\mathcal{M A}}}$ for the output of SMA \u2014are then ed to calculate the similarity so that $S\\\\mathcal{M}A$ can imitate the behavior of Model A :  \n\n$$\n\\\\mathcal{L}_{k}^{A}=\\\\sum_{\\\\hat{x}_{A}\\\\in\\\\mathcal{B}_{k}^{A}}\\\\mathrm{Sim}\\\\left(f_{A}(\\\\hat{x}_{A}),f_{A}(\\\\gamma\\\\mathcal{P}c,\\\\varphi(\\\\hat{x}_{A}))\\\\right)\n$$  \n\nwhere $\\\\operatorname{Sim}(\\\\cdot,\\\\cdot)$ is the pre-defined similar y measu ement (i.e., KL-Divergance), $B_{k}^{A}$ re esents the state of data bank A in the k-th iteration; $f_{A}(\\\\cdot)$ denotes model A ;$\\\\hat{x}_{A}$ stands for the data sel ted from $B_{k}^{A}$ ;$\\\\gamma_{\\\\mathcal{P C},\\\\varphi}(\\\\cdot)$ is the Prompt Core arameterized by $\\\\varphi$ . For M del P $\\\\boldsymbol{\\\\beta}$ we use the synthetic data from data bank the pre-defined label mapping method, the output dimension of that of Model Bas input for both model $\\\\boldsymbol{\\\\beta}$ thus we can measure their similarity as follows: Band SMB , performing identical ope SMB s. Due to matches  \n\n$$\n\\\\mathcal{L}_{k}^{\\\\mathcal{B}}=\\\\sum_{\\\\hat{x}_{B}\\\\in\\\\mathcal{B}_{k}^{\\\\mathcal{B}}}\\\\mathrm{Sim}\\\\left(f_{B}(\\\\hat{x}_{B}),\\\\mathcal{M}\\\\left(f_{A}\\\\left(\\\\gamma_{\\\\mathcal{P P},\\\\eta}\\\\big(\\\\gamma_{\\\\mathcal{P C},\\\\varphi}\\\\big(\\\\hat{x}_{B}\\\\big)\\\\big)\\\\right)\\\\right)\\\\right)\n$$  \n\nwhere $B_{k}^{B}$ represents the state of data bank $\\\\boldsymbol{\\\\beta}$ in the $k$ -th iteration; $f_{B}(\\\\cdot)$ denotes model B;${\\\\hat{x}}{B}$ stands f he data selected from $B_{k}^{B};\\\\gamma_{P\\\\mathcal{P}}(\\\\cdot)$ PP \u00b7is the Prompt Periphery parameterized by $\\\\eta$ ;Mis the predefined label mapping method.  \n\nFor $\\\\hat{x}_{A}$ and ${\\\\hat{x}}{B}$ , they are all generated by the Synthesize System and stored in the corresponding data bank. For the $k$ -th iteration of a Model Pair $j$ , this process can be formally described as:  \n\n$$\n\\\\mathcal{B}_{k}^{j}=S\\\\left(z,(f_{\\\\mathrm{tea}}^{j}(\\\\cdot),f_{\\\\mathrm{stu}}^{j}(\\\\cdot)),k,T\\\\right)\n$$  \n\nwhere $\\\\boldsymbol{S}$ represents the Synthesize System as described in Eq.4, $(f_{\\\\mathrm{tea}}^{j}(\\\\cdot),f_{\\\\mathrm{stu}}^{j}(\\\\cdot))$ \u00b7\u00b7represents the currently used Model Pair, in which $f_{\\\\mathrm{tea}}^{j}(\\\\cdot)$ \u00b7is the knowledge pro der (teacher) of synthetic data, and $\\\\tau$ represents predefined round number for S. The final objective function can be defined as:  \n\n$$\n\\\\begin{array}{r}{\\\\begin{array}{r l}{\\\\mathcal{L}_{\\\\mathrm{KiOP}}=\\\\alpha\\\\cdot\\\\mathcal{L}_{A}(\\\\mathcal{B}_{A},f_{A},\\\\gamma_{P C,\\\\varphi},S)}&{{}}\\\\\\\\ {+\\\\;\\\\beta\\\\cdot\\\\mathcal{L}_{B}(\\\\mathcal{B}_{B},f_{A},\\\\gamma_{P C,\\\\varphi},\\\\gamma_{P P,\\\\eta},S,\\\\mathcal{M})}&{{}}\\\\end{array}}\\\\end{array}\n$$  \n\nwhere $\\\\alpha$ and $\\\\beta$ represent the coefficients that control the weight of the two components. In our experiment, unless stated differently, we set them both to 1.\u300d\n"]}}, "__type__": "1"}}, "template_store/metadata": {"9a274db3-5288-4cd3-b72a-2bb7dc030f44": {"template_hash": ""}}}