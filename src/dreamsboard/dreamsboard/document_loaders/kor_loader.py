from __future__ import annotations
from langchain_core.messages import ( 
    BaseMessage,
)
from langchain_core.language_models import LanguageModelInput
from langchain_core.runnables import Runnable
from kor.extraction import create_extraction_chain
from kor.nodes import Object, Text, Number
from langchain.chains import LLMChain
from typing import Union


class KorLoader:

    @classmethod
    def form_kor_dreams_guidance_builder(cls,
                                         llm_runable: Runnable[LanguageModelInput, BaseMessage]) -> LLMChain:
        """
        生成开放问题的抽取链
        :param llm:
        :return:
        """
        # @title 长的prompt
        schema = Object(
            id="script",
            description="开放性引导问题",
            attributes=[
                Text(
                    id="step_advice",
                    description='''在这一步骤中提供的建议，例如“我想说这样的话：‘我理解这对你来说是一个困难的情况。’” ''',
                ),
                Text(
                    id="step_description",
                    description="""咨询步骤的描述，例如“建立信任”""",
                )
            ],
            examples=[
                (
                    """Please output the extracted information in CSV format in Excel dialect. Please use a | as the delimiter. 
Do NOT add any clarifying information. Output MUST follow the schema above. Do NOT add any additional columns that do not appear in the schema.
不要包含"```plaintext"或"```"。


根据提供的故事场景，您作为心理咨询工作者可以使用开放性问题来引导来访者表达他们的感受和思维。以下是一步一步的分解：

**Step 1: 建立情感连接**
开始时，您可以通过表达理解和共鸣来建立情感连接，让来访者感到舒适。您可以说：“我注意到这个对话中有许多愉快的时刻和互动。你对这些时刻有什么特别的感受吗？”

**Step 2: 探索来访者的感受**
继续引导来访者表达他们的感受。您可以问：“在这个对话中，有哪些瞬间让你感到开心或快乐？”

**Step 3: 询问是否有反感情绪**
除了积极的情感，也要问询是否有一些负面情感或担忧。您可以说：“除了快乐的瞬间，是否有一些让你感到不安或担忧的地方？”

**Step 4: 深入探讨个人反应**
一旦来访者开始分享他们的感受，可以深入探讨他们的个人反应。例如：“你觉得自己在这些互动中扮演了什么角色？”

**Step 5: 探索与他人的互动**
继续引导来访者思考他们与他人的互动。您可以问：“这些互动对你与他人的关系有什么影响？你觉得与朋友之间的互动如何影响你的情感状态？”

**Step 6: 引导自我反思**
最后，鼓励来访者进行自我反思。您可以问：“在这个故事场景中，你是否注意到了自己的情感变化或思维模式？有没有什么你想要深入探讨或解决的问题？”

通过这种方式，您可以引导来访者自由表达他们的情感和思维，帮助他们更好地理解自己和他们与他人的互动。同时，保持开放和倾听，以便在需要时提供支持和建议。""",
                    [
                        {"step_advice": "我注意到这个对话中有许多愉快的时刻和互动。你对这些时刻有什么特别的感受吗？",
                         "step_description": "建立情感连接"},
                        {"step_advice": "在这个对话中，有哪些瞬间让你感到开心或快乐?", "step_description": "探索来访者的感受"},
                        {"step_advice": "除了快乐的瞬间，是否有一些让你感到不安或担忧的地方？",
                         "step_description": "询问是否有反感情绪"},
                        {"step_advice": "你觉得自己在这些互动中扮演了什么角色?", "step_description": "深入探讨个人反应"},
                        {"step_advice": "这些互动对你与他人的关系有什么影响？你觉得与朋友之间的互动如何影响你的情感状态?", "step_description": "探索与他人的互动"},
                        {"step_advice": "在这个故事场景中，你是否注意到了自己的情感变化或思维模式？有没有什么你想要深入探讨或解决的问题?", "step_description": "引导自我反思"},
                    ],
                )
            ],
            many=True,
        )

        chain = create_extraction_chain(llm_runable, schema)
        return chain

    @classmethod
    def form_kor_dreams_personality_builder(cls,
                                            llm_runable: Runnable[LanguageModelInput, BaseMessage]) -> LLMChain:
        """
        生成性格分析的抽取链
        :param llm:
        :return:
        """
        schema = Object(
            id="personality_script",
            description="性格信息(personality,subj)",
            attributes=[
                Text(
                    id="personality",
                    description='''包含的性格评价，例如：“率真和直接、愿意与人互动并享受社交乐趣、对预期之外的情况敏感” ''',
                ),
                Text(
                    id="subj",
                    description='''包含的特定人物，例如：“张毛峰” ''',
                )
            ],
            examples=[
                (
                    """
 根据您提供的信息，您的性格特点可以总结如下：
        
1. 热情和温柔：您在描述天气和气氛时使用了"温柔长裙风"这样的形容词，表现出您对温暖和舒适的情感。

2. 情感表达：您在文本中表达了对一个叫"宝宝"的角色的期待和关心，这显示了您的感性和情感表达能力。

3. 好奇心和幽默感：您提到了要做大胆的事情，并且以"嘻嘻"结束，这暗示了您对新奇事物的好奇心和幽默感。

4. 关心家人和亲情：您提到了弟弟给了三颗糖，表现出您关心家人的情感。

5. 乐于分享和帮助：您提到要给宝宝剥虾并询问宝宝是否想知道小鱼在说什么，显示出您愿意分享和帮助他人的特点。

6. 可能有一些难以理解的部分：在文本中也出现了一些不太清楚的情节，如呼救情节和提到"小肚小肚"，这可能表现出您的思维有时候会有些混乱或不太连贯。

总的来说，您的性格特点包括热情、情感表达能力、好奇心、幽默感、亲情关怀以及乐于分享和帮助他人。
            
            """,
                    [
                        {"personality": "热情、情感表达能力、好奇心、幽默感、亲情关怀以及乐于分享和帮助他人"}
                    ],
                ),
                (
                    """根据以上分析，以下是对片段中人物性格的总结：

### 张毛峰

- **情感表达直接**：他对视觉刺激有积极的情感反应，并直接表达出来，表现出一种率真和直接的个性。
- **社交积极性**：他的情绪快感转移到社交行为上，提出共同活动的邀请，显示出他是一个愿意与人互动并享受社交乐趣的人。
- **对预期之外的情况敏感**：对话中他对某些意外的反应显示他可能对预期落空的情况有所关注。

### 露ᥫᩣ

- **自我展示的谨慎性**：她发送表情图片后撤回，表明她在自我展示方面可能更加谨慎，对自我形象有着不确定感。
- **情感表达的积极性**：她用词如“好激动”显示她能够在交流中表达积极的情感。
- **对反馈的敏感性**：撤回行为和对“别扭”的提及可能意味着她对社会反馈，尤其是潜在的负面反馈较为敏感。

这些性格特点是从对话中所使用的交流媒介和语义信息中推断出来的，可以为理解他们在社会互动中的行为和反应提供一些洞见。然而，这样的分析仅基于有限的信息，真实性格可能更为复杂。
            
            """,
                    [
                        {"personality": "率真和直接、愿意与人互动并享受社交乐趣、对预期之外的情况敏感", "subj": "张毛峰"},
                        {"personality": "对自我形象有着不确定、能够在交流中表达积极的情感、对社会反馈，尤其是潜在的负面反馈较为敏感", "subj": "露ᥫᩣ"},
                    ],
                ),
                (
                    """根据以上分解，以下是关于片段中人物性格的总结：
### 张毛峰和露ᥫᩣ的性格特征：
1. **社交技巧**：两人都展现出良好的社交技巧，能够通过幽默和轻松的方式进行交流，这表明他们在社交场合中可能是受欢迎和具有亲和力的人。
2. **适应性和灵活性**：他们能够适应数字通信媒介，使用表情符号和图片来丰富交流，显示他们能够适应现代社交方式，并具有一定的灵活性。
3. **共享快感**：通过转账和分享食物照片，两人表现出愿意在社交行为中共享快感，这暗示他们可能是慷慨和注重人际关系的人。
4. **处理误解和不确定性**：在遇到语义信息不清或预期落空时，他们能够以平和的方式处理，这显示了他们的耐心和解决问题的能力。
5. **真实和直接**：当需要澄清事实或情感状态时，他们会直接表达否定物，这表明他们倾向于诚实和直接的沟通方式。
综上所述，张毛峰和露ᥫᩣ在对话中表现出的是一种现代社交网络下成熟的交流方式，他们性格中包含了幽默感、社交技巧、适应性和直接性等特征。这些特点使他们能够在数字媒介主导的社交环境中游刃有余。
            """,
                    [
                        {"personality": "幽默感、社交技巧、适应性和直接性等特征、在社交场合中可能是受欢迎和具有亲和力的人",
                         "subj": "张毛峰"},
                        {"personality":  "幽默感、社交技巧、适应性和直接性等特征、在社交场合中可能是受欢迎和具有亲和力的人、",
                         "subj": "露ᥫᩣ"},
                    ],
                )
            ],
            many=True,
        )
        chain = create_extraction_chain(llm_runable, schema)
        return chain


    @classmethod
    def form_kor_dreams_task_step_builder(cls,
                                            llm_runable: Runnable[LanguageModelInput, BaseMessage]) -> Union[LLMChain,Object]:
        """
        生成任务步骤的抽取链
        :param llm:
        :return:
        """
        # @title 长的prompt
        schema = Object(
            id="script",
            description="Adapted from the novel into script",
            attributes=[
                Text(
                    id="task_step_name",
                    description='''提取步骤的名称，例如"分析近几年研究领域的技术框架与方法论"、"基于规则的方法"、"研究论文中采用的主要框架"、"Transformer"等''',
                ),
                Text(
                    id="task_step_description",
                    description="""提取每个步骤的具体建议和问题，例如"Text2SQL 是将自然语言查询（NLQ）转换为结构化查询语言（SQL）的任务，近年来在数据库和自然语言处理（NLP）领域受到广泛关注。主要技术框架和方法论包括："等""",
                ),
                Text(
                    id="task_step_level",
                    description="""提取步骤的层级编号，只有root>child层级，例如"0"、"0>1"、"0>2"、"1"、"1>1"、"1>2"等""",
                )
            ],
            examples=[
                (
                    """### Text2SQL 研究现状与挑战

#### 1. 分析近几年研究领域的技术框架与方法论

Text2SQL 是将自然语言查询（NLQ）转换为结构化查询语言（SQL）的任务，近年来在数据库和自然语言处理（NLP）领域受到广泛关注。主要技术框架和方法论包括：

- **基于规则的方法**：早期方法依赖于手工编写的规则和模板，将自然语言映射到 SQL。这种方法在小规模、特定领域的数据库上表现良好，但缺乏泛化能力。
- **基于统计的方法**：随着机器学习的发展，统计模型（如序列到序列模型）被引入，通过学习大量标注数据来生成 SQL 查询。
- **基于深度学习的方法**：近年来，深度学习模型（如 Transformer、BERT、GPT）在 Text2SQL 任务中取得了显著进展。这些模型通过预训练和微调，能够更好地理解自然语言和数据库模式之间的关系。

#### 2. 研究论文中采用的主要框架（如 Transformer、GAN、BERT等）在不同任务中的应用与变体

- **Transformer**：Transformer 模型在 Text2SQL 任务中表现出色，特别是在处理长文本和复杂查询时。通过自注意力机制，Transformer 能够捕捉自然语言和数据库模式之间的复杂关系。
- **BERT**：BERT 及其变体（如 RoBERTa、ALBERT）通过预训练语言模型，显著提升了模型对自然语言的理解能力。在 Text2SQL 任务中，BERT 通常用于编码自然语言查询和数据库模式。
- **GAN**：生成对抗网络（GAN）在 Text2SQL 任务中的应用较少，但在数据增强和生成多样化查询方面具有潜力。

#### 3. 评估学术界的技术进步与局限性

- **技术进步**：
- **模型性能提升**：深度学习模型在标准数据集（如 WikiSQL、Spider）上的表现显著提升，特别是在复杂查询和跨领域任务中。
- **多模态学习**：一些研究开始探索将文本和数据库模式的多模态信息结合起来，以提升模型的理解能力。

- **局限性**：
- **模型偏差**：模型在处理未见过的数据库模式或复杂查询时，仍然存在偏差和错误。
- **数据依赖**：模型的性能高度依赖于标注数据的质量和数量，缺乏泛化能力。

#### 4. 探讨计算模型在不同数据集与应用场景下的适用性与泛化能力

- **数据集**：WikiSQL、Spider 等标准数据集为 Text2SQL 研究提供了基准，但这些数据集通常局限于特定领域和简单查询。
- **应用场景**：模型在现实应用中的表现仍然有限，特别是在多领域、多模态的数据场景下，模型的泛化能力不足。

#### 5. 分析最新算法的稳定性与容错性

- **稳定性**：深度学习模型在复杂、动态环境下的稳定性仍然是一个挑战，特别是在处理大规模数据和复杂查询时。
- **容错性**：模型在面对噪声数据和错误查询时的容错性较差，容易生成错误的 SQL 查询。

#### 6. 评估论文中提出的未来研究方向与挑战

- **未来研究方向**：
- **跨领域泛化**：研究如何提升模型在跨领域任务中的泛化能力。
- **多模态学习**：探索将文本、图像和数据库模式结合起来的多模态学习方法。
- **自监督学习**：利用自监督学习方法减少对标注数据的依赖。

- **挑战**：
- **复杂查询处理**：如何有效处理复杂查询和嵌套查询仍然是一个挑战。
- **数据稀缺**：在缺乏标注数据的情况下，如何提升模型的性能是一个重要问题。
- **模型解释性**：提升模型的可解释性，使其生成的 SQL 查询更容易理解和调试。

### 总结

Text2SQL 研究在近年来取得了显著进展，特别是在深度学习模型的引入和应用方面。然而，模型在处理复杂查询、跨领域任务和现实应用中的表现仍然存在局限性。未来的研究需要关注跨领域泛化、多模态学习和自监督学习等方向，以应对现有挑战并推动该领域的进一步发展。""",
                    [
                        {
                            "task_step_description": "Text2SQL 是将自然语言查询（NLQ）转换为结构化查询语言（SQL）的任务，近年来在数据库和自然语言处理（NLP）领域受到广泛关注。主要技术框架和方法论包括：",
                            "task_step_name": "Text2SQL 研究现状与挑战",
                            "task_step_level": "0"
                        },
                        {
                            "task_step_description": "早期方法依赖于手工编写的规则和模板，将自然语言映射到 SQL。这种方法在小规模、特定领域的数据库上表现良好，但缺乏泛化能力。",
                            "task_step_name": "基于规则的方法",
                            "task_step_level": "0>1"
                        },
                        {
                            "task_step_description": "随着机器学习的发展，统计模型（如序列到序列模型）被引入，通过学习大量标注数据来生成 SQL 查询。",
                            "task_step_name": "基于统计的方法",
                            "task_step_level": "0>2"
                        },
                        {
                            "task_step_description": "Transformer 模型在 Text2SQL 任务中表现出色，特别是在处理长文本和复杂查询时。通过自注意力机制，Transformer 能够捕捉自然语言和数据库模式之间的复杂关系。",
                            "task_step_name": "研究论文中采用的主要框架",
                            "task_step_level": "1"
                        },
                        {
                            "task_step_description": "BERT 及其变体（如 RoBERTa、ALBERT）通过预训练语言模型，显著提升了模型对自然语言的理解能力。在 Text2SQL 任务中，BERT 通常用于编码自然语言查询和数据库模式。",
                            "task_step_name": "BERT",
                            "task_step_level": "1>1"
                        }
                    ]
                )
            ],
            many=True
        )


        chain = create_extraction_chain(llm_runable, schema)
        return chain, schema

    @classmethod
    def form_kor_task_step_refine_builder(cls,
                                            llm_runable: Runnable[LanguageModelInput, BaseMessage]) -> LLMChain:
        """
        抽取批评意见优化当前回答并续写上下文内容
        :param llm:
        :return:
        """
        # @title 长的prompt
        schema = Object(
            id="script",
            description="Adapted from the novel into script",
            attributes=[
                Text(
                    id="thought",
                    description='''提取优化后的回答的内容''',
                ),
                Text(
                    id="answer",
                    description="""提取续写的上下文内容""",
                ),
                Text(
                    id="answer_socre",
                    description="""提取答案评分分数。 转换为浮点数,例如95/100转换为 0.95""",
                )
            ],
            examples=[
                (
                    """### 优化后的回答：
在多模态大模型的技术发展路线中，早期探索与基础构建阶段对单一模态模型的深入研究，如CNN在图像处理和RNN在文本处理中的应用，为后续多模态融合技术的发展奠定了坚实的基础。具体而言，这一阶段的研究成果在以下几个方面对多模态融合技术产生了关键性的影响和启示：
1. **特征提取与表示**：
   - **CNN技术细节**：CNN通过卷积层和池化层的逐层操作，实现了图像特征的多层次提取。卷积层通过局部连接和共享权重捕捉局部特征，池化层则通过下采样减少特征维度，保留关键信息。例如，AlexNet利用多层卷积和池化层提取图像的层次化特征。
   - **RNN技术细节**：RNN通过循环结构捕捉文本序列的长期依赖关系，LSTM通过引入门控机制解决了传统RNN的梯度消失问题，从而更好地处理长序列数据。例如，LSTM在机器翻译任务中有效捕捉了源语言和目标语言之间的长距离依赖。
2. **模型架构创新**：
   - **ResNet创新**：ResNet引入了残差连接，解决了深度神经网络中的梯度消失问题，使得网络可以更深，提升了特征提取能力。
   - **LSTM改进**：LSTM通过引入遗忘门、输入门和输出门，改善了RNN在处理长序列时的性能，提升了模型对序列数据的理解和生成能力。
3. **训练与优化策略**：
   - **梯度下降**：单一模态模型中广泛使用的梯度下降算法为多模态模型提供了有效的参数优化方法。
   - **正则化技术**：如Dropout和L2正则化，在多模态模型中同样适用，有助于防止过拟合，提高模型的泛化能力。
4. **数据处理与预处理**：
   - **图像数据处理**：包括归一化、数据增强（如旋转、缩放）等，提升了图像数据的质量和多样性。
   - **文本数据处理**：包括分词、词嵌入（如Word2Vec、BERT）等，将文本转换为模型可理解的数值表示。
5. **跨模态交互机制**：
   - **早期融合方法**：如特征级融合（特征拼接）和决策级融合（模型集成），尽管存在局限性，但为后续复杂的融合策略提供了基础。
   - **注意力机制**：通过引入注意力机制，模型能够更好地关注不同模态中的关键信息，提升了融合效果。
6. **评估与基准**：
   - **具体标准**：如ImageNet图像分类任务、SQuAD文本问答数据集等，提供了统一的评估标准和基准，帮助研究者客观评估多模态模型的性能。
### 续写的上下文内容：
此外，早期探索与基础构建阶段不仅在技术上为多模态融合提供了坚实的基础，还在方法论和思维模式上为后续研究提供了重要的启示。例如，从单一模态的特征提取和模型优化中汲取的经验，被广泛应用到多模态融合中，提升了融合效果。同时，早期研究推动了从单一模态的独立研究转向多模态融合的系统性思维，促进了多模态大模型的快速发展。
### 评分分数：
**评分：95/100**
**评分理由**：
- **内容详实**：详细阐述了单一模态模型在特征提取、模型架构、训练策略、数据处理等方面的具体技术细节。
- **逻辑清晰**：结构分明，逻辑连贯，易于理解。
- **相关性高**：紧密围绕早期探索与基础构建阶段的影响和启示，提供了具体的案例和标准。
- **续写合理**：续写内容与原文衔接自然，进一步深化了主题。
通过以上优化，回答更加详实、具体，为后续多模态融合技术的发展提供了更为坚实的理论基础和实践指导。
""",
                    [
                        {
                            "thought": """在多模态大模型的技术发展路线中，早期探索与基础构建阶段对单一模态模型的深入研究，如CNN在图像处理和RNN在文本处理中的应用，为后续多模态融合技术的发展奠定了坚实的基础。具体而言，这一阶段的研究成果在以下几个方面对多模态融合技术产生了关键性的影响和启示：
1. **特征提取与表示**：
   - **CNN技术细节**：CNN通过卷积层和池化层的逐层操作，实现了图像特征的多层次提取。卷积层通过局部连接和共享权重捕捉局部特征，池化层则通过下采样减少特征维度，保留关键信息。例如，AlexNet利用多层卷积和池化层提取图像的层次化特征。
   - **RNN技术细节**：RNN通过循环结构捕捉文本序列的长期依赖关系，LSTM通过引入门控机制解决了传统RNN的梯度消失问题，从而更好地处理长序列数据。例如，LSTM在机器翻译任务中有效捕捉了源语言和目标语言之间的长距离依赖。
2. **模型架构创新**：
   - **ResNet创新**：ResNet引入了残差连接，解决了深度神经网络中的梯度消失问题，使得网络可以更深，提升了特征提取能力。
   - **LSTM改进**：LSTM通过引入遗忘门、输入门和输出门，改善了RNN在处理长序列时的性能，提升了模型对序列数据的理解和生成能力。
3. **训练与优化策略**：
   - **梯度下降**：单一模态模型中广泛使用的梯度下降算法为多模态模型提供了有效的参数优化方法。
   - **正则化技术**：如Dropout和L2正则化，在多模态模型中同样适用，有助于防止过拟合，提高模型的泛化能力。
4. **数据处理与预处理**：
   - **图像数据处理**：包括归一化、数据增强（如旋转、缩放）等，提升了图像数据的质量和多样性。
   - **文本数据处理**：包括分词、词嵌入（如Word2Vec、BERT）等，将文本转换为模型可理解的数值表示。
5. **跨模态交互机制**：
   - **早期融合方法**：如特征级融合（特征拼接）和决策级融合（模型集成），尽管存在局限性，但为后续复杂的融合策略提供了基础。
   - **注意力机制**：通过引入注意力机制，模型能够更好地关注不同模态中的关键信息，提升了融合效果。
6. **评估与基准**：
   - **具体标准**：如ImageNet图像分类任务、SQuAD文本问答数据集等，提供了统一的评估标准和基准，帮助研究者客观评估多模态模型的性能。""",
                            "answer": "此外，早期探索与基础构建阶段不仅在技术上为多模态融合提供了坚实的基础，还在方法论和思维模式上为后续研究提供了重要的启示。例如，从单一模态的特征提取和模型优化中汲取的经验，被广泛应用到多模态融合中，提升了融合效果。同时，早期研究推动了从单一模态的独立研究转向多模态融合的系统性思维，促进了多模态大模型的快速发展。",
                            "answer_socre": "0.95"
                        }
                    ]
                ),
                
                (
                    """### 优化后的回答：

在多模态大模型的技术发展路线中，早期探索与基础构建阶段对单一模态模型的深入研究，如CNN在图像处理和RNN在文本处理中的应用，为后续多模态融合技术的发展奠定了坚实的基础。具体而言，这一阶段的研究成果在以下几个方面对多模态融合技术产生了关键性的影响和启示：

1. **特征提取与表示**：单一模态模型在各自领域内的高效特征提取能力，为多模态融合提供了丰富的特征表示基础。例如，VGGNet在图像识别任务中通过多层次的特征提取，显著提升了图像特征的丰富性，为后续融合不同模态信息提供了可靠的特征输入。

2. **模型架构创新**：在特征提取与表示的基础上，模型架构的创新进一步推动了多模态融合技术的发展。如深度学习网络结构的优化和改进，ResNet通过引入残差连接解决了深层网络训练难题，为多模态融合模型的设计提供了多样化的架构选择。

3. **训练与优化策略**：单一模态模型在训练和优化过程中积累的经验，如梯度下降、正则化技术等，为多模态模型的训练提供了宝贵的参考。这些策略在多模态融合中同样适用，有助于提高模型的收敛速度和泛化能力。

4. **数据处理与预处理**：单一模态模型在数据处理和预处理方面的技术积累，如数据增强、归一化等，为多模态数据的处理提供了有效的手段。这些技术在多模态融合中同样重要，有助于提升数据的质量和模型的性能。

5. **跨模态交互机制**：单一模态模型的研究揭示了不同模态数据的特点和规律，为设计有效的跨模态交互机制提供了理论基础。例如，ViLBERT通过联合注意力机制，实现了图像和文本特征的有效对齐，显著提升了跨模态信息的融合效果。

6. **评估与基准**：单一模态模型在各自领域内建立的评估标准和基准数据集，为多模态融合技术的评估提供了参考。例如，ImageNet和SQuAD等基准数据集的建立，为多模态模型的性能评估提供了统一的标准，促进了模型的公平比较和持续优化。

综上所述，早期探索与基础构建阶段的研究成果不仅在技术上为多模态融合提供了坚实的基础，还在方法论和思维模式上为后续研究提供了重要的启示，推动了多模态大模型的快速发展。未来，随着单一模态模型的进一步成熟，跨模态交互机制的深入研究，将为多模态大模型的发展带来更多创新机遇。

### 续写的上下文：

在这一坚实的基础上，多模态大模型的研究进入了一个新的阶段——融合与创新阶段。这一阶段的研究重点在于如何有效地整合不同模态的信息，以实现更高级别的任务理解和处理能力。具体而言，以下几个方面成为了研究的焦点：

1. **多模态融合策略**：研究者们探索了多种融合策略，如早期融合、晚期融合和混合融合等，以找到最适合特定任务的融合方式。例如，在视频理解任务中，早期融合通过将图像和音频特征在早期层进行结合，能够更好地捕捉时空信息。

2. **跨模态注意力机制**：注意力机制在多模态融合中发挥了重要作用。通过设计跨模态注意力机制，模型能够动态地关注不同模态中的关键信息，从而提高融合效果。例如，Transformer-based的模型如BERT在处理文本和图像时，通过自注意力机制实现了信息的有效整合。

3. **端到端训练方法**：为了提高模型的训练效率和性能，研究者们开发了多种端到端的训练方法。这些方法能够同时优化特征提取、融合和任务特定层的参数，从而实现更好的整体性能。

4. **多任务学习**：多任务学习通过同时训练多个相关任务，能够共享不同任务之间的有用信息，提高模型的泛化能力。例如，在多模态情感分析中，同时训练情感分类和情感强度预测任务，能够提升模型对情感的理解。

5. **可解释性与鲁棒性**：随着多模态大模型的复杂度增加，可解释性和鲁棒性成为了重要的研究课题。研究者们致力于开发能够解释模型决策过程的方法，并提高模型在面对噪声和对抗攻击时的鲁棒性。

通过这些深入的研究，多模态大模型不仅在技术上取得了显著进展，也在实际应用中展示了巨大的潜力，如智能客服、自动驾驶和医疗诊断等领域。未来，随着技术的不断进步，多模态大模型有望在更多领域发挥重要作用，推动人工智能的全面发展。

### 评分分数：95/100

优化后的回答在逻辑连贯性、具体案例支撑、跨模态交互机制的细化、评估与基准的论述以及未来展望等方面均有显著提升，语言表达也更加生动有力，整体上符合任务要求，展现了较高的质量和深度。""",
                    [
                        {
                            "thought": """在多模态大模型的技术发展路线中，早期探索与基础构建阶段对单一模态模型的深入研究，如CNN在图像处理和RNN在文本处理中的应用，为后续多模态融合技术的发展奠定了坚实的基础。具体而言，这一阶段的研究成果在以下几个方面对多模态融合技术产生了关键性的影响和启示：

1. **特征提取与表示**：单一模态模型在各自领域内的高效特征提取能力，为多模态融合提供了丰富的特征表示基础。例如，VGGNet在图像识别任务中通过多层次的特征提取，显著提升了图像特征的丰富性，为后续融合不同模态信息提供了可靠的特征输入。

2. **模型架构创新**：在特征提取与表示的基础上，模型架构的创新进一步推动了多模态融合技术的发展。如深度学习网络结构的优化和改进，ResNet通过引入残差连接解决了深层网络训练难题，为多模态融合模型的设计提供了多样化的架构选择。

3. **训练与优化策略**：单一模态模型在训练和优化过程中积累的经验，如梯度下降、正则化技术等，为多模态模型的训练提供了宝贵的参考。这些策略在多模态融合中同样适用，有助于提高模型的收敛速度和泛化能力。

4. **数据处理与预处理**：单一模态模型在数据处理和预处理方面的技术积累，如数据增强、归一化等，为多模态数据的处理提供了有效的手段。这些技术在多模态融合中同样重要，有助于提升数据的质量和模型的性能。

5. **跨模态交互机制**：单一模态模型的研究揭示了不同模态数据的特点和规律，为设计有效的跨模态交互机制提供了理论基础。例如，ViLBERT通过联合注意力机制，实现了图像和文本特征的有效对齐，显著提升了跨模态信息的融合效果。

6. **评估与基准**：单一模态模型在各自领域内建立的评估标准和基准数据集，为多模态融合技术的评估提供了参考。例如，ImageNet和SQuAD等基准数据集的建立，为多模态模型的性能评估提供了统一的标准，促进了模型的公平比较和持续优化。

综上所述，早期探索与基础构建阶段的研究成果不仅在技术上为多模态融合提供了坚实的基础，还在方法论和思维模式上为后续研究提供了重要的启示，推动了多模态大模型的快速发展。未来，随着单一模态模型的进一步成熟，跨模态交互机制的深入研究，将为多模态大模型的发展带来更多创新机遇。

""",
                            "answer": """在这一坚实的基础上，多模态大模型的研究进入了一个新的阶段——融合与创新阶段。这一阶段的研究重点在于如何有效地整合不同模态的信息，以实现更高级别的任务理解和处理能力。具体而言，以下几个方面成为了研究的焦点：

1. **多模态融合策略**：研究者们探索了多种融合策略，如早期融合、晚期融合和混合融合等，以找到最适合特定任务的融合方式。例如，在视频理解任务中，早期融合通过将图像和音频特征在早期层进行结合，能够更好地捕捉时空信息。

2. **跨模态注意力机制**：注意力机制在多模态融合中发挥了重要作用。通过设计跨模态注意力机制，模型能够动态地关注不同模态中的关键信息，从而提高融合效果。例如，Transformer-based的模型如BERT在处理文本和图像时，通过自注意力机制实现了信息的有效整合。

3. **端到端训练方法**：为了提高模型的训练效率和性能，研究者们开发了多种端到端的训练方法。这些方法能够同时优化特征提取、融合和任务特定层的参数，从而实现更好的整体性能。

4. **多任务学习**：多任务学习通过同时训练多个相关任务，能够共享不同任务之间的有用信息，提高模型的泛化能力。例如，在多模态情感分析中，同时训练情感分类和情感强度预测任务，能够提升模型对情感的理解。

5. **可解释性与鲁棒性**：随着多模态大模型的复杂度增加，可解释性和鲁棒性成为了重要的研究课题。研究者们致力于开发能够解释模型决策过程的方法，并提高模型在面对噪声和对抗攻击时的鲁棒性。

通过这些深入的研究，多模态大模型不仅在技术上取得了显著进展，也在实际应用中展示了巨大的潜力，如智能客服、自动驾驶和医疗诊断等领域。未来，随着技术的不断进步，多模态大模型有望在更多领域发挥重要作用，推动人工智能的全面发展。
""",
                            "answer_socre": "0.95"
                        }
                    ]
                )
            ],
            many=True
        )


        chain = create_extraction_chain(llm_runable, schema)
        return chain


